/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3844 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3844 [00:49<52:36:24, 49.28s/it]

  0%|          | 2/3844 [00:57<26:43:49, 25.05s/it]
{'loss': 1.5529, 'grad_norm': 0.8570143146630824, 'learning_rate': 3.4482758620689656e-07, 'epoch': 0.0}

  0%|          | 3/3844 [01:05<18:40:28, 17.50s/it]

  0%|          | 4/3844 [01:14<14:56:57, 14.02s/it]


  0%|          | 6/3844 [01:29<10:57:50, 10.28s/it]
{'loss': 1.4797, 'grad_norm': 0.8279535137888594, 'learning_rate': 1.0344827586206898e-06, 'epoch': 0.0}

  0%|          | 7/3844 [01:38<10:38:20,  9.98s/it]


  0%|          | 9/3844 [01:53<9:15:08,  8.69s/it]
{'loss': 1.4674, 'grad_norm': 0.8077322262641929, 'learning_rate': 1.5517241379310346e-06, 'epoch': 0.0}

  0%|          | 10/3844 [02:00<8:30:58,  8.00s/it]

  0%|          | 11/3844 [02:07<8:16:16,  7.77s/it]

  0%|          | 12/3844 [02:13<7:37:52,  7.17s/it]

  0%|          | 13/3844 [02:19<7:19:04,  6.88s/it]


  0%|          | 15/3844 [02:33<7:23:14,  6.95s/it]

  0%|          | 16/3844 [02:39<7:08:00,  6.71s/it]
{'loss': 1.5017, 'grad_norm': 0.7690410008374042, 'learning_rate': 2.7586206896551725e-06, 'epoch': 0.0}


  0%|          | 18/3844 [02:55<7:43:58,  7.28s/it]

  0%|          | 19/3844 [03:03<7:55:28,  7.46s/it]
{'loss': 1.5423, 'grad_norm': 0.9506727084358076, 'learning_rate': 3.2758620689655175e-06, 'epoch': 0.0}

  1%|          | 20/3844 [03:12<8:21:33,  7.87s/it]

  1%|          | 21/3844 [03:18<7:50:55,  7.39s/it]

  1%|          | 22/3844 [03:26<7:58:14,  7.51s/it]

  1%|          | 23/3844 [03:36<8:49:31,  8.31s/it]

  1%|          | 24/3844 [03:42<8:07:58,  7.66s/it]

  1%|          | 25/3844 [03:52<8:50:06,  8.33s/it]

  1%|          | 26/3844 [04:02<9:25:56,  8.89s/it]

  1%|          | 27/3844 [04:09<8:35:04,  8.10s/it]

  1%|          | 28/3844 [04:15<8:03:20,  7.60s/it]


  1%|          | 30/3844 [04:27<7:23:30,  6.98s/it]
{'loss': 1.5324, 'grad_norm': 0.8349857703670932, 'learning_rate': 5.172413793103449e-06, 'epoch': 0.01}

  1%|          | 31/3844 [04:35<7:32:26,  7.12s/it]

  1%|          | 32/3844 [04:44<8:16:20,  7.81s/it]

  1%|          | 33/3844 [04:53<8:30:43,  8.04s/it]

  1%|          | 34/3844 [05:00<8:08:59,  7.70s/it]

  1%|          | 35/3844 [05:08<8:13:12,  7.77s/it]

  1%|          | 36/3844 [05:19<9:11:33,  8.69s/it]

  1%|          | 37/3844 [05:25<8:21:41,  7.91s/it]

  1%|          | 38/3844 [05:33<8:22:05,  7.92s/it]

  1%|          | 39/3844 [05:38<7:38:42,  7.23s/it]


  1%|          | 41/3844 [05:51<7:16:00,  6.88s/it]
{'loss': 1.4544, 'grad_norm': 0.5964885290752772, 'learning_rate': 7.0689655172413796e-06, 'epoch': 0.01}

  1%|          | 42/3844 [05:58<7:15:12,  6.87s/it]


  1%|          | 44/3844 [06:15<8:10:28,  7.74s/it]
{'loss': 1.4171, 'grad_norm': 0.5740646386782722, 'learning_rate': 7.586206896551724e-06, 'epoch': 0.01}

  1%|          | 45/3844 [06:22<7:54:41,  7.50s/it]

  1%|          | 46/3844 [06:30<7:56:35,  7.53s/it]

  1%|          | 47/3844 [06:36<7:28:20,  7.08s/it]

  1%|          | 48/3844 [06:41<6:57:19,  6.60s/it]

  1%|▏         | 49/3844 [06:48<6:50:59,  6.50s/it]

  1%|▏         | 50/3844 [06:54<6:46:07,  6.42s/it]

  1%|▏         | 51/3844 [07:02<7:21:01,  6.98s/it]

  1%|▏         | 52/3844 [07:12<8:24:41,  7.99s/it]

  1%|▏         | 53/3844 [07:18<7:38:23,  7.25s/it]

  1%|▏         | 54/3844 [07:28<8:31:04,  8.09s/it]

  1%|▏         | 55/3844 [07:36<8:33:12,  8.13s/it]

  1%|▏         | 56/3844 [07:45<8:47:46,  8.36s/it]

  1%|▏         | 57/3844 [07:57<9:51:35,  9.37s/it]

  2%|▏         | 58/3844 [08:03<8:44:11,  8.31s/it]

  2%|▏         | 59/3844 [08:08<7:51:52,  7.48s/it]

  2%|▏         | 60/3844 [08:15<7:32:04,  7.17s/it]

  2%|▏         | 61/3844 [08:20<7:06:28,  6.76s/it]

  2%|▏         | 62/3844 [08:27<6:54:46,  6.58s/it]

  2%|▏         | 63/3844 [08:36<7:48:41,  7.44s/it]

  2%|▏         | 64/3844 [08:42<7:20:29,  6.99s/it]

  2%|▏         | 65/3844 [08:48<7:05:48,  6.76s/it]

  2%|▏         | 66/3844 [08:56<7:27:53,  7.11s/it]

  2%|▏         | 67/3844 [09:06<8:20:30,  7.95s/it]

  2%|▏         | 68/3844 [09:13<7:56:22,  7.57s/it]

  2%|▏         | 69/3844 [09:19<7:33:11,  7.20s/it]

  2%|▏         | 70/3844 [09:27<7:43:12,  7.36s/it]

  2%|▏         | 71/3844 [09:33<7:25:40,  7.09s/it]

  2%|▏         | 72/3844 [09:39<7:01:32,  6.71s/it]

  2%|▏         | 73/3844 [09:46<7:14:47,  6.92s/it]

  2%|▏         | 74/3844 [09:52<6:53:43,  6.58s/it]

  2%|▏         | 75/3844 [09:59<6:48:00,  6.50s/it]

  2%|▏         | 76/3844 [10:06<7:11:12,  6.87s/it]


  2%|▏         | 78/3844 [10:22<7:49:25,  7.48s/it]
{'loss': 1.3931, 'grad_norm': 0.20880973042116463, 'learning_rate': 1.3448275862068967e-05, 'epoch': 0.02}

  2%|▏         | 79/3844 [10:29<7:39:05,  7.32s/it]

  2%|▏         | 80/3844 [10:35<7:23:16,  7.07s/it]

  2%|▏         | 81/3844 [10:43<7:41:01,  7.35s/it]

  2%|▏         | 82/3844 [10:52<8:06:24,  7.76s/it]

  2%|▏         | 83/3844 [11:05<9:54:15,  9.48s/it]

  2%|▏         | 84/3844 [11:15<10:01:38,  9.60s/it]

  2%|▏         | 85/3844 [11:23<9:17:34,  8.90s/it]

  2%|▏         | 86/3844 [11:28<8:20:28,  7.99s/it]

  2%|▏         | 87/3844 [11:37<8:25:58,  8.08s/it]

  2%|▏         | 88/3844 [11:44<8:00:47,  7.68s/it]

  2%|▏         | 89/3844 [11:49<7:22:19,  7.07s/it]

  2%|▏         | 90/3844 [11:57<7:45:36,  7.44s/it]

  2%|▏         | 91/3844 [12:03<7:11:05,  6.89s/it]

  2%|▏         | 92/3844 [12:12<7:42:33,  7.40s/it]

  2%|▏         | 93/3844 [12:20<7:51:35,  7.54s/it]

  2%|▏         | 94/3844 [12:26<7:24:01,  7.10s/it]

  2%|▏         | 95/3844 [12:33<7:26:45,  7.15s/it]

  2%|▏         | 96/3844 [12:42<8:02:56,  7.73s/it]

  3%|▎         | 97/3844 [12:48<7:26:20,  7.15s/it]


  3%|▎         | 99/3844 [13:02<7:23:59,  7.11s/it]
{'loss': 1.1798, 'grad_norm': 0.17410985281101948, 'learning_rate': 1.706896551724138e-05, 'epoch': 0.03}

  3%|▎         | 100/3844 [13:09<7:23:09,  7.10s/it]

  3%|▎         | 101/3844 [13:15<6:56:42,  6.68s/it]

  3%|▎         | 102/3844 [13:22<7:18:55,  7.04s/it]

  3%|▎         | 103/3844 [13:29<7:00:37,  6.75s/it]

  3%|▎         | 104/3844 [13:35<6:59:09,  6.72s/it]

  3%|▎         | 105/3844 [13:42<6:52:04,  6.61s/it]

  3%|▎         | 106/3844 [13:49<7:03:24,  6.80s/it]

  3%|▎         | 107/3844 [13:55<6:55:39,  6.67s/it]

  3%|▎         | 108/3844 [14:03<7:20:29,  7.07s/it]

  3%|▎         | 109/3844 [14:12<7:48:41,  7.53s/it]

  3%|▎         | 110/3844 [14:19<7:45:11,  7.48s/it]

  3%|▎         | 111/3844 [14:27<8:01:08,  7.73s/it]

  3%|▎         | 112/3844 [14:35<7:57:40,  7.68s/it]

  3%|▎         | 113/3844 [14:46<8:55:11,  8.61s/it]

  3%|▎         | 114/3844 [14:52<8:17:59,  8.01s/it]

  3%|▎         | 115/3844 [14:59<7:49:03,  7.55s/it]

  3%|▎         | 116/3844 [15:06<7:33:05,  7.29s/it]

  3%|▎         | 117/3844 [15:12<7:11:07,  6.94s/it]

  3%|▎         | 118/3844 [15:18<6:50:05,  6.60s/it]

  3%|▎         | 119/3844 [15:24<6:50:08,  6.61s/it]

  3%|▎         | 120/3844 [15:33<7:40:29,  7.42s/it]

  3%|▎         | 121/3844 [15:39<7:07:47,  6.89s/it]

  3%|▎         | 122/3844 [15:45<6:51:39,  6.64s/it]

  3%|▎         | 123/3844 [15:51<6:46:02,  6.55s/it]

  3%|▎         | 124/3844 [15:59<6:55:59,  6.71s/it]

  3%|▎         | 125/3844 [16:04<6:40:01,  6.45s/it]

  3%|▎         | 126/3844 [16:13<7:13:28,  7.00s/it]

  3%|▎         | 127/3844 [16:20<7:10:40,  6.95s/it]

  3%|▎         | 128/3844 [16:29<8:00:19,  7.76s/it]

  3%|▎         | 129/3844 [16:37<7:53:29,  7.65s/it]

  3%|▎         | 130/3844 [16:44<7:41:48,  7.46s/it]

  3%|▎         | 131/3844 [16:54<8:28:34,  8.22s/it]

  3%|▎         | 132/3844 [17:01<8:05:08,  7.84s/it]

  3%|▎         | 133/3844 [17:11<9:02:06,  8.76s/it]

  3%|▎         | 134/3844 [17:18<8:23:10,  8.14s/it]

  4%|▎         | 135/3844 [17:24<7:41:54,  7.47s/it]

  4%|▎         | 136/3844 [17:35<8:47:54,  8.54s/it]

  4%|▎         | 137/3844 [17:45<9:04:16,  8.81s/it]

  4%|▎         | 138/3844 [17:54<9:09:59,  8.90s/it]

  4%|▎         | 139/3844 [18:00<8:16:06,  8.03s/it]

  4%|▎         | 140/3844 [18:05<7:31:51,  7.32s/it]

  4%|▎         | 141/3844 [18:12<7:23:52,  7.19s/it]

  4%|▎         | 142/3844 [18:20<7:31:42,  7.32s/it]

  4%|▎         | 143/3844 [18:28<7:50:36,  7.63s/it]

  4%|▎         | 144/3844 [18:36<8:02:19,  7.82s/it]

  4%|▍         | 145/3844 [18:44<7:51:34,  7.65s/it]

  4%|▍         | 146/3844 [18:50<7:22:39,  7.18s/it]

  4%|▍         | 147/3844 [18:57<7:31:15,  7.32s/it]

  4%|▍         | 148/3844 [19:03<6:58:05,  6.79s/it]

  4%|▍         | 149/3844 [19:09<6:48:11,  6.63s/it]

  4%|▍         | 150/3844 [19:16<6:59:57,  6.82s/it]

  4%|▍         | 151/3844 [19:22<6:41:00,  6.52s/it]

  4%|▍         | 152/3844 [19:29<6:48:18,  6.64s/it]

  4%|▍         | 153/3844 [19:35<6:39:59,  6.50s/it]

  4%|▍         | 154/3844 [19:41<6:26:25,  6.28s/it]

  4%|▍         | 155/3844 [19:47<6:25:10,  6.26s/it]

  4%|▍         | 156/3844 [19:54<6:23:25,  6.24s/it]

  4%|▍         | 157/3844 [20:01<6:44:33,  6.58s/it]

  4%|▍         | 158/3844 [20:10<7:21:50,  7.19s/it]

  4%|▍         | 159/3844 [20:16<7:09:13,  6.99s/it]

  4%|▍         | 160/3844 [20:22<6:52:58,  6.73s/it]

  4%|▍         | 161/3844 [20:28<6:30:12,  6.36s/it]

  4%|▍         | 162/3844 [20:35<6:52:34,  6.72s/it]

  4%|▍         | 163/3844 [20:42<6:44:27,  6.59s/it]

  4%|▍         | 164/3844 [20:48<6:33:27,  6.42s/it]

  4%|▍         | 165/3844 [20:55<6:50:07,  6.69s/it]

  4%|▍         | 166/3844 [21:05<7:55:39,  7.76s/it]

  4%|▍         | 167/3844 [21:11<7:28:24,  7.32s/it]

  4%|▍         | 168/3844 [21:17<7:00:52,  6.87s/it]

  4%|▍         | 169/3844 [21:23<6:37:52,  6.50s/it]

  4%|▍         | 170/3844 [21:28<6:15:43,  6.14s/it]

  4%|▍         | 171/3844 [21:35<6:31:40,  6.40s/it]

  4%|▍         | 172/3844 [21:41<6:17:22,  6.17s/it]

  5%|▍         | 173/3844 [21:46<6:03:32,  5.94s/it]

  5%|▍         | 174/3844 [21:57<7:30:10,  7.36s/it]

  5%|▍         | 175/3844 [22:07<8:18:27,  8.15s/it]

  5%|▍         | 176/3844 [22:15<8:16:36,  8.12s/it]

  5%|▍         | 177/3844 [22:24<8:24:24,  8.25s/it]

  5%|▍         | 178/3844 [22:30<7:45:59,  7.63s/it]

  5%|▍         | 179/3844 [22:36<7:22:24,  7.24s/it]


  5%|▍         | 181/3844 [22:51<7:31:24,  7.39s/it]

  5%|▍         | 182/3844 [22:56<7:01:06,  6.90s/it]
{'loss': 1.4038, 'grad_norm': 0.19903292670346243, 'learning_rate': 1.998453699640878e-05, 'epoch': 0.05}

  5%|▍         | 183/3844 [23:02<6:34:23,  6.46s/it]

  5%|▍         | 184/3844 [23:14<8:10:52,  8.05s/it]

  5%|▍         | 185/3844 [23:23<8:34:49,  8.44s/it]

  5%|▍         | 186/3844 [23:30<8:04:57,  7.95s/it]

  5%|▍         | 187/3844 [23:37<7:56:36,  7.82s/it]

  5%|▍         | 188/3844 [23:46<8:16:36,  8.15s/it]


  5%|▍         | 190/3844 [24:05<8:38:38,  8.52s/it]
{'loss': 1.1934, 'grad_norm': 0.22005025669349515, 'learning_rate': 1.9980562489621136e-05, 'epoch': 0.05}


  5%|▍         | 192/3844 [24:17<7:17:37,  7.19s/it]
{'loss': 1.1836, 'grad_norm': 0.199382535829481, 'learning_rate': 1.997949797907034e-05, 'epoch': 0.05}

  5%|▌         | 193/3844 [24:22<6:45:49,  6.67s/it]

  5%|▌         | 194/3844 [24:31<7:34:11,  7.47s/it]

  5%|▌         | 195/3844 [24:39<7:38:04,  7.53s/it]

  5%|▌         | 196/3844 [24:49<8:31:16,  8.41s/it]

  5%|▌         | 197/3844 [24:58<8:38:34,  8.53s/it]

  5%|▌         | 198/3844 [25:05<7:58:04,  7.87s/it]

  5%|▌         | 199/3844 [25:11<7:22:38,  7.29s/it]

  5%|▌         | 200/3844 [25:16<6:53:33,  6.81s/it]

  5%|▌         | 201/3844 [25:22<6:30:42,  6.43s/it]

  5%|▌         | 202/3844 [25:27<6:16:40,  6.21s/it]

  5%|▌         | 203/3844 [25:33<6:01:47,  5.96s/it]

  5%|▌         | 204/3844 [25:41<6:43:34,  6.65s/it]

  5%|▌         | 205/3844 [25:47<6:20:46,  6.28s/it]

  5%|▌         | 206/3844 [25:57<7:32:06,  7.46s/it]

  5%|▌         | 207/3844 [26:03<7:16:16,  7.20s/it]

  5%|▌         | 208/3844 [26:13<7:56:43,  7.87s/it]

  5%|▌         | 209/3844 [26:19<7:19:13,  7.25s/it]

  5%|▌         | 210/3844 [26:24<6:40:56,  6.62s/it]

  5%|▌         | 211/3844 [26:30<6:38:25,  6.58s/it]

  6%|▌         | 212/3844 [26:38<7:04:46,  7.02s/it]

  6%|▌         | 213/3844 [26:47<7:34:55,  7.52s/it]

  6%|▌         | 214/3844 [26:53<7:05:24,  7.03s/it]

  6%|▌         | 215/3844 [27:02<7:39:47,  7.60s/it]

  6%|▌         | 216/3844 [27:09<7:25:33,  7.37s/it]

  6%|▌         | 217/3844 [27:15<7:15:29,  7.20s/it]

  6%|▌         | 218/3844 [27:27<8:26:59,  8.39s/it]

  6%|▌         | 219/3844 [27:33<7:59:10,  7.93s/it]

  6%|▌         | 220/3844 [27:39<7:15:50,  7.22s/it]

  6%|▌         | 221/3844 [27:44<6:43:51,  6.69s/it]

  6%|▌         | 222/3844 [27:53<7:14:34,  7.20s/it]

  6%|▌         | 223/3844 [27:58<6:45:57,  6.73s/it]

  6%|▌         | 224/3844 [28:05<6:38:09,  6.60s/it]

  6%|▌         | 225/3844 [28:11<6:34:13,  6.54s/it]

  6%|▌         | 226/3844 [28:19<6:49:56,  6.80s/it]

  6%|▌         | 227/3844 [28:24<6:28:25,  6.44s/it]

  6%|▌         | 228/3844 [28:31<6:31:00,  6.49s/it]

  6%|▌         | 229/3844 [28:37<6:21:39,  6.33s/it]

  6%|▌         | 230/3844 [28:42<6:10:08,  6.15s/it]

  6%|▌         | 231/3844 [28:49<6:14:02,  6.21s/it]

  6%|▌         | 232/3844 [28:57<6:52:31,  6.85s/it]

  6%|▌         | 233/3844 [29:05<7:08:07,  7.11s/it]

  6%|▌         | 234/3844 [29:18<8:55:49,  8.91s/it]

  6%|▌         | 235/3844 [29:25<8:30:26,  8.49s/it]

  6%|▌         | 236/3844 [29:34<8:35:26,  8.57s/it]

  6%|▌         | 237/3844 [29:42<8:21:05,  8.34s/it]

  6%|▌         | 238/3844 [29:48<7:44:40,  7.73s/it]

  6%|▌         | 239/3844 [29:55<7:21:57,  7.36s/it]

  6%|▌         | 240/3844 [30:05<8:06:47,  8.10s/it]

  6%|▋         | 241/3844 [30:11<7:43:11,  7.71s/it]

  6%|▋         | 242/3844 [30:21<8:07:30,  8.12s/it]

  6%|▋         | 243/3844 [30:29<8:10:51,  8.18s/it]

  6%|▋         | 244/3844 [30:38<8:35:18,  8.59s/it]

  6%|▋         | 245/3844 [30:46<8:17:55,  8.30s/it]

  6%|▋         | 246/3844 [30:54<8:08:09,  8.14s/it]

  6%|▋         | 247/3844 [31:01<7:44:18,  7.74s/it]

  6%|▋         | 248/3844 [31:08<7:31:44,  7.54s/it]

  6%|▋         | 249/3844 [31:14<7:12:30,  7.22s/it]

  7%|▋         | 250/3844 [31:20<6:52:25,  6.89s/it]

  7%|▋         | 251/3844 [31:26<6:40:33,  6.69s/it]

  7%|▋         | 252/3844 [31:32<6:25:11,  6.43s/it]

  7%|▋         | 253/3844 [31:39<6:36:46,  6.63s/it]

  7%|▋         | 254/3844 [31:46<6:29:42,  6.51s/it]

  7%|▋         | 255/3844 [31:52<6:32:05,  6.55s/it]

  7%|▋         | 256/3844 [32:04<7:57:07,  7.98s/it]

  7%|▋         | 257/3844 [32:09<7:16:59,  7.31s/it]

  7%|▋         | 258/3844 [32:15<6:47:38,  6.82s/it]

  7%|▋         | 259/3844 [32:24<7:25:16,  7.45s/it]

  7%|▋         | 260/3844 [32:31<7:17:03,  7.32s/it]

  7%|▋         | 261/3844 [32:38<7:10:00,  7.20s/it]

  7%|▋         | 262/3844 [32:46<7:30:50,  7.55s/it]

  7%|▋         | 263/3844 [32:52<7:06:58,  7.15s/it]

  7%|▋         | 264/3844 [32:58<6:45:56,  6.80s/it]

  7%|▋         | 265/3844 [33:08<7:29:38,  7.54s/it]

  7%|▋         | 266/3844 [33:13<6:48:15,  6.85s/it]

  7%|▋         | 267/3844 [33:20<6:54:09,  6.95s/it]

  7%|▋         | 268/3844 [33:26<6:30:13,  6.55s/it]

  7%|▋         | 269/3844 [33:32<6:32:28,  6.59s/it]

  7%|▋         | 270/3844 [33:38<6:11:43,  6.24s/it]

  7%|▋         | 271/3844 [33:43<5:58:53,  6.03s/it]

  7%|▋         | 272/3844 [33:52<6:53:24,  6.94s/it]

  7%|▋         | 273/3844 [33:59<6:53:19,  6.94s/it]

  7%|▋         | 274/3844 [34:06<6:50:15,  6.90s/it]

  7%|▋         | 275/3844 [34:12<6:38:22,  6.70s/it]

  7%|▋         | 276/3844 [34:21<7:09:24,  7.22s/it]

  7%|▋         | 277/3844 [34:27<6:46:31,  6.84s/it]

  7%|▋         | 278/3844 [34:33<6:30:34,  6.57s/it]

  7%|▋         | 279/3844 [34:39<6:25:45,  6.49s/it]

  7%|▋         | 280/3844 [34:48<7:05:55,  7.17s/it]

  7%|▋         | 281/3844 [34:55<7:14:15,  7.31s/it]

  7%|▋         | 282/3844 [35:03<7:24:06,  7.48s/it]

  7%|▋         | 283/3844 [35:13<8:04:07,  8.16s/it]

  7%|▋         | 284/3844 [35:23<8:37:28,  8.72s/it]

  7%|▋         | 285/3844 [35:29<7:50:27,  7.93s/it]

  7%|▋         | 286/3844 [35:36<7:36:25,  7.70s/it]

  7%|▋         | 287/3844 [35:46<8:05:25,  8.19s/it]

  7%|▋         | 288/3844 [35:53<7:56:21,  8.04s/it]

  8%|▊         | 289/3844 [35:59<7:08:26,  7.23s/it]

  8%|▊         | 290/3844 [36:07<7:35:49,  7.70s/it]

  8%|▊         | 291/3844 [36:20<9:02:22,  9.16s/it]

  8%|▊         | 292/3844 [36:29<8:51:25,  8.98s/it]

  8%|▊         | 293/3844 [36:34<7:46:39,  7.89s/it]

  8%|▊         | 294/3844 [36:41<7:31:07,  7.62s/it]

  8%|▊         | 295/3844 [36:47<7:00:47,  7.11s/it]

  8%|▊         | 296/3844 [36:54<6:54:54,  7.02s/it]

  8%|▊         | 297/3844 [37:00<6:50:50,  6.95s/it]

  8%|▊         | 298/3844 [37:08<7:07:05,  7.23s/it]

  8%|▊         | 299/3844 [37:14<6:41:35,  6.80s/it]

  8%|▊         | 300/3844 [37:24<7:27:18,  7.57s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0853, 'grad_norm': 0.22032758771544253, 'learning_rate': 1.9878722118446153e-05, 'epoch': 0.08}
  8%|▊         | 301/3844 [38:06<17:39:55, 17.95s/it]

  8%|▊         | 302/3844 [38:11<14:00:22, 14.24s/it]

  8%|▊         | 303/3844 [38:19<12:11:15, 12.39s/it]

  8%|▊         | 304/3844 [38:27<10:51:42, 11.05s/it]

  8%|▊         | 305/3844 [38:35<9:51:38, 10.03s/it]

  8%|▊         | 306/3844 [38:41<8:45:05,  8.90s/it]

  8%|▊         | 307/3844 [38:48<8:04:48,  8.22s/it]

  8%|▊         | 308/3844 [38:55<7:42:03,  7.84s/it]

  8%|▊         | 309/3844 [39:03<7:43:50,  7.87s/it]

  8%|▊         | 310/3844 [39:09<7:06:47,  7.25s/it]

  8%|▊         | 311/3844 [39:17<7:20:22,  7.48s/it]

  8%|▊         | 312/3844 [39:24<7:25:54,  7.57s/it]

  8%|▊         | 313/3844 [39:30<6:49:40,  6.96s/it]
[2024-05-27 11:25:24,527] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 314/3844 [39:42<8:24:43,  8.58s/it]

  8%|▊         | 315/3844 [39:49<7:51:57,  8.02s/it]

  8%|▊         | 316/3844 [39:55<7:15:42,  7.41s/it]

  8%|▊         | 317/3844 [40:05<8:01:40,  8.19s/it]

  8%|▊         | 318/3844 [40:12<7:49:56,  8.00s/it]

  8%|▊         | 319/3844 [40:21<7:51:34,  8.03s/it]

  8%|▊         | 320/3844 [40:28<7:38:08,  7.80s/it]

  8%|▊         | 321/3844 [40:40<8:47:16,  8.98s/it]

  8%|▊         | 322/3844 [40:47<8:15:35,  8.44s/it]

  8%|▊         | 323/3844 [40:52<7:25:12,  7.59s/it]

  8%|▊         | 324/3844 [40:58<6:54:25,  7.06s/it]

  8%|▊         | 325/3844 [41:06<7:14:30,  7.41s/it]

  8%|▊         | 326/3844 [41:13<6:53:10,  7.05s/it]

  9%|▊         | 327/3844 [41:19<6:36:48,  6.77s/it]

  9%|▊         | 328/3844 [41:24<6:17:07,  6.44s/it]

  9%|▊         | 329/3844 [41:32<6:42:20,  6.87s/it]

  9%|▊         | 330/3844 [41:40<7:00:57,  7.19s/it]

  9%|▊         | 331/3844 [41:48<7:03:29,  7.23s/it]

  9%|▊         | 332/3844 [41:54<6:56:22,  7.11s/it]

  9%|▊         | 333/3844 [42:04<7:47:37,  7.99s/it]

  9%|▊         | 334/3844 [42:12<7:35:11,  7.78s/it]

  9%|▊         | 335/3844 [42:20<7:38:39,  7.84s/it]

  9%|▊         | 336/3844 [42:28<7:46:36,  7.98s/it]

  9%|▉         | 337/3844 [42:36<7:45:32,  7.96s/it]

  9%|▉         | 338/3844 [42:44<7:47:09,  7.99s/it]

  9%|▉         | 339/3844 [42:50<7:20:45,  7.55s/it]

  9%|▉         | 340/3844 [42:57<6:59:57,  7.19s/it]

  9%|▉         | 341/3844 [43:04<6:54:17,  7.10s/it]

  9%|▉         | 342/3844 [43:10<6:41:37,  6.88s/it]

  9%|▉         | 343/3844 [43:17<6:34:36,  6.76s/it]

  9%|▉         | 344/3844 [43:25<7:10:35,  7.38s/it]

  9%|▉         | 345/3844 [43:32<6:58:53,  7.18s/it]

  9%|▉         | 346/3844 [43:42<7:43:03,  7.94s/it]

  9%|▉         | 347/3844 [43:48<7:03:34,  7.27s/it]

  9%|▉         | 348/3844 [43:53<6:31:54,  6.73s/it]

  9%|▉         | 349/3844 [44:01<6:49:32,  7.03s/it]

  9%|▉         | 350/3844 [44:07<6:39:41,  6.86s/it]

  9%|▉         | 351/3844 [44:14<6:29:19,  6.69s/it]

  9%|▉         | 352/3844 [44:22<7:04:39,  7.30s/it]

  9%|▉         | 353/3844 [44:28<6:42:12,  6.91s/it]

  9%|▉         | 354/3844 [44:34<6:16:49,  6.48s/it]

  9%|▉         | 355/3844 [44:43<7:07:44,  7.36s/it]

  9%|▉         | 356/3844 [44:49<6:49:02,  7.04s/it]

  9%|▉         | 357/3844 [44:56<6:49:41,  7.05s/it]

  9%|▉         | 358/3844 [45:03<6:39:28,  6.88s/it]

  9%|▉         | 359/3844 [45:10<6:38:13,  6.86s/it]

  9%|▉         | 360/3844 [45:16<6:33:54,  6.78s/it]

  9%|▉         | 361/3844 [45:24<6:56:49,  7.18s/it]

  9%|▉         | 362/3844 [45:31<6:45:55,  6.99s/it]

  9%|▉         | 363/3844 [45:36<6:18:35,  6.53s/it]

  9%|▉         | 364/3844 [45:47<7:32:27,  7.80s/it]

  9%|▉         | 365/3844 [45:54<7:21:13,  7.61s/it]

 10%|▉         | 366/3844 [46:02<7:19:41,  7.59s/it]

 10%|▉         | 367/3844 [46:08<7:01:02,  7.27s/it]

 10%|▉         | 368/3844 [46:16<7:02:08,  7.29s/it]

 10%|▉         | 369/3844 [46:23<6:57:34,  7.21s/it]

 10%|▉         | 370/3844 [46:30<6:50:58,  7.10s/it]

 10%|▉         | 371/3844 [46:39<7:28:16,  7.74s/it]

 10%|▉         | 372/3844 [46:45<6:57:35,  7.22s/it]

 10%|▉         | 373/3844 [46:50<6:26:46,  6.69s/it]

 10%|▉         | 374/3844 [46:58<6:44:57,  7.00s/it]

 10%|▉         | 375/3844 [47:04<6:26:16,  6.68s/it]

 10%|▉         | 376/3844 [47:10<6:11:12,  6.42s/it]

 10%|▉         | 377/3844 [47:18<6:39:56,  6.92s/it]

 10%|▉         | 378/3844 [47:24<6:24:37,  6.66s/it]

 10%|▉         | 379/3844 [47:30<6:17:58,  6.55s/it]

 10%|▉         | 380/3844 [47:39<7:04:02,  7.34s/it]

 10%|▉         | 381/3844 [47:45<6:39:35,  6.92s/it]

 10%|▉         | 382/3844 [47:52<6:34:40,  6.84s/it]

 10%|▉         | 383/3844 [48:02<7:20:37,  7.64s/it]

 10%|▉         | 384/3844 [48:10<7:37:39,  7.94s/it]

 10%|█         | 385/3844 [48:19<7:47:00,  8.10s/it]

 10%|█         | 386/3844 [48:27<7:58:43,  8.31s/it]

 10%|█         | 387/3844 [48:38<8:33:44,  8.92s/it]

 10%|█         | 388/3844 [48:46<8:22:30,  8.72s/it]

 10%|█         | 389/3844 [48:53<7:51:41,  8.19s/it]

 10%|█         | 390/3844 [49:02<8:03:00,  8.39s/it]

 10%|█         | 391/3844 [49:10<7:57:25,  8.30s/it]

 10%|█         | 392/3844 [49:17<7:39:04,  7.98s/it]

 10%|█         | 393/3844 [49:23<7:09:40,  7.47s/it]

 10%|█         | 394/3844 [49:31<7:08:04,  7.44s/it]

 10%|█         | 395/3844 [49:37<6:45:36,  7.06s/it]

 10%|█         | 396/3844 [49:45<6:58:39,  7.29s/it]

 10%|█         | 397/3844 [49:51<6:35:44,  6.89s/it]

 10%|█         | 398/3844 [50:00<7:14:38,  7.57s/it]

 10%|█         | 399/3844 [50:07<7:09:38,  7.48s/it]

 10%|█         | 400/3844 [50:15<7:21:36,  7.69s/it]

 10%|█         | 401/3844 [50:23<7:26:45,  7.79s/it]

 10%|█         | 402/3844 [50:29<6:49:18,  7.14s/it]

 10%|█         | 403/3844 [50:41<8:12:12,  8.58s/it]

 11%|█         | 404/3844 [50:49<8:09:55,  8.55s/it]

 11%|█         | 405/3844 [50:58<8:15:36,  8.65s/it]

 11%|█         | 406/3844 [51:04<7:28:30,  7.83s/it]

 11%|█         | 407/3844 [51:10<6:50:38,  7.17s/it]

 11%|█         | 408/3844 [51:16<6:40:58,  7.00s/it]

 11%|█         | 409/3844 [51:22<6:15:01,  6.55s/it]

 11%|█         | 410/3844 [51:29<6:20:03,  6.64s/it]

 11%|█         | 411/3844 [51:35<6:16:24,  6.58s/it]

 11%|█         | 412/3844 [51:46<7:34:55,  7.95s/it]

 11%|█         | 413/3844 [51:53<7:02:35,  7.39s/it]

 11%|█         | 414/3844 [51:58<6:26:32,  6.76s/it]

 11%|█         | 415/3844 [52:06<6:53:46,  7.24s/it]

 11%|█         | 416/3844 [52:14<7:12:23,  7.57s/it]

 11%|█         | 417/3844 [52:22<7:04:48,  7.44s/it]

 11%|█         | 418/3844 [52:28<6:49:53,  7.18s/it]

 11%|█         | 419/3844 [52:34<6:31:13,  6.85s/it]

 11%|█         | 420/3844 [52:43<7:06:12,  7.47s/it]

 11%|█         | 421/3844 [52:52<7:22:45,  7.76s/it]

 11%|█         | 422/3844 [52:59<7:19:40,  7.71s/it]

 11%|█         | 423/3844 [53:05<6:48:59,  7.17s/it]

 11%|█         | 424/3844 [53:12<6:47:45,  7.15s/it]

 11%|█         | 425/3844 [53:19<6:44:58,  7.11s/it]

 11%|█         | 426/3844 [53:27<6:56:35,  7.31s/it]

 11%|█         | 427/3844 [53:33<6:31:07,  6.87s/it]

 11%|█         | 428/3844 [53:40<6:38:07,  6.99s/it]

 11%|█         | 429/3844 [53:46<6:17:26,  6.63s/it]

 11%|█         | 430/3844 [53:55<6:57:43,  7.34s/it]

 11%|█         | 431/3844 [54:06<7:52:28,  8.31s/it]

 11%|█         | 432/3844 [54:13<7:36:56,  8.04s/it]

 11%|█▏        | 433/3844 [54:21<7:34:46,  8.00s/it]

 11%|█▏        | 434/3844 [54:30<7:49:41,  8.26s/it]

 11%|█▏        | 435/3844 [54:38<7:48:24,  8.24s/it]

 11%|█▏        | 436/3844 [54:46<7:52:35,  8.32s/it]

 11%|█▏        | 437/3844 [54:53<7:30:54,  7.94s/it]

 11%|█▏        | 438/3844 [55:01<7:18:37,  7.73s/it]

 11%|█▏        | 439/3844 [55:08<7:03:57,  7.47s/it]

 11%|█▏        | 440/3844 [55:14<6:39:29,  7.04s/it]

 11%|█▏        | 441/3844 [55:22<6:58:24,  7.38s/it]

 11%|█▏        | 442/3844 [55:30<7:04:59,  7.50s/it]

 12%|█▏        | 443/3844 [55:37<6:58:35,  7.38s/it]

 12%|█▏        | 444/3844 [55:44<7:06:01,  7.52s/it]

 12%|█▏        | 445/3844 [55:51<6:48:06,  7.20s/it]

 12%|█▏        | 446/3844 [56:00<7:20:17,  7.77s/it]

 12%|█▏        | 447/3844 [56:06<6:41:43,  7.10s/it]

 12%|█▏        | 448/3844 [56:12<6:33:13,  6.95s/it]

 12%|█▏        | 449/3844 [56:18<6:16:16,  6.65s/it]

 12%|█▏        | 450/3844 [56:27<6:49:48,  7.24s/it]

 12%|█▏        | 451/3844 [56:33<6:31:30,  6.92s/it]

 12%|█▏        | 452/3844 [56:40<6:37:33,  7.03s/it]

 12%|█▏        | 453/3844 [56:46<6:22:31,  6.77s/it]

 12%|█▏        | 454/3844 [56:54<6:42:42,  7.13s/it]

 12%|█▏        | 455/3844 [57:02<6:49:17,  7.25s/it]

 12%|█▏        | 456/3844 [57:08<6:23:53,  6.80s/it]

 12%|█▏        | 457/3844 [57:16<6:43:56,  7.16s/it]

 12%|█▏        | 458/3844 [57:22<6:36:42,  7.03s/it]

 12%|█▏        | 459/3844 [57:28<6:16:58,  6.68s/it]

 12%|█▏        | 460/3844 [57:36<6:31:08,  6.94s/it]

 12%|█▏        | 461/3844 [57:42<6:22:44,  6.79s/it]

 12%|█▏        | 462/3844 [57:50<6:35:33,  7.02s/it]

 12%|█▏        | 463/3844 [57:57<6:41:14,  7.12s/it]

 12%|█▏        | 464/3844 [58:03<6:23:35,  6.81s/it]

 12%|█▏        | 465/3844 [58:09<6:00:05,  6.39s/it]

 12%|█▏        | 466/3844 [58:15<5:53:10,  6.27s/it]

 12%|█▏        | 467/3844 [58:21<5:55:09,  6.31s/it]

 12%|█▏        | 468/3844 [58:30<6:48:12,  7.25s/it]

 12%|█▏        | 469/3844 [58:39<7:13:31,  7.71s/it]

 12%|█▏        | 470/3844 [58:46<7:02:16,  7.51s/it]

 12%|█▏        | 471/3844 [58:52<6:36:09,  7.05s/it]

 12%|█▏        | 472/3844 [59:00<6:48:16,  7.26s/it]

 12%|█▏        | 473/3844 [59:06<6:20:04,  6.76s/it]

 12%|█▏        | 474/3844 [59:15<7:08:38,  7.63s/it]

 12%|█▏        | 475/3844 [59:21<6:34:42,  7.03s/it]

 12%|█▏        | 476/3844 [59:28<6:28:25,  6.92s/it]

 12%|█▏        | 477/3844 [59:33<6:06:31,  6.53s/it]

 12%|█▏        | 478/3844 [59:41<6:21:16,  6.80s/it]

 12%|█▏        | 479/3844 [59:50<7:08:35,  7.64s/it]

 12%|█▏        | 480/3844 [59:56<6:35:53,  7.06s/it]

 13%|█▎        | 481/3844 [1:00:04<6:50:17,  7.32s/it]

 13%|█▎        | 482/3844 [1:00:11<6:55:49,  7.42s/it]

 13%|█▎        | 483/3844 [1:00:20<7:15:57,  7.78s/it]

 13%|█▎        | 484/3844 [1:00:27<7:01:45,  7.53s/it]

 13%|█▎        | 485/3844 [1:00:33<6:39:32,  7.14s/it]

 13%|█▎        | 486/3844 [1:00:42<6:59:34,  7.50s/it]

 13%|█▎        | 487/3844 [1:00:47<6:25:33,  6.89s/it]

 13%|█▎        | 488/3844 [1:00:54<6:24:27,  6.87s/it]

 13%|█▎        | 489/3844 [1:01:01<6:21:46,  6.83s/it]

 13%|█▎        | 490/3844 [1:01:07<6:10:40,  6.63s/it]

 13%|█▎        | 491/3844 [1:01:16<6:55:43,  7.44s/it]

 13%|█▎        | 492/3844 [1:01:25<7:23:05,  7.93s/it]

 13%|█▎        | 493/3844 [1:01:34<7:40:56,  8.25s/it]

 13%|█▎        | 494/3844 [1:01:41<7:19:06,  7.86s/it]

 13%|█▎        | 495/3844 [1:01:48<6:56:41,  7.47s/it]


 13%|█▎        | 497/3844 [1:02:07<7:47:11,  8.38s/it]
{'loss': 1.1257, 'grad_norm': 0.26848121575880135, 'learning_rate': 1.9488984712987106e-05, 'epoch': 0.13}

 13%|█▎        | 498/3844 [1:02:15<7:36:42,  8.19s/it]

 13%|█▎        | 499/3844 [1:02:21<7:12:04,  7.75s/it]

 13%|█▎        | 500/3844 [1:02:28<6:50:14,  7.36s/it]

 13%|█▎        | 501/3844 [1:02:36<7:09:17,  7.70s/it]

 13%|█▎        | 502/3844 [1:02:42<6:41:23,  7.21s/it]

 13%|█▎        | 503/3844 [1:02:48<6:23:03,  6.88s/it]

 13%|█▎        | 504/3844 [1:02:55<6:18:39,  6.80s/it]

 13%|█▎        | 505/3844 [1:03:02<6:19:43,  6.82s/it]

 13%|█▎        | 506/3844 [1:03:07<5:56:55,  6.42s/it]

 13%|█▎        | 507/3844 [1:03:14<6:06:08,  6.58s/it]

 13%|█▎        | 508/3844 [1:03:21<6:03:33,  6.54s/it]

 13%|█▎        | 509/3844 [1:03:27<6:05:30,  6.58s/it]

 13%|█▎        | 510/3844 [1:03:36<6:46:48,  7.32s/it]

 13%|█▎        | 511/3844 [1:03:44<6:44:04,  7.27s/it]

 13%|█▎        | 512/3844 [1:03:52<7:08:56,  7.72s/it]

 13%|█▎        | 513/3844 [1:04:06<8:42:22,  9.41s/it]

 13%|█▎        | 514/3844 [1:04:12<7:42:59,  8.34s/it]

 13%|█▎        | 515/3844 [1:04:20<7:43:03,  8.35s/it]

 13%|█▎        | 516/3844 [1:04:27<7:23:35,  8.00s/it]

 13%|█▎        | 517/3844 [1:04:38<8:05:52,  8.76s/it]

 13%|█▎        | 518/3844 [1:04:46<7:51:57,  8.51s/it]

 14%|█▎        | 519/3844 [1:04:54<7:50:27,  8.49s/it]

 14%|█▎        | 520/3844 [1:05:00<7:08:41,  7.74s/it]

 14%|█▎        | 521/3844 [1:05:06<6:37:34,  7.18s/it]

 14%|█▎        | 522/3844 [1:05:12<6:11:05,  6.70s/it]

 14%|█▎        | 523/3844 [1:05:20<6:33:01,  7.10s/it]

 14%|█▎        | 524/3844 [1:05:26<6:16:42,  6.81s/it]

 14%|█▎        | 525/3844 [1:05:32<6:05:01,  6.60s/it]

 14%|█▎        | 526/3844 [1:05:38<6:05:49,  6.62s/it]

 14%|█▎        | 527/3844 [1:05:45<5:59:14,  6.50s/it]

 14%|█▎        | 528/3844 [1:05:52<6:10:29,  6.70s/it]

 14%|█▍        | 529/3844 [1:05:58<5:56:06,  6.45s/it]

 14%|█▍        | 530/3844 [1:06:04<6:01:20,  6.54s/it]

 14%|█▍        | 531/3844 [1:06:12<6:23:38,  6.95s/it]

 14%|█▍        | 532/3844 [1:06:21<6:46:25,  7.36s/it]

 14%|█▍        | 533/3844 [1:06:31<7:36:24,  8.27s/it]

 14%|█▍        | 534/3844 [1:06:37<7:04:12,  7.69s/it]
[2024-05-27 11:52:29,752] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 535/3844 [1:06:47<7:43:02,  8.40s/it]

 14%|█▍        | 536/3844 [1:06:55<7:34:13,  8.24s/it]

 14%|█▍        | 537/3844 [1:07:01<6:53:44,  7.51s/it]

 14%|█▍        | 538/3844 [1:07:07<6:20:02,  6.90s/it]

 14%|█▍        | 539/3844 [1:07:13<6:10:58,  6.73s/it]

 14%|█▍        | 540/3844 [1:07:20<6:12:12,  6.76s/it]

 14%|█▍        | 541/3844 [1:07:31<7:31:51,  8.21s/it]

 14%|█▍        | 542/3844 [1:07:38<7:02:05,  7.67s/it]

 14%|█▍        | 543/3844 [1:07:44<6:36:44,  7.21s/it]

 14%|█▍        | 544/3844 [1:07:52<6:44:55,  7.36s/it]

 14%|█▍        | 545/3844 [1:08:00<7:05:16,  7.73s/it]

 14%|█▍        | 546/3844 [1:08:08<7:06:49,  7.77s/it]

 14%|█▍        | 547/3844 [1:08:16<7:09:54,  7.82s/it]

 14%|█▍        | 548/3844 [1:08:23<7:03:24,  7.71s/it]

 14%|█▍        | 549/3844 [1:08:31<7:02:04,  7.69s/it]

 14%|█▍        | 550/3844 [1:08:37<6:31:40,  7.13s/it]

 14%|█▍        | 551/3844 [1:08:43<6:11:13,  6.76s/it]

 14%|█▍        | 552/3844 [1:08:51<6:41:01,  7.31s/it]

 14%|█▍        | 553/3844 [1:09:01<7:14:57,  7.93s/it]

 14%|█▍        | 554/3844 [1:09:06<6:31:29,  7.14s/it]

 14%|█▍        | 555/3844 [1:09:14<6:43:18,  7.36s/it]

 14%|█▍        | 556/3844 [1:09:20<6:16:46,  6.88s/it]

 14%|█▍        | 557/3844 [1:09:29<7:04:29,  7.75s/it]

 15%|█▍        | 558/3844 [1:09:36<6:46:34,  7.42s/it]

 15%|█▍        | 559/3844 [1:09:43<6:34:44,  7.21s/it]

 15%|█▍        | 560/3844 [1:09:49<6:14:39,  6.85s/it]

 15%|█▍        | 561/3844 [1:09:55<6:06:29,  6.70s/it]

 15%|█▍        | 562/3844 [1:10:01<5:49:45,  6.39s/it]

 15%|█▍        | 563/3844 [1:10:07<5:41:34,  6.25s/it]

 15%|█▍        | 564/3844 [1:10:16<6:27:00,  7.08s/it]

 15%|█▍        | 565/3844 [1:10:21<6:03:08,  6.64s/it]

 15%|█▍        | 566/3844 [1:10:28<6:01:38,  6.62s/it]

 15%|█▍        | 567/3844 [1:10:34<5:50:21,  6.41s/it]

 15%|█▍        | 568/3844 [1:10:40<5:44:43,  6.31s/it]

 15%|█▍        | 569/3844 [1:10:52<7:15:49,  7.98s/it]

 15%|█▍        | 570/3844 [1:11:00<7:15:21,  7.98s/it]

 15%|█▍        | 571/3844 [1:11:07<7:02:12,  7.74s/it]

 15%|█▍        | 572/3844 [1:11:13<6:24:59,  7.06s/it]

 15%|█▍        | 573/3844 [1:11:18<6:03:31,  6.67s/it]

 15%|█▍        | 574/3844 [1:11:25<6:06:31,  6.73s/it]

 15%|█▍        | 575/3844 [1:11:33<6:32:01,  7.20s/it]

 15%|█▍        | 576/3844 [1:11:41<6:40:24,  7.35s/it]

 15%|█▌        | 577/3844 [1:11:48<6:37:27,  7.30s/it]

 15%|█▌        | 578/3844 [1:11:54<6:13:11,  6.86s/it]

 15%|█▌        | 579/3844 [1:12:01<6:05:54,  6.72s/it]

 15%|█▌        | 580/3844 [1:12:06<5:52:42,  6.48s/it]

 15%|█▌        | 581/3844 [1:12:13<5:53:20,  6.50s/it]

 15%|█▌        | 582/3844 [1:12:20<5:55:36,  6.54s/it]

 15%|█▌        | 583/3844 [1:12:27<6:10:57,  6.83s/it]

 15%|█▌        | 584/3844 [1:12:35<6:22:46,  7.04s/it]

 15%|█▌        | 585/3844 [1:12:40<5:56:54,  6.57s/it]

 15%|█▌        | 586/3844 [1:12:49<6:36:07,  7.30s/it]

 15%|█▌        | 587/3844 [1:12:57<6:48:24,  7.52s/it]

 15%|█▌        | 588/3844 [1:13:03<6:17:46,  6.96s/it]

 15%|█▌        | 589/3844 [1:13:11<6:39:12,  7.36s/it]

 15%|█▌        | 590/3844 [1:13:19<6:49:27,  7.55s/it]

 15%|█▌        | 591/3844 [1:13:29<7:30:42,  8.31s/it]

 15%|█▌        | 592/3844 [1:13:38<7:41:18,  8.51s/it]

 15%|█▌        | 593/3844 [1:13:44<6:51:48,  7.60s/it]

 15%|█▌        | 594/3844 [1:13:51<6:42:14,  7.43s/it]
[2024-05-27 11:59:44,589] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 595/3844 [1:14:02<7:49:15,  8.67s/it]

 16%|█▌        | 596/3844 [1:14:10<7:32:31,  8.36s/it]

 16%|█▌        | 597/3844 [1:14:18<7:29:47,  8.31s/it]

 16%|█▌        | 598/3844 [1:14:25<7:10:08,  7.95s/it]

 16%|█▌        | 599/3844 [1:14:31<6:27:55,  7.17s/it]

 16%|█▌        | 600/3844 [1:14:40<7:04:48,  7.86s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2095, 'grad_norm': 0.24894233238366842, 'learning_rate': 1.9176340996326727e-05, 'epoch': 0.16}
 16%|█▌        | 601/3844 [1:15:21<16:05:54, 17.87s/it]

 16%|█▌        | 602/3844 [1:15:30<13:37:19, 15.13s/it]

 16%|█▌        | 603/3844 [1:15:35<10:56:10, 12.15s/it]

 16%|█▌        | 604/3844 [1:15:42<9:32:50, 10.61s/it]

 16%|█▌        | 605/3844 [1:15:48<8:08:07,  9.04s/it]

 16%|█▌        | 606/3844 [1:15:54<7:19:16,  8.14s/it]

 16%|█▌        | 607/3844 [1:16:01<7:06:05,  7.90s/it]

 16%|█▌        | 608/3844 [1:16:08<6:46:13,  7.53s/it]

 16%|█▌        | 609/3844 [1:16:13<6:15:46,  6.97s/it]

 16%|█▌        | 610/3844 [1:16:21<6:31:08,  7.26s/it]

 16%|█▌        | 611/3844 [1:16:31<7:04:40,  7.88s/it]

 16%|█▌        | 612/3844 [1:16:40<7:30:10,  8.36s/it]

 16%|█▌        | 613/3844 [1:16:45<6:41:00,  7.45s/it]

 16%|█▌        | 614/3844 [1:16:52<6:26:38,  7.18s/it]

 16%|█▌        | 615/3844 [1:17:01<7:00:57,  7.82s/it]

 16%|█▌        | 616/3844 [1:17:10<7:22:42,  8.23s/it]

 16%|█▌        | 617/3844 [1:17:18<7:04:17,  7.89s/it]

 16%|█▌        | 618/3844 [1:17:24<6:45:50,  7.55s/it]

 16%|█▌        | 619/3844 [1:17:30<6:19:07,  7.05s/it]

 16%|█▌        | 620/3844 [1:17:39<6:50:31,  7.64s/it]

 16%|█▌        | 621/3844 [1:17:45<6:20:36,  7.09s/it]

 16%|█▌        | 622/3844 [1:17:53<6:31:57,  7.30s/it]

 16%|█▌        | 623/3844 [1:18:04<7:31:52,  8.42s/it]

 16%|█▌        | 624/3844 [1:18:09<6:46:07,  7.57s/it]

 16%|█▋        | 625/3844 [1:18:16<6:25:21,  7.18s/it]

 16%|█▋        | 626/3844 [1:18:24<6:45:59,  7.57s/it]

 16%|█▋        | 627/3844 [1:18:30<6:21:22,  7.11s/it]

 16%|█▋        | 628/3844 [1:18:38<6:31:48,  7.31s/it]

 16%|█▋        | 629/3844 [1:18:46<6:41:11,  7.49s/it]

 16%|█▋        | 630/3844 [1:18:54<6:59:02,  7.82s/it]


 16%|█▋        | 632/3844 [1:19:08<6:22:20,  7.14s/it]
{'loss': 1.3119, 'grad_norm': 0.27517026267545536, 'learning_rate': 1.9069399096619898e-05, 'epoch': 0.16}

 16%|█▋        | 633/3844 [1:19:13<5:55:47,  6.65s/it]

 16%|█▋        | 634/3844 [1:19:20<5:56:41,  6.67s/it]

 17%|█▋        | 635/3844 [1:19:27<5:54:32,  6.63s/it]

 17%|█▋        | 636/3844 [1:19:33<5:45:33,  6.46s/it]

 17%|█▋        | 637/3844 [1:19:40<5:54:24,  6.63s/it]

 17%|█▋        | 638/3844 [1:19:52<7:22:19,  8.28s/it]

 17%|█▋        | 639/3844 [1:19:58<6:46:59,  7.62s/it]

 17%|█▋        | 640/3844 [1:20:05<6:34:13,  7.38s/it]

 17%|█▋        | 641/3844 [1:20:10<6:04:54,  6.84s/it]

 17%|█▋        | 642/3844 [1:20:17<5:55:02,  6.65s/it]

 17%|█▋        | 643/3844 [1:20:25<6:23:02,  7.18s/it]

 17%|█▋        | 644/3844 [1:20:32<6:13:37,  7.01s/it]

 17%|█▋        | 645/3844 [1:20:39<6:12:27,  6.99s/it]

 17%|█▋        | 646/3844 [1:20:44<5:53:59,  6.64s/it]

 17%|█▋        | 647/3844 [1:20:51<5:49:55,  6.57s/it]

 17%|█▋        | 648/3844 [1:20:57<5:40:13,  6.39s/it]

 17%|█▋        | 649/3844 [1:21:06<6:27:17,  7.27s/it]

 17%|█▋        | 650/3844 [1:21:13<6:24:37,  7.23s/it]


 17%|█▋        | 652/3844 [1:21:35<8:11:22,  9.24s/it]

 17%|█▋        | 653/3844 [1:21:40<7:11:30,  8.11s/it]

 17%|█▋        | 654/3844 [1:21:49<7:16:21,  8.21s/it]

 17%|█▋        | 655/3844 [1:22:00<8:09:16,  9.21s/it]

 17%|█▋        | 656/3844 [1:22:06<7:14:18,  8.17s/it]

 17%|█▋        | 657/3844 [1:22:14<7:14:06,  8.17s/it]

 17%|█▋        | 658/3844 [1:22:20<6:40:06,  7.53s/it]

 17%|█▋        | 659/3844 [1:22:27<6:36:32,  7.47s/it]

 17%|█▋        | 660/3844 [1:22:34<6:14:54,  7.06s/it]

 17%|█▋        | 661/3844 [1:22:39<5:55:40,  6.70s/it]

 17%|█▋        | 662/3844 [1:22:46<5:52:22,  6.64s/it]

 17%|█▋        | 663/3844 [1:22:53<5:58:35,  6.76s/it]

 17%|█▋        | 664/3844 [1:23:04<7:13:21,  8.18s/it]

 17%|█▋        | 665/3844 [1:23:13<7:20:16,  8.31s/it]

 17%|█▋        | 666/3844 [1:23:19<6:45:30,  7.66s/it]

 17%|█▋        | 667/3844 [1:23:26<6:24:57,  7.27s/it]

 17%|█▋        | 668/3844 [1:23:37<7:27:56,  8.46s/it]

 17%|█▋        | 669/3844 [1:23:44<7:10:44,  8.14s/it]

 17%|█▋        | 670/3844 [1:23:54<7:44:13,  8.78s/it]

 17%|█▋        | 671/3844 [1:24:01<7:10:29,  8.14s/it]

 17%|█▋        | 672/3844 [1:24:12<8:00:50,  9.10s/it]

 18%|█▊        | 673/3844 [1:24:23<8:20:54,  9.48s/it]

 18%|█▊        | 674/3844 [1:24:32<8:18:50,  9.44s/it]

 18%|█▊        | 675/3844 [1:24:38<7:25:22,  8.43s/it]

 18%|█▊        | 676/3844 [1:24:44<6:49:04,  7.75s/it]

 18%|█▊        | 677/3844 [1:24:50<6:13:19,  7.07s/it]

 18%|█▊        | 678/3844 [1:24:56<5:58:45,  6.80s/it]

 18%|█▊        | 679/3844 [1:25:04<6:10:53,  7.03s/it]

 18%|█▊        | 680/3844 [1:25:11<6:22:52,  7.26s/it]

 18%|█▊        | 681/3844 [1:25:18<6:17:10,  7.15s/it]

 18%|█▊        | 682/3844 [1:25:25<6:03:21,  6.89s/it]

 18%|█▊        | 683/3844 [1:25:33<6:27:16,  7.35s/it]

 18%|█▊        | 684/3844 [1:25:41<6:44:01,  7.67s/it]

 18%|█▊        | 685/3844 [1:25:47<6:16:04,  7.14s/it]

 18%|█▊        | 686/3844 [1:25:56<6:38:12,  7.57s/it]

 18%|█▊        | 687/3844 [1:26:05<7:06:13,  8.10s/it]

 18%|█▊        | 688/3844 [1:26:13<7:00:02,  7.99s/it]

 18%|█▊        | 689/3844 [1:26:19<6:22:00,  7.26s/it]

 18%|█▊        | 690/3844 [1:26:26<6:23:07,  7.29s/it]

 18%|█▊        | 691/3844 [1:26:34<6:28:48,  7.40s/it]

 18%|█▊        | 692/3844 [1:26:39<6:05:28,  6.96s/it]

 18%|█▊        | 693/3844 [1:26:45<5:43:41,  6.54s/it]

 18%|█▊        | 694/3844 [1:26:51<5:28:03,  6.25s/it]

 18%|█▊        | 695/3844 [1:26:59<5:55:40,  6.78s/it]

 18%|█▊        | 696/3844 [1:27:06<5:58:27,  6.83s/it]

 18%|█▊        | 697/3844 [1:27:19<7:40:19,  8.78s/it]

 18%|█▊        | 698/3844 [1:27:25<7:00:29,  8.02s/it]

 18%|█▊        | 699/3844 [1:27:31<6:28:22,  7.41s/it]

 18%|█▊        | 700/3844 [1:27:38<6:17:36,  7.21s/it]

 18%|█▊        | 701/3844 [1:27:45<6:24:33,  7.34s/it]

 18%|█▊        | 702/3844 [1:27:53<6:19:29,  7.25s/it]

 18%|█▊        | 703/3844 [1:28:02<6:59:50,  8.02s/it]

 18%|█▊        | 704/3844 [1:28:13<7:45:00,  8.89s/it]

 18%|█▊        | 705/3844 [1:28:23<7:57:35,  9.13s/it]

 18%|█▊        | 706/3844 [1:28:29<7:12:26,  8.27s/it]

 18%|█▊        | 707/3844 [1:28:39<7:35:27,  8.71s/it]

 18%|█▊        | 708/3844 [1:28:45<7:00:20,  8.04s/it]

 18%|█▊        | 709/3844 [1:28:53<6:58:58,  8.02s/it]

 18%|█▊        | 710/3844 [1:29:03<7:18:21,  8.39s/it]

 18%|█▊        | 711/3844 [1:29:09<6:43:51,  7.73s/it]

 19%|█▊        | 712/3844 [1:29:15<6:12:37,  7.14s/it]

 19%|█▊        | 713/3844 [1:29:23<6:25:53,  7.39s/it]

 19%|█▊        | 714/3844 [1:29:30<6:22:41,  7.34s/it]

 19%|█▊        | 715/3844 [1:29:36<6:06:41,  7.03s/it]

 19%|█▊        | 716/3844 [1:29:42<5:43:27,  6.59s/it]

 19%|█▊        | 717/3844 [1:29:48<5:37:11,  6.47s/it]

 19%|█▊        | 718/3844 [1:29:53<5:23:44,  6.21s/it]

 19%|█▊        | 719/3844 [1:30:00<5:22:31,  6.19s/it]

 19%|█▊        | 720/3844 [1:30:09<6:05:22,  7.02s/it]

 19%|█▉        | 721/3844 [1:30:17<6:25:19,  7.40s/it]

 19%|█▉        | 722/3844 [1:30:24<6:21:02,  7.32s/it]

 19%|█▉        | 723/3844 [1:30:30<6:03:39,  6.99s/it]

 19%|█▉        | 724/3844 [1:30:37<6:00:47,  6.94s/it]

 19%|█▉        | 725/3844 [1:30:45<6:10:09,  7.12s/it]

 19%|█▉        | 726/3844 [1:30:53<6:29:26,  7.49s/it]

 19%|█▉        | 727/3844 [1:30:59<6:10:10,  7.13s/it]

 19%|█▉        | 728/3844 [1:31:09<6:45:30,  7.81s/it]

 19%|█▉        | 729/3844 [1:31:16<6:31:22,  7.54s/it]

 19%|█▉        | 730/3844 [1:31:21<6:00:25,  6.94s/it]

 19%|█▉        | 731/3844 [1:31:29<6:10:29,  7.14s/it]

 19%|█▉        | 732/3844 [1:31:36<6:13:39,  7.20s/it]

 19%|█▉        | 733/3844 [1:31:43<6:07:22,  7.09s/it]

 19%|█▉        | 734/3844 [1:31:51<6:23:31,  7.40s/it]

 19%|█▉        | 735/3844 [1:31:58<6:12:40,  7.19s/it]

 19%|█▉        | 736/3844 [1:32:03<5:50:25,  6.76s/it]

 19%|█▉        | 737/3844 [1:32:10<5:41:23,  6.59s/it]

 19%|█▉        | 738/3844 [1:32:15<5:27:23,  6.32s/it]

 19%|█▉        | 739/3844 [1:32:21<5:16:32,  6.12s/it]

 19%|█▉        | 740/3844 [1:32:27<5:21:44,  6.22s/it]

 19%|█▉        | 741/3844 [1:32:33<5:15:36,  6.10s/it]

 19%|█▉        | 742/3844 [1:32:43<6:16:43,  7.29s/it]

 19%|█▉        | 743/3844 [1:32:49<5:46:46,  6.71s/it]

 19%|█▉        | 744/3844 [1:32:57<6:06:39,  7.10s/it]

 19%|█▉        | 745/3844 [1:33:07<6:49:03,  7.92s/it]

 19%|█▉        | 746/3844 [1:33:14<6:46:11,  7.87s/it]

 19%|█▉        | 747/3844 [1:33:21<6:26:35,  7.49s/it]

 19%|█▉        | 748/3844 [1:33:29<6:33:02,  7.62s/it]

 19%|█▉        | 749/3844 [1:33:35<6:15:49,  7.29s/it]

 20%|█▉        | 750/3844 [1:33:41<5:48:03,  6.75s/it]

 20%|█▉        | 751/3844 [1:33:47<5:44:23,  6.68s/it]

 20%|█▉        | 752/3844 [1:33:56<6:11:03,  7.20s/it]

 20%|█▉        | 753/3844 [1:34:04<6:29:48,  7.57s/it]

 20%|█▉        | 754/3844 [1:34:15<7:23:23,  8.61s/it]

 20%|█▉        | 755/3844 [1:34:23<7:13:49,  8.43s/it]

 20%|█▉        | 756/3844 [1:34:30<6:56:20,  8.09s/it]

 20%|█▉        | 757/3844 [1:34:36<6:20:55,  7.40s/it]

 20%|█▉        | 758/3844 [1:34:44<6:25:42,  7.50s/it]

 20%|█▉        | 759/3844 [1:34:53<6:42:48,  7.83s/it]

 20%|█▉        | 760/3844 [1:34:59<6:17:38,  7.35s/it]

 20%|█▉        | 761/3844 [1:35:11<7:29:52,  8.76s/it]

 20%|█▉        | 762/3844 [1:35:20<7:28:54,  8.74s/it]

 20%|█▉        | 763/3844 [1:35:29<7:43:17,  9.02s/it]

 20%|█▉        | 764/3844 [1:35:36<7:09:22,  8.36s/it]

 20%|█▉        | 765/3844 [1:35:46<7:31:39,  8.80s/it]

 20%|█▉        | 766/3844 [1:35:53<6:58:19,  8.15s/it]

 20%|█▉        | 767/3844 [1:36:01<7:04:25,  8.28s/it]

 20%|█▉        | 768/3844 [1:36:07<6:24:04,  7.49s/it]

 20%|██        | 769/3844 [1:36:14<6:17:00,  7.36s/it]

 20%|██        | 770/3844 [1:36:21<6:21:51,  7.45s/it]

 20%|██        | 771/3844 [1:36:30<6:40:52,  7.83s/it]

 20%|██        | 772/3844 [1:36:37<6:32:26,  7.66s/it]

 20%|██        | 773/3844 [1:36:46<6:41:35,  7.85s/it]

 20%|██        | 774/3844 [1:36:52<6:17:04,  7.37s/it]

 20%|██        | 775/3844 [1:36:59<6:15:35,  7.34s/it]

 20%|██        | 776/3844 [1:37:06<6:11:07,  7.26s/it]

 20%|██        | 777/3844 [1:37:15<6:30:47,  7.64s/it]

 20%|██        | 778/3844 [1:37:20<5:58:58,  7.02s/it]

 20%|██        | 779/3844 [1:37:27<5:45:43,  6.77s/it]

 20%|██        | 780/3844 [1:37:32<5:31:32,  6.49s/it]

 20%|██        | 781/3844 [1:37:39<5:27:15,  6.41s/it]

 20%|██        | 782/3844 [1:37:46<5:32:52,  6.52s/it]

 20%|██        | 783/3844 [1:37:53<5:43:38,  6.74s/it]

 20%|██        | 784/3844 [1:38:01<6:04:49,  7.15s/it]

 20%|██        | 785/3844 [1:38:07<5:48:03,  6.83s/it]

 20%|██        | 786/3844 [1:38:16<6:23:36,  7.53s/it]

 20%|██        | 787/3844 [1:38:22<5:59:42,  7.06s/it]

 20%|██        | 788/3844 [1:38:33<6:54:05,  8.13s/it]

 21%|██        | 789/3844 [1:38:41<6:53:53,  8.13s/it]

 21%|██        | 790/3844 [1:38:47<6:24:40,  7.56s/it]

 21%|██        | 791/3844 [1:38:54<6:16:19,  7.40s/it]

 21%|██        | 792/3844 [1:39:00<5:54:37,  6.97s/it]

 21%|██        | 793/3844 [1:39:07<5:53:54,  6.96s/it]

 21%|██        | 794/3844 [1:39:16<6:18:05,  7.44s/it]

 21%|██        | 795/3844 [1:39:28<7:33:23,  8.92s/it]

 21%|██        | 796/3844 [1:39:38<7:47:24,  9.20s/it]

 21%|██        | 797/3844 [1:39:44<7:08:09,  8.43s/it]

 21%|██        | 798/3844 [1:39:51<6:47:43,  8.03s/it]

 21%|██        | 799/3844 [1:39:58<6:24:33,  7.58s/it]

 21%|██        | 800/3844 [1:40:06<6:32:39,  7.74s/it]

 21%|██        | 801/3844 [1:40:13<6:17:19,  7.44s/it]

 21%|██        | 802/3844 [1:40:21<6:32:17,  7.74s/it]

 21%|██        | 803/3844 [1:40:27<6:07:13,  7.25s/it]

 21%|██        | 804/3844 [1:40:42<7:54:53,  9.37s/it]

 21%|██        | 805/3844 [1:40:48<7:10:17,  8.50s/it]

 21%|██        | 806/3844 [1:40:54<6:34:01,  7.78s/it]

 21%|██        | 807/3844 [1:41:06<7:32:04,  8.93s/it]

 21%|██        | 808/3844 [1:41:14<7:19:23,  8.68s/it]

 21%|██        | 809/3844 [1:41:24<7:31:26,  8.92s/it]

 21%|██        | 810/3844 [1:41:30<6:48:44,  8.08s/it]

 21%|██        | 811/3844 [1:41:37<6:35:24,  7.82s/it]

 21%|██        | 812/3844 [1:41:47<7:12:55,  8.57s/it]

 21%|██        | 813/3844 [1:41:54<6:52:23,  8.16s/it]

 21%|██        | 814/3844 [1:42:01<6:27:42,  7.68s/it]

 21%|██        | 815/3844 [1:42:13<7:31:31,  8.94s/it]

 21%|██        | 816/3844 [1:42:19<6:54:18,  8.21s/it]

 21%|██▏       | 817/3844 [1:42:25<6:23:05,  7.59s/it]

 21%|██▏       | 818/3844 [1:42:31<5:56:10,  7.06s/it]

 21%|██▏       | 819/3844 [1:42:37<5:36:52,  6.68s/it]

 21%|██▏       | 820/3844 [1:42:43<5:25:21,  6.46s/it]

 21%|██▏       | 821/3844 [1:42:49<5:21:05,  6.37s/it]

 21%|██▏       | 822/3844 [1:42:55<5:06:25,  6.08s/it]

 21%|██▏       | 823/3844 [1:43:05<6:09:20,  7.34s/it]

 21%|██▏       | 824/3844 [1:43:12<5:59:36,  7.14s/it]

 21%|██▏       | 825/3844 [1:43:18<5:43:32,  6.83s/it]

 21%|██▏       | 826/3844 [1:43:24<5:30:40,  6.57s/it]

 22%|██▏       | 827/3844 [1:43:30<5:27:58,  6.52s/it]

 22%|██▏       | 828/3844 [1:43:39<6:03:04,  7.22s/it]

 22%|██▏       | 829/3844 [1:43:45<5:42:13,  6.81s/it]

 22%|██▏       | 830/3844 [1:43:50<5:25:43,  6.48s/it]

 22%|██▏       | 831/3844 [1:43:57<5:33:28,  6.64s/it]

 22%|██▏       | 832/3844 [1:44:05<5:42:26,  6.82s/it]

 22%|██▏       | 833/3844 [1:44:10<5:25:14,  6.48s/it]

 22%|██▏       | 834/3844 [1:44:17<5:25:46,  6.49s/it]

 22%|██▏       | 835/3844 [1:44:23<5:12:48,  6.24s/it]

 22%|██▏       | 836/3844 [1:44:28<5:02:04,  6.03s/it]

 22%|██▏       | 837/3844 [1:44:34<4:57:42,  5.94s/it]

 22%|██▏       | 838/3844 [1:44:43<5:42:53,  6.84s/it]

 22%|██▏       | 839/3844 [1:44:49<5:35:42,  6.70s/it]

 22%|██▏       | 840/3844 [1:45:00<6:42:53,  8.05s/it]

 22%|██▏       | 841/3844 [1:45:06<6:09:38,  7.39s/it]

 22%|██▏       | 842/3844 [1:45:12<5:44:35,  6.89s/it]

 22%|██▏       | 843/3844 [1:45:18<5:27:46,  6.55s/it]

 22%|██▏       | 844/3844 [1:45:26<5:47:01,  6.94s/it]

 22%|██▏       | 845/3844 [1:45:31<5:30:34,  6.61s/it]

 22%|██▏       | 846/3844 [1:45:38<5:34:32,  6.70s/it]

 22%|██▏       | 847/3844 [1:45:47<6:12:33,  7.46s/it]

 22%|██▏       | 848/3844 [1:45:55<6:08:27,  7.38s/it]

 22%|██▏       | 849/3844 [1:46:01<5:56:45,  7.15s/it]

 22%|██▏       | 850/3844 [1:46:08<5:56:07,  7.14s/it]

 22%|██▏       | 851/3844 [1:46:14<5:33:25,  6.68s/it]

 22%|██▏       | 852/3844 [1:46:27<7:08:53,  8.60s/it]

 22%|██▏       | 853/3844 [1:46:35<6:52:56,  8.28s/it]

 22%|██▏       | 854/3844 [1:46:42<6:44:02,  8.11s/it]

 22%|██▏       | 855/3844 [1:46:50<6:32:30,  7.88s/it]

 22%|██▏       | 856/3844 [1:47:00<7:04:45,  8.53s/it]

 22%|██▏       | 857/3844 [1:47:07<6:50:41,  8.25s/it]

 22%|██▏       | 858/3844 [1:47:19<7:39:13,  9.23s/it]

 22%|██▏       | 859/3844 [1:47:25<6:54:54,  8.34s/it]

 22%|██▏       | 860/3844 [1:47:35<7:14:53,  8.74s/it]

 22%|██▏       | 861/3844 [1:47:42<6:49:27,  8.24s/it]

 22%|██▏       | 862/3844 [1:47:48<6:19:29,  7.64s/it]

 22%|██▏       | 863/3844 [1:47:54<5:57:30,  7.20s/it]

 22%|██▏       | 864/3844 [1:48:02<6:04:33,  7.34s/it]

 23%|██▎       | 865/3844 [1:48:08<5:47:56,  7.01s/it]

 23%|██▎       | 866/3844 [1:48:14<5:35:32,  6.76s/it]

 23%|██▎       | 867/3844 [1:48:21<5:27:16,  6.60s/it]

 23%|██▎       | 868/3844 [1:48:26<5:15:59,  6.37s/it]

 23%|██▎       | 869/3844 [1:48:32<5:01:35,  6.08s/it]

 23%|██▎       | 870/3844 [1:48:38<5:02:40,  6.11s/it]

 23%|██▎       | 871/3844 [1:48:44<4:59:21,  6.04s/it]

 23%|██▎       | 872/3844 [1:48:53<5:45:22,  6.97s/it]

 23%|██▎       | 873/3844 [1:49:00<5:44:54,  6.97s/it]

 23%|██▎       | 874/3844 [1:49:07<5:46:57,  7.01s/it]

 23%|██▎       | 875/3844 [1:49:13<5:34:22,  6.76s/it]

 23%|██▎       | 876/3844 [1:49:20<5:38:59,  6.85s/it]

 23%|██▎       | 877/3844 [1:49:26<5:26:26,  6.60s/it]

 23%|██▎       | 878/3844 [1:49:35<5:52:46,  7.14s/it]

 23%|██▎       | 879/3844 [1:49:40<5:32:25,  6.73s/it]

 23%|██▎       | 880/3844 [1:49:47<5:31:39,  6.71s/it]

 23%|██▎       | 881/3844 [1:49:56<6:01:04,  7.31s/it]

 23%|██▎       | 882/3844 [1:50:04<6:07:38,  7.45s/it]

 23%|██▎       | 883/3844 [1:50:13<6:33:49,  7.98s/it]

 23%|██▎       | 884/3844 [1:50:19<6:09:38,  7.49s/it]

 23%|██▎       | 885/3844 [1:50:29<6:44:21,  8.20s/it]

 23%|██▎       | 886/3844 [1:50:35<6:09:48,  7.50s/it]

 23%|██▎       | 887/3844 [1:50:43<6:19:06,  7.69s/it]

 23%|██▎       | 888/3844 [1:50:54<7:06:47,  8.66s/it]

 23%|██▎       | 889/3844 [1:51:02<6:57:16,  8.47s/it]

 23%|██▎       | 890/3844 [1:51:08<6:13:32,  7.59s/it]

 23%|██▎       | 891/3844 [1:51:13<5:43:48,  6.99s/it]

 23%|██▎       | 892/3844 [1:51:20<5:35:14,  6.81s/it]

 23%|██▎       | 893/3844 [1:51:26<5:35:49,  6.83s/it]

 23%|██▎       | 894/3844 [1:51:33<5:26:35,  6.64s/it]

 23%|██▎       | 895/3844 [1:51:39<5:22:29,  6.56s/it]

 23%|██▎       | 896/3844 [1:51:45<5:18:47,  6.49s/it]

 23%|██▎       | 897/3844 [1:51:55<6:10:39,  7.55s/it]

 23%|██▎       | 898/3844 [1:52:02<5:56:14,  7.26s/it]

 23%|██▎       | 899/3844 [1:52:14<7:00:36,  8.57s/it]
{'loss': 1.0745, 'grad_norm': 0.3178649819406996, 'learning_rate': 1.790093028583638e-05, 'epoch': 0.23}

 23%|██▎       | 900/3844 [1:52:20<6:35:57,  8.07s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1256, 'grad_norm': 0.30510885289884015, 'learning_rate': 1.789058776377008e-05, 'epoch': 0.23}

 23%|██▎       | 902/3844 [1:53:17<13:41:19, 16.75s/it]

 23%|██▎       | 903/3844 [1:53:25<11:36:52, 14.22s/it]

 24%|██▎       | 904/3844 [1:53:35<10:26:56, 12.79s/it]
{'loss': 1.0501, 'grad_norm': 0.3009986148651481, 'learning_rate': 1.7875031963830492e-05, 'epoch': 0.24}


 24%|██▎       | 906/3844 [1:53:47<7:41:01,  9.41s/it]

 24%|██▎       | 907/3844 [1:53:53<6:48:18,  8.34s/it]
{'loss': 1.1332, 'grad_norm': 0.2754252351710659, 'learning_rate': 1.785942583208173e-05, 'epoch': 0.24}


 24%|██▎       | 909/3844 [1:54:13<7:21:48,  9.03s/it]

 24%|██▎       | 910/3844 [1:54:20<6:41:12,  8.20s/it]
{'loss': 1.2765, 'grad_norm': 0.3267885258126736, 'learning_rate': 1.7843769468267496e-05, 'epoch': 0.24}


 24%|██▎       | 912/3844 [1:54:32<5:51:05,  7.18s/it]

 24%|██▍       | 913/3844 [1:54:38<5:28:12,  6.72s/it]

 24%|██▍       | 914/3844 [1:54:44<5:14:31,  6.44s/it]

 24%|██▍       | 915/3844 [1:54:50<5:10:20,  6.36s/it]

 24%|██▍       | 916/3844 [1:54:57<5:24:14,  6.64s/it]

 24%|██▍       | 917/3844 [1:55:06<5:54:53,  7.27s/it]

 24%|██▍       | 918/3844 [1:55:13<5:55:39,  7.29s/it]
{'loss': 1.2536, 'grad_norm': 0.3142348641312606, 'learning_rate': 1.7801774347777395e-05, 'epoch': 0.24}


 24%|██▍       | 920/3844 [1:55:31<6:38:21,  8.17s/it]

 24%|██▍       | 921/3844 [1:55:38<6:10:26,  7.60s/it]

 24%|██▍       | 922/3844 [1:55:43<5:43:02,  7.04s/it]

 24%|██▍       | 923/3844 [1:55:53<6:23:46,  7.88s/it]

 24%|██▍       | 924/3844 [1:56:01<6:30:12,  8.02s/it]

 24%|██▍       | 925/3844 [1:56:07<5:59:41,  7.39s/it]

 24%|██▍       | 926/3844 [1:56:14<5:47:40,  7.15s/it]
{'loss': 1.0592, 'grad_norm': 0.2883325246458024, 'learning_rate': 1.7759424642865017e-05, 'epoch': 0.24}


 24%|██▍       | 928/3844 [1:56:30<6:07:42,  7.57s/it]

 24%|██▍       | 929/3844 [1:56:36<5:41:52,  7.04s/it]

 24%|██▍       | 930/3844 [1:56:44<5:59:51,  7.41s/it]

 24%|██▍       | 931/3844 [1:56:51<5:54:09,  7.29s/it]

 24%|██▍       | 932/3844 [1:56:58<5:45:03,  7.11s/it]

 24%|██▍       | 933/3844 [1:57:05<5:50:11,  7.22s/it]

 24%|██▍       | 934/3844 [1:57:14<6:07:48,  7.58s/it]

 24%|██▍       | 935/3844 [1:57:19<5:36:27,  6.94s/it]

 24%|██▍       | 936/3844 [1:57:29<6:23:54,  7.92s/it]
{'loss': 1.11, 'grad_norm': 0.32649635565468144, 'learning_rate': 1.7705991811160124e-05, 'epoch': 0.24}


 24%|██▍       | 938/3844 [1:57:42<5:44:59,  7.12s/it]
{'loss': 1.2155, 'grad_norm': 0.3158307172035803, 'learning_rate': 1.7695239454502954e-05, 'epoch': 0.24}


 24%|██▍       | 940/3844 [1:57:55<5:23:30,  6.68s/it]
{'loss': 1.0301, 'grad_norm': 0.30167912644340406, 'learning_rate': 1.768446523886215e-05, 'epoch': 0.24}

 24%|██▍       | 941/3844 [1:58:01<5:11:16,  6.43s/it]


 25%|██▍       | 943/3844 [1:58:15<5:31:58,  6.87s/it]

 25%|██▍       | 944/3844 [1:58:21<5:12:20,  6.46s/it]

 25%|██▍       | 945/3844 [1:58:26<5:00:35,  6.22s/it]

 25%|██▍       | 946/3844 [1:58:35<5:45:14,  7.15s/it]

 25%|██▍       | 947/3844 [1:58:44<6:10:07,  7.67s/it]
{'loss': 1.2436, 'grad_norm': 0.28857926598814665, 'learning_rate': 1.7646583787043267e-05, 'epoch': 0.25}


 25%|██▍       | 949/3844 [1:59:01<6:41:52,  8.33s/it]

 25%|██▍       | 950/3844 [1:59:08<6:09:56,  7.67s/it]

 25%|██▍       | 951/3844 [1:59:16<6:13:56,  7.76s/it]

 25%|██▍       | 952/3844 [1:59:24<6:22:45,  7.94s/it]

 25%|██▍       | 953/3844 [1:59:31<6:03:44,  7.55s/it]

 25%|██▍       | 954/3844 [1:59:38<5:58:35,  7.44s/it]

 25%|██▍       | 955/3844 [1:59:46<6:07:06,  7.62s/it]

 25%|██▍       | 956/3844 [1:59:52<5:39:15,  7.05s/it]

 25%|██▍       | 957/3844 [1:59:57<5:21:19,  6.68s/it]

 25%|██▍       | 958/3844 [2:00:04<5:20:57,  6.67s/it]

 25%|██▍       | 959/3844 [2:00:10<5:04:42,  6.34s/it]

 25%|██▍       | 960/3844 [2:00:18<5:33:46,  6.94s/it]

 25%|██▌       | 961/3844 [2:00:25<5:38:53,  7.05s/it]

 25%|██▌       | 962/3844 [2:00:32<5:28:13,  6.83s/it]

 25%|██▌       | 963/3844 [2:00:44<6:56:00,  8.66s/it]

 25%|██▌       | 964/3844 [2:00:52<6:39:12,  8.32s/it]
{'loss': 1.1314, 'grad_norm': 0.3120066448336493, 'learning_rate': 1.7553480852589635e-05, 'epoch': 0.25}


 25%|██▌       | 966/3844 [2:01:05<5:44:58,  7.19s/it]

 25%|██▌       | 967/3844 [2:01:10<5:20:12,  6.68s/it]

 25%|██▌       | 968/3844 [2:01:18<5:33:04,  6.95s/it]

 25%|██▌       | 969/3844 [2:01:23<5:17:52,  6.63s/it]

 25%|██▌       | 970/3844 [2:01:36<6:45:39,  8.47s/it]

 25%|██▌       | 971/3844 [2:01:42<6:06:59,  7.66s/it]

 25%|██▌       | 972/3844 [2:01:50<6:11:54,  7.77s/it]

 25%|██▌       | 973/3844 [2:01:56<5:42:25,  7.16s/it]
{'loss': 1.191, 'grad_norm': 0.3245822879419658, 'learning_rate': 1.7503562251150374e-05, 'epoch': 0.25}


 25%|██▌       | 975/3844 [2:02:09<5:34:06,  6.99s/it]

 25%|██▌       | 976/3844 [2:02:16<5:23:15,  6.76s/it]

 25%|██▌       | 977/3844 [2:02:24<5:41:43,  7.15s/it]

 25%|██▌       | 978/3844 [2:02:30<5:32:17,  6.96s/it]
{'loss': 1.0747, 'grad_norm': 0.2994743692460506, 'learning_rate': 1.747564300886952e-05, 'epoch': 0.25}


 25%|██▌       | 980/3844 [2:02:46<5:48:19,  7.30s/it]

 26%|██▌       | 981/3844 [2:02:51<5:23:14,  6.77s/it]

 26%|██▌       | 982/3844 [2:03:00<5:46:24,  7.26s/it]

 26%|██▌       | 983/3844 [2:03:06<5:27:54,  6.88s/it]

 26%|██▌       | 984/3844 [2:03:14<5:51:53,  7.38s/it]

 26%|██▌       | 985/3844 [2:03:20<5:31:23,  6.95s/it]
{'loss': 1.1698, 'grad_norm': 0.33592176336552027, 'learning_rate': 1.7436333211819153e-05, 'epoch': 0.26}


 26%|██▌       | 987/3844 [2:03:36<5:50:27,  7.36s/it]

 26%|██▌       | 988/3844 [2:03:45<6:14:01,  7.86s/it]
{'loss': 1.1122, 'grad_norm': 0.2998247247830638, 'learning_rate': 1.7419406862804183e-05, 'epoch': 0.26}


 26%|██▌       | 990/3844 [2:03:59<5:58:26,  7.54s/it]
{'loss': 1.197, 'grad_norm': 0.2868090328358661, 'learning_rate': 1.7408096279122915e-05, 'epoch': 0.26}


 26%|██▌       | 992/3844 [2:04:12<5:31:27,  6.97s/it]

 26%|██▌       | 993/3844 [2:04:20<5:47:15,  7.31s/it]

 26%|██▌       | 994/3844 [2:04:26<5:34:57,  7.05s/it]

 26%|██▌       | 995/3844 [2:04:32<5:11:41,  6.56s/it]

 26%|██▌       | 996/3844 [2:04:38<5:02:17,  6.37s/it]

 26%|██▌       | 997/3844 [2:04:45<5:11:33,  6.57s/it]

 26%|██▌       | 998/3844 [2:04:50<4:59:12,  6.31s/it]

 26%|██▌       | 999/3844 [2:05:00<5:47:34,  7.33s/it]

 26%|██▌       | 1000/3844 [2:05:06<5:22:46,  6.81s/it]

 26%|██▌       | 1001/3844 [2:05:13<5:25:11,  6.86s/it]

 26%|██▌       | 1002/3844 [2:05:19<5:14:19,  6.64s/it]

 26%|██▌       | 1003/3844 [2:05:28<5:45:29,  7.30s/it]

 26%|██▌       | 1004/3844 [2:05:34<5:39:22,  7.17s/it]
{'loss': 1.1126, 'grad_norm': 0.3239465471085108, 'learning_rate': 1.7328334786868967e-05, 'epoch': 0.26}


 26%|██▌       | 1006/3844 [2:05:48<5:26:50,  6.91s/it]

 26%|██▌       | 1007/3844 [2:05:54<5:15:10,  6.67s/it]

 26%|██▌       | 1008/3844 [2:06:00<5:01:26,  6.38s/it]

 26%|██▌       | 1009/3844 [2:06:06<5:03:16,  6.42s/it]
{'loss': 1.2093, 'grad_norm': 0.3208336564118644, 'learning_rate': 1.7299600762531497e-05, 'epoch': 0.26}


 26%|██▋       | 1011/3844 [2:06:24<6:01:48,  7.66s/it]

 26%|██▋       | 1012/3844 [2:06:30<5:43:56,  7.29s/it]

 26%|██▋       | 1013/3844 [2:06:40<6:13:43,  7.92s/it]

 26%|██▋       | 1014/3844 [2:06:46<5:55:42,  7.54s/it]

 26%|██▋       | 1015/3844 [2:06:52<5:35:11,  7.11s/it]

 26%|██▋       | 1016/3844 [2:06:59<5:22:24,  6.84s/it]
{'loss': 1.1155, 'grad_norm': 0.3003140174820267, 'learning_rate': 1.7259155524486598e-05, 'epoch': 0.26}

 26%|██▋       | 1017/3844 [2:07:05<5:22:23,  6.84s/it]


 27%|██▋       | 1019/3844 [2:07:21<5:41:18,  7.25s/it]

 27%|██▋       | 1020/3844 [2:07:27<5:26:15,  6.93s/it]
{'loss': 1.1586, 'grad_norm': 0.3030784761192811, 'learning_rate': 1.723593045818418e-05, 'epoch': 0.27}


 27%|██▋       | 1022/3844 [2:07:45<6:11:47,  7.90s/it]

 27%|██▋       | 1023/3844 [2:07:56<7:07:39,  9.10s/it]
[2024-05-27 12:53:38,758] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 1024/3844 [2:08:02<6:23:52,  8.17s/it]

 27%|██▋       | 1025/3844 [2:08:08<5:48:57,  7.43s/it]

 27%|██▋       | 1026/3844 [2:08:15<5:38:57,  7.22s/it]

 27%|██▋       | 1027/3844 [2:08:20<5:15:12,  6.71s/it]

 27%|██▋       | 1028/3844 [2:08:29<5:36:43,  7.17s/it]

 27%|██▋       | 1029/3844 [2:08:36<5:35:47,  7.16s/it]

 27%|██▋       | 1030/3844 [2:08:45<5:58:58,  7.65s/it]

 27%|██▋       | 1031/3844 [2:08:50<5:30:19,  7.05s/it]

 27%|██▋       | 1032/3844 [2:08:56<5:11:16,  6.64s/it]

 27%|██▋       | 1033/3844 [2:09:02<5:06:35,  6.54s/it]

 27%|██▋       | 1034/3844 [2:09:09<5:09:36,  6.61s/it]

 27%|██▋       | 1035/3844 [2:09:15<4:57:25,  6.35s/it]

 27%|██▋       | 1036/3844 [2:09:21<4:51:57,  6.24s/it]

 27%|██▋       | 1037/3844 [2:09:27<4:50:56,  6.22s/it]
{'loss': 1.1752, 'grad_norm': 0.31152489669041084, 'learning_rate': 1.713630989941171e-05, 'epoch': 0.27}


 27%|██▋       | 1039/3844 [2:09:41<5:15:56,  6.76s/it]

 27%|██▋       | 1040/3844 [2:09:48<5:13:03,  6.70s/it]

 27%|██▋       | 1041/3844 [2:09:56<5:35:32,  7.18s/it]

 27%|██▋       | 1042/3844 [2:10:03<5:24:16,  6.94s/it]

 27%|██▋       | 1043/3844 [2:10:12<6:03:04,  7.78s/it]

 27%|██▋       | 1044/3844 [2:10:18<5:37:15,  7.23s/it]

 27%|██▋       | 1045/3844 [2:10:27<6:02:03,  7.76s/it]

 27%|██▋       | 1046/3844 [2:10:33<5:32:21,  7.13s/it]

 27%|██▋       | 1047/3844 [2:10:38<5:11:14,  6.68s/it]

 27%|██▋       | 1048/3844 [2:10:46<5:21:32,  6.90s/it]

 27%|██▋       | 1049/3844 [2:10:52<5:15:23,  6.77s/it]

 27%|██▋       | 1050/3844 [2:11:01<5:42:25,  7.35s/it]

 27%|██▋       | 1051/3844 [2:11:07<5:26:35,  7.02s/it]
{'loss': 1.3495, 'grad_norm': 0.3238314106306364, 'learning_rate': 1.7053168827525443e-05, 'epoch': 0.27}


 27%|██▋       | 1053/3844 [2:11:23<5:53:26,  7.60s/it]
{'loss': 1.0192, 'grad_norm': 0.3037869763288055, 'learning_rate': 1.7041211120323584e-05, 'epoch': 0.27}


 27%|██▋       | 1055/3844 [2:11:37<5:39:58,  7.31s/it]

 27%|██▋       | 1056/3844 [2:11:42<5:14:52,  6.78s/it]

 27%|██▋       | 1057/3844 [2:11:51<5:39:06,  7.30s/it]

 28%|██▊       | 1058/3844 [2:11:57<5:24:59,  7.00s/it]

 28%|██▊       | 1059/3844 [2:12:05<5:38:51,  7.30s/it]
{'loss': 1.0491, 'grad_norm': 0.299114783783855, 'learning_rate': 1.7005218127907112e-05, 'epoch': 0.28}

 28%|██▊       | 1060/3844 [2:12:14<5:55:23,  7.66s/it]


 28%|██▊       | 1062/3844 [2:12:28<5:41:29,  7.36s/it]
{'loss': 1.1272, 'grad_norm': 0.304446525730725, 'learning_rate': 1.6987154415283957e-05, 'epoch': 0.28}


 28%|██▊       | 1064/3844 [2:12:47<6:25:30,  8.32s/it]

 28%|██▊       | 1065/3844 [2:12:53<6:01:44,  7.81s/it]

 28%|██▊       | 1066/3844 [2:13:02<6:18:01,  8.16s/it]

 28%|██▊       | 1067/3844 [2:13:08<5:45:36,  7.47s/it]

 28%|██▊       | 1068/3844 [2:13:20<6:47:16,  8.80s/it]

 28%|██▊       | 1069/3844 [2:13:26<6:11:22,  8.03s/it]

 28%|██▊       | 1070/3844 [2:13:32<5:41:58,  7.40s/it]

 28%|██▊       | 1071/3844 [2:13:39<5:34:49,  7.24s/it]

 28%|██▊       | 1072/3844 [2:13:46<5:30:53,  7.16s/it]

 28%|██▊       | 1073/3844 [2:13:52<5:16:13,  6.85s/it]

 28%|██▊       | 1074/3844 [2:13:59<5:22:57,  7.00s/it]
{'loss': 1.0683, 'grad_norm': 0.2856964338282314, 'learning_rate': 1.6914454152535285e-05, 'epoch': 0.28}

 28%|██▊       | 1075/3844 [2:14:12<6:35:38,  8.57s/it]


 28%|██▊       | 1077/3844 [2:14:27<6:08:53,  8.00s/it]
{'loss': 1.095, 'grad_norm': 0.2928277092388963, 'learning_rate': 1.689616831459732e-05, 'epoch': 0.28}


 28%|██▊       | 1079/3844 [2:14:41<5:46:50,  7.53s/it]

 28%|██▊       | 1080/3844 [2:14:51<6:11:06,  8.06s/it]

 28%|██▊       | 1081/3844 [2:15:00<6:29:43,  8.46s/it]

 28%|██▊       | 1082/3844 [2:15:09<6:36:13,  8.61s/it]

 28%|██▊       | 1083/3844 [2:15:15<5:54:11,  7.70s/it]

 28%|██▊       | 1084/3844 [2:15:23<5:58:13,  7.79s/it]
{'loss': 1.1128, 'grad_norm': 0.3083908955782034, 'learning_rate': 1.685333015688824e-05, 'epoch': 0.28}


 28%|██▊       | 1086/3844 [2:15:38<5:52:18,  7.66s/it]

 28%|██▊       | 1087/3844 [2:15:43<5:22:35,  7.02s/it]
{'loss': 1.0593, 'grad_norm': 0.3232162725744754, 'learning_rate': 1.6834897856416702e-05, 'epoch': 0.28}

 28%|██▊       | 1088/3844 [2:15:50<5:22:12,  7.01s/it]

 28%|██▊       | 1089/3844 [2:15:56<5:06:02,  6.67s/it]


 28%|██▊       | 1091/3844 [2:16:10<5:22:09,  7.02s/it]

 28%|██▊       | 1092/3844 [2:16:18<5:37:32,  7.36s/it]
{'loss': 1.1827, 'grad_norm': 0.2919211163270451, 'learning_rate': 1.6804080338412108e-05, 'epoch': 0.28}


 28%|██▊       | 1094/3844 [2:16:37<6:15:17,  8.19s/it]

 28%|██▊       | 1095/3844 [2:16:42<5:41:04,  7.44s/it]

 29%|██▊       | 1096/3844 [2:16:48<5:20:16,  6.99s/it]

 29%|██▊       | 1097/3844 [2:16:55<5:10:05,  6.77s/it]
{'loss': 1.1398, 'grad_norm': 0.3168951016993741, 'learning_rate': 1.677314202324387e-05, 'epoch': 0.29}

 29%|██▊       | 1098/3844 [2:17:00<4:53:30,  6.41s/it]


 29%|██▊       | 1100/3844 [2:17:13<4:52:33,  6.40s/it]
{'loss': 1.0088, 'grad_norm': 0.30393299960054043, 'learning_rate': 1.675452127991077e-05, 'epoch': 0.29}


 29%|██▊       | 1102/3844 [2:17:26<5:00:33,  6.58s/it]

 29%|██▊       | 1103/3844 [2:17:34<5:11:30,  6.82s/it]
{'loss': 1.1476, 'grad_norm': 0.29498288339858747, 'learning_rate': 1.673585736630513e-05, 'epoch': 0.29}


 29%|██▊       | 1105/3844 [2:17:51<6:08:59,  8.08s/it]

 29%|██▉       | 1106/3844 [2:18:00<6:20:21,  8.34s/it]

 29%|██▉       | 1107/3844 [2:18:07<5:59:41,  7.88s/it]

 29%|██▉       | 1108/3844 [2:18:15<5:51:48,  7.71s/it]
{'loss': 1.0856, 'grad_norm': 0.29935932173549007, 'learning_rate': 1.6704655233803912e-05, 'epoch': 0.29}


 29%|██▉       | 1110/3844 [2:18:29<5:35:03,  7.35s/it]
{'loss': 1.1615, 'grad_norm': 0.3080978479999662, 'learning_rate': 1.6692141020749208e-05, 'epoch': 0.29}


 29%|██▉       | 1112/3844 [2:18:40<4:49:28,  6.36s/it]
{'loss': 1.2833, 'grad_norm': 0.32494133523591234, 'learning_rate': 1.6679607798097473e-05, 'epoch': 0.29}

 29%|██▉       | 1113/3844 [2:18:46<4:52:54,  6.44s/it]


 29%|██▉       | 1115/3844 [2:19:04<5:41:42,  7.51s/it]

 29%|██▉       | 1116/3844 [2:19:17<7:02:29,  9.29s/it]
{'loss': 1.0419, 'grad_norm': 0.29548086958734143, 'learning_rate': 1.665448446646357e-05, 'epoch': 0.29}


 29%|██▉       | 1118/3844 [2:19:34<6:42:22,  8.86s/it]
{'loss': 1.0848, 'grad_norm': 0.297467138657037, 'learning_rate': 1.664189442884636e-05, 'epoch': 0.29}


 29%|██▉       | 1120/3844 [2:19:47<5:49:47,  7.70s/it]

 29%|██▉       | 1121/3844 [2:19:54<5:35:59,  7.40s/it]

 29%|██▉       | 1122/3844 [2:20:01<5:41:06,  7.52s/it]

 29%|██▉       | 1123/3844 [2:20:08<5:24:00,  7.14s/it]

 29%|██▉       | 1124/3844 [2:20:14<5:07:31,  6.78s/it]
{'loss': 1.1803, 'grad_norm': 0.3074935545671495, 'learning_rate': 1.6604011258111097e-05, 'epoch': 0.29}

 29%|██▉       | 1125/3844 [2:20:22<5:34:41,  7.39s/it]


 29%|██▉       | 1127/3844 [2:20:36<5:19:40,  7.06s/it]

 29%|██▉       | 1128/3844 [2:20:41<4:55:07,  6.52s/it]

 29%|██▉       | 1129/3844 [2:20:49<5:15:06,  6.96s/it]

 29%|██▉       | 1130/3844 [2:20:55<5:03:49,  6.72s/it]

 29%|██▉       | 1131/3844 [2:21:01<4:51:42,  6.45s/it]

 29%|██▉       | 1132/3844 [2:21:07<4:43:53,  6.28s/it]

 29%|██▉       | 1133/3844 [2:21:13<4:37:58,  6.15s/it]

 30%|██▉       | 1134/3844 [2:21:21<5:15:11,  6.98s/it]

 30%|██▉       | 1135/3844 [2:21:33<6:19:17,  8.40s/it]

 30%|██▉       | 1136/3844 [2:21:44<6:45:47,  8.99s/it]

 30%|██▉       | 1137/3844 [2:21:51<6:24:42,  8.53s/it]
{'loss': 1.2503, 'grad_norm': 0.31978060168062167, 'learning_rate': 1.6521353161957103e-05, 'epoch': 0.3}


 30%|██▉       | 1139/3844 [2:22:11<7:04:04,  9.41s/it]

 30%|██▉       | 1140/3844 [2:22:17<6:23:43,  8.51s/it]

 30%|██▉       | 1141/3844 [2:22:23<5:43:32,  7.63s/it]

 30%|██▉       | 1142/3844 [2:22:34<6:27:37,  8.61s/it]

 30%|██▉       | 1143/3844 [2:22:41<6:12:52,  8.28s/it]
{'loss': 1.2001, 'grad_norm': 0.3149970756814897, 'learning_rate': 1.6482938696406906e-05, 'epoch': 0.3}

 30%|██▉       | 1144/3844 [2:22:48<5:59:40,  7.99s/it]


 30%|██▉       | 1146/3844 [2:23:01<5:20:25,  7.13s/it]

 30%|██▉       | 1147/3844 [2:23:09<5:33:33,  7.42s/it]

 30%|██▉       | 1148/3844 [2:23:16<5:21:14,  7.15s/it]

 30%|██▉       | 1149/3844 [2:23:21<4:58:41,  6.65s/it]

 30%|██▉       | 1150/3844 [2:23:27<4:52:20,  6.51s/it]
{'loss': 0.9714, 'grad_norm': 0.31934192169593717, 'learning_rate': 1.6437912415029078e-05, 'epoch': 0.3}

 30%|██▉       | 1151/3844 [2:23:33<4:36:41,  6.16s/it]

 30%|██▉       | 1152/3844 [2:23:38<4:31:09,  6.04s/it]


 30%|███       | 1154/3844 [2:23:57<5:44:27,  7.68s/it]

 30%|███       | 1155/3844 [2:24:04<5:30:10,  7.37s/it]
{'loss': 1.1238, 'grad_norm': 0.313143972967163, 'learning_rate': 1.640561353837962e-05, 'epoch': 0.3}


 30%|███       | 1157/3844 [2:24:20<5:47:23,  7.76s/it]
{'loss': 1.0465, 'grad_norm': 0.27311234490638314, 'learning_rate': 1.6392662113120046e-05, 'epoch': 0.3}


 30%|███       | 1159/3844 [2:24:36<6:02:59,  8.11s/it]

 30%|███       | 1160/3844 [2:24:44<6:01:34,  8.08s/it]

 30%|███       | 1161/3844 [2:24:50<5:33:42,  7.46s/it]

 30%|███       | 1162/3844 [2:24:56<5:07:45,  6.88s/it]

 30%|███       | 1163/3844 [2:25:03<5:14:35,  7.04s/it]

 30%|███       | 1164/3844 [2:25:10<5:10:24,  6.95s/it]

 30%|███       | 1165/3844 [2:25:16<5:02:24,  6.77s/it]
{'loss': 1.0985, 'grad_norm': 0.3141855409579209, 'learning_rate': 1.63406751917345e-05, 'epoch': 0.3}


 30%|███       | 1167/3844 [2:25:31<5:12:20,  7.00s/it]

 30%|███       | 1168/3844 [2:25:38<5:16:52,  7.10s/it]
{'loss': 1.2268, 'grad_norm': 0.30950844519710835, 'learning_rate': 1.6321105672836572e-05, 'epoch': 0.3}


 30%|███       | 1170/3844 [2:25:51<5:01:09,  6.76s/it]

 30%|███       | 1171/3844 [2:25:57<4:55:53,  6.64s/it]

 30%|███       | 1172/3844 [2:26:03<4:46:05,  6.42s/it]
{'loss': 1.0533, 'grad_norm': 0.2906759913346067, 'learning_rate': 1.6294950157908133e-05, 'epoch': 0.3}


 31%|███       | 1174/3844 [2:26:18<5:11:21,  7.00s/it]
{'loss': 1.1518, 'grad_norm': 0.29802333021106636, 'learning_rate': 1.6281845559843157e-05, 'epoch': 0.31}

 31%|███       | 1175/3844 [2:26:25<5:02:19,  6.80s/it]


 31%|███       | 1177/3844 [2:26:39<5:09:53,  6.97s/it]

 31%|███       | 1178/3844 [2:26:46<5:12:04,  7.02s/it]
{'loss': 1.1141, 'grad_norm': 0.30379305509405863, 'learning_rate': 1.6255582868635237e-05, 'epoch': 0.31}


 31%|███       | 1180/3844 [2:26:58<4:49:45,  6.53s/it]

 31%|███       | 1181/3844 [2:27:06<5:01:50,  6.80s/it]

 31%|███       | 1182/3844 [2:27:12<4:58:40,  6.73s/it]
{'loss': 1.1258, 'grad_norm': 0.33202819098654685, 'learning_rate': 1.6229249099412253e-05, 'epoch': 0.31}

 31%|███       | 1183/3844 [2:27:18<4:49:42,  6.53s/it]


 31%|███       | 1185/3844 [2:27:31<4:50:16,  6.55s/it]
{'loss': 1.1394, 'grad_norm': 0.3303746125381368, 'learning_rate': 1.620945230751272e-05, 'epoch': 0.31}


 31%|███       | 1187/3844 [2:27:44<4:44:51,  6.43s/it]

 31%|███       | 1188/3844 [2:27:56<5:59:14,  8.12s/it]

 31%|███       | 1189/3844 [2:28:04<5:56:00,  8.05s/it]

 31%|███       | 1190/3844 [2:28:10<5:30:30,  7.47s/it]

 31%|███       | 1191/3844 [2:28:19<5:46:10,  7.83s/it]
{'loss': 1.19, 'grad_norm': 0.296653783073052, 'learning_rate': 1.616973979079956e-05, 'epoch': 0.31}

 31%|███       | 1192/3844 [2:28:31<6:35:13,  8.94s/it]


 31%|███       | 1194/3844 [2:28:46<6:07:04,  8.31s/it]

 31%|███       | 1195/3844 [2:28:52<5:41:34,  7.74s/it]
{'loss': 1.1271, 'grad_norm': 0.320451863731906, 'learning_rate': 1.6143177088627834e-05, 'epoch': 0.31}


 31%|███       | 1197/3844 [2:29:07<5:31:59,  7.53s/it]

 31%|███       | 1198/3844 [2:29:14<5:20:11,  7.26s/it]

 31%|███       | 1199/3844 [2:29:20<5:00:37,  6.82s/it]

 31%|███       | 1200/3844 [2:29:26<4:51:34,  6.62s/it]
 31%|███       | 1200/3844 [2:29:26<4:51:34,  6.62s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 31%|███       | 1201/3844 [2:30:04<11:51:24, 16.15s/it]

 31%|███▏      | 1202/3844 [2:30:11<9:44:02, 13.26s/it]

 31%|███▏      | 1203/3844 [2:30:16<8:05:23, 11.03s/it]
{'loss': 1.047, 'grad_norm': 0.29766848867605156, 'learning_rate': 1.6089842584422245e-05, 'epoch': 0.31}


 31%|███▏      | 1205/3844 [2:30:31<6:51:34,  9.36s/it]

 31%|███▏      | 1206/3844 [2:30:37<6:05:25,  8.31s/it]

 31%|███▏      | 1207/3844 [2:30:48<6:38:32,  9.07s/it]

 31%|███▏      | 1208/3844 [2:30:55<6:13:22,  8.50s/it]

 31%|███▏      | 1209/3844 [2:31:03<6:08:32,  8.39s/it]
{'loss': 1.1231, 'grad_norm': 0.2969735569693743, 'learning_rate': 1.6049659937347647e-05, 'epoch': 0.31}


 32%|███▏      | 1211/3844 [2:31:17<5:23:39,  7.38s/it]

 32%|███▏      | 1212/3844 [2:31:24<5:23:37,  7.38s/it]

 32%|███▏      | 1213/3844 [2:31:30<5:09:18,  7.05s/it]

 32%|███▏      | 1214/3844 [2:31:38<5:22:29,  7.36s/it]

 32%|███▏      | 1215/3844 [2:31:45<5:06:41,  7.00s/it]
{'loss': 0.9432, 'grad_norm': 0.31355865957946055, 'learning_rate': 1.6009322629393242e-05, 'epoch': 0.32}

 32%|███▏      | 1216/3844 [2:31:51<5:01:53,  6.89s/it]


 32%|███▏      | 1218/3844 [2:32:05<4:53:24,  6.70s/it]

 32%|███▏      | 1219/3844 [2:32:11<4:46:20,  6.54s/it]

 32%|███▏      | 1220/3844 [2:32:17<4:38:14,  6.36s/it]

 32%|███▏      | 1221/3844 [2:32:25<5:00:52,  6.88s/it]
{'loss': 1.2149, 'grad_norm': 0.2870690601845199, 'learning_rate': 1.596883169179113e-05, 'epoch': 0.32}


 32%|███▏      | 1223/3844 [2:32:38<4:53:35,  6.72s/it]

 32%|███▏      | 1224/3844 [2:32:45<4:53:47,  6.73s/it]
{'loss': 1.19, 'grad_norm': 0.3293978541799122, 'learning_rate': 1.5948528935206886e-05, 'epoch': 0.32}

 32%|███▏      | 1225/3844 [2:32:55<5:43:40,  7.87s/it]


 32%|███▏      | 1227/3844 [2:33:09<5:31:11,  7.59s/it]
{'loss': 0.9369, 'grad_norm': 0.3192605221550567, 'learning_rate': 1.592818815970099e-05, 'epoch': 0.32}


 32%|███▏      | 1229/3844 [2:33:25<5:32:39,  7.63s/it]
{'loss': 1.2726, 'grad_norm': 0.2795587647718504, 'learning_rate': 1.59146065852636e-05, 'epoch': 0.32}

 32%|███▏      | 1230/3844 [2:33:31<5:08:35,  7.08s/it]

 32%|███▏      | 1231/3844 [2:33:37<4:56:07,  6.80s/it]


 32%|███▏      | 1233/3844 [2:33:52<5:17:29,  7.30s/it]

 32%|███▏      | 1234/3844 [2:34:00<5:26:28,  7.51s/it]

 32%|███▏      | 1235/3844 [2:34:08<5:24:09,  7.45s/it]

 32%|███▏      | 1236/3844 [2:34:15<5:28:33,  7.56s/it]

 32%|███▏      | 1237/3844 [2:34:23<5:35:00,  7.71s/it]
{'loss': 1.1596, 'grad_norm': 0.3211291867602239, 'learning_rate': 1.5860112664583606e-05, 'epoch': 0.32}


 32%|███▏      | 1239/3844 [2:34:36<5:00:44,  6.93s/it]
{'loss': 1.0885, 'grad_norm': 0.3012565354199192, 'learning_rate': 1.5846447472174332e-05, 'epoch': 0.32}

 32%|███▏      | 1240/3844 [2:34:45<5:33:19,  7.68s/it]


 32%|███▏      | 1242/3844 [2:35:02<6:05:11,  8.42s/it]

 32%|███▏      | 1243/3844 [2:35:09<5:37:41,  7.79s/it]

 32%|███▏      | 1244/3844 [2:35:16<5:34:05,  7.71s/it]

 32%|███▏      | 1245/3844 [2:35:23<5:20:13,  7.39s/it]

 32%|███▏      | 1246/3844 [2:35:31<5:26:49,  7.55s/it]
{'loss': 1.0312, 'grad_norm': 0.32504271541696406, 'learning_rate': 1.579848877116201e-05, 'epoch': 0.32}


 32%|███▏      | 1248/3844 [2:35:46<5:30:08,  7.63s/it]

 32%|███▏      | 1249/3844 [2:35:52<5:10:24,  7.18s/it]

 33%|███▎      | 1250/3844 [2:35:58<4:59:28,  6.93s/it]

 33%|███▎      | 1251/3844 [2:36:04<4:39:45,  6.47s/it]

 33%|███▎      | 1252/3844 [2:36:13<5:11:16,  7.21s/it]
{'loss': 1.1856, 'grad_norm': 0.3039596284721773, 'learning_rate': 1.5757220656897896e-05, 'epoch': 0.33}


 33%|███▎      | 1254/3844 [2:36:25<4:42:15,  6.54s/it]

 33%|███▎      | 1255/3844 [2:36:32<4:49:02,  6.70s/it]

 33%|███▎      | 1256/3844 [2:36:39<4:50:32,  6.74s/it]

 33%|███▎      | 1257/3844 [2:36:46<5:02:09,  7.01s/it]

 33%|███▎      | 1258/3844 [2:36:52<4:45:36,  6.63s/it]
{'loss': 1.0734, 'grad_norm': 0.30145708952904315, 'learning_rate': 1.571580535802816e-05, 'epoch': 0.33}


 33%|███▎      | 1260/3844 [2:37:06<4:55:37,  6.86s/it]

 33%|███▎      | 1261/3844 [2:37:14<5:01:51,  7.01s/it]

 33%|███▎      | 1262/3844 [2:37:21<5:04:56,  7.09s/it]

 33%|███▎      | 1263/3844 [2:37:27<4:55:22,  6.87s/it]

 33%|███▎      | 1264/3844 [2:37:36<5:19:04,  7.42s/it]

 33%|███▎      | 1265/3844 [2:37:43<5:15:34,  7.34s/it]
{'loss': 1.1544, 'grad_norm': 0.3201361839542063, 'learning_rate': 1.566730289710558e-05, 'epoch': 0.33}


 33%|███▎      | 1267/3844 [2:37:55<4:45:30,  6.65s/it]

 33%|███▎      | 1268/3844 [2:38:05<5:25:07,  7.57s/it]

 33%|███▎      | 1269/3844 [2:38:12<5:18:20,  7.42s/it]

 33%|███▎      | 1270/3844 [2:38:18<5:02:05,  7.04s/it]

 33%|███▎      | 1271/3844 [2:38:25<4:56:24,  6.91s/it]

 33%|███▎      | 1272/3844 [2:38:33<5:16:57,  7.39s/it]

 33%|███▎      | 1273/3844 [2:38:39<4:53:20,  6.85s/it]

 33%|███▎      | 1274/3844 [2:38:45<4:48:04,  6.73s/it]

 33%|███▎      | 1275/3844 [2:38:52<4:47:47,  6.72s/it]

 33%|███▎      | 1276/3844 [2:38:59<4:48:42,  6.75s/it]

 33%|███▎      | 1277/3844 [2:39:07<5:06:15,  7.16s/it]

 33%|███▎      | 1278/3844 [2:39:14<5:07:32,  7.19s/it]
{'loss': 1.1404, 'grad_norm': 0.32466493848636324, 'learning_rate': 1.557670498414736e-05, 'epoch': 0.33}


 33%|███▎      | 1280/3844 [2:39:28<5:03:26,  7.10s/it]

 33%|███▎      | 1281/3844 [2:39:34<4:47:16,  6.73s/it]

 33%|███▎      | 1282/3844 [2:39:40<4:37:06,  6.49s/it]

 33%|███▎      | 1283/3844 [2:39:46<4:34:38,  6.43s/it]
{'loss': 1.0996, 'grad_norm': 0.32070764601367957, 'learning_rate': 1.5541680825797966e-05, 'epoch': 0.33}


 33%|███▎      | 1285/3844 [2:40:00<4:43:14,  6.64s/it]
{'loss': 1.1636, 'grad_norm': 0.29876986973706116, 'learning_rate': 1.5527643579783134e-05, 'epoch': 0.33}

 33%|███▎      | 1286/3844 [2:40:07<4:51:27,  6.84s/it]

 33%|███▎      | 1287/3844 [2:40:13<4:40:36,  6.58s/it]


 34%|███▎      | 1289/3844 [2:40:26<4:39:17,  6.56s/it]

 34%|███▎      | 1290/3844 [2:40:33<4:34:30,  6.45s/it]

 34%|███▎      | 1291/3844 [2:40:41<5:02:31,  7.11s/it]

 34%|███▎      | 1292/3844 [2:40:47<4:41:07,  6.61s/it]
{'loss': 1.1949, 'grad_norm': 0.31964847419370745, 'learning_rate': 1.5478389829582057e-05, 'epoch': 0.34}


 34%|███▎      | 1294/3844 [2:41:00<4:46:25,  6.74s/it]
{'loss': 1.1364, 'grad_norm': 0.29705446902585503, 'learning_rate': 1.5464282240375327e-05, 'epoch': 0.34}


 34%|███▎      | 1296/3844 [2:41:11<4:16:56,  6.05s/it]

 34%|███▎      | 1297/3844 [2:41:20<4:51:51,  6.88s/it]

 34%|███▍      | 1298/3844 [2:41:25<4:33:35,  6.45s/it]

 34%|███▍      | 1299/3844 [2:41:31<4:27:26,  6.31s/it]
{'loss': 1.0411, 'grad_norm': 0.30217774559231914, 'learning_rate': 1.5428945447430647e-05, 'epoch': 0.34}


 34%|███▍      | 1301/3844 [2:41:45<4:45:15,  6.73s/it]
{'loss': 1.0758, 'grad_norm': 0.3125732284975375, 'learning_rate': 1.541478370767766e-05, 'epoch': 0.34}


 34%|███▍      | 1303/3844 [2:41:59<4:45:50,  6.75s/it]

 34%|███▍      | 1304/3844 [2:42:06<4:49:48,  6.85s/it]

 34%|███▍      | 1305/3844 [2:42:12<4:37:26,  6.56s/it]

 34%|███▍      | 1306/3844 [2:42:18<4:26:24,  6.30s/it]

 34%|███▍      | 1307/3844 [2:42:27<5:06:20,  7.24s/it]

 34%|███▍      | 1308/3844 [2:42:35<5:08:07,  7.29s/it]

 34%|███▍      | 1309/3844 [2:42:41<4:49:00,  6.84s/it]
{'loss': 1.0575, 'grad_norm': 0.29725883886417787, 'learning_rate': 1.5357983340049763e-05, 'epoch': 0.34}


 34%|███▍      | 1311/3844 [2:42:57<5:11:54,  7.39s/it]

 34%|███▍      | 1312/3844 [2:43:05<5:22:53,  7.65s/it]

 34%|███▍      | 1313/3844 [2:43:11<4:55:36,  7.01s/it]

 34%|███▍      | 1314/3844 [2:43:17<4:44:55,  6.76s/it]

 34%|███▍      | 1315/3844 [2:43:24<4:55:01,  7.00s/it]

 34%|███▍      | 1316/3844 [2:43:31<4:48:08,  6.84s/it]

 34%|███▍      | 1317/3844 [2:43:37<4:47:13,  6.82s/it]
{'loss': 1.0816, 'grad_norm': 0.2854325826447771, 'learning_rate': 1.530093945635199e-05, 'epoch': 0.34}


 34%|███▍      | 1319/3844 [2:43:52<4:57:14,  7.06s/it]
{'loss': 1.333, 'grad_norm': 0.3544400931308541, 'learning_rate': 1.5286640739620323e-05, 'epoch': 0.34}


 34%|███▍      | 1321/3844 [2:44:10<5:41:08,  8.11s/it]

 34%|███▍      | 1322/3844 [2:44:16<5:08:01,  7.33s/it]

 34%|███▍      | 1323/3844 [2:44:27<6:00:15,  8.57s/it]

 34%|███▍      | 1324/3844 [2:44:33<5:33:47,  7.95s/it]

 34%|███▍      | 1325/3844 [2:44:43<5:54:05,  8.43s/it]

 34%|███▍      | 1326/3844 [2:44:50<5:37:54,  8.05s/it]

 35%|███▍      | 1327/3844 [2:44:56<5:10:23,  7.40s/it]

 35%|███▍      | 1328/3844 [2:45:02<4:52:18,  6.97s/it]

 35%|███▍      | 1329/3844 [2:45:11<5:15:33,  7.53s/it]

 35%|███▍      | 1330/3844 [2:45:19<5:17:09,  7.57s/it]

 35%|███▍      | 1331/3844 [2:45:24<4:53:44,  7.01s/it]

 35%|███▍      | 1332/3844 [2:45:33<5:12:01,  7.45s/it]

 35%|███▍      | 1333/3844 [2:45:39<4:55:03,  7.05s/it]

 35%|███▍      | 1334/3844 [2:45:46<4:57:38,  7.12s/it]

 35%|███▍      | 1335/3844 [2:45:53<4:56:26,  7.09s/it]

 35%|███▍      | 1336/3844 [2:45:59<4:42:45,  6.76s/it]

 35%|███▍      | 1337/3844 [2:46:06<4:37:41,  6.65s/it]

 35%|███▍      | 1338/3844 [2:46:15<5:16:19,  7.57s/it]

 35%|███▍      | 1339/3844 [2:46:25<5:46:31,  8.30s/it]

 35%|███▍      | 1340/3844 [2:46:31<5:19:56,  7.67s/it]

 35%|███▍      | 1341/3844 [2:46:37<4:58:30,  7.16s/it]

 35%|███▍      | 1342/3844 [2:46:43<4:42:55,  6.78s/it]

 35%|███▍      | 1343/3844 [2:46:49<4:33:29,  6.56s/it]

 35%|███▍      | 1344/3844 [2:46:55<4:26:06,  6.39s/it]

 35%|███▍      | 1345/3844 [2:47:05<5:09:19,  7.43s/it]

 35%|███▌      | 1346/3844 [2:47:11<4:46:31,  6.88s/it]

 35%|███▌      | 1347/3844 [2:47:17<4:32:50,  6.56s/it]

 35%|███▌      | 1348/3844 [2:47:23<4:25:35,  6.38s/it]

 35%|███▌      | 1349/3844 [2:47:28<4:18:01,  6.20s/it]

 35%|███▌      | 1350/3844 [2:47:37<4:48:30,  6.94s/it]

 35%|███▌      | 1351/3844 [2:47:43<4:35:19,  6.63s/it]

 35%|███▌      | 1352/3844 [2:47:49<4:28:31,  6.47s/it]

 35%|███▌      | 1353/3844 [2:47:55<4:24:14,  6.36s/it]

 35%|███▌      | 1354/3844 [2:48:01<4:15:21,  6.15s/it]

 35%|███▌      | 1355/3844 [2:48:07<4:14:27,  6.13s/it]

 35%|███▌      | 1356/3844 [2:48:14<4:31:14,  6.54s/it]

 35%|███▌      | 1357/3844 [2:48:21<4:30:52,  6.53s/it]

 35%|███▌      | 1358/3844 [2:48:33<5:39:51,  8.20s/it]

 35%|███▌      | 1359/3844 [2:48:39<5:15:51,  7.63s/it]
{'loss': 1.1571, 'grad_norm': 0.34877955005347305, 'learning_rate': 1.4997567131947955e-05, 'epoch': 0.35}


 35%|███▌      | 1361/3844 [2:48:55<5:29:21,  7.96s/it]

 35%|███▌      | 1362/3844 [2:49:04<5:33:41,  8.07s/it]

 35%|███▌      | 1363/3844 [2:49:12<5:33:54,  8.08s/it]

 35%|███▌      | 1364/3844 [2:49:19<5:21:15,  7.77s/it]
{'loss': 1.0369, 'grad_norm': 0.32009814428115735, 'learning_rate': 1.4961026898679703e-05, 'epoch': 0.35}


 36%|███▌      | 1366/3844 [2:49:32<4:57:03,  7.19s/it]

 36%|███▌      | 1367/3844 [2:49:38<4:35:44,  6.68s/it]

 36%|███▌      | 1368/3844 [2:49:44<4:24:10,  6.40s/it]

 36%|███▌      | 1369/3844 [2:49:51<4:38:30,  6.75s/it]

 36%|███▌      | 1370/3844 [2:49:57<4:31:01,  6.57s/it]

 36%|███▌      | 1371/3844 [2:50:04<4:32:04,  6.60s/it]

 36%|███▌      | 1372/3844 [2:50:11<4:33:00,  6.63s/it]

 36%|███▌      | 1373/3844 [2:50:17<4:31:21,  6.59s/it]
{'loss': 1.2082, 'grad_norm': 0.31606685628931847, 'learning_rate': 1.489503296355057e-05, 'epoch': 0.36}


 36%|███▌      | 1375/3844 [2:50:31<4:41:24,  6.84s/it]

 36%|███▌      | 1376/3844 [2:50:38<4:35:44,  6.70s/it]

 36%|███▌      | 1377/3844 [2:50:46<4:54:58,  7.17s/it]

 36%|███▌      | 1378/3844 [2:50:53<4:54:47,  7.17s/it]

 36%|███▌      | 1379/3844 [2:51:01<4:59:50,  7.30s/it]

 36%|███▌      | 1380/3844 [2:51:07<4:42:46,  6.89s/it]

 36%|███▌      | 1381/3844 [2:51:14<4:48:03,  7.02s/it]

 36%|███▌      | 1382/3844 [2:51:21<4:52:11,  7.12s/it]

 36%|███▌      | 1383/3844 [2:51:30<5:09:54,  7.56s/it]

 36%|███▌      | 1384/3844 [2:51:38<5:12:49,  7.63s/it]

 36%|███▌      | 1385/3844 [2:51:47<5:27:08,  7.98s/it]

 36%|███▌      | 1386/3844 [2:51:53<5:07:31,  7.51s/it]

 36%|███▌      | 1387/3844 [2:52:00<4:57:34,  7.27s/it]

 36%|███▌      | 1388/3844 [2:52:08<5:07:05,  7.50s/it]

 36%|███▌      | 1389/3844 [2:52:17<5:26:33,  7.98s/it]

 36%|███▌      | 1390/3844 [2:52:23<5:09:07,  7.56s/it]
{'loss': 1.2054, 'grad_norm': 0.3138477770385265, 'learning_rate': 1.4769612599882146e-05, 'epoch': 0.36}

 36%|███▌      | 1391/3844 [2:52:30<5:01:46,  7.38s/it]


 36%|███▌      | 1393/3844 [2:52:49<5:28:19,  8.04s/it]

 36%|███▋      | 1394/3844 [2:52:57<5:26:51,  8.00s/it]

 36%|███▋      | 1395/3844 [2:53:03<5:04:30,  7.46s/it]

 36%|███▋      | 1396/3844 [2:53:10<4:55:21,  7.24s/it]

 36%|███▋      | 1397/3844 [2:53:18<5:10:12,  7.61s/it]

 36%|███▋      | 1398/3844 [2:53:25<4:58:07,  7.31s/it]

 36%|███▋      | 1399/3844 [2:53:31<4:48:56,  7.09s/it]

 36%|███▋      | 1400/3844 [2:53:37<4:36:36,  6.79s/it]

 36%|███▋      | 1401/3844 [2:53:45<4:49:56,  7.12s/it]

 36%|███▋      | 1402/3844 [2:53:51<4:34:26,  6.74s/it]

 36%|███▋      | 1403/3844 [2:53:57<4:30:33,  6.65s/it]

 37%|███▋      | 1404/3844 [2:54:05<4:41:49,  6.93s/it]

 37%|███▋      | 1405/3844 [2:54:11<4:30:33,  6.66s/it]

 37%|███▋      | 1406/3844 [2:54:19<4:45:04,  7.02s/it]
{'loss': 1.1432, 'grad_norm': 0.3298460224965472, 'learning_rate': 1.4650675255624732e-05, 'epoch': 0.37}


 37%|███▋      | 1408/3844 [2:54:35<5:16:12,  7.79s/it]
{'loss': 1.029, 'grad_norm': 0.3091921700681272, 'learning_rate': 1.4635748195486984e-05, 'epoch': 0.37}


 37%|███▋      | 1410/3844 [2:54:49<4:59:57,  7.39s/it]

 37%|███▋      | 1411/3844 [2:54:55<4:46:27,  7.06s/it]

 37%|███▋      | 1412/3844 [2:55:01<4:32:20,  6.72s/it]

 37%|███▋      | 1413/3844 [2:55:11<5:10:12,  7.66s/it]

 37%|███▋      | 1414/3844 [2:55:17<4:54:58,  7.28s/it]

 37%|███▋      | 1415/3844 [2:55:25<4:59:53,  7.41s/it]
{'loss': 1.1156, 'grad_norm': 0.3494721628588378, 'learning_rate': 1.458340006372889e-05, 'epoch': 0.37}


 37%|███▋      | 1417/3844 [2:55:38<4:40:34,  6.94s/it]
{'loss': 1.1373, 'grad_norm': 0.31705362161764816, 'learning_rate': 1.4568414080971972e-05, 'epoch': 0.37}


 37%|███▋      | 1419/3844 [2:55:52<4:36:23,  6.84s/it]

 37%|███▋      | 1420/3844 [2:55:58<4:28:43,  6.65s/it]
{'loss': 1.1874, 'grad_norm': 0.3230226407660563, 'learning_rate': 1.4545910788331434e-05, 'epoch': 0.37}


 37%|███▋      | 1422/3844 [2:56:14<4:50:40,  7.20s/it]

 37%|███▋      | 1423/3844 [2:56:22<4:53:41,  7.28s/it]

 37%|███▋      | 1424/3844 [2:56:29<4:55:23,  7.32s/it]

 37%|███▋      | 1425/3844 [2:56:35<4:37:02,  6.87s/it]

 37%|███▋      | 1426/3844 [2:56:42<4:36:39,  6.86s/it]

 37%|███▋      | 1427/3844 [2:56:52<5:18:58,  7.92s/it]
{'loss': 1.0371, 'grad_norm': 0.34578511299522463, 'learning_rate': 1.4493290365309904e-05, 'epoch': 0.37}

 37%|███▋      | 1428/3844 [2:56:59<5:02:24,  7.51s/it]


 37%|███▋      | 1430/3844 [2:57:15<5:25:01,  8.08s/it]

 37%|███▋      | 1431/3844 [2:57:21<5:03:15,  7.54s/it]

 37%|███▋      | 1432/3844 [2:57:30<5:11:27,  7.75s/it]
{'loss': 1.1475, 'grad_norm': 0.32964448283842607, 'learning_rate': 1.4455608515331677e-05, 'epoch': 0.37}


 37%|███▋      | 1434/3844 [2:57:47<5:24:26,  8.08s/it]

 37%|███▋      | 1435/3844 [2:57:54<5:13:11,  7.80s/it]
{'loss': 1.0367, 'grad_norm': 0.3196272089475093, 'learning_rate': 1.4432961392910624e-05, 'epoch': 0.37}


 37%|███▋      | 1437/3844 [2:58:07<4:47:55,  7.18s/it]

 37%|███▋      | 1438/3844 [2:58:14<4:37:50,  6.93s/it]

 37%|███▋      | 1439/3844 [2:58:24<5:16:42,  7.90s/it]

 37%|███▋      | 1440/3844 [2:58:32<5:13:17,  7.82s/it]

 37%|███▋      | 1441/3844 [2:58:37<4:47:56,  7.19s/it]

 38%|███▊      | 1442/3844 [2:58:46<5:00:56,  7.52s/it]

 38%|███▊      | 1443/3844 [2:58:52<4:41:55,  7.05s/it]

 38%|███▊      | 1444/3844 [2:59:02<5:20:04,  8.00s/it]

 38%|███▊      | 1445/3844 [2:59:11<5:29:43,  8.25s/it]
{'loss': 1.1377, 'grad_norm': 0.3121104851484658, 'learning_rate': 1.4357267176001912e-05, 'epoch': 0.38}


 38%|███▊      | 1447/3844 [2:59:23<4:51:40,  7.30s/it]
{'loss': 1.0447, 'grad_norm': 0.3162498496017035, 'learning_rate': 1.4342091028863713e-05, 'epoch': 0.38}

 38%|███▊      | 1448/3844 [2:59:31<4:55:24,  7.40s/it]

 38%|███▊      | 1449/3844 [2:59:37<4:38:33,  6.98s/it]


 38%|███▊      | 1451/3844 [2:59:52<4:48:21,  7.23s/it]

 38%|███▊      | 1452/3844 [2:59:59<4:40:12,  7.03s/it]

 38%|███▊      | 1453/3844 [3:00:05<4:34:08,  6.88s/it]

 38%|███▊      | 1454/3844 [3:00:11<4:18:11,  6.48s/it]
{'loss': 1.1802, 'grad_norm': 0.32625770303340584, 'learning_rate': 1.4288877666221918e-05, 'epoch': 0.38}

 38%|███▊      | 1455/3844 [3:00:17<4:14:37,  6.40s/it]


 38%|███▊      | 1457/3844 [3:00:29<4:12:31,  6.35s/it]

 38%|███▊      | 1458/3844 [3:00:38<4:44:40,  7.16s/it]

 38%|███▊      | 1459/3844 [3:00:46<4:49:05,  7.27s/it]

 38%|███▊      | 1460/3844 [3:00:52<4:29:36,  6.79s/it]
{'loss': 0.9706, 'grad_norm': 0.3394005664867309, 'learning_rate': 1.4243147359028968e-05, 'epoch': 0.38}


 38%|███▊      | 1462/3844 [3:01:08<4:57:43,  7.50s/it]

 38%|███▊      | 1463/3844 [3:01:18<5:26:20,  8.22s/it]
{'loss': 1.1955, 'grad_norm': 0.3101066851485762, 'learning_rate': 1.4220241453383281e-05, 'epoch': 0.38}


 38%|███▊      | 1465/3844 [3:01:33<5:06:45,  7.74s/it]

 38%|███▊      | 1466/3844 [3:01:40<5:02:02,  7.62s/it]

 38%|███▊      | 1467/3844 [3:01:49<5:22:24,  8.14s/it]

 38%|███▊      | 1468/3844 [3:01:57<5:19:54,  8.08s/it]

 38%|███▊      | 1469/3844 [3:02:05<5:11:23,  7.87s/it]

 38%|███▊      | 1470/3844 [3:02:11<4:56:14,  7.49s/it]
{'loss': 1.0841, 'grad_norm': 0.30894963804339526, 'learning_rate': 1.4166689698987335e-05, 'epoch': 0.38}

 38%|███▊      | 1471/3844 [3:02:17<4:35:46,  6.97s/it]

 38%|███▊      | 1472/3844 [3:02:23<4:24:00,  6.68s/it]


 38%|███▊      | 1474/3844 [3:02:41<5:10:26,  7.86s/it]

 38%|███▊      | 1475/3844 [3:02:49<5:11:53,  7.90s/it]

 38%|███▊      | 1476/3844 [3:02:55<4:56:01,  7.50s/it]

 38%|███▊      | 1477/3844 [3:03:02<4:41:08,  7.13s/it]
{'loss': 1.1884, 'grad_norm': 0.3137840436788585, 'learning_rate': 1.4112992955970925e-05, 'epoch': 0.38}


 38%|███▊      | 1479/3844 [3:03:17<4:41:20,  7.14s/it]

 39%|███▊      | 1480/3844 [3:03:27<5:14:38,  7.99s/it]

 39%|███▊      | 1481/3844 [3:03:36<5:27:30,  8.32s/it]

 39%|███▊      | 1482/3844 [3:03:43<5:10:55,  7.90s/it]

 39%|███▊      | 1483/3844 [3:03:51<5:21:13,  8.16s/it]
{'loss': 1.1335, 'grad_norm': 0.3298667452523116, 'learning_rate': 1.4066853193349123e-05, 'epoch': 0.39}


 39%|███▊      | 1485/3844 [3:04:09<5:36:18,  8.55s/it]
{'loss': 1.1232, 'grad_norm': 0.3078813791586889, 'learning_rate': 1.405145010970523e-05, 'epoch': 0.39}


 39%|███▊      | 1487/3844 [3:04:25<5:21:41,  8.19s/it]

 39%|███▊      | 1488/3844 [3:04:30<4:51:35,  7.43s/it]

 39%|███▊      | 1489/3844 [3:04:36<4:29:13,  6.86s/it]

 39%|███▉      | 1490/3844 [3:04:45<4:54:16,  7.50s/it]

 39%|███▉      | 1491/3844 [3:04:52<4:54:53,  7.52s/it]

 39%|███▉      | 1492/3844 [3:05:01<5:02:52,  7.73s/it]

 39%|███▉      | 1493/3844 [3:05:10<5:22:38,  8.23s/it]

 39%|███▉      | 1494/3844 [3:05:18<5:13:18,  8.00s/it]

 39%|███▉      | 1495/3844 [3:05:24<4:52:23,  7.47s/it]
{'loss': 1.1982, 'grad_norm': 0.3126384383333945, 'learning_rate': 1.3974262940361831e-05, 'epoch': 0.39}


 39%|███▉      | 1497/3844 [3:05:41<5:12:05,  7.98s/it]

 39%|███▉      | 1498/3844 [3:05:48<4:58:51,  7.64s/it]

 39%|███▉      | 1499/3844 [3:05:56<5:07:19,  7.86s/it]

 39%|███▉      | 1500/3844 [3:06:08<5:50:18,  8.97s/it]
 39%|███▉      | 1500/3844 [3:06:08<5:50:18,  8.97s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 39%|███▉      | 1501/3844 [3:06:45<11:16:24, 17.32s/it]
{'loss': 1.3155, 'grad_norm': 0.3171052628847278, 'learning_rate': 1.3927814817086413e-05, 'epoch': 0.39}


 39%|███▉      | 1503/3844 [3:06:57<7:29:24, 11.52s/it]

 39%|███▉      | 1504/3844 [3:07:02<6:20:24,  9.75s/it]

 39%|███▉      | 1505/3844 [3:07:08<5:35:28,  8.61s/it]

 39%|███▉      | 1506/3844 [3:07:15<5:10:42,  7.97s/it]

 39%|███▉      | 1507/3844 [3:07:24<5:25:17,  8.35s/it]

 39%|███▉      | 1508/3844 [3:07:30<5:02:10,  7.76s/it]

 39%|███▉      | 1509/3844 [3:07:37<4:47:18,  7.38s/it]

 39%|███▉      | 1510/3844 [3:07:44<4:49:51,  7.45s/it]

 39%|███▉      | 1511/3844 [3:07:56<5:32:21,  8.55s/it]

 39%|███▉      | 1512/3844 [3:08:02<5:12:25,  8.04s/it]
{'loss': 1.1815, 'grad_norm': 0.32403150554785676, 'learning_rate': 1.3842399981656112e-05, 'epoch': 0.39}

 39%|███▉      | 1513/3844 [3:08:09<4:58:05,  7.67s/it]


 39%|███▉      | 1515/3844 [3:08:23<4:41:44,  7.26s/it]

 39%|███▉      | 1516/3844 [3:08:30<4:41:49,  7.26s/it]

 39%|███▉      | 1517/3844 [3:08:38<4:50:33,  7.49s/it]
{'loss': 1.0495, 'grad_norm': 0.31197351925201433, 'learning_rate': 1.3803465468376818e-05, 'epoch': 0.39}

 39%|███▉      | 1518/3844 [3:08:45<4:46:52,  7.40s/it]


 40%|███▉      | 1520/3844 [3:09:03<5:21:58,  8.31s/it]

 40%|███▉      | 1521/3844 [3:09:11<5:15:58,  8.16s/it]

 40%|███▉      | 1522/3844 [3:09:17<4:53:28,  7.58s/it]
{'loss': 1.3312, 'grad_norm': 0.30606920152515155, 'learning_rate': 1.3764463429757183e-05, 'epoch': 0.4}


 40%|███▉      | 1524/3844 [3:09:32<4:55:59,  7.66s/it]

 40%|███▉      | 1525/3844 [3:09:41<5:04:55,  7.89s/it]

 40%|███▉      | 1526/3844 [3:09:47<4:43:11,  7.33s/it]

 40%|███▉      | 1527/3844 [3:09:56<5:02:56,  7.84s/it]
{'loss': 1.1274, 'grad_norm': 0.30084903159375675, 'learning_rate': 1.3725394558225184e-05, 'epoch': 0.4}


 40%|███▉      | 1529/3844 [3:10:11<4:58:15,  7.73s/it]

 40%|███▉      | 1530/3844 [3:10:18<4:44:54,  7.39s/it]

 40%|███▉      | 1531/3844 [3:10:26<5:00:23,  7.79s/it]

 40%|███▉      | 1532/3844 [3:10:32<4:37:51,  7.21s/it]

 40%|███▉      | 1533/3844 [3:10:40<4:41:00,  7.30s/it]

 40%|███▉      | 1534/3844 [3:10:47<4:37:08,  7.20s/it]

 40%|███▉      | 1535/3844 [3:10:53<4:22:39,  6.83s/it]
{'loss': 0.9786, 'grad_norm': 0.30848228062708627, 'learning_rate': 1.3662747083018095e-05, 'epoch': 0.4}


 40%|███▉      | 1537/3844 [3:11:06<4:22:30,  6.83s/it]
{'loss': 1.1672, 'grad_norm': 0.32912751146123664, 'learning_rate': 1.3647059092056374e-05, 'epoch': 0.4}


 40%|████      | 1539/3844 [3:11:22<4:46:37,  7.46s/it]

 40%|████      | 1540/3844 [3:11:32<5:18:05,  8.28s/it]

 40%|████      | 1541/3844 [3:11:40<5:08:01,  8.02s/it]

 40%|████      | 1542/3844 [3:11:46<4:51:15,  7.59s/it]

 40%|████      | 1543/3844 [3:11:53<4:38:23,  7.26s/it]

 40%|████      | 1544/3844 [3:11:58<4:18:49,  6.75s/it]

 40%|████      | 1545/3844 [3:12:05<4:10:47,  6.55s/it]

 40%|████      | 1546/3844 [3:12:12<4:22:41,  6.86s/it]

 40%|████      | 1547/3844 [3:12:18<4:14:30,  6.65s/it]

 40%|████      | 1548/3844 [3:12:31<5:18:36,  8.33s/it]

 40%|████      | 1549/3844 [3:12:37<4:58:34,  7.81s/it]

 40%|████      | 1550/3844 [3:12:45<4:58:12,  7.80s/it]

 40%|████      | 1551/3844 [3:12:52<4:53:52,  7.69s/it]

 40%|████      | 1552/3844 [3:13:03<5:27:23,  8.57s/it]

 40%|████      | 1553/3844 [3:13:13<5:47:23,  9.10s/it]
{'loss': 1.1534, 'grad_norm': 0.33432917269347084, 'learning_rate': 1.3521185961648281e-05, 'epoch': 0.4}


 40%|████      | 1555/3844 [3:13:27<5:07:24,  8.06s/it]

 40%|████      | 1556/3844 [3:13:33<4:44:41,  7.47s/it]

 41%|████      | 1557/3844 [3:13:43<5:05:15,  8.01s/it]

 41%|████      | 1558/3844 [3:13:49<4:42:07,  7.40s/it]

 41%|████      | 1559/3844 [3:13:57<4:51:46,  7.66s/it]

 41%|████      | 1560/3844 [3:14:04<4:47:31,  7.55s/it]

 41%|████      | 1561/3844 [3:14:10<4:32:33,  7.16s/it]

 41%|████      | 1562/3844 [3:14:17<4:26:55,  7.02s/it]

 41%|████      | 1563/3844 [3:14:24<4:26:45,  7.02s/it]

 41%|████      | 1564/3844 [3:14:31<4:26:32,  7.01s/it]

 41%|████      | 1565/3844 [3:14:41<4:55:45,  7.79s/it]

 41%|████      | 1566/3844 [3:14:49<5:01:20,  7.94s/it]

 41%|████      | 1567/3844 [3:14:56<4:54:18,  7.76s/it]

 41%|████      | 1568/3844 [3:15:02<4:35:15,  7.26s/it]

 41%|████      | 1569/3844 [3:15:09<4:30:10,  7.13s/it]

 41%|████      | 1570/3844 [3:15:17<4:32:38,  7.19s/it]

 41%|████      | 1571/3844 [3:15:23<4:18:34,  6.83s/it]

 41%|████      | 1572/3844 [3:15:29<4:18:02,  6.81s/it]
{'loss': 1.2644, 'grad_norm': 0.3159561657474985, 'learning_rate': 1.3370882061557635e-05, 'epoch': 0.41}


 41%|████      | 1574/3844 [3:15:44<4:27:04,  7.06s/it]

 41%|████      | 1575/3844 [3:15:53<4:41:14,  7.44s/it]
{'loss': 1.1967, 'grad_norm': 0.333226387388855, 'learning_rate': 1.334706988060825e-05, 'epoch': 0.41}


 41%|████      | 1577/3844 [3:16:11<5:09:19,  8.19s/it]

 41%|████      | 1578/3844 [3:16:17<4:41:07,  7.44s/it]

 41%|████      | 1579/3844 [3:16:28<5:27:07,  8.67s/it]

 41%|████      | 1580/3844 [3:16:34<4:55:21,  7.83s/it]

 41%|████      | 1581/3844 [3:16:41<4:47:15,  7.62s/it]

 41%|████      | 1582/3844 [3:16:48<4:30:21,  7.17s/it]

 41%|████      | 1583/3844 [3:16:53<4:10:59,  6.66s/it]

 41%|████      | 1584/3844 [3:16:59<4:04:46,  6.50s/it]

 41%|████      | 1585/3844 [3:17:06<4:11:53,  6.69s/it]

 41%|████▏     | 1586/3844 [3:17:15<4:33:13,  7.26s/it]
{'loss': 1.1885, 'grad_norm': 0.3062567504603513, 'learning_rate': 1.3259576687370242e-05, 'epoch': 0.41}


 41%|████▏     | 1588/3844 [3:17:29<4:31:21,  7.22s/it]
{'loss': 1.1372, 'grad_norm': 0.33056255118420846, 'learning_rate': 1.3243638521493425e-05, 'epoch': 0.41}


 41%|████▏     | 1590/3844 [3:17:44<4:31:37,  7.23s/it]

 41%|████▏     | 1591/3844 [3:17:53<4:53:16,  7.81s/it]

 41%|████▏     | 1592/3844 [3:18:03<5:20:23,  8.54s/it]

 41%|████▏     | 1593/3844 [3:18:09<4:55:20,  7.87s/it]
{'loss': 1.2148, 'grad_norm': 0.3206743882426803, 'learning_rate': 1.3203752895388248e-05, 'epoch': 0.41}


 41%|████▏     | 1595/3844 [3:18:24<4:46:05,  7.63s/it]

 42%|████▏     | 1596/3844 [3:18:31<4:35:23,  7.35s/it]

 42%|████▏     | 1597/3844 [3:18:37<4:20:07,  6.95s/it]

 42%|████▏     | 1598/3844 [3:18:43<4:11:44,  6.73s/it]

 42%|████▏     | 1599/3844 [3:18:48<3:58:51,  6.38s/it]

 42%|████▏     | 1600/3844 [3:18:57<4:28:03,  7.17s/it]

 42%|████▏     | 1601/3844 [3:19:05<4:33:10,  7.31s/it]

 42%|████▏     | 1602/3844 [3:19:11<4:14:14,  6.80s/it]

 42%|████▏     | 1603/3844 [3:19:17<4:05:22,  6.57s/it]

 42%|████▏     | 1604/3844 [3:19:27<4:50:38,  7.78s/it]

 42%|████▏     | 1605/3844 [3:19:34<4:44:30,  7.62s/it]

 42%|████▏     | 1606/3844 [3:19:41<4:27:38,  7.18s/it]

 42%|████▏     | 1607/3844 [3:19:50<4:56:49,  7.96s/it]

 42%|████▏     | 1608/3844 [3:19:57<4:38:05,  7.46s/it]
{'loss': 1.2273, 'grad_norm': 0.32838586870843545, 'learning_rate': 1.3083757584997839e-05, 'epoch': 0.42}

 42%|████▏     | 1609/3844 [3:20:02<4:16:13,  6.88s/it]


 42%|████▏     | 1611/3844 [3:20:17<4:21:10,  7.02s/it]

 42%|████▏     | 1612/3844 [3:20:25<4:28:08,  7.21s/it]

 42%|████▏     | 1613/3844 [3:20:31<4:14:59,  6.86s/it]
{'loss': 1.3089, 'grad_norm': 0.3282961658506482, 'learning_rate': 1.3043648704569725e-05, 'epoch': 0.42}

 42%|████▏     | 1614/3844 [3:20:38<4:18:11,  6.95s/it]


 42%|████▏     | 1616/3844 [3:20:56<4:56:31,  7.99s/it]
{'loss': 1.1908, 'grad_norm': 0.32961388766579053, 'learning_rate': 1.3019557393501228e-05, 'epoch': 0.42}


 42%|████▏     | 1618/3844 [3:21:15<5:29:02,  8.87s/it]

 42%|████▏     | 1619/3844 [3:21:25<5:39:15,  9.15s/it]

 42%|████▏     | 1620/3844 [3:21:31<5:09:19,  8.35s/it]
{'loss': 1.2272, 'grad_norm': 0.3689314450423354, 'learning_rate': 1.2987405651463954e-05, 'epoch': 0.42}


 42%|████▏     | 1622/3844 [3:21:47<4:58:57,  8.07s/it]

 42%|████▏     | 1623/3844 [3:21:55<5:00:40,  8.12s/it]

 42%|████▏     | 1624/3844 [3:22:05<5:23:04,  8.73s/it]

 42%|████▏     | 1625/3844 [3:22:10<4:46:49,  7.76s/it]
{'loss': 1.0627, 'grad_norm': 0.31688797741202707, 'learning_rate': 1.2947168283159038e-05, 'epoch': 0.42}


 42%|████▏     | 1627/3844 [3:22:25<4:38:43,  7.54s/it]

 42%|████▏     | 1628/3844 [3:22:31<4:19:35,  7.03s/it]

 42%|████▏     | 1629/3844 [3:22:37<4:17:25,  6.97s/it]

 42%|████▏     | 1630/3844 [3:22:44<4:13:18,  6.86s/it]

 42%|████▏     | 1631/3844 [3:22:53<4:32:51,  7.40s/it]
{'loss': 1.18, 'grad_norm': 0.3206304645077246, 'learning_rate': 1.2898814437825597e-05, 'epoch': 0.42}

 42%|████▏     | 1632/3844 [3:23:01<4:38:18,  7.55s/it]


 43%|████▎     | 1634/3844 [3:23:17<4:53:25,  7.97s/it]

 43%|████▎     | 1635/3844 [3:23:27<5:16:54,  8.61s/it]

 43%|████▎     | 1636/3844 [3:23:37<5:33:46,  9.07s/it]

 43%|████▎     | 1637/3844 [3:23:46<5:29:02,  8.95s/it]

 43%|████▎     | 1638/3844 [3:23:53<5:04:06,  8.27s/it]

 43%|████▎     | 1639/3844 [3:23:59<4:46:57,  7.81s/it]
{'loss': 1.2363, 'grad_norm': 0.36815217731816197, 'learning_rate': 1.2834227577586433e-05, 'epoch': 0.43}


 43%|████▎     | 1641/3844 [3:24:12<4:18:35,  7.04s/it]

 43%|████▎     | 1642/3844 [3:24:19<4:17:53,  7.03s/it]
{'loss': 0.982, 'grad_norm': 0.3203174210520075, 'learning_rate': 1.2809974137444087e-05, 'epoch': 0.43}


 43%|████▎     | 1644/3844 [3:24:37<5:00:50,  8.20s/it]

 43%|████▎     | 1645/3844 [3:24:46<5:09:04,  8.43s/it]

 43%|████▎     | 1646/3844 [3:24:53<4:50:36,  7.93s/it]

 43%|████▎     | 1647/3844 [3:25:01<4:48:19,  7.87s/it]

 43%|████▎     | 1648/3844 [3:25:07<4:29:19,  7.36s/it]

 43%|████▎     | 1649/3844 [3:25:12<4:06:11,  6.73s/it]

 43%|████▎     | 1650/3844 [3:25:22<4:39:55,  7.66s/it]

 43%|████▎     | 1651/3844 [3:25:28<4:23:05,  7.20s/it]

 43%|████▎     | 1652/3844 [3:25:38<4:50:35,  7.95s/it]

 43%|████▎     | 1653/3844 [3:25:44<4:29:27,  7.38s/it]

 43%|████▎     | 1654/3844 [3:25:49<4:07:48,  6.79s/it]

 43%|████▎     | 1655/3844 [3:25:56<4:09:52,  6.85s/it]

 43%|████▎     | 1656/3844 [3:26:03<4:08:41,  6.82s/it]

 43%|████▎     | 1657/3844 [3:26:10<4:09:56,  6.86s/it]

 43%|████▎     | 1658/3844 [3:26:18<4:26:43,  7.32s/it]

 43%|████▎     | 1659/3844 [3:26:25<4:21:48,  7.19s/it]

 43%|████▎     | 1660/3844 [3:26:31<4:11:22,  6.91s/it]

 43%|████▎     | 1661/3844 [3:26:38<4:03:47,  6.70s/it]

 43%|████▎     | 1662/3844 [3:26:46<4:23:32,  7.25s/it]

 43%|████▎     | 1663/3844 [3:26:54<4:24:41,  7.28s/it]

 43%|████▎     | 1664/3844 [3:27:03<4:52:23,  8.05s/it]

 43%|████▎     | 1665/3844 [3:27:09<4:28:25,  7.39s/it]

 43%|████▎     | 1666/3844 [3:27:16<4:19:27,  7.15s/it]

 43%|████▎     | 1667/3844 [3:27:23<4:21:06,  7.20s/it]
{'loss': 1.0484, 'grad_norm': 0.32580074507221785, 'learning_rate': 1.2607178477524853e-05, 'epoch': 0.43}

 43%|████▎     | 1668/3844 [3:27:29<4:01:14,  6.65s/it]


 43%|████▎     | 1670/3844 [3:27:44<4:27:48,  7.39s/it]

 43%|████▎     | 1671/3844 [3:27:54<4:50:05,  8.01s/it]
{'loss': 1.0082, 'grad_norm': 0.32884270139875116, 'learning_rate': 1.257462144402597e-05, 'epoch': 0.43}


 44%|████▎     | 1673/3844 [3:28:09<4:41:02,  7.77s/it]

 44%|████▎     | 1674/3844 [3:28:16<4:28:44,  7.43s/it]

 44%|████▎     | 1675/3844 [3:28:22<4:12:37,  6.99s/it]

 44%|████▎     | 1676/3844 [3:28:27<3:56:42,  6.55s/it]

 44%|████▎     | 1677/3844 [3:28:37<4:36:10,  7.65s/it]

 44%|████▎     | 1678/3844 [3:28:44<4:20:09,  7.21s/it]
{'loss': 1.3041, 'grad_norm': 0.333403842945478, 'learning_rate': 1.251757646639288e-05, 'epoch': 0.44}


 44%|████▎     | 1680/3844 [3:29:00<4:47:23,  7.97s/it]

 44%|████▎     | 1681/3844 [3:29:08<4:40:44,  7.79s/it]

 44%|████▍     | 1682/3844 [3:29:14<4:25:57,  7.38s/it]

 44%|████▍     | 1683/3844 [3:29:24<4:51:52,  8.10s/it]
{'loss': 1.0258, 'grad_norm': 0.30507588899125365, 'learning_rate': 1.2476776302646793e-05, 'epoch': 0.44}

 44%|████▍     | 1684/3844 [3:29:31<4:37:42,  7.71s/it]

 44%|████▍     | 1685/3844 [3:29:37<4:18:14,  7.18s/it]

 44%|████▍     | 1686/3844 [3:29:43<4:03:45,  6.78s/it]


 44%|████▍     | 1688/3844 [3:29:59<4:18:12,  7.19s/it]
{'loss': 1.247, 'grad_norm': 0.35054318982145205, 'learning_rate': 1.2435932167116502e-05, 'epoch': 0.44}

 44%|████▍     | 1689/3844 [3:30:09<4:51:42,  8.12s/it]


 44%|████▍     | 1691/3844 [3:30:22<4:30:32,  7.54s/it]

 44%|████▍     | 1692/3844 [3:30:28<4:11:13,  7.00s/it]
{'loss': 1.055, 'grad_norm': 0.3313018200343624, 'learning_rate': 1.2403225686270383e-05, 'epoch': 0.44}


 44%|████▍     | 1694/3844 [3:30:41<3:59:49,  6.69s/it]

 44%|████▍     | 1695/3844 [3:30:47<3:54:56,  6.56s/it]

 44%|████▍     | 1696/3844 [3:30:53<3:48:21,  6.38s/it]

 44%|████▍     | 1697/3844 [3:31:03<4:19:12,  7.24s/it]
{'loss': 1.088, 'grad_norm': 0.31538107928964937, 'learning_rate': 1.2362304229377311e-05, 'epoch': 0.44}


 44%|████▍     | 1699/3844 [3:31:18<4:23:23,  7.37s/it]

 44%|████▍     | 1700/3844 [3:31:27<4:38:33,  7.80s/it]
{'loss': 1.2006, 'grad_norm': 0.3131817345862932, 'learning_rate': 1.233773117775946e-05, 'epoch': 0.44}


 44%|████▍     | 1702/3844 [3:31:39<4:12:01,  7.06s/it]

 44%|████▍     | 1703/3844 [3:31:47<4:15:27,  7.16s/it]

 44%|████▍     | 1704/3844 [3:31:53<4:07:23,  6.94s/it]
{'loss': 1.1343, 'grad_norm': 0.345321709589283, 'learning_rate': 1.2304943894265065e-05, 'epoch': 0.44}


 44%|████▍     | 1706/3844 [3:32:04<3:43:25,  6.27s/it]

 44%|████▍     | 1707/3844 [3:32:12<3:57:59,  6.68s/it]

 44%|████▍     | 1708/3844 [3:32:19<4:02:14,  6.80s/it]

 44%|████▍     | 1709/3844 [3:32:30<4:43:16,  7.96s/it]

 44%|████▍     | 1710/3844 [3:32:40<5:09:21,  8.70s/it]
[2024-05-27 14:18:22,553] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 1711/3844 [3:32:48<5:01:15,  8.47s/it]

 45%|████▍     | 1712/3844 [3:32:55<4:38:24,  7.83s/it]

 45%|████▍     | 1713/3844 [3:33:01<4:20:19,  7.33s/it]

 45%|████▍     | 1714/3844 [3:33:06<4:00:02,  6.76s/it]

 45%|████▍     | 1715/3844 [3:33:12<3:49:28,  6.47s/it]

 45%|████▍     | 1716/3844 [3:33:20<4:07:41,  6.98s/it]
{'loss': 1.0823, 'grad_norm': 0.31436827115306826, 'learning_rate': 1.2206426398186534e-05, 'epoch': 0.45}


 45%|████▍     | 1718/3844 [3:33:33<4:01:11,  6.81s/it]

 45%|████▍     | 1719/3844 [3:33:40<4:03:15,  6.87s/it]

 45%|████▍     | 1720/3844 [3:33:49<4:17:45,  7.28s/it]

 45%|████▍     | 1721/3844 [3:33:54<3:58:12,  6.73s/it]

 45%|████▍     | 1722/3844 [3:34:02<4:05:10,  6.93s/it]

 45%|████▍     | 1723/3844 [3:34:09<4:15:27,  7.23s/it]

 45%|████▍     | 1724/3844 [3:34:18<4:26:16,  7.54s/it]

 45%|████▍     | 1725/3844 [3:34:25<4:25:57,  7.53s/it]

 45%|████▍     | 1726/3844 [3:34:32<4:20:59,  7.39s/it]

 45%|████▍     | 1727/3844 [3:34:39<4:11:28,  7.13s/it]
{'loss': 1.0246, 'grad_norm': 0.32956305916017553, 'learning_rate': 1.2115920237815284e-05, 'epoch': 0.45}


 45%|████▍     | 1729/3844 [3:34:53<4:17:42,  7.31s/it]

 45%|████▌     | 1730/3844 [3:35:00<4:08:32,  7.05s/it]

 45%|████▌     | 1731/3844 [3:35:07<4:05:50,  6.98s/it]

 45%|████▌     | 1732/3844 [3:35:14<4:11:19,  7.14s/it]

 45%|████▌     | 1733/3844 [3:35:20<3:57:34,  6.75s/it]
{'loss': 1.2129, 'grad_norm': 0.3102212944455911, 'learning_rate': 1.2066476111227852e-05, 'epoch': 0.45}


 45%|████▌     | 1735/3844 [3:35:34<3:56:45,  6.74s/it]

 45%|████▌     | 1736/3844 [3:35:40<3:49:34,  6.53s/it]

 45%|████▌     | 1737/3844 [3:35:47<4:02:47,  6.91s/it]

 45%|████▌     | 1738/3844 [3:35:58<4:39:34,  7.97s/it]

 45%|████▌     | 1739/3844 [3:36:08<5:00:52,  8.58s/it]

 45%|████▌     | 1740/3844 [3:36:14<4:39:52,  7.98s/it]

 45%|████▌     | 1741/3844 [3:36:21<4:21:03,  7.45s/it]

 45%|████▌     | 1742/3844 [3:36:26<4:00:09,  6.85s/it]

 45%|████▌     | 1743/3844 [3:36:32<3:49:26,  6.55s/it]

 45%|████▌     | 1744/3844 [3:36:39<3:51:24,  6.61s/it]

 45%|████▌     | 1745/3844 [3:36:44<3:40:54,  6.31s/it]

 45%|████▌     | 1746/3844 [3:36:52<3:58:30,  6.82s/it]

 45%|████▌     | 1747/3844 [3:37:02<4:30:53,  7.75s/it]

 45%|████▌     | 1748/3844 [3:37:08<4:06:10,  7.05s/it]

 45%|████▌     | 1749/3844 [3:37:14<4:01:05,  6.90s/it]

 46%|████▌     | 1750/3844 [3:37:20<3:46:30,  6.49s/it]

 46%|████▌     | 1751/3844 [3:37:26<3:48:02,  6.54s/it]

 46%|████▌     | 1752/3844 [3:37:35<4:06:15,  7.06s/it]

 46%|████▌     | 1753/3844 [3:37:42<4:12:17,  7.24s/it]

 46%|████▌     | 1754/3844 [3:37:49<4:04:10,  7.01s/it]

 46%|████▌     | 1755/3844 [3:37:55<3:54:24,  6.73s/it]

 46%|████▌     | 1756/3844 [3:38:01<3:43:55,  6.43s/it]

 46%|████▌     | 1757/3844 [3:38:07<3:38:19,  6.28s/it]

 46%|████▌     | 1758/3844 [3:38:13<3:39:40,  6.32s/it]

 46%|████▌     | 1759/3844 [3:38:20<3:44:48,  6.47s/it]

 46%|████▌     | 1760/3844 [3:38:27<3:49:12,  6.60s/it]
{'loss': 1.0725, 'grad_norm': 0.33741839832170656, 'learning_rate': 1.1843342052427786e-05, 'epoch': 0.46}


 46%|████▌     | 1762/3844 [3:38:42<4:04:35,  7.05s/it]

 46%|████▌     | 1763/3844 [3:38:48<3:53:43,  6.74s/it]
{'loss': 1.1059, 'grad_norm': 0.35175283112868394, 'learning_rate': 1.1818488357172534e-05, 'epoch': 0.46}


 46%|████▌     | 1765/3844 [3:39:03<4:10:56,  7.24s/it]
{'loss': 1.1247, 'grad_norm': 0.3379248571146577, 'learning_rate': 1.1801912760231802e-05, 'epoch': 0.46}


 46%|████▌     | 1767/3844 [3:39:22<4:53:27,  8.48s/it]

 46%|████▌     | 1768/3844 [3:39:28<4:30:03,  7.80s/it]

 46%|████▌     | 1769/3844 [3:39:36<4:24:00,  7.63s/it]
{'loss': 1.0888, 'grad_norm': 0.3510351741817669, 'learning_rate': 1.1768746257988884e-05, 'epoch': 0.46}


 46%|████▌     | 1771/3844 [3:39:48<3:56:06,  6.83s/it]

 46%|████▌     | 1772/3844 [3:39:54<3:45:54,  6.54s/it]

 46%|████▌     | 1773/3844 [3:40:00<3:44:57,  6.52s/it]

 46%|████▌     | 1774/3844 [3:40:12<4:39:47,  8.11s/it]

 46%|████▌     | 1775/3844 [3:40:19<4:26:35,  7.73s/it]

 46%|████▌     | 1776/3844 [3:40:27<4:26:30,  7.73s/it]

 46%|████▌     | 1777/3844 [3:40:33<4:16:03,  7.43s/it]

 46%|████▋     | 1778/3844 [3:40:41<4:18:24,  7.50s/it]

 46%|████▋     | 1779/3844 [3:40:47<4:03:38,  7.08s/it]

 46%|████▋     | 1780/3844 [3:40:57<4:35:48,  8.02s/it]
{'loss': 1.047, 'grad_norm': 0.2929442388142907, 'learning_rate': 1.1677435885536453e-05, 'epoch': 0.46}


 46%|████▋     | 1782/3844 [3:41:16<4:55:10,  8.59s/it]

 46%|████▋     | 1783/3844 [3:41:22<4:31:42,  7.91s/it]

 46%|████▋     | 1784/3844 [3:41:28<4:13:54,  7.40s/it]

 46%|████▋     | 1785/3844 [3:41:34<3:56:33,  6.89s/it]

 46%|████▋     | 1786/3844 [3:41:42<4:08:08,  7.23s/it]

 46%|████▋     | 1787/3844 [3:41:49<4:05:39,  7.17s/it]

 47%|████▋     | 1788/3844 [3:41:57<4:09:42,  7.29s/it]

 47%|████▋     | 1789/3844 [3:42:02<3:53:19,  6.81s/it]

 47%|████▋     | 1790/3844 [3:42:12<4:20:26,  7.61s/it]

 47%|████▋     | 1791/3844 [3:42:18<4:07:37,  7.24s/it]

 47%|████▋     | 1792/3844 [3:42:25<4:04:58,  7.16s/it]

 47%|████▋     | 1793/3844 [3:42:31<3:54:17,  6.85s/it]

 47%|████▋     | 1794/3844 [3:42:41<4:24:17,  7.74s/it]
{'loss': 0.9879, 'grad_norm': 0.339547816615707, 'learning_rate': 1.156101525425872e-05, 'epoch': 0.47}

 47%|████▋     | 1795/3844 [3:42:48<4:14:55,  7.47s/it]

 47%|████▋     | 1796/3844 [3:42:54<3:59:08,  7.01s/it]


 47%|████▋     | 1798/3844 [3:43:09<4:05:52,  7.21s/it]

 47%|████▋     | 1799/3844 [3:43:15<3:50:02,  6.75s/it]

 47%|████▋     | 1800/3844 [3:43:21<3:44:44,  6.60s/it]
 47%|████▋     | 1800/3844 [3:43:21<3:44:44,  6.60s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1441, 'grad_norm': 0.3331548015589102, 'learning_rate': 1.1502722447204313e-05, 'epoch': 0.47}

 47%|████▋     | 1802/3844 [3:44:05<7:29:29, 13.21s/it]
{'loss': 1.1122, 'grad_norm': 0.3300180322104295, 'learning_rate': 1.1494390587406423e-05, 'epoch': 0.47}


 47%|████▋     | 1804/3844 [3:44:19<5:52:14, 10.36s/it]
{'loss': 1.1819, 'grad_norm': 0.362147755668473, 'learning_rate': 1.1477723690019788e-05, 'epoch': 0.47}

 47%|████▋     | 1805/3844 [3:44:28<5:33:18,  9.81s/it]

 47%|████▋     | 1806/3844 [3:44:36<5:15:50,  9.30s/it]


 47%|████▋     | 1808/3844 [3:44:52<4:42:33,  8.33s/it]

 47%|████▋     | 1809/3844 [3:45:03<5:16:03,  9.32s/it]

 47%|████▋     | 1810/3844 [3:45:11<4:58:24,  8.80s/it]
{'loss': 1.1653, 'grad_norm': 0.33923672831555923, 'learning_rate': 1.1427698001693688e-05, 'epoch': 0.47}

 47%|████▋     | 1811/3844 [3:45:16<4:26:36,  7.87s/it]


 47%|████▋     | 1813/3844 [3:45:31<4:19:16,  7.66s/it]

 47%|████▋     | 1814/3844 [3:45:37<3:58:01,  7.04s/it]

 47%|████▋     | 1815/3844 [3:45:42<3:43:17,  6.60s/it]
{'loss': 1.1327, 'grad_norm': 0.3254594211156196, 'learning_rate': 1.138598199229018e-05, 'epoch': 0.47}

 47%|████▋     | 1816/3844 [3:45:48<3:33:36,  6.32s/it]


 47%|████▋     | 1818/3844 [3:46:03<3:50:13,  6.82s/it]

 47%|████▋     | 1819/3844 [3:46:15<4:46:27,  8.49s/it]
{'loss': 1.1452, 'grad_norm': 0.33861561626696607, 'learning_rate': 1.135259143272531e-05, 'epoch': 0.47}

 47%|████▋     | 1820/3844 [3:46:24<4:46:31,  8.49s/it]

 47%|████▋     | 1821/3844 [3:46:30<4:22:20,  7.78s/it]


 47%|████▋     | 1823/3844 [3:46:46<4:21:16,  7.76s/it]
{'loss': 1.1654, 'grad_norm': 0.3421374165248461, 'learning_rate': 1.1319185504566299e-05, 'epoch': 0.47}


 47%|████▋     | 1825/3844 [3:47:03<4:39:03,  8.29s/it]

 48%|████▊     | 1826/3844 [3:47:09<4:16:46,  7.63s/it]

 48%|████▊     | 1827/3844 [3:47:19<4:33:50,  8.15s/it]

 48%|████▊     | 1828/3844 [3:47:27<4:36:39,  8.23s/it]

 48%|████▊     | 1829/3844 [3:47:34<4:20:14,  7.75s/it]

 48%|████▊     | 1830/3844 [3:47:41<4:14:57,  7.60s/it]

 48%|████▊     | 1831/3844 [3:47:49<4:18:06,  7.69s/it]

 48%|████▊     | 1832/3844 [3:47:56<4:10:01,  7.46s/it]

 48%|████▊     | 1833/3844 [3:48:06<4:35:57,  8.23s/it]
{'loss': 0.94, 'grad_norm': 0.30745868277696187, 'learning_rate': 1.1235605937910848e-05, 'epoch': 0.48}


 48%|████▊     | 1835/3844 [3:48:19<4:05:09,  7.32s/it]
{'loss': 1.1279, 'grad_norm': 0.3279020829815143, 'learning_rate': 1.121887930506443e-05, 'epoch': 0.48}

 48%|████▊     | 1836/3844 [3:48:28<4:28:28,  8.02s/it]

 48%|████▊     | 1837/3844 [3:48:34<4:02:30,  7.25s/it]

 48%|████▊     | 1838/3844 [3:48:41<3:56:25,  7.07s/it]


 48%|████▊     | 1840/3844 [3:48:57<4:22:55,  7.87s/it]

 48%|████▊     | 1841/3844 [3:49:07<4:44:22,  8.52s/it]
{'loss': 1.1, 'grad_norm': 0.3378822962265451, 'learning_rate': 1.1168678822644512e-05, 'epoch': 0.48}

 48%|████▊     | 1842/3844 [3:49:17<4:53:32,  8.80s/it]

 48%|████▊     | 1843/3844 [3:49:24<4:38:19,  8.35s/it]


 48%|████▊     | 1845/3844 [3:49:37<4:09:04,  7.48s/it]

 48%|████▊     | 1846/3844 [3:49:43<3:56:35,  7.11s/it]

 48%|████▊     | 1847/3844 [3:49:50<3:49:49,  6.91s/it]

 48%|████▊     | 1848/3844 [3:49:55<3:35:51,  6.49s/it]

 48%|████▊     | 1849/3844 [3:50:04<3:54:28,  7.05s/it]
{'loss': 1.1606, 'grad_norm': 0.3177098091330949, 'learning_rate': 1.110169859185769e-05, 'epoch': 0.48}

 48%|████▊     | 1850/3844 [3:50:14<4:26:16,  8.01s/it]

 48%|████▊     | 1851/3844 [3:50:21<4:12:29,  7.60s/it]

 48%|████▊     | 1852/3844 [3:50:26<3:54:53,  7.08s/it]

 48%|████▊     | 1853/3844 [3:50:33<3:46:07,  6.81s/it]

 48%|████▊     | 1854/3844 [3:50:40<3:52:10,  7.00s/it]

 48%|████▊     | 1855/3844 [3:50:47<3:48:14,  6.89s/it]


 48%|████▊     | 1857/3844 [3:50:59<3:38:17,  6.59s/it]
{'loss': 1.3254, 'grad_norm': 0.36090972558397755, 'learning_rate': 1.1034668289748467e-05, 'epoch': 0.48}


 48%|████▊     | 1859/3844 [3:51:17<4:31:29,  8.21s/it]

 48%|████▊     | 1860/3844 [3:51:23<4:08:03,  7.50s/it]
{'loss': 1.2174, 'grad_norm': 0.3415510474029239, 'learning_rate': 1.1009519639165162e-05, 'epoch': 0.48}


 48%|████▊     | 1862/3844 [3:51:37<3:57:39,  7.19s/it]

 48%|████▊     | 1863/3844 [3:51:45<4:06:05,  7.45s/it]

 48%|████▊     | 1864/3844 [3:51:51<3:54:57,  7.12s/it]
{'loss': 1.0761, 'grad_norm': 0.3609932939656658, 'learning_rate': 1.0975978096150958e-05, 'epoch': 0.48}

 49%|████▊     | 1865/3844 [3:51:58<3:53:28,  7.08s/it]


 49%|████▊     | 1867/3844 [3:52:10<3:29:46,  6.37s/it]
{'loss': 1.1411, 'grad_norm': 0.3581868379751644, 'learning_rate': 1.0950814640632668e-05, 'epoch': 0.49}

 49%|████▊     | 1868/3844 [3:52:17<3:36:18,  6.57s/it]


 49%|████▊     | 1870/3844 [3:52:29<3:29:34,  6.37s/it]
{'loss': 1.2151, 'grad_norm': 0.3276134652746847, 'learning_rate': 1.092564510815859e-05, 'epoch': 0.49}


 49%|████▊     | 1872/3844 [3:52:43<3:40:02,  6.69s/it]

 49%|████▊     | 1873/3844 [3:52:52<3:57:02,  7.22s/it]

 49%|████▉     | 1874/3844 [3:52:58<3:47:43,  6.94s/it]
{'loss': 1.1827, 'grad_norm': 0.3566978202588791, 'learning_rate': 1.0892076556530324e-05, 'epoch': 0.49}


 49%|████▉     | 1876/3844 [3:53:14<4:07:39,  7.55s/it]
{'loss': 1.1277, 'grad_norm': 0.3123745042875047, 'learning_rate': 1.0875288455846521e-05, 'epoch': 0.49}


 49%|████▉     | 1878/3844 [3:53:29<4:13:00,  7.72s/it]

 49%|████▉     | 1879/3844 [3:53:35<3:54:48,  7.17s/it]
{'loss': 1.2597, 'grad_norm': 0.34237395467951787, 'learning_rate': 1.0850101657853755e-05, 'epoch': 0.49}


 49%|████▉     | 1881/3844 [3:53:56<4:49:51,  8.86s/it]

 49%|████▉     | 1882/3844 [3:54:05<4:57:01,  9.08s/it]

 49%|████▉     | 1883/3844 [3:54:14<4:48:12,  8.82s/it]
{'loss': 1.0124, 'grad_norm': 0.339637429439818, 'learning_rate': 1.0816510836608946e-05, 'epoch': 0.49}


 49%|████▉     | 1885/3844 [3:54:32<4:56:42,  9.09s/it]
{'loss': 1.0438, 'grad_norm': 0.3391155893004523, 'learning_rate': 1.0799711923077513e-05, 'epoch': 0.49}


 49%|████▉     | 1887/3844 [3:54:44<4:01:53,  7.42s/it]

 49%|████▉     | 1888/3844 [3:54:50<3:50:11,  7.06s/it]

 49%|████▉     | 1889/3844 [3:55:02<4:32:16,  8.36s/it]

 49%|████▉     | 1890/3844 [3:55:08<4:08:12,  7.62s/it]

 49%|████▉     | 1891/3844 [3:55:18<4:38:39,  8.56s/it]

 49%|████▉     | 1892/3844 [3:55:24<4:07:17,  7.60s/it]

 49%|████▉     | 1893/3844 [3:55:30<3:56:02,  7.26s/it]

 49%|████▉     | 1894/3844 [3:55:36<3:43:53,  6.89s/it]
{'loss': 1.1986, 'grad_norm': 0.3423206371366712, 'learning_rate': 1.07240893896079e-05, 'epoch': 0.49}

 49%|████▉     | 1895/3844 [3:55:43<3:42:18,  6.84s/it]

 49%|████▉     | 1896/3844 [3:55:49<3:31:14,  6.51s/it]


 49%|████▉     | 1898/3844 [3:56:00<3:20:58,  6.20s/it]

 49%|████▉     | 1899/3844 [3:56:06<3:17:38,  6.10s/it]

 49%|████▉     | 1900/3844 [3:56:12<3:14:23,  6.00s/it]

 49%|████▉     | 1901/3844 [3:56:18<3:10:33,  5.88s/it]

 49%|████▉     | 1902/3844 [3:56:23<3:10:28,  5.89s/it]

 50%|████▉     | 1903/3844 [3:56:30<3:13:39,  5.99s/it]

 50%|████▉     | 1904/3844 [3:56:37<3:28:33,  6.45s/it]
{'loss': 0.931, 'grad_norm': 0.3307139389969371, 'learning_rate': 1.064001569178502e-05, 'epoch': 0.5}

 50%|████▉     | 1905/3844 [3:56:45<3:37:59,  6.75s/it]

 50%|████▉     | 1906/3844 [3:56:50<3:28:22,  6.45s/it]


 50%|████▉     | 1908/3844 [3:57:04<3:34:25,  6.65s/it]

 50%|████▉     | 1909/3844 [3:57:10<3:26:06,  6.39s/it]

 50%|████▉     | 1910/3844 [3:57:15<3:19:16,  6.18s/it]

 50%|████▉     | 1911/3844 [3:57:21<3:15:30,  6.07s/it]
{'loss': 1.1294, 'grad_norm': 0.34016413208265106, 'learning_rate': 1.0581136705053518e-05, 'epoch': 0.5}

 50%|████▉     | 1912/3844 [3:57:31<3:46:25,  7.03s/it]


 50%|████▉     | 1914/3844 [3:57:46<4:05:44,  7.64s/it]

 50%|████▉     | 1915/3844 [3:57:54<4:06:29,  7.67s/it]
{'loss': 1.1189, 'grad_norm': 0.35252131627756866, 'learning_rate': 1.0547482359135806e-05, 'epoch': 0.5}


 50%|████▉     | 1917/3844 [3:58:12<4:24:25,  8.23s/it]

 50%|████▉     | 1918/3844 [3:58:20<4:22:25,  8.18s/it]
{'loss': 1.1095, 'grad_norm': 0.3186392314895989, 'learning_rate': 1.0522237496463602e-05, 'epoch': 0.5}


 50%|████▉     | 1920/3844 [3:58:32<3:45:13,  7.02s/it]

 50%|████▉     | 1921/3844 [3:58:38<3:34:23,  6.69s/it]

 50%|█████     | 1922/3844 [3:58:45<3:37:46,  6.80s/it]
{'loss': 1.0449, 'grad_norm': 0.35344099354251884, 'learning_rate': 1.0488572515348804e-05, 'epoch': 0.5}

 50%|█████     | 1923/3844 [3:58:55<4:03:09,  7.59s/it]

 50%|█████     | 1924/3844 [3:59:03<4:05:54,  7.68s/it]

 50%|█████     | 1925/3844 [3:59:13<4:27:08,  8.35s/it]

 50%|█████     | 1926/3844 [3:59:19<4:05:20,  7.67s/it]

 50%|█████     | 1927/3844 [3:59:27<4:10:59,  7.86s/it]


 50%|█████     | 1929/3844 [3:59:43<4:10:04,  7.84s/it]
{'loss': 1.1855, 'grad_norm': 0.3531722119730046, 'learning_rate': 1.0429645670666582e-05, 'epoch': 0.5}

 50%|█████     | 1930/3844 [3:59:51<4:13:24,  7.94s/it]


 50%|█████     | 1932/3844 [4:00:06<4:07:02,  7.75s/it]
{'loss': 1.1779, 'grad_norm': 0.30708584730029787, 'learning_rate': 1.0404386612421941e-05, 'epoch': 0.5}

 50%|█████     | 1933/3844 [4:00:15<4:22:22,  8.24s/it]


 50%|█████     | 1935/3844 [4:00:28<3:51:56,  7.29s/it]

 50%|█████     | 1936/3844 [4:00:34<3:44:54,  7.07s/it]

 50%|█████     | 1937/3844 [4:00:46<4:33:53,  8.62s/it]

 50%|█████     | 1938/3844 [4:00:56<4:47:42,  9.06s/it]

 50%|█████     | 1939/3844 [4:01:04<4:33:30,  8.61s/it]

 50%|█████     | 1940/3844 [4:01:11<4:13:30,  7.99s/it]

 50%|█████     | 1941/3844 [4:01:22<4:42:44,  8.91s/it]

 51%|█████     | 1942/3844 [4:01:28<4:17:55,  8.14s/it]
{'loss': 1.1723, 'grad_norm': 0.31446616115664555, 'learning_rate': 1.0320171992292134e-05, 'epoch': 0.51}


 51%|█████     | 1944/3844 [4:01:41<3:46:59,  7.17s/it]

 51%|█████     | 1945/3844 [4:01:49<3:55:36,  7.44s/it]
{'loss': 1.0821, 'grad_norm': 0.3315717865503876, 'learning_rate': 1.0294902900427808e-05, 'epoch': 0.51}


 51%|█████     | 1947/3844 [4:02:03<3:44:17,  7.09s/it]
{'loss': 1.0551, 'grad_norm': 0.34055313919625363, 'learning_rate': 1.0278055782094391e-05, 'epoch': 0.51}


 51%|█████     | 1949/3844 [4:02:14<3:22:52,  6.42s/it]
{'loss': 1.1465, 'grad_norm': 0.34798161753746365, 'learning_rate': 1.0261207873919818e-05, 'epoch': 0.51}

 51%|█████     | 1950/3844 [4:02:23<3:44:27,  7.11s/it]


 51%|█████     | 1952/3844 [4:02:37<3:36:39,  6.87s/it]

 51%|█████     | 1953/3844 [4:02:42<3:25:06,  6.51s/it]

 51%|█████     | 1954/3844 [4:02:51<3:40:40,  7.01s/it]
{'loss': 1.1445, 'grad_norm': 0.3351436398389625, 'learning_rate': 1.0219084962001105e-05, 'epoch': 0.51}


 51%|█████     | 1956/3844 [4:03:04<3:34:15,  6.81s/it]
{'loss': 1.2165, 'grad_norm': 0.3587020226392789, 'learning_rate': 1.0202234666277116e-05, 'epoch': 0.51}

 51%|█████     | 1957/3844 [4:03:11<3:32:20,  6.75s/it]


 51%|█████     | 1959/3844 [4:03:26<3:46:29,  7.21s/it]

 51%|█████     | 1960/3844 [4:03:34<3:57:20,  7.56s/it]
{'loss': 0.9846, 'grad_norm': 0.36112306139189043, 'learning_rate': 1.0168532399301047e-05, 'epoch': 0.51}


 51%|█████     | 1962/3844 [4:03:49<3:53:29,  7.44s/it]
{'loss': 1.1106, 'grad_norm': 0.31558023604090946, 'learning_rate': 1.0151680523783128e-05, 'epoch': 0.51}

 51%|█████     | 1963/3844 [4:03:55<3:46:42,  7.23s/it]

 51%|█████     | 1964/3844 [4:04:03<3:52:13,  7.41s/it]

 51%|█████     | 1965/3844 [4:04:16<4:37:05,  8.85s/it]

 51%|█████     | 1966/3844 [4:04:21<4:09:52,  7.98s/it]

 51%|█████     | 1967/3844 [4:04:29<4:08:59,  7.96s/it]

 51%|█████     | 1968/3844 [4:04:36<3:53:51,  7.48s/it]

 51%|█████     | 1969/3844 [4:04:41<3:37:23,  6.96s/it]


 51%|█████▏    | 1971/3844 [4:04:53<3:16:49,  6.31s/it]

 51%|█████▏    | 1972/3844 [4:05:00<3:27:35,  6.65s/it]

 51%|█████▏    | 1973/3844 [4:05:07<3:24:16,  6.55s/it]

 51%|█████▏    | 1974/3844 [4:05:13<3:20:17,  6.43s/it]
{'loss': 1.1852, 'grad_norm': 0.3483519386140939, 'learning_rate': 1.0050561898084677e-05, 'epoch': 0.51}

 51%|█████▏    | 1975/3844 [4:05:19<3:18:55,  6.39s/it]


 51%|█████▏    | 1977/3844 [4:05:34<3:35:08,  6.91s/it]
{'loss': 1.2192, 'grad_norm': 0.3310468177408401, 'learning_rate': 1.002528102983185e-05, 'epoch': 0.51}


 51%|█████▏    | 1979/3844 [4:05:50<3:49:15,  7.38s/it]
{'loss': 1.0423, 'grad_norm': 0.3605700760379073, 'learning_rate': 1.0008427017923173e-05, 'epoch': 0.51}


 52%|█████▏    | 1981/3844 [4:06:04<3:39:47,  7.08s/it]

 52%|█████▏    | 1982/3844 [4:06:12<3:50:09,  7.42s/it]

 52%|█████▏    | 1983/3844 [4:06:19<3:40:50,  7.12s/it]
{'loss': 1.0631, 'grad_norm': 0.3604412013947376, 'learning_rate': 9.974718970168152e-06, 'epoch': 0.52}

 52%|█████▏    | 1984/3844 [4:06:25<3:39:13,  7.07s/it]

 52%|█████▏    | 1985/3844 [4:06:31<3:28:56,  6.74s/it]


 52%|█████▏    | 1987/3844 [4:06:46<3:37:50,  7.04s/it]
{'loss': 1.0704, 'grad_norm': 0.34737932369826163, 'learning_rate': 9.94101120966461e-06, 'epoch': 0.52}


 52%|█████▏    | 1989/3844 [4:07:00<3:33:08,  6.89s/it]

 52%|█████▏    | 1990/3844 [4:07:06<3:28:38,  6.75s/it]

 52%|█████▏    | 1991/3844 [4:07:13<3:24:35,  6.62s/it]
{'loss': 1.1538, 'grad_norm': 0.3723373432607885, 'learning_rate': 9.90730411941134e-06, 'epoch': 0.52}


 52%|█████▏    | 1993/3844 [4:07:27<3:24:21,  6.62s/it]
{'loss': 1.1716, 'grad_norm': 0.3348561890596019, 'learning_rate': 9.89045094531372e-06, 'epoch': 0.52}


 52%|█████▏    | 1995/3844 [4:07:41<3:36:20,  7.02s/it]

 52%|█████▏    | 1996/3844 [4:07:49<3:47:42,  7.39s/it]
{'loss': 1.0672, 'grad_norm': 0.34014140363434525, 'learning_rate': 9.865171782596253e-06, 'epoch': 0.52}

 52%|█████▏    | 1997/3844 [4:07:56<3:41:06,  7.18s/it]


 52%|█████▏    | 1999/3844 [4:08:07<3:17:16,  6.42s/it]

 52%|█████▏    | 2000/3844 [4:08:14<3:22:42,  6.60s/it]

 52%|█████▏    | 2001/3844 [4:08:21<3:21:12,  6.55s/it]
{'loss': 1.0912, 'grad_norm': 0.3152756629940684, 'learning_rate': 9.823041839472215e-06, 'epoch': 0.52}


 52%|█████▏    | 2003/3844 [4:08:41<4:19:32,  8.46s/it]
[2024-05-27 14:54:23,183] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 2004/3844 [4:08:48<4:08:11,  8.09s/it]

 52%|█████▏    | 2005/3844 [4:08:56<4:09:47,  8.15s/it]

 52%|█████▏    | 2006/3844 [4:09:03<3:56:58,  7.74s/it]

 52%|█████▏    | 2007/3844 [4:09:10<3:49:59,  7.51s/it]

 52%|█████▏    | 2008/3844 [4:09:17<3:44:15,  7.33s/it]
{'loss': 0.9517, 'grad_norm': 0.3424781676979867, 'learning_rate': 9.764065364604419e-06, 'epoch': 0.52}

 52%|█████▏    | 2009/3844 [4:09:23<3:34:37,  7.02s/it]


 52%|█████▏    | 2011/3844 [4:09:42<4:15:25,  8.36s/it]
[2024-05-27 14:55:24,473] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 2012/3844 [4:09:49<3:59:35,  7.85s/it]

 52%|█████▏    | 2013/3844 [4:09:55<3:41:46,  7.27s/it]

 52%|█████▏    | 2014/3844 [4:10:01<3:33:52,  7.01s/it]

 52%|█████▏    | 2015/3844 [4:10:10<3:52:35,  7.63s/it]

 52%|█████▏    | 2016/3844 [4:10:17<3:45:48,  7.41s/it]
{'loss': 1.0909, 'grad_norm': 0.33248173425672345, 'learning_rate': 9.696673851550907e-06, 'epoch': 0.52}

 52%|█████▏    | 2017/3844 [4:10:23<3:34:47,  7.05s/it]

 52%|█████▏    | 2018/3844 [4:10:30<3:31:38,  6.95s/it]

 53%|█████▎    | 2019/3844 [4:10:36<3:20:21,  6.59s/it]


 53%|█████▎    | 2021/3844 [4:10:47<3:05:58,  6.12s/it]

 53%|█████▎    | 2022/3844 [4:10:53<3:01:57,  5.99s/it]

 53%|█████▎    | 2023/3844 [4:10:59<3:01:06,  5.97s/it]

 53%|█████▎    | 2024/3844 [4:11:05<3:01:28,  5.98s/it]

 53%|█████▎    | 2025/3844 [4:11:11<3:04:30,  6.09s/it]

 53%|█████▎    | 2026/3844 [4:11:19<3:21:41,  6.66s/it]

 53%|█████▎    | 2027/3844 [4:11:25<3:14:16,  6.42s/it]

 53%|█████▎    | 2028/3844 [4:11:35<3:46:32,  7.49s/it]

 53%|█████▎    | 2029/3844 [4:11:40<3:28:48,  6.90s/it]

 53%|█████▎    | 2030/3844 [4:11:49<3:42:24,  7.36s/it]

 53%|█████▎    | 2031/3844 [4:11:56<3:43:44,  7.40s/it]

 53%|█████▎    | 2032/3844 [4:12:02<3:30:08,  6.96s/it]

 53%|█████▎    | 2033/3844 [4:12:13<4:05:46,  8.14s/it]

 53%|█████▎    | 2034/3844 [4:12:21<3:59:23,  7.94s/it]

 53%|█████▎    | 2035/3844 [4:12:31<4:18:56,  8.59s/it]

 53%|█████▎    | 2036/3844 [4:12:37<3:57:40,  7.89s/it]
{'loss': 1.2665, 'grad_norm': 0.33345554509696684, 'learning_rate': 9.528262080864495e-06, 'epoch': 0.53}


 53%|█████▎    | 2038/3844 [4:12:49<3:29:51,  6.97s/it]
{'loss': 1.1282, 'grad_norm': 0.35978485510042674, 'learning_rate': 9.5114274846512e-06, 'epoch': 0.53}

 53%|█████▎    | 2039/3844 [4:12:56<3:21:58,  6.71s/it]


 53%|█████▎    | 2041/3844 [4:13:09<3:20:13,  6.66s/it]

 53%|█████▎    | 2042/3844 [4:13:15<3:18:18,  6.60s/it]
{'loss': 1.0632, 'grad_norm': 0.34986573948606653, 'learning_rate': 9.477762503536402e-06, 'epoch': 0.53}


 53%|█████▎    | 2044/3844 [4:13:30<3:31:01,  7.03s/it]

 53%|█████▎    | 2045/3844 [4:13:39<3:41:33,  7.39s/it]
{'loss': 1.2135, 'grad_norm': 0.3442509396801791, 'learning_rate': 9.452517640864197e-06, 'epoch': 0.53}


 53%|█████▎    | 2047/3844 [4:13:53<3:36:36,  7.23s/it]
{'loss': 1.0998, 'grad_norm': 0.3560719477390712, 'learning_rate': 9.435689666419538e-06, 'epoch': 0.53}


 53%|█████▎    | 2049/3844 [4:14:07<3:39:35,  7.34s/it]

 53%|█████▎    | 2050/3844 [4:14:17<3:59:08,  8.00s/it]

 53%|█████▎    | 2051/3844 [4:14:23<3:42:23,  7.44s/it]

 53%|█████▎    | 2052/3844 [4:14:30<3:40:15,  7.37s/it]

 53%|█████▎    | 2053/3844 [4:14:37<3:29:48,  7.03s/it]

 53%|█████▎    | 2054/3844 [4:14:45<3:41:34,  7.43s/it]

 53%|█████▎    | 2055/3844 [4:14:51<3:27:34,  6.96s/it]

 53%|█████▎    | 2056/3844 [4:14:57<3:19:24,  6.69s/it]

 54%|█████▎    | 2057/3844 [4:15:05<3:28:03,  6.99s/it]

 54%|█████▎    | 2058/3844 [4:15:11<3:19:58,  6.72s/it]
{'loss': 1.0355, 'grad_norm': 0.36737604517891004, 'learning_rate': 9.343165741529927e-06, 'epoch': 0.54}


 54%|█████▎    | 2060/3844 [4:15:27<3:36:00,  7.27s/it]

 54%|█████▎    | 2061/3844 [4:15:35<3:46:30,  7.62s/it]

 54%|█████▎    | 2062/3844 [4:15:42<3:35:39,  7.26s/it]

 54%|█████▎    | 2063/3844 [4:15:53<4:12:30,  8.51s/it]
{'loss': 0.9748, 'grad_norm': 0.3081645408481605, 'learning_rate': 9.301127592158516e-06, 'epoch': 0.54}


 54%|█████▎    | 2065/3844 [4:16:05<3:36:38,  7.31s/it]

 54%|█████▎    | 2066/3844 [4:16:19<4:33:03,  9.21s/it]

 54%|█████▍    | 2067/3844 [4:16:27<4:24:59,  8.95s/it]
{'loss': 0.9887, 'grad_norm': 0.3214527713130307, 'learning_rate': 9.267505970263662e-06, 'epoch': 0.54}


 54%|█████▍    | 2069/3844 [4:16:41<3:50:56,  7.81s/it]

 54%|█████▍    | 2070/3844 [4:16:47<3:33:37,  7.23s/it]
{'loss': 1.2149, 'grad_norm': 0.3517792517702284, 'learning_rate': 9.242295194818729e-06, 'epoch': 0.54}


 54%|█████▍    | 2072/3844 [4:17:01<3:34:36,  7.27s/it]

 54%|█████▍    | 2073/3844 [4:17:07<3:27:37,  7.03s/it]
{'loss': 1.1076, 'grad_norm': 0.34246355838939047, 'learning_rate': 9.21708926210381e-06, 'epoch': 0.54}


 54%|█████▍    | 2075/3844 [4:17:21<3:22:23,  6.86s/it]

 54%|█████▍    | 2076/3844 [4:17:28<3:22:10,  6.86s/it]

 54%|█████▍    | 2077/3844 [4:17:33<3:10:41,  6.47s/it]

 54%|█████▍    | 2078/3844 [4:17:39<3:09:20,  6.43s/it]

 54%|█████▍    | 2079/3844 [4:17:45<3:00:52,  6.15s/it]

 54%|█████▍    | 2080/3844 [4:17:53<3:17:56,  6.73s/it]

 54%|█████▍    | 2081/3844 [4:18:04<3:52:43,  7.92s/it]

 54%|█████▍    | 2082/3844 [4:18:14<4:10:05,  8.52s/it]

 54%|█████▍    | 2083/3844 [4:18:23<4:17:40,  8.78s/it]
{'loss': 1.1352, 'grad_norm': 0.3306491347498727, 'learning_rate': 9.133106529850461e-06, 'epoch': 0.54}


 54%|█████▍    | 2085/3844 [4:18:37<3:51:33,  7.90s/it]

 54%|█████▍    | 2086/3844 [4:18:45<3:47:04,  7.75s/it]

 54%|█████▍    | 2087/3844 [4:18:53<3:52:08,  7.93s/it]

 54%|█████▍    | 2088/3844 [4:19:00<3:39:25,  7.50s/it]

 54%|█████▍    | 2089/3844 [4:19:06<3:26:09,  7.05s/it]

 54%|█████▍    | 2090/3844 [4:19:15<3:50:45,  7.89s/it]

 54%|█████▍    | 2091/3844 [4:19:21<3:34:45,  7.35s/it]
{'loss': 1.1226, 'grad_norm': 0.3449179020709236, 'learning_rate': 9.065964382395315e-06, 'epoch': 0.54}


 54%|█████▍    | 2093/3844 [4:19:35<3:27:03,  7.10s/it]
{'loss': 1.111, 'grad_norm': 0.34798894215523163, 'learning_rate': 9.049185359367337e-06, 'epoch': 0.54}

 54%|█████▍    | 2094/3844 [4:19:43<3:28:45,  7.16s/it]

 55%|█████▍    | 2095/3844 [4:19:50<3:31:41,  7.26s/it]

 55%|█████▍    | 2096/3844 [4:19:56<3:23:46,  6.99s/it]

 55%|█████▍    | 2097/3844 [4:20:05<3:33:46,  7.34s/it]

 55%|█████▍    | 2098/3844 [4:20:15<3:58:13,  8.19s/it]

 55%|█████▍    | 2099/3844 [4:20:20<3:33:36,  7.34s/it]

 55%|█████▍    | 2100/3844 [4:20:27<3:26:48,  7.11s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 55%|█████▍    | 2101/3844 [4:21:06<8:04:17, 16.67s/it]

 55%|█████▍    | 2102/3844 [4:21:13<6:41:32, 13.83s/it]
{'loss': 0.9926, 'grad_norm': 0.3382885239736705, 'learning_rate': 8.973713866950221e-06, 'epoch': 0.55}


 55%|█████▍    | 2104/3844 [4:21:34<5:49:48, 12.06s/it]

 55%|█████▍    | 2105/3844 [4:21:40<4:58:23, 10.30s/it]

 55%|█████▍    | 2106/3844 [4:21:46<4:20:09,  8.98s/it]
{'loss': 1.1321, 'grad_norm': 0.3351567543244951, 'learning_rate': 8.940189672559094e-06, 'epoch': 0.55}


 55%|█████▍    | 2108/3844 [4:22:00<3:52:28,  8.03s/it]
{'loss': 1.0901, 'grad_norm': 0.34202312938345675, 'learning_rate': 8.923432067280914e-06, 'epoch': 0.55}

 55%|█████▍    | 2109/3844 [4:22:07<3:39:23,  7.59s/it]

 55%|█████▍    | 2110/3844 [4:22:12<3:22:51,  7.02s/it]

 55%|█████▍    | 2111/3844 [4:22:18<3:14:32,  6.74s/it]


 55%|█████▍    | 2113/3844 [4:22:31<3:09:15,  6.56s/it]
{'loss': 1.0842, 'grad_norm': 0.3461258469592682, 'learning_rate': 8.881551537305188e-06, 'epoch': 0.55}


 55%|█████▌    | 2115/3844 [4:22:50<3:47:51,  7.91s/it]

 55%|█████▌    | 2116/3844 [4:22:55<3:29:47,  7.28s/it]

 55%|█████▌    | 2117/3844 [4:23:01<3:18:59,  6.91s/it]

 55%|█████▌    | 2118/3844 [4:23:07<3:10:31,  6.62s/it]
{'loss': 1.1665, 'grad_norm': 0.32309025384715645, 'learning_rate': 8.83969086385602e-06, 'epoch': 0.55}

 55%|█████▌    | 2119/3844 [4:23:13<3:00:06,  6.26s/it]

 55%|█████▌    | 2120/3844 [4:23:18<2:53:56,  6.05s/it]

 55%|█████▌    | 2121/3844 [4:23:25<2:57:09,  6.17s/it]


 55%|█████▌    | 2123/3844 [4:23:39<3:12:23,  6.71s/it]
{'loss': 1.1448, 'grad_norm': 0.32792205678600905, 'learning_rate': 8.797850790112561e-06, 'epoch': 0.55}

 55%|█████▌    | 2124/3844 [4:23:45<3:01:31,  6.33s/it]


 55%|█████▌    | 2126/3844 [4:24:00<3:14:45,  6.80s/it]
{'loss': 1.1991, 'grad_norm': 0.3451017085330374, 'learning_rate': 8.77275694275122e-06, 'epoch': 0.55}


 55%|█████▌    | 2128/3844 [4:24:15<3:26:21,  7.22s/it]
{'loss': 1.1577, 'grad_norm': 0.3288665146539378, 'learning_rate': 8.756032058888243e-06, 'epoch': 0.55}

 55%|█████▌    | 2129/3844 [4:24:23<3:31:25,  7.40s/it]


 55%|█████▌    | 2131/3844 [4:24:38<3:35:03,  7.53s/it]

 55%|█████▌    | 2132/3844 [4:24:45<3:34:02,  7.50s/it]
{'loss': 1.0647, 'grad_norm': 0.3535664455018033, 'learning_rate': 8.722592939451626e-06, 'epoch': 0.55}

 55%|█████▌    | 2133/3844 [4:24:51<3:19:26,  6.99s/it]


 56%|█████▌    | 2135/3844 [4:25:13<4:19:05,  9.10s/it]

 56%|█████▌    | 2136/3844 [4:25:20<3:56:03,  8.29s/it]

 56%|█████▌    | 2137/3844 [4:25:26<3:40:43,  7.76s/it]
{'loss': 1.154, 'grad_norm': 0.3448547860753773, 'learning_rate': 8.680814495433704e-06, 'epoch': 0.56}


 56%|█████▌    | 2139/3844 [4:25:40<3:28:29,  7.34s/it]

 56%|█████▌    | 2140/3844 [4:25:45<3:12:24,  6.77s/it]

 56%|█████▌    | 2141/3844 [4:25:56<3:44:24,  7.91s/it]

 56%|█████▌    | 2142/3844 [4:26:03<3:40:20,  7.77s/it]

 56%|█████▌    | 2143/3844 [4:26:16<4:18:04,  9.10s/it]
{'loss': 1.1052, 'grad_norm': 0.3200934675762797, 'learning_rate': 8.63071134270168e-06, 'epoch': 0.56}

 56%|█████▌    | 2144/3844 [4:26:23<3:59:30,  8.45s/it]


 56%|█████▌    | 2146/3844 [4:26:37<3:44:00,  7.92s/it]

 56%|█████▌    | 2147/3844 [4:26:44<3:30:02,  7.43s/it]

 56%|█████▌    | 2148/3844 [4:26:52<3:36:04,  7.64s/it]
{'loss': 1.2173, 'grad_norm': 0.3275099137647338, 'learning_rate': 8.588985401912357e-06, 'epoch': 0.56}

 56%|█████▌    | 2149/3844 [4:26:59<3:30:23,  7.45s/it]

 56%|█████▌    | 2150/3844 [4:27:05<3:18:42,  7.04s/it]

 56%|█████▌    | 2151/3844 [4:27:11<3:06:36,  6.61s/it]


 56%|█████▌    | 2153/3844 [4:27:26<3:18:45,  7.05s/it]
{'loss': 1.0526, 'grad_norm': 0.3701241282643324, 'learning_rate': 8.547284511762189e-06, 'epoch': 0.56}

 56%|█████▌    | 2154/3844 [4:27:33<3:19:59,  7.10s/it]


 56%|█████▌    | 2156/3844 [4:27:48<3:27:28,  7.37s/it]

 56%|█████▌    | 2157/3844 [4:27:54<3:11:20,  6.81s/it]

 56%|█████▌    | 2158/3844 [4:27:59<3:00:05,  6.41s/it]
{'loss': 1.1024, 'grad_norm': 0.32532666378245956, 'learning_rate': 8.505609412593579e-06, 'epoch': 0.56}


 56%|█████▌    | 2160/3844 [4:28:14<3:15:01,  6.95s/it]
{'loss': 1.2433, 'grad_norm': 0.32143433969260365, 'learning_rate': 8.488946760150791e-06, 'epoch': 0.56}

 56%|█████▌    | 2161/3844 [4:28:21<3:15:31,  6.97s/it]

 56%|█████▌    | 2162/3844 [4:28:27<3:07:05,  6.67s/it]


 56%|█████▋    | 2164/3844 [4:28:44<3:36:11,  7.72s/it]
{'loss': 1.097, 'grad_norm': 0.35803125350169557, 'learning_rate': 8.455634379411314e-06, 'epoch': 0.56}


 56%|█████▋    | 2166/3844 [4:28:57<3:18:38,  7.10s/it]
{'loss': 1.0721, 'grad_norm': 0.33639964315760057, 'learning_rate': 8.438984745741281e-06, 'epoch': 0.56}


 56%|█████▋    | 2168/3844 [4:29:12<3:20:10,  7.17s/it]
{'loss': 0.9944, 'grad_norm': 0.3289098706751551, 'learning_rate': 8.422339546268145e-06, 'epoch': 0.56}


 56%|█████▋    | 2170/3844 [4:29:26<3:17:47,  7.09s/it]

 56%|█████▋    | 2171/3844 [4:29:35<3:36:38,  7.77s/it]

 57%|█████▋    | 2172/3844 [4:29:46<3:56:30,  8.49s/it]

 57%|█████▋    | 2173/3844 [4:29:52<3:43:27,  8.02s/it]

 57%|█████▋    | 2174/3844 [4:29:58<3:23:11,  7.30s/it]

 57%|█████▋    | 2175/3844 [4:30:07<3:40:03,  7.91s/it]
{'loss': 1.0045, 'grad_norm': 0.3499536666795563, 'learning_rate': 8.364116949932914e-06, 'epoch': 0.57}

 57%|█████▋    | 2176/3844 [4:30:13<3:21:51,  7.26s/it]

 57%|█████▋    | 2177/3844 [4:30:19<3:10:12,  6.85s/it]


 57%|█████▋    | 2179/3844 [4:30:36<3:27:47,  7.49s/it]
{'loss': 0.9928, 'grad_norm': 0.3322770084681107, 'learning_rate': 8.330872322709887e-06, 'epoch': 0.57}

 57%|█████▋    | 2180/3844 [4:30:45<3:42:55,  8.04s/it]

 57%|█████▋    | 2181/3844 [4:30:57<4:11:58,  9.09s/it]


 57%|█████▋    | 2183/3844 [4:31:10<3:34:50,  7.76s/it]

 57%|█████▋    | 2184/3844 [4:31:18<3:36:18,  7.82s/it]
{'loss': 1.158, 'grad_norm': 0.3387639516279088, 'learning_rate': 8.2893432527171e-06, 'epoch': 0.57}

 57%|█████▋    | 2185/3844 [4:31:23<3:15:09,  7.06s/it]

 57%|█████▋    | 2186/3844 [4:31:31<3:23:39,  7.37s/it]


 57%|█████▋    | 2188/3844 [4:31:48<3:32:36,  7.70s/it]

 57%|█████▋    | 2189/3844 [4:31:56<3:34:13,  7.77s/it]

 57%|█████▋    | 2190/3844 [4:32:07<4:02:14,  8.79s/it]
{'loss': 0.9424, 'grad_norm': 0.3444813880667947, 'learning_rate': 8.239548522466904e-06, 'epoch': 0.57}


 57%|█████▋    | 2192/3844 [4:32:20<3:32:11,  7.71s/it]

 57%|█████▋    | 2193/3844 [4:32:26<3:19:47,  7.26s/it]

 57%|█████▋    | 2194/3844 [4:32:34<3:19:58,  7.27s/it]
{'loss': 1.2211, 'grad_norm': 0.32664980259353515, 'learning_rate': 8.206376960614459e-06, 'epoch': 0.57}


 57%|█████▋    | 2196/3844 [4:32:49<3:20:53,  7.31s/it]

 57%|█████▋    | 2197/3844 [4:32:54<3:09:48,  6.91s/it]

 57%|█████▋    | 2198/3844 [4:33:00<3:01:03,  6.60s/it]

 57%|█████▋    | 2199/3844 [4:33:10<3:23:43,  7.43s/it]
{'loss': 1.1625, 'grad_norm': 0.34647715924547023, 'learning_rate': 8.164941211457927e-06, 'epoch': 0.57}


 57%|█████▋    | 2201/3844 [4:33:26<3:36:15,  7.90s/it]
{'loss': 1.1722, 'grad_norm': 0.3421458838897114, 'learning_rate': 8.1483759927293e-06, 'epoch': 0.57}


 57%|█████▋    | 2203/3844 [4:33:40<3:18:58,  7.27s/it]

 57%|█████▋    | 2204/3844 [4:33:47<3:15:46,  7.16s/it]
{'loss': 1.249, 'grad_norm': 0.3298089480767333, 'learning_rate': 8.123538041266622e-06, 'epoch': 0.57}


 57%|█████▋    | 2206/3844 [4:34:00<3:06:57,  6.85s/it]

 57%|█████▋    | 2207/3844 [4:34:09<3:23:29,  7.46s/it]

 57%|█████▋    | 2208/3844 [4:34:15<3:12:14,  7.05s/it]
{'loss': 1.1131, 'grad_norm': 0.3367543657976715, 'learning_rate': 8.090439455946483e-06, 'epoch': 0.57}


 57%|█████▋    | 2210/3844 [4:34:29<3:09:08,  6.95s/it]

 58%|█████▊    | 2211/3844 [4:34:36<3:11:39,  7.04s/it]
{'loss': 1.1725, 'grad_norm': 0.36434728770027003, 'learning_rate': 8.065629735096219e-06, 'epoch': 0.58}

 58%|█████▊    | 2212/3844 [4:34:42<3:00:02,  6.62s/it]

 58%|█████▊    | 2213/3844 [4:34:53<3:41:23,  8.14s/it]

 58%|█████▊    | 2214/3844 [4:35:01<3:36:55,  7.98s/it]


 58%|█████▊    | 2216/3844 [4:35:16<3:24:48,  7.55s/it]

 58%|█████▊    | 2217/3844 [4:35:22<3:17:07,  7.27s/it]

 58%|█████▊    | 2218/3844 [4:35:33<3:41:57,  8.19s/it]

 58%|█████▊    | 2219/3844 [4:35:41<3:41:42,  8.19s/it]

 58%|█████▊    | 2220/3844 [4:35:47<3:22:00,  7.46s/it]

 58%|█████▊    | 2221/3844 [4:35:52<3:06:28,  6.89s/it]

 58%|█████▊    | 2222/3844 [4:35:58<2:58:42,  6.61s/it]
{'loss': 1.2314, 'grad_norm': 0.3558650857802502, 'learning_rate': 7.97476773746414e-06, 'epoch': 0.58}

 58%|█████▊    | 2223/3844 [4:36:06<3:06:56,  6.92s/it]

 58%|█████▊    | 2224/3844 [4:36:13<3:11:52,  7.11s/it]

 58%|█████▊    | 2225/3844 [4:36:19<3:03:09,  6.79s/it]


 58%|█████▊    | 2227/3844 [4:36:31<2:45:14,  6.13s/it]

 58%|█████▊    | 2228/3844 [4:36:36<2:43:52,  6.08s/it]

 58%|█████▊    | 2229/3844 [4:36:44<2:56:20,  6.55s/it]
{'loss': 0.9944, 'grad_norm': 0.335255171278022, 'learning_rate': 7.917036580802258e-06, 'epoch': 0.58}

 58%|█████▊    | 2230/3844 [4:36:54<3:19:28,  7.42s/it]


 58%|█████▊    | 2232/3844 [4:37:13<3:43:32,  8.32s/it]

 58%|█████▊    | 2233/3844 [4:37:19<3:24:20,  7.61s/it]
{'loss': 1.2343, 'grad_norm': 0.33118815328988693, 'learning_rate': 7.88407976218472e-06, 'epoch': 0.58}

 58%|█████▊    | 2234/3844 [4:37:25<3:16:52,  7.34s/it]


 58%|█████▊    | 2236/3844 [4:37:40<3:23:21,  7.59s/it]

 58%|█████▊    | 2237/3844 [4:37:46<3:09:24,  7.07s/it]

 58%|█████▊    | 2238/3844 [4:37:52<2:59:48,  6.72s/it]
{'loss': 1.1931, 'grad_norm': 0.34434611904679274, 'learning_rate': 7.842917591537008e-06, 'epoch': 0.58}


 58%|█████▊    | 2240/3844 [4:38:08<3:13:05,  7.22s/it]

 58%|█████▊    | 2241/3844 [4:38:14<3:01:56,  6.81s/it]

 58%|█████▊    | 2242/3844 [4:38:21<3:02:01,  6.82s/it]

 58%|█████▊    | 2243/3844 [4:38:28<3:05:29,  6.95s/it]

 58%|█████▊    | 2244/3844 [4:38:35<3:03:30,  6.88s/it]

 58%|█████▊    | 2245/3844 [4:38:41<2:58:57,  6.72s/it]
{'loss': 1.1135, 'grad_norm': 0.3266338624320403, 'learning_rate': 7.785355053568217e-06, 'epoch': 0.58}


 58%|█████▊    | 2247/3844 [4:38:59<3:31:49,  7.96s/it]

 58%|█████▊    | 2248/3844 [4:39:06<3:25:24,  7.72s/it]
{'loss': 0.9841, 'grad_norm': 0.34569898575622177, 'learning_rate': 7.760708868505675e-06, 'epoch': 0.58}

 59%|█████▊    | 2249/3844 [4:39:15<3:34:37,  8.07s/it]


 59%|█████▊    | 2251/3844 [4:39:32<3:37:34,  8.19s/it]

 59%|█████▊    | 2252/3844 [4:39:39<3:21:35,  7.60s/it]

 59%|█████▊    | 2253/3844 [4:39:46<3:22:13,  7.63s/it]

 59%|█████▊    | 2254/3844 [4:39:52<3:08:10,  7.10s/it]

 59%|█████▊    | 2255/3844 [4:39:59<3:06:58,  7.06s/it]
{'loss': 1.0621, 'grad_norm': 0.3823655370807322, 'learning_rate': 7.703257033283052e-06, 'epoch': 0.59}


 59%|█████▊    | 2257/3844 [4:40:12<3:01:30,  6.86s/it]
{'loss': 0.9337, 'grad_norm': 0.3419763567391985, 'learning_rate': 7.686856815034516e-06, 'epoch': 0.59}

 59%|█████▊    | 2258/3844 [4:40:18<2:48:46,  6.39s/it]


 59%|█████▉    | 2260/3844 [4:40:30<2:49:45,  6.43s/it]

 59%|█████▉    | 2261/3844 [4:40:37<2:46:50,  6.32s/it]

 59%|█████▉    | 2262/3844 [4:40:43<2:45:43,  6.29s/it]
{'loss': 1.0306, 'grad_norm': 0.3468512155138124, 'learning_rate': 7.645885118001291e-06, 'epoch': 0.59}

 59%|█████▉    | 2263/3844 [4:40:50<2:53:28,  6.58s/it]

 59%|█████▉    | 2264/3844 [4:40:56<2:47:03,  6.34s/it]

 59%|█████▉    | 2265/3844 [4:41:01<2:40:52,  6.11s/it]


 59%|█████▉    | 2267/3844 [4:41:25<3:52:53,  8.86s/it]

 59%|█████▉    | 2268/3844 [4:41:35<4:03:16,  9.26s/it]

 59%|█████▉    | 2269/3844 [4:41:44<3:59:55,  9.14s/it]

 59%|█████▉    | 2270/3844 [4:41:50<3:34:49,  8.19s/it]

 59%|█████▉    | 2271/3844 [4:42:00<3:49:08,  8.74s/it]
{'loss': 1.146, 'grad_norm': 0.33069482651762844, 'learning_rate': 7.57224187280565e-06, 'epoch': 0.59}


 59%|█████▉    | 2273/3844 [4:42:12<3:13:30,  7.39s/it]

 59%|█████▉    | 2274/3844 [4:42:21<3:24:15,  7.81s/it]
{'loss': 1.176, 'grad_norm': 0.33654645884786694, 'learning_rate': 7.547724948448384e-06, 'epoch': 0.59}


 59%|█████▉    | 2276/3844 [4:42:35<3:11:39,  7.33s/it]

 59%|█████▉    | 2277/3844 [4:42:43<3:16:31,  7.52s/it]
{'loss': 1.2111, 'grad_norm': 0.32577925530514695, 'learning_rate': 7.523223697353208e-06, 'epoch': 0.59}


 59%|█████▉    | 2279/3844 [4:42:59<3:23:10,  7.79s/it]

 59%|█████▉    | 2280/3844 [4:43:05<3:07:46,  7.20s/it]
{'loss': 1.0871, 'grad_norm': 0.3781858897583973, 'learning_rate': 7.498738276115336e-06, 'epoch': 0.59}

 59%|█████▉    | 2281/3844 [4:43:14<3:23:12,  7.80s/it]


 59%|█████▉    | 2283/3844 [4:43:35<4:00:15,  9.23s/it]

 59%|█████▉    | 2284/3844 [4:43:47<4:22:33, 10.10s/it]

 59%|█████▉    | 2285/3844 [4:43:53<3:46:52,  8.73s/it]

 59%|█████▉    | 2286/3844 [4:44:01<3:44:16,  8.64s/it]

 59%|█████▉    | 2287/3844 [4:44:10<3:47:30,  8.77s/it]
{'loss': 1.1388, 'grad_norm': 0.37284151074309196, 'learning_rate': 7.44166806599648e-06, 'epoch': 0.59}


 60%|█████▉    | 2289/3844 [4:44:24<3:27:48,  8.02s/it]

 60%|█████▉    | 2290/3844 [4:44:31<3:16:15,  7.58s/it]

 60%|█████▉    | 2291/3844 [4:44:37<3:01:25,  7.01s/it]
{'loss': 1.2375, 'grad_norm': 0.3336626276287864, 'learning_rate': 7.409096359383271e-06, 'epoch': 0.6}


 60%|█████▉    | 2293/3844 [4:44:52<3:08:28,  7.29s/it]
{'loss': 1.1212, 'grad_norm': 0.36782202900972055, 'learning_rate': 7.392821522475151e-06, 'epoch': 0.6}

 60%|█████▉    | 2294/3844 [4:45:00<3:08:51,  7.31s/it]

 60%|█████▉    | 2295/3844 [4:45:06<2:57:34,  6.88s/it]


 60%|█████▉    | 2297/3844 [4:45:19<2:52:11,  6.68s/it]

 60%|█████▉    | 2298/3844 [4:45:25<2:48:12,  6.53s/it]
{'loss': 1.2026, 'grad_norm': 0.3588534244985992, 'learning_rate': 7.352166932149416e-06, 'epoch': 0.6}


 60%|█████▉    | 2300/3844 [4:45:42<3:14:54,  7.57s/it]

 60%|█████▉    | 2301/3844 [4:45:47<3:00:47,  7.03s/it]
{'loss': 1.0493, 'grad_norm': 0.3563863970779174, 'learning_rate': 7.327796695954046e-06, 'epoch': 0.6}


 60%|█████▉    | 2303/3844 [4:46:05<3:28:59,  8.14s/it]

 60%|█████▉    | 2304/3844 [4:46:11<3:15:22,  7.61s/it]

 60%|█████▉    | 2305/3844 [4:46:19<3:12:06,  7.49s/it]

 60%|█████▉    | 2306/3844 [4:46:25<3:01:37,  7.09s/it]

 60%|██████    | 2307/3844 [4:46:36<3:31:00,  8.24s/it]
{'loss': 1.1038, 'grad_norm': 0.31958153613570106, 'learning_rate': 7.279107615890426e-06, 'epoch': 0.6}


 60%|██████    | 2309/3844 [4:46:51<3:24:19,  7.99s/it]
{'loss': 1.0612, 'grad_norm': 0.3339531535841683, 'learning_rate': 7.262893318942062e-06, 'epoch': 0.6}

 60%|██████    | 2310/3844 [4:46:58<3:17:40,  7.73s/it]


 60%|██████    | 2312/3844 [4:47:15<3:36:27,  8.48s/it]

 60%|██████    | 2313/3844 [4:47:21<3:14:10,  7.61s/it]

 60%|██████    | 2314/3844 [4:47:27<3:01:17,  7.11s/it]
{'loss': 1.1729, 'grad_norm': 0.34906025156897086, 'learning_rate': 7.222391692827538e-06, 'epoch': 0.6}

 60%|██████    | 2315/3844 [4:47:34<3:04:00,  7.22s/it]


 60%|██████    | 2317/3844 [4:47:47<2:55:33,  6.90s/it]
{'loss': 1.037, 'grad_norm': 0.32248780327461735, 'learning_rate': 7.198114341245914e-06, 'epoch': 0.6}


 60%|██████    | 2319/3844 [4:48:04<3:09:06,  7.44s/it]
{'loss': 1.0453, 'grad_norm': 0.3225264092507252, 'learning_rate': 7.181939379359036e-06, 'epoch': 0.6}

 60%|██████    | 2320/3844 [4:48:11<3:05:03,  7.29s/it]


 60%|██████    | 2322/3844 [4:48:25<3:07:01,  7.37s/it]
{'loss': 1.2454, 'grad_norm': 0.33368375800575933, 'learning_rate': 7.1576919601458884e-06, 'epoch': 0.6}

 60%|██████    | 2323/3844 [4:48:31<2:54:14,  6.87s/it]


 60%|██████    | 2325/3844 [4:48:42<2:36:04,  6.16s/it]
{'loss': 1.1361, 'grad_norm': 0.3590157927613093, 'learning_rate': 7.133462707018484e-06, 'epoch': 0.6}

 61%|██████    | 2326/3844 [4:48:52<3:12:17,  7.60s/it]

 61%|██████    | 2327/3844 [4:49:00<3:13:17,  7.65s/it]


 61%|██████    | 2329/3844 [4:49:18<3:30:15,  8.33s/it]

 61%|██████    | 2330/3844 [4:49:24<3:13:06,  7.65s/it]

 61%|██████    | 2331/3844 [4:49:31<3:10:33,  7.56s/it]
{'loss': 1.0087, 'grad_norm': 0.3256356571273798, 'learning_rate': 7.085059318330958e-06, 'epoch': 0.61}


 61%|██████    | 2333/3844 [4:49:47<3:18:47,  7.89s/it]
{'loss': 1.0025, 'grad_norm': 0.3354700839058042, 'learning_rate': 7.068941354624994e-06, 'epoch': 0.61}

 61%|██████    | 2334/3844 [4:49:54<3:12:15,  7.64s/it]


 61%|██████    | 2336/3844 [4:50:13<3:27:17,  8.25s/it]

 61%|██████    | 2337/3844 [4:50:19<3:13:59,  7.72s/it]

 61%|██████    | 2338/3844 [4:50:27<3:11:13,  7.62s/it]

 61%|██████    | 2339/3844 [4:50:37<3:29:40,  8.36s/it]

 61%|██████    | 2340/3844 [4:50:44<3:16:45,  7.85s/it]

 61%|██████    | 2341/3844 [4:50:49<3:01:30,  7.25s/it]
{'loss': 1.2002, 'grad_norm': 0.3510548304704665, 'learning_rate': 7.004553216509447e-06, 'epoch': 0.61}

 61%|██████    | 2342/3844 [4:50:57<3:02:11,  7.28s/it]

 61%|██████    | 2343/3844 [4:51:04<3:04:53,  7.39s/it]

 61%|██████    | 2344/3844 [4:51:12<3:07:45,  7.51s/it]

 61%|██████    | 2345/3844 [4:51:18<2:58:03,  7.13s/it]


 61%|██████    | 2347/3844 [4:51:33<3:05:39,  7.44s/it]

 61%|██████    | 2348/3844 [4:51:39<2:54:56,  7.02s/it]
{'loss': 1.1681, 'grad_norm': 0.35028184483418623, 'learning_rate': 6.948325173677826e-06, 'epoch': 0.61}

 61%|██████    | 2349/3844 [4:51:46<2:55:06,  7.03s/it]


 61%|██████    | 2351/3844 [4:52:02<3:09:37,  7.62s/it]

 61%|██████    | 2352/3844 [4:52:13<3:34:10,  8.61s/it]

 61%|██████    | 2353/3844 [4:52:19<3:16:30,  7.91s/it]

 61%|██████    | 2354/3844 [4:52:27<3:17:02,  7.93s/it]

 61%|██████▏   | 2355/3844 [4:52:34<3:05:17,  7.47s/it]

 61%|██████▏   | 2356/3844 [4:52:43<3:20:08,  8.07s/it]

 61%|██████▏   | 2357/3844 [4:52:50<3:12:13,  7.76s/it]
{'loss': 1.0545, 'grad_norm': 0.32546807944570294, 'learning_rate': 6.876188282482823e-06, 'epoch': 0.61}


 61%|██████▏   | 2359/3844 [4:53:02<2:48:51,  6.82s/it]
{'loss': 1.2267, 'grad_norm': 0.3674751492265902, 'learning_rate': 6.860182118210337e-06, 'epoch': 0.61}


 61%|██████▏   | 2361/3844 [4:53:14<2:37:11,  6.36s/it]
{'loss': 1.2376, 'grad_norm': 0.3512155887950094, 'learning_rate': 6.8441848728581975e-06, 'epoch': 0.61}


 61%|██████▏   | 2363/3844 [4:53:27<2:40:37,  6.51s/it]

 61%|██████▏   | 2364/3844 [4:53:33<2:34:38,  6.27s/it]
{'loss': 1.1567, 'grad_norm': 0.3765493212797552, 'learning_rate': 6.820205827202029e-06, 'epoch': 0.61}


 62%|██████▏   | 2366/3844 [4:53:46<2:39:51,  6.49s/it]

 62%|██████▏   | 2367/3844 [4:53:55<2:57:29,  7.21s/it]

 62%|██████▏   | 2368/3844 [4:54:04<3:11:31,  7.79s/it]

 62%|██████▏   | 2369/3844 [4:54:12<3:12:06,  7.81s/it]

 62%|██████▏   | 2370/3844 [4:54:19<3:08:00,  7.65s/it]
{'loss': 1.2025, 'grad_norm': 0.33656747934240566, 'learning_rate': 6.772308858215118e-06, 'epoch': 0.62}


 62%|██████▏   | 2372/3844 [4:54:35<3:16:00,  7.99s/it]

 62%|██████▏   | 2373/3844 [4:54:43<3:17:10,  8.04s/it]
{'loss': 1.1276, 'grad_norm': 0.3245789618936361, 'learning_rate': 6.748391241008985e-06, 'epoch': 0.62}

 62%|██████▏   | 2374/3844 [4:54:51<3:11:47,  7.83s/it]

 62%|██████▏   | 2375/3844 [4:54:57<3:00:37,  7.38s/it]


 62%|██████▏   | 2377/3844 [4:55:09<2:46:52,  6.82s/it]
{'loss': 1.1142, 'grad_norm': 0.33564522095654914, 'learning_rate': 6.716533438778079e-06, 'epoch': 0.62}

 62%|██████▏   | 2378/3844 [4:55:20<3:18:42,  8.13s/it]

 62%|██████▏   | 2379/3844 [4:55:29<3:19:07,  8.16s/it]


 62%|██████▏   | 2381/3844 [4:55:43<3:09:18,  7.76s/it]
{'loss': 1.0489, 'grad_norm': 0.3267084354921786, 'learning_rate': 6.684712944387315e-06, 'epoch': 0.62}


 62%|██████▏   | 2383/3844 [4:55:58<3:09:34,  7.79s/it]

 62%|██████▏   | 2384/3844 [4:56:06<3:07:06,  7.69s/it]
{'loss': 1.2237, 'grad_norm': 0.35178143893975117, 'learning_rate': 6.6608722743783536e-06, 'epoch': 0.62}

 62%|██████▏   | 2385/3844 [4:56:11<2:49:35,  6.97s/it]


 62%|██████▏   | 2387/3844 [4:56:24<2:47:00,  6.88s/it]

 62%|██████▏   | 2388/3844 [4:56:32<2:54:21,  7.19s/it]
{'loss': 1.2217, 'grad_norm': 0.3296722421919986, 'learning_rate': 6.6291179384423674e-06, 'epoch': 0.62}


 62%|██████▏   | 2390/3844 [4:56:45<2:47:45,  6.92s/it]

 62%|██████▏   | 2391/3844 [4:56:51<2:40:40,  6.63s/it]

 62%|██████▏   | 2392/3844 [4:56:58<2:37:38,  6.51s/it]
{'loss': 1.1139, 'grad_norm': 0.33404719916260667, 'learning_rate': 6.597401903590543e-06, 'epoch': 0.62}


 62%|██████▏   | 2394/3844 [4:57:12<2:47:36,  6.94s/it]
{'loss': 0.9799, 'grad_norm': 0.3399679526469441, 'learning_rate': 6.581558361703349e-06, 'epoch': 0.62}


 62%|██████▏   | 2396/3844 [4:57:28<2:56:31,  7.31s/it]
{'loss': 1.2385, 'grad_norm': 0.3208429957066102, 'learning_rate': 6.565724530191027e-06, 'epoch': 0.62}


 62%|██████▏   | 2398/3844 [4:57:40<2:41:12,  6.69s/it]
{'loss': 1.1633, 'grad_norm': 0.36144582708546624, 'learning_rate': 6.549900454030926e-06, 'epoch': 0.62}


 62%|██████▏   | 2400/3844 [4:57:54<2:41:40,  6.72s/it]
 62%|██████▏   | 2400/3844 [4:57:54<2:41:40,  6.72s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 62%|██████▏   | 2401/3844 [4:58:24<5:32:52, 13.84s/it]

 62%|██████▏   | 2402/3844 [4:58:30<4:34:26, 11.42s/it]

 63%|██████▎   | 2403/3844 [4:58:36<3:57:28,  9.89s/it]

 63%|██████▎   | 2404/3844 [4:58:42<3:27:43,  8.66s/it]

 63%|██████▎   | 2405/3844 [4:58:50<3:20:19,  8.35s/it]
{'loss': 1.0691, 'grad_norm': 0.34139072502209217, 'learning_rate': 6.494593659578144e-06, 'epoch': 0.63}

 63%|██████▎   | 2406/3844 [4:59:01<3:37:55,  9.09s/it]

 63%|██████▎   | 2407/3844 [4:59:11<3:47:19,  9.49s/it]


 63%|██████▎   | 2409/3844 [4:59:28<3:33:58,  8.95s/it]

 63%|██████▎   | 2410/3844 [4:59:34<3:13:27,  8.09s/it]
{'loss': 1.1264, 'grad_norm': 0.34108218775997395, 'learning_rate': 6.455163374690777e-06, 'epoch': 0.63}

 63%|██████▎   | 2411/3844 [4:59:43<3:21:48,  8.45s/it]

 63%|██████▎   | 2412/3844 [4:59:49<3:02:36,  7.65s/it]


 63%|██████▎   | 2414/3844 [5:00:02<2:47:24,  7.02s/it]

 63%|██████▎   | 2415/3844 [5:00:07<2:39:18,  6.69s/it]
{'loss': 1.0903, 'grad_norm': 0.3397706117074509, 'learning_rate': 6.415796023541492e-06, 'epoch': 0.63}

 63%|██████▎   | 2416/3844 [5:00:15<2:45:44,  6.96s/it]


 63%|██████▎   | 2418/3844 [5:00:28<2:39:31,  6.71s/it]
{'loss': 1.3128, 'grad_norm': 0.3645157578467875, 'learning_rate': 6.392206111841116e-06, 'epoch': 0.63}


 63%|██████▎   | 2420/3844 [5:00:42<2:43:34,  6.89s/it]
{'loss': 1.0413, 'grad_norm': 0.33381323193264356, 'learning_rate': 6.3764923050439e-06, 'epoch': 0.63}


 63%|██████▎   | 2422/3844 [5:00:58<2:53:02,  7.30s/it]

 63%|██████▎   | 2423/3844 [5:01:06<2:58:52,  7.55s/it]
{'loss': 1.0212, 'grad_norm': 0.32817154316738956, 'learning_rate': 6.352940907943632e-06, 'epoch': 0.63}

 63%|██████▎   | 2424/3844 [5:01:17<3:24:28,  8.64s/it]


 63%|██████▎   | 2426/3844 [5:01:34<3:18:28,  8.40s/it]
{'loss': 1.1515, 'grad_norm': 0.3720262071417191, 'learning_rate': 6.329412820346497e-06, 'epoch': 0.63}

 63%|██████▎   | 2427/3844 [5:01:41<3:06:54,  7.91s/it]


 63%|██████▎   | 2429/3844 [5:02:01<3:29:07,  8.87s/it]

 63%|██████▎   | 2430/3844 [5:02:08<3:17:46,  8.39s/it]
{'loss': 1.1564, 'grad_norm': 0.3566680309334029, 'learning_rate': 6.2980785559973035e-06, 'epoch': 0.63}


 63%|██████▎   | 2432/3844 [5:02:23<3:06:43,  7.93s/it]
{'loss': 0.9152, 'grad_norm': 0.3219065869378685, 'learning_rate': 6.282427175013371e-06, 'epoch': 0.63}

 63%|██████▎   | 2433/3844 [5:02:29<2:58:13,  7.58s/it]


 63%|██████▎   | 2435/3844 [5:02:43<2:45:31,  7.05s/it]

 63%|██████▎   | 2436/3844 [5:02:50<2:44:09,  7.00s/it]
{'loss': 1.0966, 'grad_norm': 0.31890069508709856, 'learning_rate': 6.251156137722102e-06, 'epoch': 0.63}


 63%|██████▎   | 2438/3844 [5:03:02<2:36:10,  6.66s/it]

 63%|██████▎   | 2439/3844 [5:03:12<2:56:51,  7.55s/it]
{'loss': 1.0819, 'grad_norm': 0.33180919271689563, 'learning_rate': 6.227730793711044e-06, 'epoch': 0.63}


 64%|██████▎   | 2441/3844 [5:03:26<2:48:37,  7.21s/it]
{'loss': 1.0745, 'grad_norm': 0.3503235368659146, 'learning_rate': 6.212127282779443e-06, 'epoch': 0.63}

 64%|██████▎   | 2442/3844 [5:03:31<2:36:36,  6.70s/it]

 64%|██████▎   | 2443/3844 [5:03:37<2:34:17,  6.61s/it]

 64%|██████▎   | 2444/3844 [5:03:43<2:27:46,  6.33s/it]

 64%|██████▎   | 2445/3844 [5:03:51<2:41:00,  6.91s/it]

 64%|██████▎   | 2446/3844 [5:03:58<2:35:32,  6.68s/it]


 64%|██████▎   | 2448/3844 [5:04:17<3:10:06,  8.17s/it]

 64%|██████▎   | 2449/3844 [5:04:22<2:51:38,  7.38s/it]

 64%|██████▎   | 2450/3844 [5:04:29<2:43:46,  7.05s/it]

 64%|██████▍   | 2451/3844 [5:04:35<2:38:37,  6.83s/it]

 64%|██████▍   | 2452/3844 [5:04:43<2:45:49,  7.15s/it]
{'loss': 1.1109, 'grad_norm': 0.3175787706512212, 'learning_rate': 6.1265014905121e-06, 'epoch': 0.64}


 64%|██████▍   | 2454/3844 [5:04:56<2:41:21,  6.97s/it]

 64%|██████▍   | 2455/3844 [5:05:02<2:30:29,  6.50s/it]

 64%|██████▍   | 2456/3844 [5:05:08<2:27:43,  6.39s/it]
{'loss': 1.1258, 'grad_norm': 0.33253443703435615, 'learning_rate': 6.0954469746598756e-06, 'epoch': 0.64}

 64%|██████▍   | 2457/3844 [5:05:14<2:23:00,  6.19s/it]


 64%|██████▍   | 2459/3844 [5:05:29<2:37:16,  6.81s/it]

 64%|██████▍   | 2460/3844 [5:05:35<2:32:15,  6.60s/it]

 64%|██████▍   | 2461/3844 [5:05:40<2:25:04,  6.29s/it]

 64%|██████▍   | 2462/3844 [5:05:48<2:35:26,  6.75s/it]
{'loss': 1.1316, 'grad_norm': 0.3370944448411928, 'learning_rate': 6.04894849506869e-06, 'epoch': 0.64}

 64%|██████▍   | 2463/3844 [5:05:55<2:39:30,  6.93s/it]


 64%|██████▍   | 2465/3844 [5:06:10<2:48:36,  7.34s/it]
{'loss': 1.1573, 'grad_norm': 0.3270486970774998, 'learning_rate': 6.025737059638171e-06, 'epoch': 0.64}

 64%|██████▍   | 2466/3844 [5:06:19<2:58:05,  7.75s/it]

 64%|██████▍   | 2467/3844 [5:06:25<2:45:38,  7.22s/it]


 64%|██████▍   | 2469/3844 [5:06:43<3:05:45,  8.11s/it]

 64%|██████▍   | 2470/3844 [5:06:49<2:50:48,  7.46s/it]

 64%|██████▍   | 2471/3844 [5:06:55<2:40:06,  7.00s/it]
{'loss': 1.2097, 'grad_norm': 0.34586108955817213, 'learning_rate': 5.979390539264289e-06, 'epoch': 0.64}


 64%|██████▍   | 2473/3844 [5:07:07<2:28:18,  6.49s/it]
{'loss': 1.0655, 'grad_norm': 0.3577617678957768, 'learning_rate': 5.963964482428065e-06, 'epoch': 0.64}


 64%|██████▍   | 2475/3844 [5:07:19<2:21:12,  6.19s/it]
{'loss': 1.1042, 'grad_norm': 0.3583791733934443, 'learning_rate': 5.9485498902947745e-06, 'epoch': 0.64}


 64%|██████▍   | 2477/3844 [5:07:36<2:56:01,  7.73s/it]

 64%|██████▍   | 2478/3844 [5:07:44<2:54:51,  7.68s/it]

 64%|██████▍   | 2479/3844 [5:07:50<2:44:42,  7.24s/it]

 65%|██████▍   | 2480/3844 [5:07:59<2:56:46,  7.78s/it]

 65%|██████▍   | 2481/3844 [5:08:05<2:41:27,  7.11s/it]

 65%|██████▍   | 2482/3844 [5:08:11<2:36:18,  6.89s/it]
{'loss': 1.1055, 'grad_norm': 0.3507022866028809, 'learning_rate': 5.894689734235581e-06, 'epoch': 0.65}


 65%|██████▍   | 2484/3844 [5:08:25<2:37:56,  6.97s/it]

 65%|██████▍   | 2485/3844 [5:08:31<2:30:09,  6.63s/it]

 65%|██████▍   | 2486/3844 [5:08:36<2:22:08,  6.28s/it]

 65%|██████▍   | 2487/3844 [5:08:43<2:21:18,  6.25s/it]
{'loss': 1.1214, 'grad_norm': 0.33065999931916673, 'learning_rate': 5.856305546003202e-06, 'epoch': 0.65}


 65%|██████▍   | 2489/3844 [5:08:57<2:33:42,  6.81s/it]

 65%|██████▍   | 2490/3844 [5:09:03<2:27:06,  6.52s/it]

 65%|██████▍   | 2491/3844 [5:09:12<2:44:49,  7.31s/it]

 65%|██████▍   | 2492/3844 [5:09:20<2:52:02,  7.63s/it]

 65%|██████▍   | 2493/3844 [5:09:30<3:05:43,  8.25s/it]

 65%|██████▍   | 2494/3844 [5:09:40<3:17:41,  8.79s/it]

 65%|██████▍   | 2495/3844 [5:09:46<3:00:04,  8.01s/it]

 65%|██████▍   | 2496/3844 [5:09:53<2:50:14,  7.58s/it]

 65%|██████▍   | 2497/3844 [5:10:02<3:00:45,  8.05s/it]

 65%|██████▍   | 2498/3844 [5:10:09<2:54:03,  7.76s/it]
{'loss': 1.1666, 'grad_norm': 0.3353514724540234, 'learning_rate': 5.772120240513686e-06, 'epoch': 0.65}

 65%|██████▌   | 2499/3844 [5:10:16<2:45:05,  7.36s/it]


 65%|██████▌   | 2501/3844 [5:10:31<2:48:56,  7.55s/it]

 65%|██████▌   | 2502/3844 [5:10:38<2:46:23,  7.44s/it]
{'loss': 1.101, 'grad_norm': 0.3626535925650721, 'learning_rate': 5.741597094450158e-06, 'epoch': 0.65}

 65%|██████▌   | 2503/3844 [5:10:46<2:46:44,  7.46s/it]

 65%|██████▌   | 2504/3844 [5:10:52<2:36:26,  7.00s/it]


 65%|██████▌   | 2506/3844 [5:11:03<2:21:46,  6.36s/it]

 65%|██████▌   | 2507/3844 [5:11:09<2:18:19,  6.21s/it]

 65%|██████▌   | 2508/3844 [5:11:15<2:16:57,  6.15s/it]

 65%|██████▌   | 2509/3844 [5:11:20<2:11:37,  5.92s/it]

 65%|██████▌   | 2510/3844 [5:11:33<2:57:51,  8.00s/it]
{'loss': 1.1953, 'grad_norm': 0.3560700820005842, 'learning_rate': 5.6806963047618365e-06, 'epoch': 0.65}


 65%|██████▌   | 2512/3844 [5:11:46<2:39:32,  7.19s/it]

 65%|██████▌   | 2513/3844 [5:11:53<2:39:03,  7.17s/it]

 65%|██████▌   | 2514/3844 [5:12:02<2:50:06,  7.67s/it]

 65%|██████▌   | 2515/3844 [5:12:17<3:35:12,  9.72s/it]

 65%|██████▌   | 2516/3844 [5:12:23<3:14:27,  8.79s/it]

 65%|██████▌   | 2517/3844 [5:12:29<2:53:07,  7.83s/it]
{'loss': 1.1383, 'grad_norm': 0.351915212298226, 'learning_rate': 5.627569054048731e-06, 'epoch': 0.65}


 66%|██████▌   | 2519/3844 [5:12:43<2:47:00,  7.56s/it]

 66%|██████▌   | 2520/3844 [5:12:51<2:45:28,  7.50s/it]

 66%|██████▌   | 2521/3844 [5:12:57<2:39:56,  7.25s/it]

 66%|██████▌   | 2522/3844 [5:13:03<2:30:05,  6.81s/it]

 66%|██████▌   | 2523/3844 [5:13:09<2:21:21,  6.42s/it]
{'loss': 1.0592, 'grad_norm': 0.35566984655021416, 'learning_rate': 5.582152437896986e-06, 'epoch': 0.66}

 66%|██████▌   | 2524/3844 [5:13:14<2:14:48,  6.13s/it]

 66%|██████▌   | 2525/3844 [5:13:20<2:12:16,  6.02s/it]


 66%|██████▌   | 2527/3844 [5:13:35<2:31:47,  6.92s/it]
{'loss': 1.157, 'grad_norm': 0.33181025746938525, 'learning_rate': 5.551937368486492e-06, 'epoch': 0.66}

 66%|██████▌   | 2528/3844 [5:13:40<2:21:34,  6.45s/it]

 66%|██████▌   | 2529/3844 [5:13:46<2:15:12,  6.17s/it]


 66%|██████▌   | 2531/3844 [5:13:59<2:22:51,  6.53s/it]

 66%|██████▌   | 2532/3844 [5:14:06<2:26:52,  6.72s/it]

 66%|██████▌   | 2533/3844 [5:14:13<2:25:32,  6.66s/it]

 66%|██████▌   | 2534/3844 [5:14:21<2:32:56,  7.01s/it]

 66%|██████▌   | 2535/3844 [5:14:27<2:24:23,  6.62s/it]
{'loss': 1.1614, 'grad_norm': 0.35675325889255777, 'learning_rate': 5.491659193510109e-06, 'epoch': 0.66}


 66%|██████▌   | 2537/3844 [5:14:39<2:20:29,  6.45s/it]
{'loss': 1.1465, 'grad_norm': 0.35083220927412256, 'learning_rate': 5.476621558656485e-06, 'epoch': 0.66}


 66%|██████▌   | 2539/3844 [5:14:53<2:26:27,  6.73s/it]

 66%|██████▌   | 2540/3844 [5:15:01<2:33:10,  7.05s/it]

 66%|██████▌   | 2541/3844 [5:15:09<2:40:13,  7.38s/it]

 66%|██████▌   | 2542/3844 [5:15:15<2:30:56,  6.96s/it]

 66%|██████▌   | 2543/3844 [5:15:25<2:54:19,  8.04s/it]

 66%|██████▌   | 2544/3844 [5:15:31<2:41:53,  7.47s/it]

 66%|██████▌   | 2545/3844 [5:15:38<2:38:56,  7.34s/it]
{'loss': 1.1049, 'grad_norm': 0.33671607770603773, 'learning_rate': 5.416599936271116e-06, 'epoch': 0.66}

 66%|██████▌   | 2546/3844 [5:15:44<2:25:46,  6.74s/it]


 66%|██████▋   | 2548/3844 [5:16:03<2:56:42,  8.18s/it]

 66%|██████▋   | 2549/3844 [5:16:11<2:52:22,  7.99s/it]

 66%|██████▋   | 2550/3844 [5:16:19<2:55:15,  8.13s/it]
{'loss': 1.2089, 'grad_norm': 0.337097914351247, 'learning_rate': 5.3791920328886875e-06, 'epoch': 0.66}


 66%|██████▋   | 2552/3844 [5:16:35<2:47:57,  7.80s/it]

 66%|██████▋   | 2553/3844 [5:16:41<2:39:06,  7.39s/it]

 66%|██████▋   | 2554/3844 [5:16:51<2:52:08,  8.01s/it]
{'loss': 1.1525, 'grad_norm': 0.3441000116903926, 'learning_rate': 5.349324744375271e-06, 'epoch': 0.66}


 66%|██████▋   | 2556/3844 [5:17:05<2:43:48,  7.63s/it]
{'loss': 1.1568, 'grad_norm': 0.3824147284569601, 'learning_rate': 5.3344108948770205e-06, 'epoch': 0.66}


 67%|██████▋   | 2558/3844 [5:17:19<2:35:23,  7.25s/it]

 67%|██████▋   | 2559/3844 [5:17:26<2:28:19,  6.93s/it]

 67%|██████▋   | 2560/3844 [5:17:33<2:29:37,  6.99s/it]
{'loss': 1.1276, 'grad_norm': 0.35656993385980534, 'learning_rate': 5.304622997217627e-06, 'epoch': 0.67}


 67%|██████▋   | 2562/3844 [5:17:45<2:22:37,  6.67s/it]
{'loss': 1.2865, 'grad_norm': 0.35893181151844045, 'learning_rate': 5.289749033671542e-06, 'epoch': 0.67}


 67%|██████▋   | 2564/3844 [5:17:59<2:23:22,  6.72s/it]

 67%|██████▋   | 2565/3844 [5:18:05<2:17:42,  6.46s/it]

 67%|██████▋   | 2566/3844 [5:18:11<2:14:52,  6.33s/it]
{'loss': 0.9927, 'grad_norm': 0.3394260918266075, 'learning_rate': 5.2600412884002835e-06, 'epoch': 0.67}


 67%|██████▋   | 2568/3844 [5:18:27<2:35:33,  7.31s/it]
{'loss': 1.1503, 'grad_norm': 0.3770770477625353, 'learning_rate': 5.245207591062495e-06, 'epoch': 0.67}


 67%|██████▋   | 2570/3844 [5:18:42<2:34:40,  7.28s/it]

 67%|██████▋   | 2571/3844 [5:18:48<2:27:31,  6.95s/it]

 67%|██████▋   | 2572/3844 [5:18:54<2:20:06,  6.61s/it]

 67%|██████▋   | 2573/3844 [5:19:01<2:24:10,  6.81s/it]
{'loss': 1.2195, 'grad_norm': 0.3479679202523615, 'learning_rate': 5.2081825302669495e-06, 'epoch': 0.67}


 67%|██████▋   | 2575/3844 [5:19:19<2:50:29,  8.06s/it]

 67%|██████▋   | 2576/3844 [5:19:26<2:38:14,  7.49s/it]

 67%|██████▋   | 2577/3844 [5:19:33<2:35:22,  7.36s/it]
{'loss': 1.2724, 'grad_norm': 0.3553103843929505, 'learning_rate': 5.178623702100192e-06, 'epoch': 0.67}


 67%|██████▋   | 2579/3844 [5:19:43<2:13:45,  6.34s/it]

 67%|██████▋   | 2580/3844 [5:19:49<2:11:41,  6.25s/it]

 67%|██████▋   | 2581/3844 [5:19:55<2:08:40,  6.11s/it]

 67%|██████▋   | 2582/3844 [5:20:01<2:04:35,  5.92s/it]

 67%|██████▋   | 2583/3844 [5:20:09<2:22:09,  6.76s/it]
{'loss': 1.1148, 'grad_norm': 0.32629048719609965, 'learning_rate': 5.134388281040135e-06, 'epoch': 0.67}

 67%|██████▋   | 2584/3844 [5:20:19<2:37:24,  7.50s/it]


 67%|██████▋   | 2586/3844 [5:20:33<2:31:30,  7.23s/it]
{'loss': 1.185, 'grad_norm': 0.36358485988813305, 'learning_rate': 5.112317146363998e-06, 'epoch': 0.67}

 67%|██████▋   | 2587/3844 [5:20:39<2:23:08,  6.83s/it]


 67%|██████▋   | 2589/3844 [5:20:54<2:33:02,  7.32s/it]

 67%|██████▋   | 2590/3844 [5:20:59<2:24:02,  6.89s/it]
{'loss': 1.1215, 'grad_norm': 0.38089916699563786, 'learning_rate': 5.082937584713311e-06, 'epoch': 0.67}


 67%|██████▋   | 2592/3844 [5:21:13<2:23:54,  6.90s/it]

 67%|██████▋   | 2593/3844 [5:21:21<2:31:05,  7.25s/it]
{'loss': 1.0736, 'grad_norm': 0.340169675298683, 'learning_rate': 5.060939559471497e-06, 'epoch': 0.67}


 68%|██████▊   | 2595/3844 [5:21:40<2:54:50,  8.40s/it]

 68%|██████▊   | 2596/3844 [5:21:49<3:01:09,  8.71s/it]

 68%|██████▊   | 2597/3844 [5:21:56<2:50:28,  8.20s/it]
{'loss': 1.1874, 'grad_norm': 0.3647090102344592, 'learning_rate': 5.0316579877838144e-06, 'epoch': 0.68}

 68%|██████▊   | 2598/3844 [5:22:03<2:40:09,  7.71s/it]


 68%|██████▊   | 2600/3844 [5:22:15<2:24:35,  6.97s/it]
{'loss': 1.1815, 'grad_norm': 0.3495378564398759, 'learning_rate': 5.009733837443292e-06, 'epoch': 0.68}


 68%|██████▊   | 2602/3844 [5:22:35<2:52:23,  8.33s/it]

 68%|██████▊   | 2603/3844 [5:22:42<2:40:58,  7.78s/it]
{'loss': 1.1106, 'grad_norm': 0.3408807928588154, 'learning_rate': 4.987841581465276e-06, 'epoch': 0.68}

 68%|██████▊   | 2604/3844 [5:22:51<2:45:41,  8.02s/it]


 68%|██████▊   | 2606/3844 [5:23:04<2:30:06,  7.27s/it]

 68%|██████▊   | 2607/3844 [5:23:10<2:21:09,  6.85s/it]

 68%|██████▊   | 2608/3844 [5:23:15<2:13:22,  6.47s/it]
{'loss': 1.1468, 'grad_norm': 0.372252332823489, 'learning_rate': 4.9514257444675105e-06, 'epoch': 0.68}


 68%|██████▊   | 2610/3844 [5:23:29<2:18:18,  6.73s/it]

 68%|██████▊   | 2611/3844 [5:23:36<2:16:16,  6.63s/it]

 68%|██████▊   | 2612/3844 [5:23:43<2:19:55,  6.81s/it]

 68%|██████▊   | 2613/3844 [5:23:49<2:15:41,  6.61s/it]

 68%|██████▊   | 2614/3844 [5:23:57<2:22:15,  6.94s/it]

 68%|██████▊   | 2615/3844 [5:24:07<2:43:21,  7.98s/it]

 68%|██████▊   | 2616/3844 [5:24:14<2:34:12,  7.53s/it]

 68%|██████▊   | 2617/3844 [5:24:20<2:23:29,  7.02s/it]
{'loss': 1.129, 'grad_norm': 0.3572738748980946, 'learning_rate': 4.8861035402944204e-06, 'epoch': 0.68}

 68%|██████▊   | 2618/3844 [5:24:29<2:35:21,  7.60s/it]


 68%|██████▊   | 2620/3844 [5:24:40<2:14:21,  6.59s/it]

 68%|██████▊   | 2621/3844 [5:24:46<2:08:35,  6.31s/it]
{'loss': 1.2046, 'grad_norm': 0.34913213001975835, 'learning_rate': 4.857165648364932e-06, 'epoch': 0.68}


 68%|██████▊   | 2623/3844 [5:25:05<2:44:28,  8.08s/it]
{'loss': 1.0566, 'grad_norm': 0.35910100731421857, 'learning_rate': 4.842718594849446e-06, 'epoch': 0.68}

 68%|██████▊   | 2624/3844 [5:25:11<2:29:51,  7.37s/it]

 68%|██████▊   | 2625/3844 [5:25:17<2:22:35,  7.02s/it]

 68%|██████▊   | 2626/3844 [5:25:25<2:30:54,  7.43s/it]


 68%|██████▊   | 2628/3844 [5:25:40<2:28:23,  7.32s/it]
{'loss': 1.1616, 'grad_norm': 0.32962352114972754, 'learning_rate': 4.806665143155476e-06, 'epoch': 0.68}


 68%|██████▊   | 2630/3844 [5:25:54<2:23:00,  7.07s/it]
{'loss': 1.0128, 'grad_norm': 0.33672348316159484, 'learning_rate': 4.792269542849332e-06, 'epoch': 0.68}


 68%|██████▊   | 2632/3844 [5:26:08<2:21:55,  7.03s/it]
{'loss': 1.03, 'grad_norm': 0.33854750572333697, 'learning_rate': 4.777888735545477e-06, 'epoch': 0.68}

 68%|██████▊   | 2633/3844 [5:26:15<2:20:58,  6.98s/it]

 69%|██████▊   | 2634/3844 [5:26:23<2:28:31,  7.37s/it]

 69%|██████▊   | 2635/3844 [5:26:29<2:18:57,  6.90s/it]

 69%|██████▊   | 2636/3844 [5:26:35<2:16:52,  6.80s/it]

 69%|██████▊   | 2637/3844 [5:26:41<2:10:03,  6.47s/it]


 69%|██████▊   | 2639/3844 [5:26:56<2:19:26,  6.94s/it]
{'loss': 1.0688, 'grad_norm': 0.34962027733266005, 'learning_rate': 4.727672994265017e-06, 'epoch': 0.69}


 69%|██████▊   | 2641/3844 [5:27:12<2:34:46,  7.72s/it]

 69%|██████▊   | 2642/3844 [5:27:24<2:59:36,  8.97s/it]
{'loss': 1.169, 'grad_norm': 0.3569749557921949, 'learning_rate': 4.70620802233009e-06, 'epoch': 0.69}


 69%|██████▉   | 2644/3844 [5:27:38<2:39:23,  7.97s/it]

 69%|██████▉   | 2645/3844 [5:27:44<2:26:55,  7.35s/it]

 69%|██████▉   | 2646/3844 [5:27:50<2:15:23,  6.78s/it]

 69%|██████▉   | 2647/3844 [5:27:56<2:13:50,  6.71s/it]
{'loss': 1.038, 'grad_norm': 0.3715674422438656, 'learning_rate': 4.6705083240698105e-06, 'epoch': 0.69}

 69%|██████▉   | 2648/3844 [5:28:05<2:25:12,  7.28s/it]

 69%|██████▉   | 2649/3844 [5:28:13<2:32:15,  7.64s/it]

 69%|██████▉   | 2650/3844 [5:28:19<2:21:53,  7.13s/it]

 69%|██████▉   | 2651/3844 [5:28:26<2:16:46,  6.88s/it]

 69%|██████▉   | 2652/3844 [5:28:33<2:20:39,  7.08s/it]


 69%|██████▉   | 2654/3844 [5:28:50<2:37:18,  7.93s/it]

 69%|██████▉   | 2655/3844 [5:28:58<2:37:25,  7.94s/it]
{'loss': 1.204, 'grad_norm': 0.341326222733222, 'learning_rate': 4.613585875021068e-06, 'epoch': 0.69}


 69%|██████▉   | 2657/3844 [5:29:12<2:25:28,  7.35s/it]
{'loss': 1.0464, 'grad_norm': 0.3537063078846579, 'learning_rate': 4.599393413230017e-06, 'epoch': 0.69}


 69%|██████▉   | 2659/3844 [5:29:24<2:15:31,  6.86s/it]
{'loss': 1.1278, 'grad_norm': 0.35283200173105334, 'learning_rate': 4.5852162923223434e-06, 'epoch': 0.69}

 69%|██████▉   | 2660/3844 [5:29:31<2:13:19,  6.76s/it]

 69%|██████▉   | 2661/3844 [5:29:38<2:13:32,  6.77s/it]


 69%|██████▉   | 2663/3844 [5:29:52<2:18:22,  7.03s/it]

 69%|██████▉   | 2664/3844 [5:29:58<2:13:37,  6.79s/it]

 69%|██████▉   | 2665/3844 [5:30:09<2:32:48,  7.78s/it]

 69%|██████▉   | 2666/3844 [5:30:16<2:32:34,  7.77s/it]
{'loss': 1.0212, 'grad_norm': 0.3626879049917274, 'learning_rate': 4.535717759624677e-06, 'epoch': 0.69}


 69%|██████▉   | 2668/3844 [5:30:36<2:54:15,  8.89s/it]

 69%|██████▉   | 2669/3844 [5:30:43<2:40:42,  8.21s/it]

 69%|██████▉   | 2670/3844 [5:30:50<2:35:07,  7.93s/it]

 69%|██████▉   | 2671/3844 [5:30:58<2:38:03,  8.08s/it]
{'loss': 0.9956, 'grad_norm': 0.36726303488887685, 'learning_rate': 4.5004779775562825e-06, 'epoch': 0.69}


 70%|██████▉   | 2673/3844 [5:31:10<2:16:34,  7.00s/it]

 70%|██████▉   | 2674/3844 [5:31:16<2:09:44,  6.65s/it]

 70%|██████▉   | 2675/3844 [5:31:22<2:05:54,  6.46s/it]
{'loss': 1.2675, 'grad_norm': 0.361856934783807, 'learning_rate': 4.472356420216867e-06, 'epoch': 0.7}


 70%|██████▉   | 2677/3844 [5:31:39<2:27:15,  7.57s/it]

 70%|██████▉   | 2678/3844 [5:31:44<2:17:13,  7.06s/it]
{'loss': 1.1414, 'grad_norm': 0.35419023413191936, 'learning_rate': 4.45130645181053e-06, 'epoch': 0.7}


 70%|██████▉   | 2680/3844 [5:31:58<2:19:11,  7.17s/it]

 70%|██████▉   | 2681/3844 [5:32:04<2:10:10,  6.72s/it]

 70%|██████▉   | 2682/3844 [5:32:10<2:05:40,  6.49s/it]

 70%|██████▉   | 2683/3844 [5:32:16<2:02:43,  6.34s/it]
{'loss': 1.1582, 'grad_norm': 0.32592252433916513, 'learning_rate': 4.416302045130479e-06, 'epoch': 0.7}


 70%|██████▉   | 2685/3844 [5:32:33<2:16:53,  7.09s/it]
{'loss': 1.1326, 'grad_norm': 0.34055465467461227, 'learning_rate': 4.402328004378992e-06, 'epoch': 0.7}

 70%|██████▉   | 2686/3844 [5:32:39<2:15:08,  7.00s/it]


 70%|██████▉   | 2688/3844 [5:32:52<2:09:08,  6.70s/it]

 70%|██████▉   | 2689/3844 [5:32:58<2:04:09,  6.45s/it]
{'loss': 1.0918, 'grad_norm': 0.35939089857137685, 'learning_rate': 4.374427664518706e-06, 'epoch': 0.7}

 70%|██████▉   | 2690/3844 [5:33:05<2:07:59,  6.66s/it]


 70%|███████   | 2692/3844 [5:33:18<2:05:15,  6.52s/it]

 70%|███████   | 2693/3844 [5:33:24<2:03:49,  6.46s/it]
{'loss': 1.0418, 'grad_norm': 0.35731112802476706, 'learning_rate': 4.346591244284068e-06, 'epoch': 0.7}

 70%|███████   | 2694/3844 [5:33:31<2:04:38,  6.50s/it]

 70%|███████   | 2695/3844 [5:33:39<2:12:57,  6.94s/it]


 70%|███████   | 2697/3844 [5:33:54<2:18:08,  7.23s/it]

 70%|███████   | 2698/3844 [5:34:03<2:25:31,  7.62s/it]
{'loss': 1.1509, 'grad_norm': 0.35542271871163766, 'learning_rate': 4.311886087737877e-06, 'epoch': 0.7}


 70%|███████   | 2700/3844 [5:34:17<2:20:02,  7.34s/it]
 70%|███████   | 2700/3844 [5:34:17<2:20:02,  7.34s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2922, 'grad_norm': 0.35851160705884966, 'learning_rate': 4.291111427108563e-06, 'epoch': 0.7}
 70%|███████   | 2701/3844 [5:34:53<5:04:43, 16.00s/it]

 70%|███████   | 2702/3844 [5:34:59<4:10:04, 13.14s/it]

 70%|███████   | 2703/3844 [5:35:06<3:29:49, 11.03s/it]

 70%|███████   | 2704/3844 [5:35:14<3:13:08, 10.17s/it]

 70%|███████   | 2705/3844 [5:35:21<2:57:13,  9.34s/it]

 70%|███████   | 2706/3844 [5:35:27<2:38:06,  8.34s/it]

 70%|███████   | 2707/3844 [5:35:35<2:37:46,  8.33s/it]

 70%|███████   | 2708/3844 [5:35:43<2:33:49,  8.12s/it]


 70%|███████   | 2710/3844 [5:35:56<2:18:57,  7.35s/it]

 71%|███████   | 2711/3844 [5:36:03<2:14:31,  7.12s/it]
{'loss': 1.1435, 'grad_norm': 0.35203196969015915, 'learning_rate': 4.222126821866568e-06, 'epoch': 0.71}

 71%|███████   | 2712/3844 [5:36:11<2:21:00,  7.47s/it]

 71%|███████   | 2713/3844 [5:36:17<2:13:49,  7.10s/it]

 71%|███████   | 2714/3844 [5:36:23<2:07:50,  6.79s/it]


 71%|███████   | 2716/3844 [5:36:37<2:07:41,  6.79s/it]

 71%|███████   | 2717/3844 [5:36:45<2:15:56,  7.24s/it]

 71%|███████   | 2718/3844 [5:36:51<2:08:23,  6.84s/it]

 71%|███████   | 2719/3844 [5:36:57<2:03:33,  6.59s/it]
{'loss': 1.0206, 'grad_norm': 0.3689176810664568, 'learning_rate': 4.167234327567356e-06, 'epoch': 0.71}


 71%|███████   | 2721/3844 [5:37:13<2:19:22,  7.45s/it]

 71%|███████   | 2722/3844 [5:37:21<2:20:57,  7.54s/it]
{'loss': 1.0993, 'grad_norm': 0.3335948588693265, 'learning_rate': 4.146717853277304e-06, 'epoch': 0.71}

 71%|███████   | 2723/3844 [5:37:27<2:14:39,  7.21s/it]

 71%|███████   | 2724/3844 [5:37:36<2:22:18,  7.62s/it]

 71%|███████   | 2725/3844 [5:37:44<2:24:15,  7.73s/it]

 71%|███████   | 2726/3844 [5:37:50<2:14:02,  7.19s/it]


 71%|███████   | 2728/3844 [5:38:01<1:57:57,  6.34s/it]

 71%|███████   | 2729/3844 [5:38:07<1:56:50,  6.29s/it]
{'loss': 1.0076, 'grad_norm': 0.3296182546156247, 'learning_rate': 4.098991790117979e-06, 'epoch': 0.71}


 71%|███████   | 2731/3844 [5:38:19<1:54:23,  6.17s/it]
{'loss': 1.1877, 'grad_norm': 0.34677311996856675, 'learning_rate': 4.0853934147364e-06, 'epoch': 0.71}

 71%|███████   | 2732/3844 [5:38:26<1:57:55,  6.36s/it]

 71%|███████   | 2733/3844 [5:38:36<2:16:47,  7.39s/it]

 71%|███████   | 2734/3844 [5:38:42<2:09:00,  6.97s/it]

 71%|███████   | 2735/3844 [5:38:47<2:01:45,  6.59s/it]


 71%|███████   | 2737/3844 [5:39:01<2:02:29,  6.64s/it]
{'loss': 1.1613, 'grad_norm': 0.3476854998300075, 'learning_rate': 4.044699248527435e-06, 'epoch': 0.71}

 71%|███████   | 2738/3844 [5:39:09<2:14:31,  7.30s/it]

 71%|███████▏  | 2739/3844 [5:39:18<2:20:14,  7.61s/it]

 71%|███████▏  | 2740/3844 [5:39:24<2:14:28,  7.31s/it]

 71%|███████▏  | 2741/3844 [5:39:32<2:14:26,  7.31s/it]

 71%|███████▏  | 2742/3844 [5:39:38<2:09:32,  7.05s/it]


 71%|███████▏  | 2744/3844 [5:39:55<2:20:25,  7.66s/it]

 71%|███████▏  | 2745/3844 [5:40:07<2:44:17,  8.97s/it]
{'loss': 1.1742, 'grad_norm': 0.35782570830229776, 'learning_rate': 3.990677370606761e-06, 'epoch': 0.71}

 71%|███████▏  | 2746/3844 [5:40:13<2:29:00,  8.14s/it]


 71%|███████▏  | 2748/3844 [5:40:35<2:51:40,  9.40s/it]

 72%|███████▏  | 2749/3844 [5:40:41<2:29:42,  8.20s/it]
{'loss': 1.1295, 'grad_norm': 0.36493157333178566, 'learning_rate': 3.963768698272226e-06, 'epoch': 0.72}

 72%|███████▏  | 2750/3844 [5:40:46<2:16:21,  7.48s/it]

 72%|███████▏  | 2751/3844 [5:40:56<2:28:35,  8.16s/it]

 72%|███████▏  | 2752/3844 [5:41:04<2:29:37,  8.22s/it]

 72%|███████▏  | 2753/3844 [5:41:16<2:49:36,  9.33s/it]


 72%|███████▏  | 2755/3844 [5:41:33<2:43:57,  9.03s/it]
{'loss': 1.1692, 'grad_norm': 0.36748074697415556, 'learning_rate': 3.9235343832332116e-06, 'epoch': 0.72}


 72%|███████▏  | 2757/3844 [5:41:47<2:20:43,  7.77s/it]
{'loss': 1.0718, 'grad_norm': 0.37151746967197613, 'learning_rate': 3.910157415577757e-06, 'epoch': 0.72}


 72%|███████▏  | 2759/3844 [5:42:03<2:24:30,  7.99s/it]

 72%|███████▏  | 2760/3844 [5:42:11<2:25:39,  8.06s/it]

 72%|███████▏  | 2761/3844 [5:42:19<2:23:59,  7.98s/it]

 72%|███████▏  | 2762/3844 [5:42:31<2:46:55,  9.26s/it]
[2024-05-27 16:28:13,433] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0383, 'grad_norm': 0.3444626476307395, 'learning_rate': 3.876790761326485e-06, 'epoch': 0.72}

 72%|███████▏  | 2763/3844 [5:42:38<2:34:34,  8.58s/it]


 72%|███████▏  | 2765/3844 [5:42:51<2:16:53,  7.61s/it]

 72%|███████▏  | 2766/3844 [5:42:59<2:16:15,  7.58s/it]
{'loss': 0.9771, 'grad_norm': 0.3358639581059278, 'learning_rate': 3.850175680198868e-06, 'epoch': 0.72}


 72%|███████▏  | 2768/3844 [5:43:13<2:15:25,  7.55s/it]

 72%|███████▏  | 2769/3844 [5:43:19<2:08:24,  7.17s/it]

 72%|███████▏  | 2770/3844 [5:43:27<2:11:10,  7.33s/it]
{'loss': 1.1115, 'grad_norm': 0.3587446160233977, 'learning_rate': 3.823630475422465e-06, 'epoch': 0.72}

 72%|███████▏  | 2771/3844 [5:43:33<2:01:05,  6.77s/it]

 72%|███████▏  | 2772/3844 [5:43:39<1:56:31,  6.52s/it]


 72%|███████▏  | 2774/3844 [5:43:55<2:10:12,  7.30s/it]
{'loss': 1.0542, 'grad_norm': 0.3455831869744152, 'learning_rate': 3.797155448612736e-06, 'epoch': 0.72}

 72%|███████▏  | 2775/3844 [5:44:02<2:08:57,  7.24s/it]


 72%|███████▏  | 2777/3844 [5:44:17<2:10:00,  7.31s/it]

 72%|███████▏  | 2778/3844 [5:44:25<2:12:51,  7.48s/it]
{'loss': 1.123, 'grad_norm': 0.32476007571173915, 'learning_rate': 3.7707509005877487e-06, 'epoch': 0.72}

 72%|███████▏  | 2779/3844 [5:44:32<2:11:41,  7.42s/it]

 72%|███████▏  | 2780/3844 [5:44:38<2:02:11,  6.89s/it]


 72%|███████▏  | 2782/3844 [5:44:51<2:01:01,  6.84s/it]
{'loss': 1.0183, 'grad_norm': 0.33277505634914545, 'learning_rate': 3.744417131364766e-06, 'epoch': 0.72}

 72%|███████▏  | 2783/3844 [5:44:58<1:58:05,  6.68s/it]

 72%|███████▏  | 2784/3844 [5:45:07<2:10:13,  7.37s/it]

 72%|███████▏  | 2785/3844 [5:45:17<2:24:00,  8.16s/it]

 72%|███████▏  | 2786/3844 [5:45:24<2:20:21,  7.96s/it]

 73%|███████▎  | 2787/3844 [5:45:32<2:20:45,  7.99s/it]


 73%|███████▎  | 2789/3844 [5:45:43<1:58:33,  6.74s/it]

 73%|███████▎  | 2790/3844 [5:45:49<1:53:30,  6.46s/it]

 73%|███████▎  | 2791/3844 [5:45:55<1:51:16,  6.34s/it]

 73%|███████▎  | 2792/3844 [5:46:01<1:50:08,  6.28s/it]

 73%|███████▎  | 2793/3844 [5:46:08<1:49:23,  6.24s/it]
{'loss': 1.0455, 'grad_norm': 0.3504185889162542, 'learning_rate': 3.6723666591070485e-06, 'epoch': 0.73}
[2024-05-27 16:32:01,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 2794/3844 [5:46:19<2:15:07,  7.72s/it]

 73%|███████▎  | 2795/3844 [5:46:29<2:26:38,  8.39s/it]


 73%|███████▎  | 2797/3844 [5:46:46<2:25:13,  8.32s/it]

 73%|███████▎  | 2798/3844 [5:46:52<2:11:47,  7.56s/it]
{'loss': 1.2346, 'grad_norm': 0.34876785841748353, 'learning_rate': 3.639795814619439e-06, 'epoch': 0.73}

 73%|███████▎  | 2799/3844 [5:46:58<2:08:23,  7.37s/it]

 73%|███████▎  | 2800/3844 [5:47:05<2:02:24,  7.04s/it]


 73%|███████▎  | 2802/3844 [5:47:19<2:06:15,  7.27s/it]
{'loss': 1.0766, 'grad_norm': 0.3970107844591151, 'learning_rate': 3.613820411399234e-06, 'epoch': 0.73}

 73%|███████▎  | 2803/3844 [5:47:26<2:03:51,  7.14s/it]


 73%|███████▎  | 2805/3844 [5:47:38<1:51:43,  6.45s/it]

 73%|███████▎  | 2806/3844 [5:47:43<1:47:19,  6.20s/it]

 73%|███████▎  | 2807/3844 [5:47:49<1:44:05,  6.02s/it]

 73%|███████▎  | 2808/3844 [5:47:59<2:07:05,  7.36s/it]

 73%|███████▎  | 2809/3844 [5:48:07<2:09:47,  7.52s/it]

 73%|███████▎  | 2810/3844 [5:48:14<2:03:44,  7.18s/it]
{'loss': 1.1468, 'grad_norm': 0.3513614337661009, 'learning_rate': 3.5620875849709247e-06, 'epoch': 0.73}

 73%|███████▎  | 2811/3844 [5:48:20<1:59:54,  6.96s/it]


 73%|███████▎  | 2813/3844 [5:48:34<1:58:13,  6.88s/it]
{'loss': 1.202, 'grad_norm': 0.3514143513978156, 'learning_rate': 3.542763084615334e-06, 'epoch': 0.73}


 73%|███████▎  | 2815/3844 [5:48:49<2:05:47,  7.34s/it]

 73%|███████▎  | 2816/3844 [5:48:56<2:01:09,  7.07s/it]
{'loss': 1.0469, 'grad_norm': 0.3607697789529631, 'learning_rate': 3.523479854494292e-06, 'epoch': 0.73}


 73%|███████▎  | 2818/3844 [5:49:10<1:58:01,  6.90s/it]
{'loss': 1.2499, 'grad_norm': 0.34438394566485586, 'learning_rate': 3.5106473565277186e-06, 'epoch': 0.73}

 73%|███████▎  | 2819/3844 [5:49:17<1:57:05,  6.85s/it]


 73%|███████▎  | 2821/3844 [5:49:31<2:03:14,  7.23s/it]

 73%|███████▎  | 2822/3844 [5:49:41<2:17:38,  8.08s/it]
{'loss': 0.9738, 'grad_norm': 0.3265215615240967, 'learning_rate': 3.485037697672079e-06, 'epoch': 0.73}

 73%|███████▎  | 2823/3844 [5:49:49<2:14:16,  7.89s/it]


 73%|███████▎  | 2825/3844 [5:50:04<2:10:16,  7.67s/it]

 74%|███████▎  | 2826/3844 [5:50:10<2:02:45,  7.24s/it]

 74%|███████▎  | 2827/3844 [5:50:18<2:06:44,  7.48s/it]
{'loss': 1.1038, 'grad_norm': 0.3518143312254468, 'learning_rate': 3.4531297560184107e-06, 'epoch': 0.74}

 74%|███████▎  | 2828/3844 [5:50:25<2:02:46,  7.25s/it]


 74%|███████▎  | 2830/3844 [5:50:38<1:55:51,  6.86s/it]
{'loss': 1.0168, 'grad_norm': 0.3237676788425749, 'learning_rate': 3.4340407456167657e-06, 'epoch': 0.74}


 74%|███████▎  | 2832/3844 [5:50:55<2:16:39,  8.10s/it]

 74%|███████▎  | 2833/3844 [5:51:04<2:18:11,  8.20s/it]
{'loss': 1.2016, 'grad_norm': 0.36520799466971837, 'learning_rate': 3.4149937003283772e-06, 'epoch': 0.74}

 74%|███████▎  | 2834/3844 [5:51:09<2:03:18,  7.33s/it]

 74%|███████▍  | 2835/3844 [5:51:14<1:52:52,  6.71s/it]

 74%|███████▍  | 2836/3844 [5:51:23<2:01:19,  7.22s/it]

 74%|███████▍  | 2837/3844 [5:51:30<2:02:49,  7.32s/it]

 74%|███████▍  | 2838/3844 [5:51:39<2:09:11,  7.70s/it]

 74%|███████▍  | 2839/3844 [5:51:45<1:58:47,  7.09s/it]

 74%|███████▍  | 2840/3844 [5:51:55<2:13:08,  7.96s/it]

 74%|███████▍  | 2841/3844 [5:52:01<2:03:32,  7.39s/it]

 74%|███████▍  | 2842/3844 [5:52:07<1:56:25,  6.97s/it]


 74%|███████▍  | 2844/3844 [5:52:18<1:43:50,  6.23s/it]

 74%|███████▍  | 2845/3844 [5:52:23<1:41:10,  6.08s/it]
{'loss': 1.216, 'grad_norm': 0.34515019063550406, 'learning_rate': 3.339227600981124e-06, 'epoch': 0.74}


 74%|███████▍  | 2847/3844 [5:52:38<1:49:45,  6.61s/it]

 74%|███████▍  | 2848/3844 [5:52:43<1:45:06,  6.33s/it]

 74%|███████▍  | 2849/3844 [5:52:49<1:43:40,  6.25s/it]
{'loss': 0.8548, 'grad_norm': 0.34698122756060845, 'learning_rate': 3.314123216600875e-06, 'epoch': 0.74}


 74%|███████▍  | 2851/3844 [5:53:08<2:07:54,  7.73s/it]
{'loss': 1.0455, 'grad_norm': 0.31330966960396767, 'learning_rate': 3.3015994943008178e-06, 'epoch': 0.74}

 74%|███████▍  | 2852/3844 [5:53:15<2:05:57,  7.62s/it]


 74%|███████▍  | 2854/3844 [5:53:31<2:14:27,  8.15s/it]
{'loss': 1.0475, 'grad_norm': 0.3525474410665677, 'learning_rate': 3.2828495982861018e-06, 'epoch': 0.74}


 74%|███████▍  | 2856/3844 [5:53:46<2:04:10,  7.54s/it]

 74%|███████▍  | 2857/3844 [5:53:51<1:54:17,  6.95s/it]
{'loss': 1.3004, 'grad_norm': 0.352674257185032, 'learning_rate': 3.2641426336948734e-06, 'epoch': 0.74}

 74%|███████▍  | 2858/3844 [5:53:59<1:55:15,  7.01s/it]

 74%|███████▍  | 2859/3844 [5:54:08<2:08:04,  7.80s/it]

 74%|███████▍  | 2860/3844 [5:54:15<2:01:19,  7.40s/it]

 74%|███████▍  | 2861/3844 [5:54:21<1:56:43,  7.12s/it]


 74%|███████▍  | 2863/3844 [5:54:34<1:48:09,  6.62s/it]

 75%|███████▍  | 2864/3844 [5:54:40<1:45:24,  6.45s/it]
{'loss': 1.3135, 'grad_norm': 0.3465616279292827, 'learning_rate': 3.2206606762834325e-06, 'epoch': 0.74}


 75%|███████▍  | 2866/3844 [5:54:52<1:41:38,  6.24s/it]

 75%|███████▍  | 2867/3844 [5:54:58<1:41:15,  6.22s/it]
{'loss': 1.0447, 'grad_norm': 0.33843632598402956, 'learning_rate': 3.202097678394195e-06, 'epoch': 0.75}

 75%|███████▍  | 2868/3844 [5:55:05<1:42:52,  6.32s/it]


 75%|███████▍  | 2870/3844 [5:55:22<2:02:45,  7.56s/it]
{'loss': 1.0444, 'grad_norm': 0.31856404761294926, 'learning_rate': 3.1835781280393906e-06, 'epoch': 0.75}


 75%|███████▍  | 2872/3844 [5:55:36<1:59:07,  7.35s/it]
{'loss': 0.8818, 'grad_norm': 0.35760222670827624, 'learning_rate': 3.1712559571182643e-06, 'epoch': 0.75}

 75%|███████▍  | 2873/3844 [5:55:47<2:15:41,  8.38s/it]

 75%|███████▍  | 2874/3844 [5:55:55<2:13:09,  8.24s/it]


 75%|███████▍  | 2876/3844 [5:56:08<2:00:03,  7.44s/it]
{'loss': 1.27, 'grad_norm': 0.3571327877797665, 'learning_rate': 3.1466698431117603e-06, 'epoch': 0.75}

 75%|███████▍  | 2877/3844 [5:56:15<1:54:43,  7.12s/it]

 75%|███████▍  | 2878/3844 [5:56:25<2:10:28,  8.10s/it]

 75%|███████▍  | 2879/3844 [5:56:31<2:00:25,  7.49s/it]

 75%|███████▍  | 2880/3844 [5:56:37<1:53:33,  7.07s/it]


 75%|███████▍  | 2882/3844 [5:56:50<1:47:37,  6.71s/it]

 75%|███████▌  | 2883/3844 [5:56:56<1:42:06,  6.38s/it]

 75%|███████▌  | 2884/3844 [5:57:02<1:40:40,  6.29s/it]
{'loss': 1.1138, 'grad_norm': 0.3679951648308096, 'learning_rate': 3.0977315030258002e-06, 'epoch': 0.75}

 75%|███████▌  | 2885/3844 [5:57:07<1:36:50,  6.06s/it]


 75%|███████▌  | 2887/3844 [5:57:20<1:37:00,  6.08s/it]
{'loss': 1.092, 'grad_norm': 0.35002967141634966, 'learning_rate': 3.079460382934111e-06, 'epoch': 0.75}

 75%|███████▌  | 2888/3844 [5:57:27<1:39:23,  6.24s/it]

 75%|███████▌  | 2889/3844 [5:57:33<1:39:52,  6.27s/it]

 75%|███████▌  | 2890/3844 [5:57:39<1:38:12,  6.18s/it]


 75%|███████▌  | 2892/3844 [5:57:56<1:57:58,  7.44s/it]

 75%|███████▌  | 2893/3844 [5:58:02<1:50:58,  7.00s/it]

 75%|███████▌  | 2894/3844 [5:58:08<1:47:33,  6.79s/it]
{'loss': 1.0358, 'grad_norm': 0.3669122576694098, 'learning_rate': 3.036999981491131e-06, 'epoch': 0.75}

 75%|███████▌  | 2895/3844 [5:58:14<1:41:24,  6.41s/it]


 75%|███████▌  | 2897/3844 [5:58:34<2:16:30,  8.65s/it]
{'loss': 1.0837, 'grad_norm': 0.3374138839214683, 'learning_rate': 3.0188767517675267e-06, 'epoch': 0.75}

 75%|███████▌  | 2898/3844 [5:58:41<2:06:27,  8.02s/it]


 75%|███████▌  | 2900/3844 [5:58:54<1:55:01,  7.31s/it]

 75%|███████▌  | 2901/3844 [5:59:00<1:50:53,  7.06s/it]

 75%|███████▌  | 2902/3844 [5:59:06<1:44:58,  6.69s/it]

 76%|███████▌  | 2903/3844 [5:59:12<1:42:26,  6.53s/it]
{'loss': 1.1172, 'grad_norm': 0.36087320724361194, 'learning_rate': 2.9827642635376475e-06, 'epoch': 0.76}

 76%|███████▌  | 2904/3844 [5:59:19<1:44:44,  6.69s/it]

 76%|███████▌  | 2905/3844 [5:59:25<1:38:55,  6.32s/it]

 76%|███████▌  | 2906/3844 [5:59:33<1:46:35,  6.82s/it]

 76%|███████▌  | 2907/3844 [5:59:39<1:45:18,  6.74s/it]


 76%|███████▌  | 2909/3844 [5:59:54<1:49:11,  7.01s/it]

 76%|███████▌  | 2910/3844 [6:00:00<1:45:21,  6.77s/it]
{'loss': 1.0154, 'grad_norm': 0.3414574574766111, 'learning_rate': 2.940859829925653e-06, 'epoch': 0.76}


 76%|███████▌  | 2912/3844 [6:00:15<1:48:12,  6.97s/it]
{'loss': 1.1694, 'grad_norm': 0.3591932394540983, 'learning_rate': 2.9289321881345257e-06, 'epoch': 0.76}

 76%|███████▌  | 2913/3844 [6:00:21<1:44:44,  6.75s/it]


 76%|███████▌  | 2915/3844 [6:00:36<1:51:21,  7.19s/it]

 76%|███████▌  | 2916/3844 [6:00:42<1:46:02,  6.86s/it]
{'loss': 1.3095, 'grad_norm': 0.34683901248085625, 'learning_rate': 2.905137196289336e-06, 'epoch': 0.76}

 76%|███████▌  | 2917/3844 [6:00:51<1:56:04,  7.51s/it]

 76%|███████▌  | 2918/3844 [6:01:03<2:13:44,  8.67s/it]


 76%|███████▌  | 2920/3844 [6:01:15<1:51:33,  7.24s/it]
{'loss': 1.0985, 'grad_norm': 0.36242754137663735, 'learning_rate': 2.881422818637227e-06, 'epoch': 0.76}

 76%|███████▌  | 2921/3844 [6:01:24<1:58:34,  7.71s/it]

 76%|███████▌  | 2922/3844 [6:01:29<1:48:22,  7.05s/it]

 76%|███████▌  | 2923/3844 [6:01:39<2:00:32,  7.85s/it]


 76%|███████▌  | 2925/3844 [6:01:50<1:44:12,  6.80s/it]

 76%|███████▌  | 2926/3844 [6:01:56<1:40:02,  6.54s/it]

 76%|███████▌  | 2927/3844 [6:02:02<1:35:13,  6.23s/it]
{'loss': 1.1081, 'grad_norm': 0.37691647637780035, 'learning_rate': 2.8401174455869695e-06, 'epoch': 0.76}


 76%|███████▌  | 2929/3844 [6:02:16<1:42:30,  6.72s/it]

 76%|███████▌  | 2930/3844 [6:02:22<1:39:22,  6.52s/it]
{'loss': 1.115, 'grad_norm': 0.35511549809840104, 'learning_rate': 2.822491327609178e-06, 'epoch': 0.76}

 76%|███████▌  | 2931/3844 [6:02:33<1:59:36,  7.86s/it]


 76%|███████▋  | 2933/3844 [6:02:47<1:50:53,  7.30s/it]
{'loss': 1.2005, 'grad_norm': 0.3687769715666733, 'learning_rate': 2.8049110833495507e-06, 'epoch': 0.76}

 76%|███████▋  | 2934/3844 [6:02:53<1:47:03,  7.06s/it]


 76%|███████▋  | 2936/3844 [6:03:06<1:44:08,  6.88s/it]
{'loss': 1.1376, 'grad_norm': 0.34675706853082433, 'learning_rate': 2.787376825168966e-06, 'epoch': 0.76}

 76%|███████▋  | 2937/3844 [6:03:16<1:54:50,  7.60s/it]

 76%|███████▋  | 2938/3844 [6:03:22<1:47:08,  7.10s/it]

 76%|███████▋  | 2939/3844 [6:03:28<1:41:59,  6.76s/it]

 76%|███████▋  | 2940/3844 [6:03:37<1:55:17,  7.65s/it]

 77%|███████▋  | 2941/3844 [6:03:43<1:46:59,  7.11s/it]

 77%|███████▋  | 2942/3844 [6:03:50<1:43:23,  6.88s/it]


 77%|███████▋  | 2944/3844 [6:04:09<2:02:47,  8.19s/it]

 77%|███████▋  | 2945/3844 [6:04:15<1:52:13,  7.49s/it]

 77%|███████▋  | 2946/3844 [6:04:20<1:44:32,  6.98s/it]

 77%|███████▋  | 2947/3844 [6:04:27<1:41:01,  6.76s/it]

 77%|███████▋  | 2948/3844 [6:04:33<1:38:38,  6.61s/it]
{'loss': 1.0821, 'grad_norm': 0.35747601437710563, 'learning_rate': 2.7177018901526562e-06, 'epoch': 0.77}

 77%|███████▋  | 2949/3844 [6:04:44<1:57:55,  7.91s/it]


 77%|███████▋  | 2951/3844 [6:05:02<2:06:03,  8.47s/it]
{'loss': 0.9424, 'grad_norm': 0.3583997804296131, 'learning_rate': 2.700399237468505e-06, 'epoch': 0.77}


 77%|███████▋  | 2953/3844 [6:05:15<1:50:44,  7.46s/it]
{'loss': 1.157, 'grad_norm': 0.3441152092422793, 'learning_rate': 2.6888900477800862e-06, 'epoch': 0.77}

 77%|███████▋  | 2954/3844 [6:05:24<1:57:57,  7.95s/it]


 77%|███████▋  | 2956/3844 [6:05:42<2:09:16,  8.74s/it]

 77%|███████▋  | 2957/3844 [6:05:49<1:57:14,  7.93s/it]
{'loss': 1.0988, 'grad_norm': 0.35296132652083106, 'learning_rate': 2.665934004530203e-06, 'epoch': 0.77}

 77%|███████▋  | 2958/3844 [6:05:58<2:04:40,  8.44s/it]

 77%|███████▋  | 2959/3844 [6:06:06<2:00:40,  8.18s/it]


 77%|███████▋  | 2961/3844 [6:06:22<2:02:47,  8.34s/it]
{'loss': 1.1184, 'grad_norm': 0.33123211041951817, 'learning_rate': 2.6430612933797027e-06, 'epoch': 0.77}

 77%|███████▋  | 2962/3844 [6:06:28<1:49:52,  7.47s/it]


 77%|███████▋  | 2964/3844 [6:06:40<1:41:38,  6.93s/it]
{'loss': 1.1702, 'grad_norm': 0.34885900751886834, 'learning_rate': 2.6259616030812128e-06, 'epoch': 0.77}


 77%|███████▋  | 2966/3844 [6:06:56<1:47:15,  7.33s/it]
{'loss': 1.0583, 'grad_norm': 0.37431478076620817, 'learning_rate': 2.6145879860380773e-06, 'epoch': 0.77}


 77%|███████▋  | 2968/3844 [6:07:13<1:51:01,  7.60s/it]

 77%|███████▋  | 2969/3844 [6:07:20<1:49:38,  7.52s/it]

 77%|███████▋  | 2970/3844 [6:07:29<1:53:51,  7.82s/it]

 77%|███████▋  | 2971/3844 [6:07:35<1:44:09,  7.16s/it]
{'loss': 1.0861, 'grad_norm': 0.3670859112397077, 'learning_rate': 2.586245796610888e-06, 'epoch': 0.77}


 77%|███████▋  | 2973/3844 [6:07:47<1:36:59,  6.68s/it]
{'loss': 1.1664, 'grad_norm': 0.34931213134748446, 'learning_rate': 2.574945746646096e-06, 'epoch': 0.77}


 77%|███████▋  | 2975/3844 [6:08:05<1:55:53,  8.00s/it]

 77%|███████▋  | 2976/3844 [6:08:11<1:48:53,  7.53s/it]

 77%|███████▋  | 2977/3844 [6:08:20<1:56:51,  8.09s/it]
{'loss': 1.2473, 'grad_norm': 0.34909991011899827, 'learning_rate': 2.5524089532539863e-06, 'epoch': 0.77}


 77%|███████▋  | 2979/3844 [6:08:37<1:58:26,  8.22s/it]
{'loss': 1.1051, 'grad_norm': 0.3499050773492989, 'learning_rate': 2.5411722738443466e-06, 'epoch': 0.77}

 78%|███████▊  | 2980/3844 [6:08:44<1:50:05,  7.65s/it]

 78%|███████▊  | 2981/3844 [6:08:52<1:52:35,  7.83s/it]

 78%|███████▊  | 2982/3844 [6:09:02<2:00:54,  8.42s/it]

 78%|███████▊  | 2983/3844 [6:09:08<1:51:16,  7.75s/it]


 78%|███████▊  | 2985/3844 [6:09:21<1:43:09,  7.21s/it]

 78%|███████▊  | 2986/3844 [6:09:29<1:47:20,  7.51s/it]

 78%|███████▊  | 2987/3844 [6:09:35<1:39:19,  6.95s/it]

 78%|███████▊  | 2988/3844 [6:09:41<1:35:48,  6.72s/it]

 78%|███████▊  | 2989/3844 [6:09:51<1:48:34,  7.62s/it]

 78%|███████▊  | 2990/3844 [6:09:57<1:41:41,  7.14s/it]
{'loss': 1.1612, 'grad_norm': 0.3654591147169336, 'learning_rate': 2.4797501153017842e-06, 'epoch': 0.78}

 78%|███████▊  | 2991/3844 [6:10:04<1:41:51,  7.16s/it]

 78%|███████▊  | 2992/3844 [6:10:10<1:36:52,  6.82s/it]


 78%|███████▊  | 2994/3844 [6:10:25<1:42:27,  7.23s/it]
{'loss': 1.1555, 'grad_norm': 0.3216878233271129, 'learning_rate': 2.4575747222939526e-06, 'epoch': 0.78}


 78%|███████▊  | 2996/3844 [6:10:37<1:34:33,  6.69s/it]
{'loss': 1.1646, 'grad_norm': 0.3828831541234615, 'learning_rate': 2.446519147410368e-06, 'epoch': 0.78}

 78%|███████▊  | 2997/3844 [6:10:46<1:44:01,  7.37s/it]


 78%|███████▊  | 2999/3844 [6:10:59<1:38:12,  6.97s/it]
{'loss': 1.0746, 'grad_norm': 0.3484235012389642, 'learning_rate': 2.4299760254563786e-06, 'epoch': 0.78}

 78%|███████▊  | 3000/3844 [6:11:06<1:36:15,  6.84s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0796, 'grad_norm': 0.38016982294713547, 'learning_rate': 2.4189741500881224e-06, 'epoch': 0.78}

 78%|███████▊  | 3002/3844 [6:11:53<3:13:20, 13.78s/it]

 78%|███████▊  | 3003/3844 [6:12:01<2:49:48, 12.11s/it]
{'loss': 1.1861, 'grad_norm': 0.34359758255679584, 'learning_rate': 2.4079938092700226e-06, 'epoch': 0.78}


 78%|███████▊  | 3005/3844 [6:12:19<2:24:29, 10.33s/it]
{'loss': 1.0762, 'grad_norm': 0.35533285764957195, 'learning_rate': 2.397035034192676e-06, 'epoch': 0.78}


 78%|███████▊  | 3007/3844 [6:12:35<2:10:22,  9.35s/it]

 78%|███████▊  | 3008/3844 [6:12:45<2:11:09,  9.41s/it]

 78%|███████▊  | 3009/3844 [6:12:56<2:15:48,  9.76s/it]
{'loss': 1.1018, 'grad_norm': 0.3280325766186737, 'learning_rate': 2.3751823057162195e-06, 'epoch': 0.78}

 78%|███████▊  | 3010/3844 [6:13:02<2:01:55,  8.77s/it]

 78%|███████▊  | 3011/3844 [6:13:10<1:57:07,  8.44s/it]


 78%|███████▊  | 3013/3844 [6:13:29<2:05:35,  9.07s/it]
{'loss': 1.0581, 'grad_norm': 0.33268756383541764, 'learning_rate': 2.3534162129567383e-06, 'epoch': 0.78}


 78%|███████▊  | 3015/3844 [6:13:45<1:54:47,  8.31s/it]

 78%|███████▊  | 3016/3844 [6:13:54<1:54:28,  8.29s/it]
{'loss': 1.1342, 'grad_norm': 0.3788821967746874, 'learning_rate': 2.337148646888061e-06, 'epoch': 0.78}


 79%|███████▊  | 3018/3844 [6:14:09<1:50:22,  8.02s/it]

 79%|███████▊  | 3019/3844 [6:14:20<2:01:16,  8.82s/it]
[2024-05-27 17:00:01,861] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0696, 'grad_norm': 0.34294915719487395, 'learning_rate': 2.320930056515457e-06, 'epoch': 0.79}

 79%|███████▊  | 3020/3844 [6:14:26<1:51:30,  8.12s/it]


 79%|███████▊  | 3022/3844 [6:14:43<1:51:58,  8.17s/it]
{'loss': 1.2394, 'grad_norm': 0.33863840877087953, 'learning_rate': 2.3047605454970456e-06, 'epoch': 0.79}


 79%|███████▊  | 3024/3844 [6:14:59<1:52:23,  8.22s/it]
{'loss': 1.1386, 'grad_norm': 0.33540637325378825, 'learning_rate': 2.2940081888398747e-06, 'epoch': 0.79}

 79%|███████▊  | 3025/3844 [6:15:10<2:04:18,  9.11s/it]

 79%|███████▊  | 3026/3844 [6:15:22<2:14:53,  9.89s/it]


 79%|███████▉  | 3028/3844 [6:15:39<2:02:45,  9.03s/it]
{'loss': 1.2662, 'grad_norm': 0.34886237723868546, 'learning_rate': 2.272569174586209e-06, 'epoch': 0.79}

 79%|███████▉  | 3029/3844 [6:15:44<1:47:46,  7.93s/it]

 79%|███████▉  | 3030/3844 [6:15:50<1:39:39,  7.35s/it]


 79%|███████▉  | 3032/3844 [6:16:04<1:35:31,  7.06s/it]

 79%|███████▉  | 3033/3844 [6:16:09<1:29:21,  6.61s/it]
{'loss': 1.1634, 'grad_norm': 0.364864529376016, 'learning_rate': 2.2458939062791195e-06, 'epoch': 0.79}


 79%|███████▉  | 3035/3844 [6:16:21<1:26:21,  6.41s/it]

 79%|███████▉  | 3036/3844 [6:16:30<1:34:08,  6.99s/it]

 79%|███████▉  | 3037/3844 [6:16:35<1:28:42,  6.60s/it]
{'loss': 1.1506, 'grad_norm': 0.36056704547616936, 'learning_rate': 2.224652786757381e-06, 'epoch': 0.79}

 79%|███████▉  | 3038/3844 [6:16:44<1:35:54,  7.14s/it]

 79%|███████▉  | 3039/3844 [6:16:52<1:41:06,  7.54s/it]

 79%|███████▉  | 3040/3844 [6:16:58<1:34:15,  7.03s/it]

 79%|███████▉  | 3041/3844 [6:17:06<1:36:57,  7.24s/it]

 79%|███████▉  | 3042/3844 [6:17:13<1:35:28,  7.14s/it]

 79%|███████▉  | 3043/3844 [6:17:20<1:35:07,  7.13s/it]


 79%|███████▉  | 3045/3844 [6:17:36<1:41:35,  7.63s/it]

 79%|███████▉  | 3046/3844 [6:17:44<1:42:59,  7.74s/it]

 79%|███████▉  | 3047/3844 [6:17:49<1:34:02,  7.08s/it]
{'loss': 1.1983, 'grad_norm': 0.35336346896169973, 'learning_rate': 2.1719370275474616e-06, 'epoch': 0.79}

 79%|███████▉  | 3048/3844 [6:17:59<1:42:55,  7.76s/it]


 79%|███████▉  | 3050/3844 [6:18:15<1:44:39,  7.91s/it]

 79%|███████▉  | 3051/3844 [6:18:21<1:36:39,  7.31s/it]
{'loss': 1.1869, 'grad_norm': 0.3641319853076462, 'learning_rate': 2.151006168624059e-06, 'epoch': 0.79}


 79%|███████▉  | 3053/3844 [6:18:35<1:34:55,  7.20s/it]

 79%|███████▉  | 3054/3844 [6:18:42<1:30:41,  6.89s/it]
{'loss': 1.1475, 'grad_norm': 0.38018036421593965, 'learning_rate': 2.135366537729183e-06, 'epoch': 0.79}

 79%|███████▉  | 3055/3844 [6:18:51<1:39:07,  7.54s/it]


 80%|███████▉  | 3057/3844 [6:19:10<1:49:47,  8.37s/it]

 80%|███████▉  | 3058/3844 [6:19:18<1:48:03,  8.25s/it]
{'loss': 1.2554, 'grad_norm': 0.34028956291039303, 'learning_rate': 2.114591904309421e-06, 'epoch': 0.8}

 80%|███████▉  | 3059/3844 [6:19:28<1:57:03,  8.95s/it]

 80%|███████▉  | 3060/3844 [6:19:37<1:55:13,  8.82s/it]

 80%|███████▉  | 3061/3844 [6:19:43<1:42:50,  7.88s/it]


 80%|███████▉  | 3063/3844 [6:19:56<1:34:27,  7.26s/it]
{'loss': 1.1696, 'grad_norm': 0.3776953735410195, 'learning_rate': 2.0887496353632964e-06, 'epoch': 0.8}

 80%|███████▉  | 3064/3844 [6:20:03<1:33:32,  7.20s/it]


 80%|███████▉  | 3066/3844 [6:20:23<1:54:58,  8.87s/it]
{'loss': 1.1791, 'grad_norm': 0.3494884310900423, 'learning_rate': 2.0733116623957637e-06, 'epoch': 0.8}

 80%|███████▉  | 3067/3844 [6:20:29<1:42:00,  7.88s/it]

 80%|███████▉  | 3068/3844 [6:20:43<2:04:31,  9.63s/it]


 80%|███████▉  | 3070/3844 [6:20:58<1:50:09,  8.54s/it]
{'loss': 1.2044, 'grad_norm': 0.34707718067279314, 'learning_rate': 2.052806522941715e-06, 'epoch': 0.8}


 80%|███████▉  | 3072/3844 [6:21:14<1:46:45,  8.30s/it]
{'loss': 1.0782, 'grad_norm': 0.3629988375142412, 'learning_rate': 2.0425878006898257e-06, 'epoch': 0.8}

 80%|███████▉  | 3073/3844 [6:21:21<1:43:40,  8.07s/it]

 80%|███████▉  | 3074/3844 [6:21:26<1:33:02,  7.25s/it]

 80%|███████▉  | 3075/3844 [6:21:33<1:29:42,  7.00s/it]

 80%|████████  | 3076/3844 [6:21:39<1:25:09,  6.65s/it]


 80%|████████  | 3078/3844 [6:21:53<1:28:59,  6.97s/it]

 80%|████████  | 3079/3844 [6:22:01<1:32:35,  7.26s/it]
{'loss': 0.9858, 'grad_norm': 0.38140532194928134, 'learning_rate': 2.0070004669171726e-06, 'epoch': 0.8}

 80%|████████  | 3080/3844 [6:22:07<1:25:46,  6.74s/it]

 80%|████████  | 3081/3844 [6:22:14<1:29:07,  7.01s/it]

 80%|████████  | 3082/3844 [6:22:20<1:25:24,  6.72s/it]

 80%|████████  | 3083/3844 [6:22:26<1:21:39,  6.44s/it]

 80%|████████  | 3084/3844 [6:22:36<1:36:06,  7.59s/it]

 80%|████████  | 3085/3844 [6:22:42<1:29:25,  7.07s/it]

 80%|████████  | 3086/3844 [6:22:49<1:26:57,  6.88s/it]

 80%|████████  | 3087/3844 [6:22:59<1:39:37,  7.90s/it]

 80%|████████  | 3088/3844 [6:23:06<1:37:18,  7.72s/it]

 80%|████████  | 3089/3844 [6:23:13<1:33:38,  7.44s/it]


 80%|████████  | 3091/3844 [6:23:28<1:35:33,  7.61s/it]

 80%|████████  | 3092/3844 [6:23:36<1:37:23,  7.77s/it]
{'loss': 1.2077, 'grad_norm': 0.3284176205178854, 'learning_rate': 1.9416485397247796e-06, 'epoch': 0.8}

 80%|████████  | 3093/3844 [6:23:42<1:31:22,  7.30s/it]


 81%|████████  | 3095/3844 [6:24:00<1:39:37,  7.98s/it]

 81%|████████  | 3096/3844 [6:24:08<1:40:20,  8.05s/it]
{'loss': 1.0754, 'grad_norm': 0.34032393772047426, 'learning_rate': 1.9217344601943875e-06, 'epoch': 0.81}

 81%|████████  | 3097/3844 [6:24:13<1:30:29,  7.27s/it]


 81%|████████  | 3099/3844 [6:24:30<1:39:04,  7.98s/it]
{'loss': 1.197, 'grad_norm': 0.3464284446986587, 'learning_rate': 1.9068591240487811e-06, 'epoch': 0.81}

 81%|████████  | 3100/3844 [6:24:36<1:32:07,  7.43s/it]


 81%|████████  | 3102/3844 [6:24:50<1:28:26,  7.15s/it]
{'loss': 1.2251, 'grad_norm': 0.36297682515012986, 'learning_rate': 1.8920355137150892e-06, 'epoch': 0.81}


 81%|████████  | 3104/3844 [6:25:04<1:28:24,  7.17s/it]
{'loss': 1.0843, 'grad_norm': 0.33191518372218387, 'learning_rate': 1.8821818901881117e-06, 'epoch': 0.81}

 81%|████████  | 3105/3844 [6:25:13<1:32:29,  7.51s/it]


 81%|████████  | 3107/3844 [6:25:28<1:34:02,  7.66s/it]
{'loss': 1.1522, 'grad_norm': 0.37363365767091916, 'learning_rate': 1.8674446999159956e-06, 'epoch': 0.81}

 81%|████████  | 3108/3844 [6:25:35<1:29:19,  7.28s/it]


 81%|████████  | 3110/3844 [6:25:50<1:29:27,  7.31s/it]

 81%|████████  | 3111/3844 [6:25:56<1:26:11,  7.06s/it]
{'loss': 1.3071, 'grad_norm': 0.3358399532812997, 'learning_rate': 1.8478759833525128e-06, 'epoch': 0.81}

 81%|████████  | 3112/3844 [6:26:03<1:23:51,  6.87s/it]

 81%|████████  | 3113/3844 [6:26:15<1:45:15,  8.64s/it]

 81%|████████  | 3114/3844 [6:26:23<1:40:16,  8.24s/it]


 81%|████████  | 3116/3844 [6:26:38<1:33:43,  7.72s/it]

 81%|████████  | 3117/3844 [6:26:44<1:29:49,  7.41s/it]
{'loss': 1.1823, 'grad_norm': 0.3731019649119099, 'learning_rate': 1.8186966536033146e-06, 'epoch': 0.81}

 81%|████████  | 3118/3844 [6:26:51<1:26:00,  7.11s/it]


 81%|████████  | 3120/3844 [6:27:04<1:24:33,  7.01s/it]
{'loss': 1.1547, 'grad_norm': 0.34280050221750435, 'learning_rate': 1.8041853761175842e-06, 'epoch': 0.81}

 81%|████████  | 3121/3844 [6:27:13<1:31:20,  7.58s/it]

 81%|████████  | 3122/3844 [6:27:21<1:30:33,  7.53s/it]

 81%|████████  | 3123/3844 [6:27:27<1:26:29,  7.20s/it]


 81%|████████▏ | 3125/3844 [6:27:48<1:49:31,  9.14s/it]

 81%|████████▏ | 3126/3844 [6:27:54<1:37:50,  8.18s/it]

 81%|████████▏ | 3127/3844 [6:28:04<1:42:05,  8.54s/it]

 81%|████████▏ | 3128/3844 [6:28:10<1:33:31,  7.84s/it]

 81%|████████▏ | 3129/3844 [6:28:16<1:28:41,  7.44s/it]
{'loss': 1.1938, 'grad_norm': 0.3424866920365925, 'learning_rate': 1.7609662051634524e-06, 'epoch': 0.81}


 81%|████████▏ | 3131/3844 [6:28:34<1:35:48,  8.06s/it]
{'loss': 1.1005, 'grad_norm': 0.3833192495941385, 'learning_rate': 1.7514262177759235e-06, 'epoch': 0.81}


 82%|████████▏ | 3133/3844 [6:28:48<1:27:31,  7.39s/it]

 82%|████████▏ | 3134/3844 [6:28:54<1:20:51,  6.83s/it]

 82%|████████▏ | 3135/3844 [6:29:00<1:18:18,  6.63s/it]

 82%|████████▏ | 3136/3844 [6:29:06<1:15:18,  6.38s/it]

 82%|████████▏ | 3137/3844 [6:29:12<1:15:30,  6.41s/it]

 82%|████████▏ | 3138/3844 [6:29:19<1:14:36,  6.34s/it]
{'loss': 1.0975, 'grad_norm': 0.3668738627855929, 'learning_rate': 1.7182209565255148e-06, 'epoch': 0.82}


 82%|████████▏ | 3140/3844 [6:29:32<1:16:04,  6.48s/it]

 82%|████████▏ | 3141/3844 [6:29:42<1:28:48,  7.58s/it]

 82%|████████▏ | 3142/3844 [6:29:52<1:36:33,  8.25s/it]
[2024-05-27 17:15:34,146] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 3143/3844 [6:29:58<1:29:47,  7.69s/it]

 82%|████████▏ | 3144/3844 [6:30:05<1:25:18,  7.31s/it]
{'loss': 1.2408, 'grad_norm': 0.3422267794612353, 'learning_rate': 1.6899886296116963e-06, 'epoch': 0.82}

 82%|████████▏ | 3145/3844 [6:30:11<1:22:18,  7.06s/it]

 82%|████████▏ | 3146/3844 [6:30:17<1:18:16,  6.73s/it]


 82%|████████▏ | 3148/3844 [6:30:29<1:12:17,  6.23s/it]

 82%|████████▏ | 3149/3844 [6:30:36<1:17:33,  6.70s/it]
{'loss': 0.9516, 'grad_norm': 0.3600298197412589, 'learning_rate': 1.6666239461498989e-06, 'epoch': 0.82}


 82%|████████▏ | 3151/3844 [6:30:50<1:18:41,  6.81s/it]
{'loss': 1.1326, 'grad_norm': 0.3532511449877192, 'learning_rate': 1.6573194749864462e-06, 'epoch': 0.82}

 82%|████████▏ | 3152/3844 [6:30:57<1:18:53,  6.84s/it]


 82%|████████▏ | 3154/3844 [6:31:14<1:31:59,  8.00s/it]
{'loss': 1.2394, 'grad_norm': 0.3213498986604786, 'learning_rate': 1.643407210410759e-06, 'epoch': 0.82}

 82%|████████▏ | 3155/3844 [6:31:21<1:28:34,  7.71s/it]


 82%|████████▏ | 3157/3844 [6:31:34<1:20:35,  7.04s/it]
{'loss': 1.1685, 'grad_norm': 0.3612239395647909, 'learning_rate': 1.6295483554511293e-06, 'epoch': 0.82}


 82%|████████▏ | 3159/3844 [6:31:52<1:31:27,  8.01s/it]
{'loss': 1.1694, 'grad_norm': 0.359856582230134, 'learning_rate': 1.6203388345757476e-06, 'epoch': 0.82}


 82%|████████▏ | 3161/3844 [6:32:07<1:25:22,  7.50s/it]
{'loss': 1.025, 'grad_norm': 0.35307043913895064, 'learning_rate': 1.6111531168422179e-06, 'epoch': 0.82}

 82%|████████▏ | 3162/3844 [6:32:13<1:21:36,  7.18s/it]

 82%|████████▏ | 3163/3844 [6:32:21<1:24:15,  7.42s/it]


 82%|████████▏ | 3165/3844 [6:32:36<1:23:49,  7.41s/it]

 82%|████████▏ | 3166/3844 [6:32:42<1:19:29,  7.03s/it]

 82%|████████▏ | 3167/3844 [6:32:54<1:35:29,  8.46s/it]

 82%|████████▏ | 3168/3844 [6:33:01<1:28:39,  7.87s/it]
{'loss': 1.1107, 'grad_norm': 0.3610602239469422, 'learning_rate': 1.5791909306209274e-06, 'epoch': 0.82}

 82%|████████▏ | 3169/3844 [6:33:07<1:24:23,  7.50s/it]

 82%|████████▏ | 3170/3844 [6:33:17<1:31:50,  8.18s/it]

 82%|████████▏ | 3171/3844 [6:33:24<1:26:35,  7.72s/it]


 83%|████████▎ | 3173/3844 [6:33:36<1:18:30,  7.02s/it]

 83%|████████▎ | 3174/3844 [6:33:45<1:23:36,  7.49s/it]

 83%|████████▎ | 3175/3844 [6:33:52<1:22:36,  7.41s/it]
{'loss': 0.9848, 'grad_norm': 0.3464648380903409, 'learning_rate': 1.5475217639375385e-06, 'epoch': 0.83}


 83%|████████▎ | 3177/3844 [6:34:08<1:25:44,  7.71s/it]
{'loss': 1.1261, 'grad_norm': 0.3351338420369982, 'learning_rate': 1.5385274049849463e-06, 'epoch': 0.83}


 83%|████████▎ | 3179/3844 [6:34:26<1:33:20,  8.42s/it]
{'loss': 1.2279, 'grad_norm': 0.32285978783092417, 'learning_rate': 1.5295570815665395e-06, 'epoch': 0.83}


 83%|████████▎ | 3181/3844 [6:34:40<1:26:06,  7.79s/it]
{'loss': 1.0313, 'grad_norm': 0.36016105055309744, 'learning_rate': 1.5206108191632873e-06, 'epoch': 0.83}


 83%|████████▎ | 3183/3844 [6:34:54<1:21:49,  7.43s/it]

 83%|████████▎ | 3184/3844 [6:35:01<1:19:03,  7.19s/it]

 83%|████████▎ | 3185/3844 [6:35:09<1:20:24,  7.32s/it]
{'loss': 1.1698, 'grad_norm': 0.3620103688620097, 'learning_rate': 1.5027905789843234e-06, 'epoch': 0.83}


 83%|████████▎ | 3187/3844 [6:35:21<1:13:41,  6.73s/it]
{'loss': 1.1379, 'grad_norm': 0.34630950396150545, 'learning_rate': 1.4939166518285187e-06, 'epoch': 0.83}

 83%|████████▎ | 3188/3844 [6:35:27<1:11:31,  6.54s/it]

 83%|████████▎ | 3189/3844 [6:35:36<1:17:32,  7.10s/it]

 83%|████████▎ | 3190/3844 [6:35:42<1:15:14,  6.90s/it]

 83%|████████▎ | 3191/3844 [6:35:49<1:16:52,  7.06s/it]

 83%|████████▎ | 3192/3844 [6:35:55<1:13:27,  6.76s/it]

 83%|████████▎ | 3193/3844 [6:36:03<1:16:22,  7.04s/it]


 83%|████████▎ | 3195/3844 [6:36:18<1:18:27,  7.25s/it]
{'loss': 1.1636, 'grad_norm': 0.36848200046290486, 'learning_rate': 1.4586628167951688e-06, 'epoch': 0.83}

 83%|████████▎ | 3196/3844 [6:36:26<1:18:59,  7.31s/it]


 83%|████████▎ | 3198/3844 [6:36:41<1:20:08,  7.44s/it]

 83%|████████▎ | 3199/3844 [6:36:48<1:21:23,  7.57s/it]
{'loss': 1.0669, 'grad_norm': 0.36053331633801877, 'learning_rate': 1.441181373686923e-06, 'epoch': 0.83}


 83%|████████▎ | 3201/3844 [6:37:01<1:13:32,  6.86s/it]

 83%|████████▎ | 3202/3844 [6:37:08<1:15:39,  7.07s/it]
{'loss': 1.1188, 'grad_norm': 0.3368855576140219, 'learning_rate': 1.4281340996308223e-06, 'epoch': 0.83}


 83%|████████▎ | 3204/3844 [6:37:23<1:15:55,  7.12s/it]
{'loss': 1.246, 'grad_norm': 0.35264524247615714, 'learning_rate': 1.419466348179399e-06, 'epoch': 0.83}


 83%|████████▎ | 3206/3844 [6:37:36<1:12:40,  6.83s/it]

 83%|████████▎ | 3207/3844 [6:37:42<1:09:32,  6.55s/it]

 83%|████████▎ | 3208/3844 [6:37:49<1:08:54,  6.50s/it]
{'loss': 1.1331, 'grad_norm': 0.35176968033648437, 'learning_rate': 1.402203991040607e-06, 'epoch': 0.83}

 83%|████████▎ | 3209/3844 [6:37:55<1:09:13,  6.54s/it]


 84%|████████▎ | 3211/3844 [6:38:09<1:11:12,  6.75s/it]

 84%|████████▎ | 3212/3844 [6:38:15<1:08:00,  6.46s/it]
{'loss': 1.2022, 'grad_norm': 0.34571048697828904, 'learning_rate': 1.385039324922305e-06, 'epoch': 0.84}


 84%|████████▎ | 3214/3844 [6:38:33<1:21:50,  7.79s/it]
{'loss': 1.1429, 'grad_norm': 0.36519277020715873, 'learning_rate': 1.3764936869863354e-06, 'epoch': 0.84}


 84%|████████▎ | 3216/3844 [6:38:47<1:17:30,  7.41s/it]
{'loss': 1.092, 'grad_norm': 0.34536526867249606, 'learning_rate': 1.367972544855145e-06, 'epoch': 0.84}


 84%|████████▎ | 3218/3844 [6:39:01<1:16:05,  7.29s/it]

 84%|████████▎ | 3219/3844 [6:39:09<1:16:41,  7.36s/it]
{'loss': 1.2796, 'grad_norm': 0.36010976249641646, 'learning_rate': 1.3552368142217743e-06, 'epoch': 0.84}

 84%|████████▍ | 3220/3844 [6:39:20<1:27:52,  8.45s/it]

 84%|████████▍ | 3221/3844 [6:39:27<1:24:59,  8.18s/it]

 84%|████████▍ | 3222/3844 [6:39:35<1:24:36,  8.16s/it]

 84%|████████▍ | 3223/3844 [6:39:42<1:20:27,  7.77s/it]

 84%|████████▍ | 3224/3844 [6:39:48<1:13:32,  7.12s/it]


 84%|████████▍ | 3226/3844 [6:40:00<1:09:27,  6.74s/it]
{'loss': 0.9672, 'grad_norm': 0.3946809147616046, 'learning_rate': 1.3257351160076737e-06, 'epoch': 0.84}

 84%|████████▍ | 3227/3844 [6:40:10<1:17:04,  7.50s/it]

 84%|████████▍ | 3228/3844 [6:40:17<1:17:57,  7.59s/it]


 84%|████████▍ | 3230/3844 [6:40:31<1:13:04,  7.14s/it]

 84%|████████▍ | 3231/3844 [6:40:39<1:17:09,  7.55s/it]
{'loss': 1.1631, 'grad_norm': 0.3391854780639931, 'learning_rate': 1.3048472144069624e-06, 'epoch': 0.84}

 84%|████████▍ | 3232/3844 [6:40:48<1:20:40,  7.91s/it]

 84%|████████▍ | 3233/3844 [6:40:54<1:15:01,  7.37s/it]

 84%|████████▍ | 3234/3844 [6:41:00<1:11:32,  7.04s/it]


 84%|████████▍ | 3236/3844 [6:41:13<1:07:49,  6.69s/it]
{'loss': 1.0484, 'grad_norm': 0.38448739596795006, 'learning_rate': 1.284113683381213e-06, 'epoch': 0.84}

 84%|████████▍ | 3237/3844 [6:41:24<1:19:25,  7.85s/it]


 84%|████████▍ | 3239/3844 [6:41:37<1:12:05,  7.15s/it]
{'loss': 1.037, 'grad_norm': 0.3460461148254884, 'learning_rate': 1.2717478158837026e-06, 'epoch': 0.84}

 84%|████████▍ | 3240/3844 [6:41:44<1:12:20,  7.19s/it]


 84%|████████▍ | 3242/3844 [6:41:59<1:14:35,  7.43s/it]
{'loss': 1.1475, 'grad_norm': 0.3539196740223764, 'learning_rate': 1.2594377333944797e-06, 'epoch': 0.84}

 84%|████████▍ | 3243/3844 [6:42:06<1:13:45,  7.36s/it]


 84%|████████▍ | 3245/3844 [6:42:21<1:15:35,  7.57s/it]
{'loss': 1.1061, 'grad_norm': 0.34684475094937706, 'learning_rate': 1.247183514591156e-06, 'epoch': 0.84}

 84%|████████▍ | 3246/3844 [6:42:28<1:13:44,  7.40s/it]

 84%|████████▍ | 3247/3844 [6:42:36<1:15:38,  7.60s/it]


 85%|████████▍ | 3249/3844 [6:42:53<1:17:52,  7.85s/it]
{'loss': 1.0524, 'grad_norm': 0.3960259927622892, 'learning_rate': 1.2309315905710795e-06, 'epoch': 0.85}


 85%|████████▍ | 3251/3844 [6:43:07<1:13:51,  7.47s/it]
{'loss': 1.0457, 'grad_norm': 0.34148592659367427, 'learning_rate': 1.2228429809669506e-06, 'epoch': 0.85}

 85%|████████▍ | 3252/3844 [6:43:16<1:18:10,  7.92s/it]


 85%|████████▍ | 3254/3844 [6:43:31<1:14:51,  7.61s/it]

 85%|████████▍ | 3255/3844 [6:43:37<1:11:17,  7.26s/it]

 85%|████████▍ | 3256/3844 [6:43:43<1:06:06,  6.75s/it]

 85%|████████▍ | 3257/3844 [6:43:52<1:11:23,  7.30s/it]
{'loss': 1.1381, 'grad_norm': 0.35098366850475177, 'learning_rate': 1.1987268372821548e-06, 'epoch': 0.85}

 85%|████████▍ | 3258/3844 [6:44:00<1:14:14,  7.60s/it]


 85%|████████▍ | 3260/3844 [6:44:17<1:18:10,  8.03s/it]
{'loss': 1.17, 'grad_norm': 0.346186519719592, 'learning_rate': 1.18675310455858e-06, 'epoch': 0.85}


 85%|████████▍ | 3262/3844 [6:44:31<1:13:49,  7.61s/it]

 85%|████████▍ | 3263/3844 [6:44:38<1:11:27,  7.38s/it]
{'loss': 1.1539, 'grad_norm': 0.3666875308499404, 'learning_rate': 1.1748357000712574e-06, 'epoch': 0.85}

 85%|████████▍ | 3264/3844 [6:44:47<1:16:22,  7.90s/it]


 85%|████████▍ | 3266/3844 [6:45:01<1:12:33,  7.53s/it]

 85%|████████▍ | 3267/3844 [6:45:07<1:07:04,  6.98s/it]
{'loss': 1.1734, 'grad_norm': 0.33302974461865237, 'learning_rate': 1.159033580718607e-06, 'epoch': 0.85}


 85%|████████▌ | 3269/3844 [6:45:19<1:02:51,  6.56s/it]

 85%|████████▌ | 3270/3844 [6:45:25<1:01:08,  6.39s/it]
{'loss': 1.3037, 'grad_norm': 0.3760977290737116, 'learning_rate': 1.1472479043678276e-06, 'epoch': 0.85}

 85%|████████▌ | 3271/3844 [6:45:34<1:07:30,  7.07s/it]


 85%|████████▌ | 3273/3844 [6:45:51<1:13:55,  7.77s/it]
{'loss': 1.1486, 'grad_norm': 0.3585424610598366, 'learning_rate': 1.135518808743471e-06, 'epoch': 0.85}


 85%|████████▌ | 3275/3844 [6:46:09<1:18:14,  8.25s/it]
{'loss': 1.024, 'grad_norm': 0.35855741047749756, 'learning_rate': 1.1277308824317956e-06, 'epoch': 0.85}

 85%|████████▌ | 3276/3844 [6:46:16<1:14:02,  7.82s/it]


 85%|████████▌ | 3278/3844 [6:46:35<1:22:03,  8.70s/it]
[2024-05-27 17:32:17,221] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 3279/3844 [6:46:43<1:19:52,  8.48s/it]

 85%|████████▌ | 3280/3844 [6:46:51<1:19:09,  8.42s/it]
{'loss': 1.0738, 'grad_norm': 0.35318344658784195, 'learning_rate': 1.108371375529561e-06, 'epoch': 0.85}

 85%|████████▌ | 3281/3844 [6:46:58<1:14:31,  7.94s/it]


 85%|████████▌ | 3283/3844 [6:47:13<1:11:11,  7.61s/it]
{'loss': 1.1001, 'grad_norm': 0.34185634138657184, 'learning_rate': 1.0968314217008414e-06, 'epoch': 0.85}

 85%|████████▌ | 3284/3844 [6:47:20<1:09:56,  7.49s/it]

 85%|████████▌ | 3285/3844 [6:47:26<1:05:58,  7.08s/it]

 85%|████████▌ | 3286/3844 [6:47:34<1:07:33,  7.26s/it]

 86%|████████▌ | 3287/3844 [6:47:40<1:03:53,  6.88s/it]


 86%|████████▌ | 3289/3844 [6:47:54<1:03:54,  6.91s/it]
{'loss': 1.1329, 'grad_norm': 0.3606096761261192, 'learning_rate': 1.0739222962973227e-06, 'epoch': 0.86}


 86%|████████▌ | 3291/3844 [6:48:07<1:02:41,  6.80s/it]
{'loss': 1.157, 'grad_norm': 0.34718596997535106, 'learning_rate': 1.0663366028815937e-06, 'epoch': 0.86}

 86%|████████▌ | 3292/3844 [6:48:15<1:05:11,  7.09s/it]


 86%|████████▌ | 3294/3844 [6:48:31<1:08:55,  7.52s/it]
{'loss': 1.2373, 'grad_norm': 0.36456695523358335, 'learning_rate': 1.0550056510327534e-06, 'epoch': 0.86}

 86%|████████▌ | 3295/3844 [6:48:38<1:08:03,  7.44s/it]


 86%|████████▌ | 3297/3844 [6:48:52<1:04:47,  7.11s/it]

 86%|████████▌ | 3298/3844 [6:48:59<1:05:32,  7.20s/it]
{'loss': 1.1178, 'grad_norm': 0.3422750669958608, 'learning_rate': 1.0399866592386642e-06, 'epoch': 0.86}

 86%|████████▌ | 3299/3844 [6:49:07<1:06:24,  7.31s/it]

 86%|████████▌ | 3300/3844 [6:49:14<1:07:40,  7.46s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[2024-05-27 17:35:52,852] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8641, 'grad_norm': 0.34367338615196835, 'learning_rate': 1.0287892167210544e-06, 'epoch': 0.86}

 86%|████████▌ | 3302/3844 [6:50:17<2:37:48, 17.47s/it]
{'loss': 1.2055, 'grad_norm': 0.3566227368017163, 'learning_rate': 1.0250694740989743e-06, 'epoch': 0.86}


 86%|████████▌ | 3304/3844 [6:50:34<1:56:00, 12.89s/it]
{'loss': 1.153, 'grad_norm': 0.32661528026227465, 'learning_rate': 1.0176491120366482e-06, 'epoch': 0.86}


 86%|████████▌ | 3306/3844 [6:50:51<1:38:12, 10.95s/it]

 86%|████████▌ | 3307/3844 [6:50:57<1:24:48,  9.48s/it]

 86%|████████▌ | 3308/3844 [6:51:06<1:21:35,  9.13s/it]

 86%|████████▌ | 3309/3844 [6:51:11<1:11:45,  8.05s/it]

 86%|████████▌ | 3310/3844 [6:51:19<1:11:29,  8.03s/it]

 86%|████████▌ | 3311/3844 [6:51:26<1:08:05,  7.67s/it]
{'loss': 1.1675, 'grad_norm': 0.3716806142011012, 'learning_rate': 9.918789141652152e-07, 'epoch': 0.86}

 86%|████████▌ | 3312/3844 [6:51:32<1:03:51,  7.20s/it]
[2024-05-27 17:37:27,310] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 86%|████████▌ | 3314/3844 [6:51:51<1:11:40,  8.11s/it]
{'loss': 1.3119, 'grad_norm': 0.33260474872210377, 'learning_rate': 9.809304477511694e-07, 'epoch': 0.86}

 86%|████████▌ | 3315/3844 [6:51:58<1:08:58,  7.82s/it]

 86%|████████▋ | 3316/3844 [6:52:05<1:05:10,  7.41s/it]


 86%|████████▋ | 3318/3844 [6:52:18<1:00:20,  6.88s/it]

 86%|████████▋ | 3319/3844 [6:52:25<1:02:42,  7.17s/it]

 86%|████████▋ | 3320/3844 [6:52:32<1:01:11,  7.01s/it]
{'loss': 1.131, 'grad_norm': 0.3306277908742031, 'learning_rate': 9.59206515670753e-07, 'epoch': 0.86}


 86%|████████▋ | 3322/3844 [6:52:48<1:06:54,  7.69s/it]
{'loss': 1.1616, 'grad_norm': 0.36705952525099556, 'learning_rate': 9.520165399131808e-07, 'epoch': 0.86}

 86%|████████▋ | 3323/3844 [6:52:56<1:08:07,  7.85s/it]


 86%|████████▋ | 3325/3844 [6:53:12<1:08:34,  7.93s/it]
{'loss': 1.0435, 'grad_norm': 0.3649288749743526, 'learning_rate': 9.412797730798351e-07, 'epoch': 0.86}

 87%|████████▋ | 3326/3844 [6:53:21<1:10:10,  8.13s/it]


 87%|████████▋ | 3328/3844 [6:53:32<59:19,  6.90s/it]

 87%|████████▋ | 3329/3844 [6:53:42<1:06:50,  7.79s/it]
{'loss': 1.0673, 'grad_norm': 0.3443363899206291, 'learning_rate': 9.270541579838632e-07, 'epoch': 0.87}


 87%|████████▋ | 3331/3844 [6:53:56<1:03:34,  7.44s/it]
{'loss': 1.2051, 'grad_norm': 0.3266447332253999, 'learning_rate': 9.19979999066003e-07, 'epoch': 0.87}


 87%|████████▋ | 3333/3844 [6:54:10<1:01:43,  7.25s/it]

 87%|████████▋ | 3334/3844 [6:54:16<57:52,  6.81s/it]
{'loss': 1.1719, 'grad_norm': 0.3477267860669171, 'learning_rate': 9.094171280190811e-07, 'epoch': 0.87}


 87%|████████▋ | 3336/3844 [6:54:34<1:05:01,  7.68s/it]

 87%|████████▋ | 3337/3844 [6:54:44<1:10:42,  8.37s/it]

 87%|████████▋ | 3338/3844 [6:54:50<1:04:50,  7.69s/it]
{'loss': 1.1157, 'grad_norm': 0.3635235730115322, 'learning_rate': 8.954236905592561e-07, 'epoch': 0.87}

 87%|████████▋ | 3339/3844 [6:54:55<58:52,  7.00s/it]

 87%|████████▋ | 3340/3844 [6:55:03<1:01:32,  7.33s/it]

 87%|████████▋ | 3341/3844 [6:55:09<57:39,  6.88s/it]

 87%|████████▋ | 3342/3844 [6:55:15<55:52,  6.68s/it]


 87%|████████▋ | 3344/3844 [6:55:26<51:04,  6.13s/it]
{'loss': 1.1527, 'grad_norm': 0.3377684272376333, 'learning_rate': 8.746275510356383e-07, 'epoch': 0.87}

 87%|████████▋ | 3345/3844 [6:55:34<55:48,  6.71s/it]


 87%|████████▋ | 3347/3844 [6:55:50<59:57,  7.24s/it]
{'loss': 1.1781, 'grad_norm': 0.339739270993338, 'learning_rate': 8.643169328316759e-07, 'epoch': 0.87}

 87%|████████▋ | 3348/3844 [6:55:59<1:04:33,  7.81s/it]


 87%|████████▋ | 3350/3844 [6:56:12<58:38,  7.12s/it]

 87%|████████▋ | 3351/3844 [6:56:17<54:42,  6.66s/it]

 87%|████████▋ | 3352/3844 [6:56:26<59:26,  7.25s/it]

 87%|████████▋ | 3353/3844 [6:56:32<55:49,  6.82s/it]

 87%|████████▋ | 3354/3844 [6:56:40<58:33,  7.17s/it]
{'loss': 1.1045, 'grad_norm': 0.33133210577380556, 'learning_rate': 8.404860053285136e-07, 'epoch': 0.87}


 87%|████████▋ | 3356/3844 [6:56:56<1:00:47,  7.47s/it]

 87%|████████▋ | 3357/3844 [6:57:06<1:07:05,  8.27s/it]

 87%|████████▋ | 3358/3844 [6:57:12<1:02:24,  7.71s/it]

 87%|████████▋ | 3359/3844 [6:57:20<1:01:51,  7.65s/it]

 87%|████████▋ | 3360/3844 [6:57:26<58:58,  7.31s/it]
{'loss': 1.112, 'grad_norm': 0.3564234237236647, 'learning_rate': 8.203131434432255e-07, 'epoch': 0.87}

 87%|████████▋ | 3361/3844 [6:57:33<56:45,  7.05s/it]


 87%|████████▋ | 3363/3844 [6:57:46<55:09,  6.88s/it]
{'loss': 1.1247, 'grad_norm': 0.38835878672038626, 'learning_rate': 8.103146857656019e-07, 'epoch': 0.87}

 88%|████████▊ | 3364/3844 [6:57:53<56:07,  7.02s/it]

 88%|████████▊ | 3365/3844 [6:58:01<56:46,  7.11s/it]

 88%|████████▊ | 3366/3844 [6:58:07<55:13,  6.93s/it]

 88%|████████▊ | 3367/3844 [6:58:16<58:09,  7.31s/it]


 88%|████████▊ | 3369/3844 [6:58:30<58:33,  7.40s/it]
{'loss': 1.0358, 'grad_norm': 0.3683237660771596, 'learning_rate': 7.90494036456444e-07, 'epoch': 0.88}


 88%|████████▊ | 3371/3844 [6:58:44<56:58,  7.23s/it]
{'loss': 1.1614, 'grad_norm': 0.3433821608393335, 'learning_rate': 7.839394491778763e-07, 'epoch': 0.88}

 88%|████████▊ | 3372/3844 [6:58:51<56:35,  7.19s/it]


 88%|████████▊ | 3374/3844 [6:59:06<58:20,  7.45s/it]
{'loss': 1.1493, 'grad_norm': 0.3116930012125198, 'learning_rate': 7.741566596935912e-07, 'epoch': 0.88}

 88%|████████▊ | 3375/3844 [6:59:14<58:00,  7.42s/it]

 88%|████████▊ | 3376/3844 [6:59:21<57:34,  7.38s/it]

 88%|████████▊ | 3377/3844 [6:59:28<56:17,  7.23s/it]

 88%|████████▊ | 3378/3844 [6:59:35<56:19,  7.25s/it]

 88%|████████▊ | 3379/3844 [6:59:43<58:20,  7.53s/it]

 88%|████████▊ | 3380/3844 [6:59:51<59:38,  7.71s/it]


 88%|████████▊ | 3382/3844 [7:00:04<54:05,  7.02s/it]
{'loss': 1.0559, 'grad_norm': 0.357184900934939, 'learning_rate': 7.483576643487856e-07, 'epoch': 0.88}


 88%|████████▊ | 3384/3844 [7:00:20<57:22,  7.48s/it]

 88%|████████▊ | 3385/3844 [7:00:26<53:53,  7.04s/it]
{'loss': 1.2465, 'grad_norm': 0.3248269707162417, 'learning_rate': 7.387913836646532e-07, 'epoch': 0.88}

 88%|████████▊ | 3386/3844 [7:00:35<57:26,  7.52s/it]

 88%|████████▊ | 3387/3844 [7:00:41<54:35,  7.17s/it]

 88%|████████▊ | 3388/3844 [7:00:47<50:55,  6.70s/it]

 88%|████████▊ | 3389/3844 [7:00:53<50:17,  6.63s/it]

 88%|████████▊ | 3390/3844 [7:00:59<47:21,  6.26s/it]

 88%|████████▊ | 3391/3844 [7:01:05<47:48,  6.33s/it]

 88%|████████▊ | 3392/3844 [7:01:11<46:58,  6.24s/it]

 88%|████████▊ | 3393/3844 [7:01:18<48:15,  6.42s/it]

 88%|████████▊ | 3394/3844 [7:01:25<50:07,  6.68s/it]

 88%|████████▊ | 3395/3844 [7:01:32<48:59,  6.55s/it]
[2024-05-27 17:47:26,487] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 3397/3844 [7:01:51<58:10,  7.81s/it]

 88%|████████▊ | 3398/3844 [7:01:57<53:43,  7.23s/it]
{'loss': 1.244, 'grad_norm': 0.3735669831428219, 'learning_rate': 6.980222663906932e-07, 'epoch': 0.88}

 88%|████████▊ | 3399/3844 [7:02:06<58:19,  7.86s/it]

 88%|████████▊ | 3400/3844 [7:02:20<1:12:02,  9.74s/it]


 89%|████████▊ | 3402/3844 [7:02:35<1:02:26,  8.48s/it]
{'loss': 1.0666, 'grad_norm': 0.3257979808341099, 'learning_rate': 6.857022928602419e-07, 'epoch': 0.88}

 89%|████████▊ | 3403/3844 [7:02:40<55:52,  7.60s/it]

 89%|████████▊ | 3404/3844 [7:02:46<50:54,  6.94s/it]

 89%|████████▊ | 3405/3844 [7:02:53<52:55,  7.23s/it]

 89%|████████▊ | 3406/3844 [7:03:00<50:28,  6.91s/it]
[2024-05-27 17:48:52,656] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▊ | 3407/3844 [7:03:10<58:37,  8.05s/it]

 89%|████████▊ | 3408/3844 [7:03:17<55:25,  7.63s/it]

 89%|████████▊ | 3409/3844 [7:03:23<52:17,  7.21s/it]

 89%|████████▊ | 3410/3844 [7:03:34<1:00:20,  8.34s/it]

 89%|████████▊ | 3411/3844 [7:03:40<54:22,  7.53s/it]

 89%|████████▉ | 3412/3844 [7:03:47<54:15,  7.54s/it]

 89%|████████▉ | 3413/3844 [7:04:02<1:10:07,  9.76s/it]

 89%|████████▉ | 3414/3844 [7:04:14<1:14:44, 10.43s/it]

 89%|████████▉ | 3415/3844 [7:04:23<1:11:21,  9.98s/it]


 89%|████████▉ | 3417/3844 [7:04:37<59:24,  8.35s/it]

 89%|████████▉ | 3418/3844 [7:04:43<54:16,  7.65s/it]
{'loss': 1.1726, 'grad_norm': 0.3422666649424875, 'learning_rate': 6.374821020409994e-07, 'epoch': 0.89}

 89%|████████▉ | 3419/3844 [7:04:52<56:40,  8.00s/it]


 89%|████████▉ | 3421/3844 [7:05:05<50:56,  7.23s/it]
{'loss': 1.0188, 'grad_norm': 0.34620628463750946, 'learning_rate': 6.286300438071035e-07, 'epoch': 0.89}

 89%|████████▉ | 3422/3844 [7:05:16<59:02,  8.40s/it]

 89%|████████▉ | 3423/3844 [7:05:22<55:11,  7.87s/it]

 89%|████████▉ | 3424/3844 [7:05:32<58:59,  8.43s/it]

 89%|████████▉ | 3425/3844 [7:05:38<53:37,  7.68s/it]


 89%|████████▉ | 3427/3844 [7:05:55<56:57,  8.19s/it]
{'loss': 1.0216, 'grad_norm': 0.3235344812004316, 'learning_rate': 6.111056696622564e-07, 'epoch': 0.89}


 89%|████████▉ | 3429/3844 [7:06:07<48:53,  7.07s/it]
{'loss': 1.1534, 'grad_norm': 0.3368640517841933, 'learning_rate': 6.053175294748193e-07, 'epoch': 0.89}


 89%|████████▉ | 3431/3844 [7:06:19<45:14,  6.57s/it]

 89%|████████▉ | 3432/3844 [7:06:25<43:18,  6.31s/it]
{'loss': 1.1909, 'grad_norm': 0.34684981607505067, 'learning_rate': 5.966853613042001e-07, 'epoch': 0.89}

 89%|████████▉ | 3433/3844 [7:06:33<46:13,  6.75s/it]

 89%|████████▉ | 3434/3844 [7:06:41<49:59,  7.32s/it]

 89%|████████▉ | 3435/3844 [7:06:48<49:17,  7.23s/it]

 89%|████████▉ | 3436/3844 [7:06:55<49:05,  7.22s/it]

 89%|████████▉ | 3437/3844 [7:07:02<47:21,  6.98s/it]

 89%|████████▉ | 3438/3844 [7:07:11<50:42,  7.49s/it]

 89%|████████▉ | 3439/3844 [7:07:16<46:54,  6.95s/it]

 89%|████████▉ | 3440/3844 [7:07:23<45:45,  6.80s/it]

 90%|████████▉ | 3441/3844 [7:07:30<45:56,  6.84s/it]

 90%|████████▉ | 3442/3844 [7:07:36<44:03,  6.58s/it]

 90%|████████▉ | 3443/3844 [7:07:42<42:47,  6.40s/it]

 90%|████████▉ | 3444/3844 [7:07:52<51:16,  7.69s/it]

 90%|████████▉ | 3445/3844 [7:08:00<51:11,  7.70s/it]


 90%|████████▉ | 3447/3844 [7:08:15<49:04,  7.42s/it]
{'loss': 1.2431, 'grad_norm': 0.36167628630521004, 'learning_rate': 5.544271035930559e-07, 'epoch': 0.9}


 90%|████████▉ | 3449/3844 [7:08:27<44:29,  6.76s/it]
{'loss': 1.1377, 'grad_norm': 0.3294634755402972, 'learning_rate': 5.489065547232009e-07, 'epoch': 0.9}

 90%|████████▉ | 3450/3844 [7:08:33<42:10,  6.42s/it]

 90%|████████▉ | 3451/3844 [7:08:41<46:53,  7.16s/it]

 90%|████████▉ | 3452/3844 [7:08:49<46:31,  7.12s/it]


 90%|████████▉ | 3454/3844 [7:09:01<43:16,  6.66s/it]
{'loss': 1.2145, 'grad_norm': 0.3864802252878977, 'learning_rate': 5.35222670699358e-07, 'epoch': 0.9}

 90%|████████▉ | 3455/3844 [7:09:07<41:38,  6.42s/it]

 90%|████████▉ | 3456/3844 [7:09:12<39:59,  6.18s/it]


 90%|████████▉ | 3458/3844 [7:09:27<43:28,  6.76s/it]
{'loss': 1.1189, 'grad_norm': 0.3361338791799342, 'learning_rate': 5.24396536549725e-07, 'epoch': 0.9}

 90%|████████▉ | 3459/3844 [7:09:35<44:37,  6.95s/it]

 90%|█████████ | 3460/3844 [7:09:40<42:08,  6.59s/it]

 90%|█████████ | 3461/3844 [7:09:51<49:11,  7.71s/it]


 90%|█████████ | 3463/3844 [7:10:05<46:24,  7.31s/it]
{'loss': 1.1425, 'grad_norm': 0.37049841553253665, 'learning_rate': 5.110152870128971e-07, 'epoch': 0.9}

 90%|█████████ | 3464/3844 [7:10:11<43:08,  6.81s/it]

 90%|█████████ | 3465/3844 [7:10:20<47:00,  7.44s/it]

 90%|█████████ | 3466/3844 [7:10:28<48:17,  7.67s/it]

 90%|█████████ | 3467/3844 [7:10:35<46:34,  7.41s/it]

 90%|█████████ | 3468/3844 [7:10:44<50:18,  8.03s/it]

 90%|█████████ | 3469/3844 [7:10:52<49:14,  7.88s/it]

 90%|█████████ | 3470/3844 [7:10:58<46:15,  7.42s/it]

 90%|█████████ | 3471/3844 [7:11:07<48:20,  7.78s/it]


 90%|█████████ | 3473/3844 [7:11:21<46:24,  7.51s/it]

 90%|█████████ | 3474/3844 [7:11:27<43:21,  7.03s/it]
{'loss': 1.0314, 'grad_norm': 0.36414333080036027, 'learning_rate': 4.821698613045511e-07, 'epoch': 0.9}

 90%|█████████ | 3475/3844 [7:11:34<43:04,  7.00s/it]


 90%|█████████ | 3477/3844 [7:11:47<41:15,  6.75s/it]
{'loss': 1.1368, 'grad_norm': 0.34611298634565446, 'learning_rate': 4.744447635231764e-07, 'epoch': 0.9}


 91%|█████████ | 3479/3844 [7:12:01<41:57,  6.90s/it]

 91%|█████████ | 3480/3844 [7:12:09<43:38,  7.19s/it]
{'loss': 1.1972, 'grad_norm': 0.3583562253830199, 'learning_rate': 4.6678054656497216e-07, 'epoch': 0.91}


 91%|█████████ | 3482/3844 [7:12:25<46:21,  7.68s/it]
{'loss': 1.0192, 'grad_norm': 0.3551079027572219, 'learning_rate': 4.617049154782349e-07, 'epoch': 0.91}

 91%|█████████ | 3483/3844 [7:12:31<43:09,  7.17s/it]

 91%|█████████ | 3484/3844 [7:12:38<42:26,  7.07s/it]


 91%|█████████ | 3486/3844 [7:12:55<47:18,  7.93s/it]
{'loss': 1.0452, 'grad_norm': 0.3597183296164114, 'learning_rate': 4.516349506663431e-07, 'epoch': 0.91}


 91%|█████████ | 3488/3844 [7:13:07<40:55,  6.90s/it]

 91%|█████████ | 3489/3844 [7:13:13<38:45,  6.55s/it]
{'loss': 1.0111, 'grad_norm': 0.3855896567237032, 'learning_rate': 4.441536685261938e-07, 'epoch': 0.91}


 91%|█████████ | 3491/3844 [7:13:29<44:18,  7.53s/it]
{'loss': 0.8766, 'grad_norm': 0.33350511524807036, 'learning_rate': 4.392000743989822e-07, 'epoch': 0.91}

 91%|█████████ | 3492/3844 [7:13:36<42:23,  7.23s/it]

 91%|█████████ | 3493/3844 [7:13:44<43:56,  7.51s/it]

 91%|█████████ | 3494/3844 [7:13:52<44:47,  7.68s/it]

 91%|█████████ | 3495/3844 [7:13:58<41:59,  7.22s/it]

 91%|█████████ | 3496/3844 [7:14:05<41:03,  7.08s/it]

 91%|█████████ | 3497/3844 [7:14:14<43:57,  7.60s/it]


 91%|█████████ | 3499/3844 [7:14:29<43:49,  7.62s/it]
{'loss': 1.1141, 'grad_norm': 0.3550637103001105, 'learning_rate': 4.196574201155934e-07, 'epoch': 0.91}

 91%|█████████ | 3500/3844 [7:14:38<45:51,  8.00s/it]

 91%|█████████ | 3501/3844 [7:14:46<45:23,  7.94s/it]

 91%|█████████ | 3502/3844 [7:14:55<46:45,  8.20s/it]

 91%|█████████ | 3503/3844 [7:15:01<42:52,  7.54s/it]

 91%|█████████ | 3504/3844 [7:15:06<39:32,  6.98s/it]

 91%|█████████ | 3505/3844 [7:15:14<40:17,  7.13s/it]


 91%|█████████ | 3507/3844 [7:15:31<45:43,  8.14s/it]

 91%|█████████▏| 3508/3844 [7:15:37<42:14,  7.54s/it]
{'loss': 1.2409, 'grad_norm': 0.3605083478197623, 'learning_rate': 3.9819243920328764e-07, 'epoch': 0.91}

 91%|█████████▏| 3509/3844 [7:15:43<38:44,  6.94s/it]

 91%|█████████▏| 3510/3844 [7:15:50<39:33,  7.11s/it]

 91%|█████████▏| 3511/3844 [7:16:02<46:58,  8.46s/it]


 91%|█████████▏| 3513/3844 [7:16:17<44:16,  8.02s/it]
{'loss': 1.1652, 'grad_norm': 0.3583868717742646, 'learning_rate': 3.8650602506728143e-07, 'epoch': 0.91}


 91%|█████████▏| 3515/3844 [7:16:32<41:07,  7.50s/it]

 91%|█████████▏| 3516/3844 [7:16:39<41:17,  7.55s/it]
{'loss': 0.9922, 'grad_norm': 0.3500351397377908, 'learning_rate': 3.7957608717875017e-07, 'epoch': 0.91}

 91%|█████████▏| 3517/3844 [7:16:47<40:39,  7.46s/it]

 92%|█████████▏| 3518/3844 [7:16:54<40:25,  7.44s/it]

 92%|█████████▏| 3519/3844 [7:17:00<38:42,  7.15s/it]

 92%|█████████▏| 3520/3844 [7:17:07<37:29,  6.94s/it]

 92%|█████████▏| 3521/3844 [7:17:15<38:49,  7.21s/it]

 92%|█████████▏| 3522/3844 [7:17:21<37:00,  6.90s/it]

 92%|█████████▏| 3523/3844 [7:17:28<37:36,  7.03s/it]

 92%|█████████▏| 3524/3844 [7:17:37<40:20,  7.56s/it]


 92%|█████████▏| 3526/3844 [7:17:51<38:51,  7.33s/it]
{'loss': 1.1417, 'grad_norm': 0.3583829637846013, 'learning_rate': 3.5692061399487667e-07, 'epoch': 0.92}

 92%|█████████▏| 3527/3844 [7:17:58<37:36,  7.12s/it]


 92%|█████████▏| 3529/3844 [7:18:12<36:56,  7.04s/it]
{'loss': 1.1641, 'grad_norm': 0.3252894620810379, 'learning_rate': 3.502574352955768e-07, 'epoch': 0.92}

 92%|█████████▏| 3530/3844 [7:18:21<39:39,  7.58s/it]

 92%|█████████▏| 3531/3844 [7:18:26<36:26,  6.99s/it]

 92%|█████████▏| 3532/3844 [7:18:33<35:32,  6.83s/it]

 92%|█████████▏| 3533/3844 [7:18:38<33:34,  6.48s/it]

 92%|█████████▏| 3534/3844 [7:18:45<33:08,  6.42s/it]


 92%|█████████▏| 3536/3844 [7:18:58<33:57,  6.62s/it]
{'loss': 1.1858, 'grad_norm': 0.3513595918492003, 'learning_rate': 3.3494993664559326e-07, 'epoch': 0.92}

 92%|█████████▏| 3537/3844 [7:19:05<34:47,  6.80s/it]

 92%|█████████▏| 3538/3844 [7:19:13<35:43,  7.00s/it]

 92%|█████████▏| 3539/3844 [7:19:18<33:52,  6.66s/it]

 92%|█████████▏| 3540/3844 [7:19:24<32:26,  6.40s/it]

 92%|█████████▏| 3541/3844 [7:19:30<32:07,  6.36s/it]


 92%|█████████▏| 3543/3844 [7:19:46<34:25,  6.86s/it]
{'loss': 1.1471, 'grad_norm': 0.3574263998769154, 'learning_rate': 3.1997875348263483e-07, 'epoch': 0.92}

 92%|█████████▏| 3544/3844 [7:19:53<34:20,  6.87s/it]

 92%|█████████▏| 3545/3844 [7:19:58<32:23,  6.50s/it]

 92%|█████████▏| 3546/3844 [7:20:09<37:50,  7.62s/it]

 92%|█████████▏| 3547/3844 [7:20:15<35:58,  7.27s/it]


 92%|█████████▏| 3549/3844 [7:20:28<33:09,  6.74s/it]
{'loss': 1.0856, 'grad_norm': 0.37602685371251116, 'learning_rate': 3.0741438565386914e-07, 'epoch': 0.92}


 92%|█████████▏| 3551/3844 [7:20:40<31:15,  6.40s/it]

 92%|█████████▏| 3552/3844 [7:20:46<30:13,  6.21s/it]

 92%|█████████▏| 3553/3844 [7:20:52<29:48,  6.15s/it]
{'loss': 1.1801, 'grad_norm': 0.3434254927841332, 'learning_rate': 2.991757836850906e-07, 'epoch': 0.92}

 92%|█████████▏| 3554/3844 [7:20:57<29:18,  6.06s/it]


 93%|█████████▎| 3556/3844 [7:21:10<29:44,  6.20s/it]
{'loss': 1.1564, 'grad_norm': 0.3594065878939786, 'learning_rate': 2.930691616324854e-07, 'epoch': 0.92}

 93%|█████████▎| 3557/3844 [7:21:16<29:39,  6.20s/it]


 93%|█████████▎| 3559/3844 [7:21:32<34:20,  7.23s/it]
{'loss': 0.9612, 'grad_norm': 0.34106617810814854, 'learning_rate': 2.870245796316384e-07, 'epoch': 0.93}

 93%|█████████▎| 3560/3844 [7:21:41<37:27,  7.91s/it]

 93%|█████████▎| 3561/3844 [7:21:47<34:05,  7.23s/it]

 93%|█████████▎| 3562/3844 [7:21:55<34:19,  7.30s/it]

 93%|█████████▎| 3563/3844 [7:22:03<36:15,  7.74s/it]

 93%|█████████▎| 3564/3844 [7:22:14<40:34,  8.70s/it]

 93%|█████████▎| 3565/3844 [7:22:21<37:50,  8.14s/it]

 93%|█████████▎| 3566/3844 [7:22:29<36:48,  7.95s/it]

 93%|█████████▎| 3567/3844 [7:22:34<33:32,  7.26s/it]

 93%|█████████▎| 3568/3844 [7:22:41<33:19,  7.24s/it]

 93%|█████████▎| 3569/3844 [7:22:50<34:28,  7.52s/it]


 93%|█████████▎| 3571/3844 [7:23:14<44:50,  9.86s/it]

 93%|█████████▎| 3572/3844 [7:23:24<44:54,  9.91s/it]
{'loss': 0.9603, 'grad_norm': 0.31343603369632683, 'learning_rate': 2.6154923260801934e-07, 'epoch': 0.93}

 93%|█████████▎| 3573/3844 [7:23:30<39:19,  8.71s/it]

 93%|█████████▎| 3574/3844 [7:23:37<37:14,  8.28s/it]


 93%|█████████▎| 3576/3844 [7:23:52<35:25,  7.93s/it]
{'loss': 1.0719, 'grad_norm': 0.3432517149030166, 'learning_rate': 2.5394565845178296e-07, 'epoch': 0.93}

 93%|█████████▎| 3577/3844 [7:24:00<36:09,  8.13s/it]

 93%|█████████▎| 3578/3844 [7:24:08<34:40,  7.82s/it]

 93%|█████████▎| 3579/3844 [7:24:15<33:47,  7.65s/it]

 93%|█████████▎| 3580/3844 [7:24:22<32:43,  7.44s/it]

 93%|█████████▎| 3581/3844 [7:24:29<32:50,  7.49s/it]

 93%|█████████▎| 3582/3844 [7:24:36<32:09,  7.37s/it]

 93%|█████████▎| 3583/3844 [7:24:43<31:00,  7.13s/it]


 93%|█████████▎| 3585/3844 [7:24:56<30:07,  6.98s/it]
{'loss': 1.1937, 'grad_norm': 0.3763201760682078, 'learning_rate': 2.3724263167768214e-07, 'epoch': 0.93}

 93%|█████████▎| 3586/3844 [7:25:03<29:07,  6.77s/it]

 93%|█████████▎| 3587/3844 [7:25:09<28:38,  6.69s/it]

 93%|█████████▎| 3588/3844 [7:25:17<30:22,  7.12s/it]

 93%|█████████▎| 3589/3844 [7:25:23<28:52,  6.79s/it]


 93%|█████████▎| 3591/3844 [7:25:38<29:46,  7.06s/it]
{'loss': 1.1829, 'grad_norm': 0.34451585280543323, 'learning_rate': 2.2641920547009978e-07, 'epoch': 0.93}

 93%|█████████▎| 3592/3844 [7:25:45<29:58,  7.14s/it]


 93%|█████████▎| 3594/3844 [7:25:58<28:00,  6.72s/it]
{'loss': 1.0084, 'grad_norm': 0.33637643228280006, 'learning_rate': 2.2110117422100562e-07, 'epoch': 0.93}

 94%|█████████▎| 3595/3844 [7:26:04<26:37,  6.41s/it]


 94%|█████████▎| 3597/3844 [7:26:16<26:11,  6.36s/it]

 94%|█████████▎| 3598/3844 [7:26:22<25:21,  6.18s/it]

 94%|█████████▎| 3599/3844 [7:26:28<25:04,  6.14s/it]
{'loss': 0.9774, 'grad_norm': 0.3529615184027393, 'learning_rate': 2.1237669434722165e-07, 'epoch': 0.94}

 94%|█████████▎| 3600/3844 [7:26:34<24:21,  5.99s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1543, 'grad_norm': 0.37408875844269396, 'learning_rate': 2.0893554827903117e-07, 'epoch': 0.94}
 94%|█████████▎| 3601/3844 [7:27:25<1:19:40, 19.67s/it]

 94%|█████████▎| 3602/3844 [7:27:34<1:05:37, 16.27s/it]

 94%|█████████▎| 3603/3844 [7:27:39<52:29, 13.07s/it]


 94%|█████████▍| 3605/3844 [7:27:52<38:29,  9.66s/it]
{'loss': 1.0473, 'grad_norm': 0.3601339565868326, 'learning_rate': 2.021367028981025e-07, 'epoch': 0.94}


 94%|█████████▍| 3607/3844 [7:28:08<35:34,  9.01s/it]
{'loss': 0.9688, 'grad_norm': 0.3321421818334725, 'learning_rate': 1.987790228980646e-07, 'epoch': 0.94}

 94%|█████████▍| 3608/3844 [7:28:15<32:45,  8.33s/it]

 94%|█████████▍| 3609/3844 [7:28:21<30:17,  7.73s/it]

 94%|█████████▍| 3610/3844 [7:28:30<30:59,  7.94s/it]

 94%|█████████▍| 3611/3844 [7:28:37<29:44,  7.66s/it]

 94%|█████████▍| 3612/3844 [7:28:43<27:39,  7.15s/it]


 94%|█████████▍| 3614/3844 [7:28:55<24:51,  6.49s/it]
{'loss': 1.1895, 'grad_norm': 0.3546678703016458, 'learning_rate': 1.8724645425481137e-07, 'epoch': 0.94}

 94%|█████████▍| 3615/3844 [7:29:04<28:00,  7.34s/it]

 94%|█████████▍| 3616/3844 [7:29:12<28:12,  7.43s/it]


 94%|█████████▍| 3618/3844 [7:29:24<25:56,  6.89s/it]

 94%|█████████▍| 3619/3844 [7:29:30<24:39,  6.58s/it]
{'loss': 1.0577, 'grad_norm': 0.3849210164127614, 'learning_rate': 1.7921793645964692e-07, 'epoch': 0.94}

 94%|█████████▍| 3620/3844 [7:29:37<24:46,  6.63s/it]

 94%|█████████▍| 3621/3844 [7:29:46<27:17,  7.34s/it]

 94%|█████████▍| 3622/3844 [7:29:51<25:13,  6.82s/it]

 94%|█████████▍| 3623/3844 [7:29:58<24:39,  6.69s/it]

 94%|█████████▍| 3624/3844 [7:30:03<23:13,  6.33s/it]

 94%|█████████▍| 3625/3844 [7:30:09<22:13,  6.09s/it]

 94%|█████████▍| 3626/3844 [7:30:19<26:38,  7.33s/it]

 94%|█████████▍| 3627/3844 [7:30:26<25:32,  7.06s/it]

 94%|█████████▍| 3628/3844 [7:30:32<24:18,  6.75s/it]

 94%|█████████▍| 3629/3844 [7:30:39<25:11,  7.03s/it]

 94%|█████████▍| 3630/3844 [7:30:45<23:40,  6.64s/it]

 94%|█████████▍| 3631/3844 [7:30:51<23:08,  6.52s/it]

 94%|█████████▍| 3632/3844 [7:30:59<24:19,  6.89s/it]

 95%|█████████▍| 3633/3844 [7:31:06<23:53,  6.79s/it]

 95%|█████████▍| 3634/3844 [7:31:11<22:36,  6.46s/it]

 95%|█████████▍| 3635/3844 [7:31:22<26:43,  7.67s/it]

 95%|█████████▍| 3636/3844 [7:31:29<26:29,  7.64s/it]


 95%|█████████▍| 3638/3844 [7:31:42<24:17,  7.07s/it]
{'loss': 1.2259, 'grad_norm': 0.3483235250047393, 'learning_rate': 1.503008577175824e-07, 'epoch': 0.95}


 95%|█████████▍| 3640/3844 [7:31:54<22:10,  6.52s/it]
{'loss': 1.2009, 'grad_norm': 0.3594332717159046, 'learning_rate': 1.4740371815755937e-07, 'epoch': 0.95}

 95%|█████████▍| 3641/3844 [7:32:03<23:59,  7.09s/it]


 95%|█████████▍| 3643/3844 [7:32:15<21:44,  6.49s/it]

 95%|█████████▍| 3644/3844 [7:32:21<21:13,  6.37s/it]
{'loss': 1.1615, 'grad_norm': 0.35157214225817085, 'learning_rate': 1.4169340860639813e-07, 'epoch': 0.95}


 95%|█████████▍| 3646/3844 [7:32:35<21:32,  6.53s/it]
{'loss': 1.1826, 'grad_norm': 0.39227175911686446, 'learning_rate': 1.3888025483588142e-07, 'epoch': 0.95}


 95%|█████████▍| 3648/3844 [7:32:47<20:28,  6.27s/it]

 95%|█████████▍| 3649/3844 [7:32:53<20:02,  6.17s/it]

 95%|█████████▍| 3650/3844 [7:32:59<19:46,  6.11s/it]
{'loss': 1.049, 'grad_norm': 0.3534703611564731, 'learning_rate': 1.3333798925996843e-07, 'epoch': 0.95}

 95%|█████████▍| 3651/3844 [7:33:05<20:01,  6.23s/it]

 95%|█████████▌| 3652/3844 [7:33:11<19:24,  6.07s/it]

 95%|█████████▌| 3653/3844 [7:33:16<18:47,  5.90s/it]

 95%|█████████▌| 3654/3844 [7:33:22<18:17,  5.77s/it]

 95%|█████████▌| 3655/3844 [7:33:28<18:52,  5.99s/it]


 95%|█████████▌| 3657/3844 [7:33:45<22:44,  7.30s/it]

 95%|█████████▌| 3658/3844 [7:33:53<23:03,  7.44s/it]
{'loss': 1.1458, 'grad_norm': 0.3642105931790091, 'learning_rate': 1.225898447026319e-07, 'epoch': 0.95}


 95%|█████████▌| 3660/3844 [7:34:08<23:26,  7.64s/it]

 95%|█████████▌| 3661/3844 [7:34:15<21:58,  7.21s/it]

 95%|█████████▌| 3662/3844 [7:34:21<20:49,  6.86s/it]
{'loss': 1.0308, 'grad_norm': 0.36294552555196974, 'learning_rate': 1.1738408784520482e-07, 'epoch': 0.95}


 95%|█████████▌| 3664/3844 [7:34:35<20:59,  7.00s/it]
{'loss': 1.2205, 'grad_norm': 0.3467028975380292, 'learning_rate': 1.1482331435895833e-07, 'epoch': 0.95}


 95%|█████████▌| 3666/3844 [7:34:47<18:55,  6.38s/it]
{'loss': 1.2421, 'grad_norm': 0.4046658832264698, 'learning_rate': 1.1229062055973072e-07, 'epoch': 0.95}


 95%|█████████▌| 3668/3844 [7:35:03<21:08,  7.21s/it]
{'loss': 1.1002, 'grad_norm': 0.3462362041476159, 'learning_rate': 1.0978601364185271e-07, 'epoch': 0.95}

 95%|█████████▌| 3669/3844 [7:35:10<21:09,  7.26s/it]


 95%|█████████▌| 3671/3844 [7:35:27<23:06,  8.01s/it]
{'loss': 0.9541, 'grad_norm': 0.3531493747741943, 'learning_rate': 1.0608178170882777e-07, 'epoch': 0.95}

 96%|█████████▌| 3672/3844 [7:35:34<22:14,  7.76s/it]

 96%|█████████▌| 3673/3844 [7:35:41<21:34,  7.57s/it]

 96%|█████████▌| 3674/3844 [7:35:50<22:41,  8.01s/it]

 96%|█████████▌| 3675/3844 [7:35:58<22:14,  7.89s/it]


 96%|█████████▌| 3677/3844 [7:36:11<19:50,  7.13s/it]

 96%|█████████▌| 3678/3844 [7:36:17<19:06,  6.91s/it]

 96%|█████████▌| 3679/3844 [7:36:25<19:33,  7.11s/it]
{'loss': 1.1501, 'grad_norm': 0.3454578128890142, 'learning_rate': 9.651304251642179e-08, 'epoch': 0.96}


 96%|█████████▌| 3681/3844 [7:36:41<20:50,  7.67s/it]

 96%|█████████▌| 3682/3844 [7:36:47<19:29,  7.22s/it]
{'loss': 1.2749, 'grad_norm': 0.35682052831142125, 'learning_rate': 9.304078545897766e-08, 'epoch': 0.96}

 96%|█████████▌| 3683/3844 [7:36:54<19:14,  7.17s/it]

 96%|█████████▌| 3684/3844 [7:37:02<19:32,  7.33s/it]

 96%|█████████▌| 3685/3844 [7:37:08<18:32,  6.99s/it]


 96%|█████████▌| 3687/3844 [7:37:23<19:21,  7.40s/it]

 96%|█████████▌| 3688/3844 [7:37:29<17:55,  6.89s/it]
{'loss': 1.0944, 'grad_norm': 0.35100823273520215, 'learning_rate': 8.62862486199878e-08, 'epoch': 0.96}


 96%|█████████▌| 3690/3844 [7:37:45<19:44,  7.69s/it]

 96%|█████████▌| 3691/3844 [7:37:53<19:53,  7.80s/it]
{'loss': 1.2305, 'grad_norm': 0.32174041364762257, 'learning_rate': 8.30040120088127e-08, 'epoch': 0.96}

 96%|█████████▌| 3692/3844 [7:38:01<20:11,  7.97s/it]

 96%|█████████▌| 3693/3844 [7:38:08<19:29,  7.75s/it]

 96%|█████████▌| 3694/3844 [7:38:15<18:39,  7.46s/it]

 96%|█████████▌| 3695/3844 [7:38:22<17:40,  7.12s/it]

 96%|█████████▌| 3696/3844 [7:38:27<16:36,  6.73s/it]

 96%|█████████▌| 3697/3844 [7:38:34<16:32,  6.75s/it]

 96%|█████████▌| 3698/3844 [7:38:40<15:45,  6.48s/it]

 96%|█████████▌| 3699/3844 [7:38:47<16:15,  6.73s/it]


 96%|█████████▋| 3701/3844 [7:39:05<18:52,  7.92s/it]
{'loss': 1.1045, 'grad_norm': 0.3358111902893863, 'learning_rate': 7.25211016508831e-08, 'epoch': 0.96}


 96%|█████████▋| 3703/3844 [7:39:17<16:20,  6.95s/it]
{'loss': 1.1901, 'grad_norm': 0.3558426854023095, 'learning_rate': 7.050909570771924e-08, 'epoch': 0.96}

 96%|█████████▋| 3704/3844 [7:39:26<17:14,  7.39s/it]


 96%|█████████▋| 3706/3844 [7:39:37<15:08,  6.58s/it]
{'loss': 1.1625, 'grad_norm': 0.37330005656074583, 'learning_rate': 6.754397399209246e-08, 'epoch': 0.96}


 96%|█████████▋| 3708/3844 [7:39:51<15:21,  6.77s/it]

 96%|█████████▋| 3709/3844 [7:39:59<16:03,  7.13s/it]

 97%|█████████▋| 3710/3844 [7:40:05<15:23,  6.89s/it]
{'loss': 1.1354, 'grad_norm': 0.3794570741284365, 'learning_rate': 6.368923051713882e-08, 'epoch': 0.97}

 97%|█████████▋| 3711/3844 [7:40:14<16:16,  7.34s/it]

 97%|█████████▋| 3712/3844 [7:40:20<15:37,  7.10s/it]

 97%|█████████▋| 3713/3844 [7:40:28<16:14,  7.44s/it]

 97%|█████████▋| 3714/3844 [7:40:41<19:45,  9.12s/it]


 97%|█████████▋| 3716/3844 [7:40:55<16:54,  7.92s/it]
{'loss': 1.0409, 'grad_norm': 0.3453330377035891, 'learning_rate': 5.811881552384768e-08, 'epoch': 0.97}

 97%|█████████▋| 3717/3844 [7:41:04<17:43,  8.37s/it]


 97%|█████████▋| 3719/3844 [7:41:21<17:00,  8.16s/it]
{'loss': 1.0871, 'grad_norm': 0.35164864886251146, 'learning_rate': 5.542891176824161e-08, 'epoch': 0.97}


 97%|█████████▋| 3721/3844 [7:41:35<15:22,  7.50s/it]

 97%|█████████▋| 3722/3844 [7:41:43<15:21,  7.55s/it]
{'loss': 1.052, 'grad_norm': 0.34114977607757957, 'learning_rate': 5.280256689806473e-08, 'epoch': 0.97}

 97%|█████████▋| 3723/3844 [7:41:50<14:39,  7.27s/it]


 97%|█████████▋| 3725/3844 [7:42:09<16:50,  8.49s/it]
{'loss': 1.0669, 'grad_norm': 0.3345538490407078, 'learning_rate': 5.023979769911314e-08, 'epoch': 0.97}

 97%|█████████▋| 3726/3844 [7:42:17<16:12,  8.24s/it]

 97%|█████████▋| 3727/3844 [7:42:22<14:33,  7.47s/it]

 97%|█████████▋| 3728/3844 [7:42:29<13:45,  7.11s/it]

 97%|█████████▋| 3729/3844 [7:42:37<14:03,  7.34s/it]

 97%|█████████▋| 3730/3844 [7:42:46<15:20,  8.07s/it]

 97%|█████████▋| 3731/3844 [7:42:53<14:13,  7.55s/it]

 97%|█████████▋| 3732/3844 [7:43:00<13:55,  7.46s/it]

 97%|█████████▋| 3733/3844 [7:43:06<13:11,  7.13s/it]

 97%|█████████▋| 3734/3844 [7:43:14<13:35,  7.42s/it]

 97%|█████████▋| 3735/3844 [7:43:23<13:50,  7.62s/it]


 97%|█████████▋| 3737/3844 [7:43:39<14:05,  7.90s/it]
{'loss': 1.0244, 'grad_norm': 0.3375391405207897, 'learning_rate': 4.0624799107695786e-08, 'epoch': 0.97}

 97%|█████████▋| 3738/3844 [7:43:46<13:34,  7.69s/it]

 97%|█████████▋| 3739/3844 [7:43:53<12:49,  7.33s/it]

 97%|█████████▋| 3740/3844 [7:44:00<12:47,  7.38s/it]

 97%|█████████▋| 3741/3844 [7:44:08<12:58,  7.56s/it]


 97%|█████████▋| 3743/3844 [7:44:25<13:19,  7.92s/it]

 97%|█████████▋| 3744/3844 [7:44:31<12:06,  7.26s/it]

 97%|█████████▋| 3745/3844 [7:44:37<11:29,  6.96s/it]
{'loss': 1.184, 'grad_norm': 0.35003691233153844, 'learning_rate': 3.47805480350516e-08, 'epoch': 0.97}

 97%|█████████▋| 3746/3844 [7:44:43<10:41,  6.55s/it]

 97%|█████████▋| 3747/3844 [7:44:52<11:50,  7.32s/it]


 98%|█████████▊| 3749/3844 [7:45:07<11:25,  7.22s/it]
{'loss': 1.2547, 'grad_norm': 0.3756054874316149, 'learning_rate': 3.202824843049346e-08, 'epoch': 0.98}


 98%|█████████▊| 3751/3844 [7:45:26<12:49,  8.28s/it]

 98%|█████████▊| 3752/3844 [7:45:32<11:40,  7.61s/it]
{'loss': 1.2193, 'grad_norm': 0.36654269513843263, 'learning_rate': 3.0038348555290196e-08, 'epoch': 0.98}


 98%|█████████▊| 3754/3844 [7:45:45<10:47,  7.20s/it]

 98%|█████████▊| 3755/3844 [7:45:52<10:19,  6.96s/it]
{'loss': 1.0778, 'grad_norm': 0.3433470222155334, 'learning_rate': 2.8112169844600747e-08, 'epoch': 0.98}

 98%|█████████▊| 3756/3844 [7:46:00<10:55,  7.45s/it]

 98%|█████████▊| 3757/3844 [7:46:06<10:06,  6.97s/it]

 98%|█████████▊| 3758/3844 [7:46:13<09:46,  6.82s/it]

 98%|█████████▊| 3759/3844 [7:46:23<10:57,  7.74s/it]

 98%|█████████▊| 3760/3844 [7:46:31<11:03,  7.90s/it]

 98%|█████████▊| 3761/3844 [7:46:37<10:08,  7.34s/it]

 98%|█████████▊| 3762/3844 [7:46:44<10:05,  7.38s/it]

 98%|█████████▊| 3763/3844 [7:46:53<10:27,  7.75s/it]

 98%|█████████▊| 3764/3844 [7:47:02<10:51,  8.15s/it]


 98%|█████████▊| 3766/3844 [7:47:18<10:26,  8.04s/it]
{'loss': 1.0963, 'grad_norm': 0.37095571713698466, 'learning_rate': 2.1594879095153676e-08, 'epoch': 0.98}

 98%|█████████▊| 3767/3844 [7:47:23<09:18,  7.25s/it]


 98%|█████████▊| 3769/3844 [7:47:39<09:42,  7.77s/it]
{'loss': 1.2173, 'grad_norm': 0.3758861729879165, 'learning_rate': 1.9966222011550406e-08, 'epoch': 0.98}

 98%|█████████▊| 3770/3844 [7:47:46<09:15,  7.51s/it]

 98%|█████████▊| 3771/3844 [7:47:55<09:35,  7.89s/it]


 98%|█████████▊| 3773/3844 [7:48:11<09:42,  8.21s/it]
{'loss': 1.153, 'grad_norm': 0.3631701795237088, 'learning_rate': 1.7893902922830885e-08, 'epoch': 0.98}

 98%|█████████▊| 3774/3844 [7:48:18<09:05,  7.80s/it]

 98%|█████████▊| 3775/3844 [7:48:27<09:19,  8.11s/it]


 98%|█████████▊| 3777/3844 [7:48:42<08:44,  7.83s/it]
{'loss': 1.1199, 'grad_norm': 0.3744955751857698, 'learning_rate': 1.5935003846664353e-08, 'epoch': 0.98}

 98%|█████████▊| 3778/3844 [7:48:48<08:09,  7.41s/it]

 98%|█████████▊| 3779/3844 [7:48:54<07:42,  7.11s/it]

 98%|█████████▊| 3780/3844 [7:49:03<08:02,  7.53s/it]

 98%|█████████▊| 3781/3844 [7:49:10<07:51,  7.49s/it]

 98%|█████████▊| 3782/3844 [7:49:16<07:12,  6.97s/it]


 98%|█████████▊| 3784/3844 [7:49:31<07:17,  7.29s/it]
{'loss': 1.0594, 'grad_norm': 0.3845823623816333, 'learning_rate': 1.2779913588529814e-08, 'epoch': 0.98}


 98%|█████████▊| 3786/3844 [7:49:44<06:25,  6.65s/it]
{'loss': 1.1357, 'grad_norm': 0.3515076404786797, 'learning_rate': 1.1942286047514906e-08, 'epoch': 0.98}

 99%|█████████▊| 3787/3844 [7:49:54<07:27,  7.84s/it]

 99%|█████████▊| 3788/3844 [7:50:00<06:43,  7.20s/it]


 99%|█████████▊| 3790/3844 [7:50:14<06:24,  7.12s/it]
{'loss': 1.0887, 'grad_norm': 0.3686378824872431, 'learning_rate': 1.035214905229287e-08, 'epoch': 0.99}

 99%|█████████▊| 3791/3844 [7:50:21<06:11,  7.00s/it]

 99%|█████████▊| 3792/3844 [7:50:27<05:52,  6.78s/it]


 99%|█████████▊| 3794/3844 [7:50:42<05:54,  7.08s/it]

 99%|█████████▊| 3795/3844 [7:50:48<05:29,  6.73s/it]

 99%|█████████▉| 3796/3844 [7:50:54<05:11,  6.49s/it]

 99%|█████████▉| 3797/3844 [7:51:00<05:02,  6.44s/it]

 99%|█████████▉| 3798/3844 [7:51:08<05:14,  6.84s/it]
{'loss': 1.12, 'grad_norm': 0.3685707922855262, 'learning_rate': 7.51240895323635e-09, 'epoch': 0.99}

 99%|█████████▉| 3799/3844 [7:51:15<05:11,  6.92s/it]


 99%|█████████▉| 3801/3844 [7:51:32<05:24,  7.54s/it]
{'loss': 1.1129, 'grad_norm': 0.3722951777079682, 'learning_rate': 6.564585841674076e-09, 'epoch': 0.99}

 99%|█████████▉| 3802/3844 [7:51:38<05:05,  7.28s/it]

 99%|█████████▉| 3803/3844 [7:51:46<05:08,  7.54s/it]

 99%|█████████▉| 3804/3844 [7:51:53<04:45,  7.13s/it]


 99%|█████████▉| 3806/3844 [7:52:20<06:36, 10.43s/it]
{'loss': 1.0175, 'grad_norm': 0.3223400133956029, 'learning_rate': 5.126819446297227e-09, 'epoch': 0.99}

 99%|█████████▉| 3807/3844 [7:52:25<05:29,  8.91s/it]

 99%|█████████▉| 3808/3844 [7:52:31<04:46,  7.97s/it]

 99%|█████████▉| 3809/3844 [7:52:43<05:21,  9.20s/it]
[2024-05-27 18:38:35,319] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 99%|█████████▉| 3811/3844 [7:53:02<05:03,  9.21s/it]
{'loss': 1.1113, 'grad_norm': 0.3666857617291543, 'learning_rate': 3.866498388295225e-09, 'epoch': 0.99}


 99%|█████████▉| 3813/3844 [7:53:18<04:24,  8.54s/it]
{'loss': 1.3076, 'grad_norm': 0.35712458460812746, 'learning_rate': 3.4120597774822683e-09, 'epoch': 0.99}

 99%|█████████▉| 3814/3844 [7:53:24<03:57,  7.90s/it]

 99%|█████████▉| 3815/3844 [7:53:31<03:35,  7.42s/it]

 99%|█████████▉| 3816/3844 [7:53:36<03:13,  6.91s/it]

 99%|█████████▉| 3817/3844 [7:53:42<03:00,  6.67s/it]

 99%|█████████▉| 3818/3844 [7:53:49<02:53,  6.69s/it]


 99%|█████████▉| 3820/3844 [7:54:04<02:47,  6.99s/it]
{'loss': 1.2427, 'grad_norm': 0.3594559062449498, 'learning_rate': 2.0451521445752354e-09, 'epoch': 0.99}

 99%|█████████▉| 3821/3844 [7:54:11<02:42,  7.07s/it]

 99%|█████████▉| 3822/3844 [7:54:17<02:28,  6.73s/it]

 99%|█████████▉| 3823/3844 [7:54:27<02:43,  7.80s/it]

 99%|█████████▉| 3824/3844 [7:54:34<02:32,  7.61s/it]

100%|█████████▉| 3825/3844 [7:54:41<02:18,  7.27s/it]

100%|█████████▉| 3826/3844 [7:54:50<02:19,  7.75s/it]

100%|█████████▉| 3827/3844 [7:54:59<02:20,  8.27s/it]

100%|█████████▉| 3828/3844 [7:55:07<02:10,  8.13s/it]

100%|█████████▉| 3829/3844 [7:55:13<01:52,  7.47s/it]

100%|█████████▉| 3830/3844 [7:55:19<01:37,  6.99s/it]

100%|█████████▉| 3831/3844 [7:55:25<01:26,  6.65s/it]


100%|█████████▉| 3833/3844 [7:55:40<01:18,  7.16s/it]
{'loss': 1.206, 'grad_norm': 0.3150657155774251, 'learning_rate': 4.2963554324093605e-10, 'epoch': 1.0}

100%|█████████▉| 3834/3844 [7:55:47<01:12,  7.20s/it]

100%|█████████▉| 3835/3844 [7:55:54<01:02,  6.93s/it]


100%|█████████▉| 3837/3844 [7:56:14<00:58,  8.38s/it]

100%|█████████▉| 3838/3844 [7:56:20<00:45,  7.66s/it]
{'loss': 1.1822, 'grad_norm': 0.40402070834026793, 'learning_rate': 1.2782609387196332e-10, 'epoch': 1.0}

100%|█████████▉| 3839/3844 [7:56:26<00:35,  7.05s/it]

100%|█████████▉| 3840/3844 [7:56:37<00:33,  8.36s/it]


100%|█████████▉| 3842/3844 [7:56:52<00:16,  8.13s/it]
{'loss': 1.0835, 'grad_norm': 0.37018657005181616, 'learning_rate': 1.4202926215123313e-11, 'epoch': 1.0}


100%|██████████| 3844/3844 [7:57:10<00:00,  8.72s/it]
{'loss': 1.1504, 'grad_norm': 0.3578578295325835, 'learning_rate': 0.0, 'epoch': 1.0}

100%|██████████| 3844/3844 [7:57:11<00:00,  7.45s/it]
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(