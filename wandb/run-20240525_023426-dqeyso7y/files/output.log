/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/4236 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/4236 [00:11<13:39:10, 11.61s/it]

  0%|          | 2/4236 [00:14<7:57:04,  6.76s/it]

  0%|          | 3/4236 [00:18<6:17:49,  5.36s/it]

  0%|          | 4/4236 [00:22<5:31:45,  4.70s/it]
{'loss': 2.7663, 'grad_norm': 2.8203625069578786, 'learning_rate': 6.25e-06, 'epoch': 0.0}


  0%|          | 6/4236 [00:31<5:35:13,  4.75s/it]

  0%|          | 7/4236 [00:35<5:17:00,  4.50s/it]
{'loss': 2.7955, 'grad_norm': 3.0945865633714007, 'learning_rate': 1.09375e-05, 'epoch': 0.0}


  0%|          | 9/4236 [00:44<5:04:19,  4.32s/it]
{'loss': 2.7611, 'grad_norm': 3.096857813674199, 'learning_rate': 1.4062500000000001e-05, 'epoch': 0.0}

  0%|          | 10/4236 [00:49<5:25:20,  4.62s/it]


  0%|          | 12/4236 [00:56<4:46:09,  4.06s/it]

  0%|          | 13/4236 [01:01<5:09:45,  4.40s/it]

  0%|          | 14/4236 [01:06<5:13:28,  4.45s/it]

  0%|          | 15/4236 [01:10<5:05:03,  4.34s/it]

  0%|          | 16/4236 [01:13<4:47:57,  4.09s/it]

  0%|          | 17/4236 [01:17<4:42:17,  4.01s/it]

  0%|          | 18/4236 [01:26<6:15:17,  5.34s/it]

  0%|          | 19/4236 [01:29<5:43:21,  4.89s/it]

  0%|          | 20/4236 [01:35<5:47:39,  4.95s/it]

  0%|          | 21/4236 [01:38<5:25:40,  4.64s/it]
{'loss': 2.1117, 'grad_norm': 1.0645132431208928, 'learning_rate': 3.2812500000000005e-05, 'epoch': 0.0}


  1%|          | 23/4236 [01:48<5:28:32,  4.68s/it]

  1%|          | 24/4236 [01:52<5:09:50,  4.41s/it]

  1%|          | 25/4236 [01:56<5:08:19,  4.39s/it]

  1%|          | 26/4236 [02:00<4:56:59,  4.23s/it]

  1%|          | 27/4236 [02:05<5:24:49,  4.63s/it]
{'loss': 1.9582, 'grad_norm': 0.47271161201493644, 'learning_rate': 4.21875e-05, 'epoch': 0.01}

  1%|          | 28/4236 [02:11<5:48:28,  4.97s/it]


  1%|          | 30/4236 [02:19<5:21:56,  4.59s/it]

  1%|          | 31/4236 [02:30<7:26:40,  6.37s/it]

  1%|          | 32/4236 [02:34<6:40:58,  5.72s/it]

  1%|          | 33/4236 [02:38<5:59:35,  5.13s/it]
{'loss': 1.9182, 'grad_norm': 0.4440600996330109, 'learning_rate': 5.15625e-05, 'epoch': 0.01}


  1%|          | 35/4236 [02:56<7:54:29,  6.78s/it]
{'loss': 1.964, 'grad_norm': 0.41940077829702477, 'learning_rate': 5.46875e-05, 'epoch': 0.01}


  1%|          | 37/4236 [03:06<6:57:29,  5.97s/it]
{'loss': 1.8203, 'grad_norm': 0.40331140829064804, 'learning_rate': 5.78125e-05, 'epoch': 0.01}

  1%|          | 38/4236 [03:11<6:29:41,  5.57s/it]

  1%|          | 39/4236 [03:15<6:03:41,  5.20s/it]

  1%|          | 40/4236 [03:19<5:36:52,  4.82s/it]


  1%|          | 42/4236 [03:44<11:06:04,  9.53s/it]

  1%|          | 43/4236 [03:50<9:48:47,  8.43s/it]

  1%|          | 44/4236 [03:56<8:56:19,  7.68s/it]

  1%|          | 45/4236 [04:02<8:27:06,  7.26s/it]

  1%|          | 46/4236 [04:06<7:09:31,  6.15s/it]

  1%|          | 47/4236 [04:10<6:15:58,  5.39s/it]

  1%|          | 48/4236 [04:14<5:56:08,  5.10s/it]

  1%|          | 49/4236 [04:18<5:36:17,  4.82s/it]
{'loss': 1.8645, 'grad_norm': 0.28823942520619356, 'learning_rate': 7.65625e-05, 'epoch': 0.01}


  1%|          | 51/4236 [04:27<5:13:24,  4.49s/it]

  1%|          | 52/4236 [04:32<5:33:37,  4.78s/it]

  1%|▏         | 53/4236 [04:36<5:22:37,  4.63s/it]

  1%|▏         | 54/4236 [04:40<5:07:24,  4.41s/it]

  1%|▏         | 55/4236 [04:44<4:57:19,  4.27s/it]

  1%|▏         | 56/4236 [04:48<4:51:18,  4.18s/it]

  1%|▏         | 57/4236 [04:54<5:22:24,  4.63s/it]

  1%|▏         | 58/4236 [04:57<4:59:35,  4.30s/it]
{'loss': 1.8609, 'grad_norm': 0.3273190321673683, 'learning_rate': 9.062500000000001e-05, 'epoch': 0.01}


  1%|▏         | 60/4236 [05:06<5:07:29,  4.42s/it]

  1%|▏         | 61/4236 [05:10<4:59:17,  4.30s/it]
{'loss': 1.8869, 'grad_norm': 0.3301386355417973, 'learning_rate': 9.53125e-05, 'epoch': 0.01}


  1%|▏         | 63/4236 [05:23<6:17:44,  5.43s/it]
{'loss': 1.8591, 'grad_norm': 0.2821912106307686, 'learning_rate': 9.84375e-05, 'epoch': 0.01}


  2%|▏         | 65/4236 [05:33<6:04:23,  5.24s/it]

  2%|▏         | 66/4236 [05:37<5:41:45,  4.92s/it]

  2%|▏         | 67/4236 [05:42<5:53:23,  5.09s/it]
{'loss': 1.8141, 'grad_norm': 0.2902170210990687, 'learning_rate': 0.0001046875, 'epoch': 0.02}

  2%|▏         | 68/4236 [05:47<5:49:42,  5.03s/it]


  2%|▏         | 70/4236 [06:00<6:48:34,  5.88s/it]

  2%|▏         | 71/4236 [06:04<6:10:12,  5.33s/it]
{'loss': 2.0399, 'grad_norm': 0.3305197503339205, 'learning_rate': 0.0001109375, 'epoch': 0.02}


  2%|▏         | 73/4236 [06:12<5:23:32,  4.66s/it]

  2%|▏         | 74/4236 [06:16<5:10:00,  4.47s/it]

  2%|▏         | 75/4236 [06:20<5:06:37,  4.42s/it]

  2%|▏         | 76/4236 [06:24<4:58:12,  4.30s/it]

  2%|▏         | 77/4236 [06:30<5:40:17,  4.91s/it]

  2%|▏         | 78/4236 [06:35<5:27:50,  4.73s/it]

  2%|▏         | 79/4236 [06:43<6:32:34,  5.67s/it]
{'loss': 1.78, 'grad_norm': 0.28356621355529804, 'learning_rate': 0.0001234375, 'epoch': 0.02}

  2%|▏         | 80/4236 [06:53<8:14:16,  7.14s/it]


  2%|▏         | 82/4236 [07:07<7:49:50,  6.79s/it]

  2%|▏         | 83/4236 [07:12<7:14:48,  6.28s/it]
{'loss': 1.6995, 'grad_norm': 0.30834815515173575, 'learning_rate': 0.0001296875, 'epoch': 0.02}

  2%|▏         | 84/4236 [07:17<6:58:35,  6.05s/it]

  2%|▏         | 85/4236 [07:21<6:19:39,  5.49s/it]

  2%|▏         | 86/4236 [07:27<6:25:30,  5.57s/it]


  2%|▏         | 88/4236 [07:35<5:25:20,  4.71s/it]
{'loss': 1.8503, 'grad_norm': 0.31143891534342283, 'learning_rate': 0.0001375, 'epoch': 0.02}

  2%|▏         | 89/4236 [07:42<6:04:55,  5.28s/it]


  2%|▏         | 91/4236 [07:50<5:22:57,  4.67s/it]

  2%|▏         | 92/4236 [07:54<5:16:56,  4.59s/it]

  2%|▏         | 93/4236 [07:58<5:01:30,  4.37s/it]

  2%|▏         | 94/4236 [08:02<4:50:20,  4.21s/it]

  2%|▏         | 95/4236 [08:14<7:39:31,  6.66s/it]

  2%|▏         | 96/4236 [08:21<7:37:22,  6.63s/it]

  2%|▏         | 97/4236 [08:29<8:13:01,  7.15s/it]

  2%|▏         | 98/4236 [08:37<8:30:58,  7.41s/it]
{'loss': 1.7681, 'grad_norm': 0.31755040144289765, 'learning_rate': 0.000153125, 'epoch': 0.02}

  2%|▏         | 99/4236 [08:42<7:29:53,  6.52s/it]


  2%|▏         | 101/4236 [08:49<5:45:24,  5.01s/it]

  2%|▏         | 102/4236 [08:52<5:14:33,  4.57s/it]

  2%|▏         | 103/4236 [09:01<6:44:06,  5.87s/it]

  2%|▏         | 104/4236 [09:08<7:07:31,  6.21s/it]

  2%|▏         | 105/4236 [09:14<6:55:12,  6.03s/it]
{'loss': 1.8435, 'grad_norm': 0.30614661943653065, 'learning_rate': 0.0001640625, 'epoch': 0.02}


  3%|▎         | 107/4236 [09:25<6:30:51,  5.68s/it]

  3%|▎         | 108/4236 [09:39<9:22:57,  8.18s/it]

  3%|▎         | 109/4236 [09:43<7:54:38,  6.90s/it]
{'loss': 1.6668, 'grad_norm': 0.3405635477525885, 'learning_rate': 0.0001703125, 'epoch': 0.03}


  3%|▎         | 111/4236 [10:04<9:19:20,  8.14s/it]

  3%|▎         | 112/4236 [10:12<9:15:29,  8.08s/it]

  3%|▎         | 113/4236 [10:18<8:34:11,  7.48s/it]

  3%|▎         | 114/4236 [10:24<8:12:56,  7.18s/it]

  3%|▎         | 115/4236 [10:29<7:23:01,  6.45s/it]

  3%|▎         | 116/4236 [10:34<6:48:23,  5.95s/it]

  3%|▎         | 117/4236 [10:38<6:14:51,  5.46s/it]

  3%|▎         | 118/4236 [10:44<6:28:11,  5.66s/it]

  3%|▎         | 119/4236 [10:49<6:07:06,  5.35s/it]

  3%|▎         | 120/4236 [10:53<5:34:14,  4.87s/it]

  3%|▎         | 121/4236 [10:59<5:59:10,  5.24s/it]

  3%|▎         | 122/4236 [11:02<5:24:45,  4.74s/it]

  3%|▎         | 123/4236 [11:08<5:39:45,  4.96s/it]
{'loss': 1.7106, 'grad_norm': 0.2883984489205702, 'learning_rate': 0.0001921875, 'epoch': 0.03}

  3%|▎         | 124/4236 [11:12<5:17:06,  4.63s/it]


  3%|▎         | 126/4236 [11:20<4:58:44,  4.36s/it]

  3%|▎         | 127/4236 [11:24<4:51:39,  4.26s/it]

  3%|▎         | 128/4236 [11:29<5:06:49,  4.48s/it]

  3%|▎         | 129/4236 [11:34<5:18:29,  4.65s/it]

  3%|▎         | 130/4236 [11:39<5:25:55,  4.76s/it]

  3%|▎         | 131/4236 [11:44<5:32:02,  4.85s/it]

  3%|▎         | 132/4236 [11:55<7:38:54,  6.71s/it]

  3%|▎         | 133/4236 [12:01<7:20:20,  6.44s/it]

  3%|▎         | 134/4236 [12:09<7:48:42,  6.86s/it]

  3%|▎         | 135/4236 [12:12<6:38:40,  5.83s/it]
{'loss': 1.7138, 'grad_norm': 0.33136346187413307, 'learning_rate': 0.0001999985671394597, 'epoch': 0.03}

  3%|▎         | 136/4236 [12:18<6:34:51,  5.78s/it]

  3%|▎         | 137/4236 [12:24<6:38:17,  5.83s/it]

  3%|▎         | 138/4236 [12:32<7:24:43,  6.51s/it]


  3%|▎         | 140/4236 [12:41<6:27:06,  5.67s/it]
{'loss': 1.8773, 'grad_norm': 0.27370840001844465, 'learning_rate': 0.00019999578916403085, 'epoch': 0.03}

  3%|▎         | 141/4236 [12:46<5:57:39,  5.24s/it]

  3%|▎         | 142/4236 [13:00<9:02:25,  7.95s/it]

  3%|▎         | 143/4236 [13:04<7:41:11,  6.76s/it]


  3%|▎         | 145/4236 [13:13<6:29:10,  5.71s/it]

  3%|▎         | 146/4236 [13:18<6:18:54,  5.56s/it]

  3%|▎         | 147/4236 [13:25<6:37:42,  5.84s/it]

  3%|▎         | 148/4236 [13:29<6:01:19,  5.30s/it]

  4%|▎         | 149/4236 [13:37<7:05:17,  6.24s/it]

  4%|▎         | 150/4236 [13:41<6:24:44,  5.65s/it]

  4%|▎         | 151/4236 [13:45<5:40:44,  5.00s/it]

  4%|▎         | 152/4236 [13:49<5:15:31,  4.64s/it]

  4%|▎         | 153/4236 [13:53<5:04:06,  4.47s/it]

  4%|▎         | 154/4236 [13:59<5:29:59,  4.85s/it]

  4%|▎         | 155/4236 [14:02<5:11:05,  4.57s/it]

  4%|▎         | 156/4236 [14:06<4:55:48,  4.35s/it]
{'loss': 1.7569, 'grad_norm': 0.3338299372895293, 'learning_rate': 0.00019997707505258145, 'epoch': 0.04}

  4%|▎         | 157/4236 [14:10<4:39:25,  4.11s/it]


  4%|▍         | 159/4236 [14:17<4:22:20,  3.86s/it]
{'loss': 1.7896, 'grad_norm': 0.3455671041580853, 'learning_rate': 0.00019997189963721844, 'epoch': 0.04}


  4%|▍         | 161/4236 [14:25<4:29:37,  3.97s/it]

  4%|▍         | 162/4236 [14:29<4:20:52,  3.84s/it]

  4%|▍         | 163/4236 [14:33<4:24:46,  3.90s/it]

  4%|▍         | 164/4236 [14:47<7:56:10,  7.02s/it]

  4%|▍         | 165/4236 [14:51<6:52:04,  6.07s/it]

  4%|▍         | 166/4236 [14:57<6:46:27,  5.99s/it]

  4%|▍         | 167/4236 [15:03<6:45:36,  5.98s/it]

  4%|▍         | 168/4236 [15:10<7:16:00,  6.43s/it]

  4%|▍         | 169/4236 [15:19<8:07:38,  7.19s/it]

  4%|▍         | 170/4236 [15:24<7:21:03,  6.51s/it]

  4%|▍         | 171/4236 [15:29<6:39:02,  5.89s/it]

  4%|▍         | 172/4236 [15:32<5:53:00,  5.21s/it]

  4%|▍         | 173/4236 [15:39<6:26:01,  5.70s/it]

  4%|▍         | 174/4236 [15:49<7:49:26,  6.93s/it]

  4%|▍         | 175/4236 [15:53<6:47:39,  6.02s/it]
{'loss': 1.6586, 'grad_norm': 0.32115952797096314, 'learning_rate': 0.00019993541110729018, 'epoch': 0.04}


  4%|▍         | 177/4236 [16:02<6:03:15,  5.37s/it]
{'loss': 1.7824, 'grad_norm': 0.2736717892143408, 'learning_rate': 0.00019992979788128928, 'epoch': 0.04}

  4%|▍         | 178/4236 [16:08<6:09:50,  5.47s/it]


  4%|▍         | 180/4236 [16:15<5:05:47,  4.52s/it]
{'loss': 1.6539, 'grad_norm': 0.31249624950568083, 'learning_rate': 0.00019992093972273018, 'epoch': 0.04}

  4%|▍         | 181/4236 [16:20<5:09:52,  4.59s/it]


  4%|▍         | 183/4236 [16:40<8:31:50,  7.58s/it]
{'loss': 1.7326, 'grad_norm': 0.3075087716071145, 'learning_rate': 0.00019991155562235186, 'epoch': 0.04}


  4%|▍         | 185/4236 [17:05<11:26:09, 10.16s/it]
{'loss': 1.7415, 'grad_norm': 0.3359567629533069, 'learning_rate': 0.00019990500738981156, 'epoch': 0.04}


  4%|▍         | 187/4236 [17:15<8:12:57,  7.30s/it]

  4%|▍         | 188/4236 [17:21<7:58:43,  7.10s/it]

  4%|▍         | 189/4236 [17:25<6:57:47,  6.19s/it]

  4%|▍         | 190/4236 [17:29<5:59:49,  5.34s/it]

  5%|▍         | 191/4236 [17:35<6:28:38,  5.76s/it]
{'loss': 1.7679, 'grad_norm': 0.2857409862408562, 'learning_rate': 0.0001998839604679692, 'epoch': 0.05}

  5%|▍         | 192/4236 [17:40<6:08:41,  5.47s/it]


  5%|▍         | 194/4236 [17:58<7:43:30,  6.88s/it]

  5%|▍         | 195/4236 [18:04<7:25:47,  6.62s/it]

  5%|▍         | 196/4236 [18:16<8:57:22,  7.98s/it]

  5%|▍         | 197/4236 [18:20<7:55:57,  7.07s/it]
{'loss': 1.7939, 'grad_norm': 0.27236333286030356, 'learning_rate': 0.00019986081056019023, 'epoch': 0.05}

  5%|▍         | 198/4236 [18:24<6:51:29,  6.11s/it]


  5%|▍         | 200/4236 [18:34<6:04:10,  5.41s/it]
{'loss': 1.8763, 'grad_norm': 0.3002631465630091, 'learning_rate': 0.00019984844713715928, 'epoch': 0.05}


  5%|▍         | 202/4236 [18:43<5:37:43,  5.02s/it]

  5%|▍         | 203/4236 [18:47<5:10:13,  4.62s/it]

  5%|▍         | 204/4236 [18:51<5:02:16,  4.50s/it]

  5%|▍         | 205/4236 [18:59<6:15:58,  5.60s/it]

  5%|▍         | 206/4236 [19:04<6:01:57,  5.39s/it]

  5%|▍         | 207/4236 [19:09<6:01:33,  5.38s/it]

  5%|▍         | 208/4236 [19:13<5:32:12,  4.95s/it]

  5%|▍         | 209/4236 [19:17<5:14:00,  4.68s/it]

  5%|▍         | 210/4236 [19:31<8:19:37,  7.45s/it]

  5%|▍         | 211/4236 [19:37<7:45:21,  6.94s/it]

  5%|▌         | 212/4236 [19:41<6:52:47,  6.15s/it]

  5%|▌         | 213/4236 [19:51<8:05:25,  7.24s/it]

  5%|▌         | 214/4236 [19:55<7:03:02,  6.31s/it]

  5%|▌         | 215/4236 [19:59<6:10:59,  5.54s/it]

  5%|▌         | 216/4236 [20:03<5:46:52,  5.18s/it]

  5%|▌         | 217/4236 [20:07<5:17:30,  4.74s/it]

  5%|▌         | 218/4236 [20:13<5:49:11,  5.21s/it]
{'loss': 1.9914, 'grad_norm': 0.2940974888044932, 'learning_rate': 0.00019976323230502766, 'epoch': 0.05}

  5%|▌         | 219/4236 [20:18<5:43:41,  5.13s/it]

  5%|▌         | 220/4236 [20:22<5:18:42,  4.76s/it]

  5%|▌         | 221/4236 [20:26<5:05:56,  4.57s/it]


  5%|▌         | 223/4236 [20:40<6:34:50,  5.90s/it]

  5%|▌         | 224/4236 [20:43<5:51:24,  5.26s/it]

  5%|▌         | 225/4236 [20:52<6:55:35,  6.22s/it]

  5%|▌         | 226/4236 [20:57<6:27:54,  5.80s/it]
{'loss': 1.7843, 'grad_norm': 0.3016726111565488, 'learning_rate': 0.0001997192900919066, 'epoch': 0.05}


  5%|▌         | 228/4236 [21:04<5:14:49,  4.71s/it]

  5%|▌         | 229/4236 [21:11<5:58:19,  5.37s/it]

  5%|▌         | 230/4236 [21:19<7:00:48,  6.30s/it]

  5%|▌         | 231/4236 [21:23<6:04:00,  5.45s/it]

  5%|▌         | 232/4236 [21:28<5:52:26,  5.28s/it]

  6%|▌         | 233/4236 [21:42<8:52:38,  7.98s/it]

  6%|▌         | 234/4236 [21:48<8:12:12,  7.38s/it]

  6%|▌         | 235/4236 [21:51<6:55:41,  6.23s/it]
{'loss': 1.8131, 'grad_norm': 0.3400601972290368, 'learning_rate': 0.0001996653937178841, 'epoch': 0.06}

  6%|▌         | 236/4236 [21:57<6:34:32,  5.92s/it]

  6%|▌         | 237/4236 [22:03<6:35:35,  5.94s/it]

  6%|▌         | 238/4236 [22:08<6:29:03,  5.84s/it]


  6%|▌         | 240/4236 [22:17<5:40:48,  5.12s/it]

  6%|▌         | 241/4236 [22:21<5:12:27,  4.69s/it]

  6%|▌         | 242/4236 [22:25<5:01:27,  4.53s/it]

  6%|▌         | 243/4236 [22:29<4:50:57,  4.37s/it]

  6%|▌         | 244/4236 [22:33<4:36:35,  4.16s/it]

  6%|▌         | 245/4236 [22:38<4:50:49,  4.37s/it]
{'loss': 1.7006, 'grad_norm': 0.2893811345484527, 'learning_rate': 0.00019959997158579967, 'epoch': 0.06}

  6%|▌         | 246/4236 [22:45<5:39:36,  5.11s/it]

  6%|▌         | 247/4236 [22:59<8:36:01,  7.76s/it]


  6%|▌         | 249/4236 [23:19<10:23:50,  9.39s/it]
[2024-05-25 02:57:57,504] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 250/4236 [23:25<9:16:46,  8.38s/it]

  6%|▌         | 251/4236 [23:33<9:08:45,  8.26s/it]

  6%|▌         | 252/4236 [23:38<7:53:25,  7.13s/it]

  6%|▌         | 253/4236 [23:43<7:17:01,  6.58s/it]

  6%|▌         | 254/4236 [23:49<6:59:17,  6.32s/it]

  6%|▌         | 255/4236 [23:53<6:15:29,  5.66s/it]

  6%|▌         | 256/4236 [23:58<5:54:35,  5.35s/it]
{'loss': 1.7165, 'grad_norm': 0.3050411814120475, 'learning_rate': 0.00019952127952230163, 'epoch': 0.06}


  6%|▌         | 258/4236 [24:11<6:35:23,  5.96s/it]

  6%|▌         | 259/4236 [24:15<5:54:35,  5.35s/it]

  6%|▌         | 260/4236 [24:20<5:47:09,  5.24s/it]
{'loss': 1.7932, 'grad_norm': 0.273361373263576, 'learning_rate': 0.0001994909178004014, 'epoch': 0.06}


  6%|▌         | 262/4236 [24:30<5:47:05,  5.24s/it]

  6%|▌         | 263/4236 [24:35<5:50:05,  5.29s/it]
{'loss': 1.7094, 'grad_norm': 0.3040541214015049, 'learning_rate': 0.00019946753553522784, 'epoch': 0.06}


  6%|▋         | 265/4236 [24:45<5:31:11,  5.00s/it]

  6%|▋         | 266/4236 [24:50<5:30:49,  5.00s/it]

  6%|▋         | 267/4236 [24:54<5:10:43,  4.70s/it]

  6%|▋         | 268/4236 [24:59<5:10:18,  4.69s/it]

  6%|▋         | 269/4236 [25:09<6:59:47,  6.35s/it]

  6%|▋         | 270/4236 [25:14<6:28:37,  5.88s/it]
{'loss': 1.7176, 'grad_norm': 0.30939648921809126, 'learning_rate': 0.00019941094108601987, 'epoch': 0.06}


  6%|▋         | 272/4236 [25:24<6:01:40,  5.47s/it]
{'loss': 1.7211, 'grad_norm': 0.30582166301382263, 'learning_rate': 0.0001993942479140418, 'epoch': 0.06}


  6%|▋         | 274/4236 [25:33<5:30:36,  5.01s/it]
{'loss': 1.7744, 'grad_norm': 0.3224100380716656, 'learning_rate': 0.00019937732222221377, 'epoch': 0.06}


  7%|▋         | 276/4236 [25:48<6:34:13,  5.97s/it]

  7%|▋         | 277/4236 [25:56<7:13:45,  6.57s/it]

  7%|▋         | 278/4236 [26:02<7:14:02,  6.58s/it]

  7%|▋         | 279/4236 [26:06<6:20:56,  5.78s/it]
{'loss': 1.7016, 'grad_norm': 0.30538492394783423, 'learning_rate': 0.00019933399097948345, 'epoch': 0.07}

  7%|▋         | 280/4236 [26:13<6:37:37,  6.03s/it]


  7%|▋         | 282/4236 [26:28<7:47:06,  7.09s/it]
{'loss': 1.7868, 'grad_norm': 0.3042639695408742, 'learning_rate': 0.00019930729505646791, 'epoch': 0.07}

  7%|▋         | 283/4236 [26:35<7:44:51,  7.06s/it]

  7%|▋         | 284/4236 [26:39<6:46:22,  6.17s/it]


  7%|▋         | 286/4236 [26:50<6:19:03,  5.76s/it]

  7%|▋         | 287/4236 [26:53<5:37:50,  5.13s/it]

  7%|▋         | 288/4236 [26:57<5:15:52,  4.80s/it]

  7%|▋         | 289/4236 [27:01<4:59:12,  4.55s/it]

  7%|▋         | 290/4236 [27:12<6:51:57,  6.26s/it]

  7%|▋         | 291/4236 [27:17<6:38:25,  6.06s/it]

  7%|▋         | 292/4236 [27:22<6:13:17,  5.68s/it]

  7%|▋         | 293/4236 [27:26<5:40:53,  5.19s/it]
{'loss': 1.7614, 'grad_norm': 0.3321114978856992, 'learning_rate': 0.00019920493901337994, 'epoch': 0.07}

  7%|▋         | 294/4236 [27:33<6:10:18,  5.64s/it]


  7%|▋         | 296/4236 [27:41<5:29:16,  5.01s/it]

  7%|▋         | 297/4236 [27:45<5:07:14,  4.68s/it]

  7%|▋         | 298/4236 [27:50<5:14:41,  4.79s/it]

  7%|▋         | 299/4236 [27:54<4:54:29,  4.49s/it]

  7%|▋         | 300/4236 [27:58<4:42:13,  4.30s/it]
  7%|▋         | 300/4236 [27:58<4:42:13,  4.30s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  7%|▋         | 301/4236 [28:22<11:03:13, 10.11s/it]

  7%|▋         | 302/4236 [28:28<9:43:53,  8.91s/it]

  7%|▋         | 303/4236 [28:32<8:07:56,  7.44s/it]

  7%|▋         | 304/4236 [28:36<6:58:58,  6.39s/it]

  7%|▋         | 305/4236 [28:39<6:03:12,  5.54s/it]

  7%|▋         | 306/4236 [28:44<5:47:27,  5.30s/it]

  7%|▋         | 307/4236 [28:48<5:15:44,  4.82s/it]

  7%|▋         | 308/4236 [28:52<4:56:59,  4.54s/it]
{'loss': 1.79, 'grad_norm': 0.3245516470830935, 'learning_rate': 0.00019905405039893825, 'epoch': 0.07}

  7%|▋         | 309/4236 [29:03<7:02:47,  6.46s/it]


  7%|▋         | 311/4236 [29:14<6:30:57,  5.98s/it]

  7%|▋         | 312/4236 [29:19<6:07:36,  5.62s/it]

  7%|▋         | 313/4236 [29:23<5:37:14,  5.16s/it]
{'loss': 1.6283, 'grad_norm': 0.3246529822922228, 'learning_rate': 0.00019900085664535758, 'epoch': 0.07}

  7%|▋         | 314/4236 [29:27<5:05:28,  4.67s/it]


  7%|▋         | 316/4236 [29:40<6:02:10,  5.54s/it]

  7%|▋         | 317/4236 [29:44<5:28:11,  5.02s/it]

  8%|▊         | 318/4236 [29:47<4:56:37,  4.54s/it]
{'loss': 1.7982, 'grad_norm': 0.29871993094236465, 'learning_rate': 0.0001989462153959925, 'epoch': 0.08}


  8%|▊         | 320/4236 [29:54<4:20:31,  3.99s/it]

  8%|▊         | 321/4236 [30:00<4:53:56,  4.50s/it]

  8%|▊         | 322/4236 [30:04<4:40:32,  4.30s/it]

  8%|▊         | 323/4236 [30:07<4:27:44,  4.11s/it]
{'loss': 1.6509, 'grad_norm': 0.29582657442215804, 'learning_rate': 0.00019889012744975508, 'epoch': 0.08}


  8%|▊         | 325/4236 [30:16<4:40:27,  4.30s/it]

  8%|▊         | 326/4236 [30:21<5:00:02,  4.60s/it]

  8%|▊         | 327/4236 [30:30<6:14:55,  5.75s/it]

  8%|▊         | 328/4236 [30:33<5:32:44,  5.11s/it]
{'loss': 1.7272, 'grad_norm': 0.2900624503191636, 'learning_rate': 0.00019883259362670966, 'epoch': 0.08}

  8%|▊         | 329/4236 [30:39<5:39:22,  5.21s/it]


  8%|▊         | 331/4236 [30:48<5:17:44,  4.88s/it]

  8%|▊         | 332/4236 [31:01<7:57:42,  7.34s/it]

  8%|▊         | 333/4236 [31:05<6:52:07,  6.34s/it]

  8%|▊         | 334/4236 [31:10<6:16:21,  5.79s/it]

  8%|▊         | 335/4236 [31:14<5:49:38,  5.38s/it]

  8%|▊         | 336/4236 [31:18<5:19:40,  4.92s/it]

  8%|▊         | 337/4236 [31:22<5:02:27,  4.65s/it]

  8%|▊         | 338/4236 [31:26<4:52:18,  4.50s/it]

  8%|▊         | 339/4236 [31:31<4:46:40,  4.41s/it]

  8%|▊         | 340/4236 [31:35<4:54:07,  4.53s/it]
{'loss': 1.6865, 'grad_norm': 0.2823197082622915, 'learning_rate': 0.00019868861835203977, 'epoch': 0.08}

  8%|▊         | 341/4236 [31:41<5:18:27,  4.91s/it]

  8%|▊         | 342/4236 [31:45<4:56:04,  4.56s/it]


  8%|▊         | 344/4236 [31:52<4:27:28,  4.12s/it]

  8%|▊         | 345/4236 [31:58<4:53:59,  4.53s/it]

  8%|▊         | 346/4236 [32:02<4:52:39,  4.51s/it]

  8%|▊         | 347/4236 [32:06<4:41:29,  4.34s/it]
{'loss': 1.7243, 'grad_norm': 0.31495177087182424, 'learning_rate': 0.00019860079379547857, 'epoch': 0.08}

  8%|▊         | 348/4236 [32:11<4:45:23,  4.40s/it]


  8%|▊         | 350/4236 [32:22<5:25:58,  5.03s/it]

  8%|▊         | 351/4236 [32:26<5:20:29,  4.95s/it]

  8%|▊         | 352/4236 [32:34<6:06:18,  5.66s/it]

  8%|▊         | 353/4236 [32:39<5:48:47,  5.39s/it]

  8%|▊         | 354/4236 [32:45<6:04:03,  5.63s/it]

  8%|▊         | 355/4236 [32:49<5:29:58,  5.10s/it]
{'loss': 1.7194, 'grad_norm': 0.2900016032655475, 'learning_rate': 0.00019849696307672863, 'epoch': 0.08}


  8%|▊         | 357/4236 [33:02<6:16:52,  5.83s/it]

  8%|▊         | 358/4236 [33:06<5:34:08,  5.17s/it]
{'loss': 1.7605, 'grad_norm': 0.29756560719505676, 'learning_rate': 0.00019845707586116056, 'epoch': 0.08}

  8%|▊         | 359/4236 [33:13<6:19:12,  5.87s/it]


  9%|▊         | 361/4236 [33:21<5:06:29,  4.75s/it]

  9%|▊         | 362/4236 [33:27<5:30:07,  5.11s/it]

  9%|▊         | 363/4236 [33:36<6:45:40,  6.28s/it]

  9%|▊         | 364/4236 [33:40<6:16:14,  5.83s/it]

  9%|▊         | 365/4236 [33:44<5:41:13,  5.29s/it]

  9%|▊         | 366/4236 [33:48<5:06:58,  4.76s/it]

  9%|▊         | 367/4236 [33:52<4:52:10,  4.53s/it]
{'loss': 1.6649, 'grad_norm': 0.2767024509662844, 'learning_rate': 0.00019833430564796057, 'epoch': 0.09}


  9%|▊         | 369/4236 [34:02<5:16:13,  4.91s/it]

  9%|▊         | 370/4236 [34:07<5:07:05,  4.77s/it]

  9%|▉         | 371/4236 [34:12<5:13:42,  4.87s/it]

  9%|▉         | 372/4236 [34:17<5:10:35,  4.82s/it]

  9%|▉         | 373/4236 [34:22<5:12:49,  4.86s/it]
{'loss': 1.8545, 'grad_norm': 0.266343477968889, 'learning_rate': 0.00019824987052793037, 'epoch': 0.09}

  9%|▉         | 374/4236 [34:25<4:49:32,  4.50s/it]


  9%|▉         | 376/4236 [34:38<6:13:12,  5.80s/it]

  9%|▉         | 377/4236 [34:44<5:59:30,  5.59s/it]

  9%|▉         | 378/4236 [34:48<5:32:29,  5.17s/it]

  9%|▉         | 379/4236 [34:52<5:11:49,  4.85s/it]

  9%|▉         | 380/4236 [34:56<4:59:28,  4.66s/it]

  9%|▉         | 381/4236 [35:00<4:51:55,  4.54s/it]
{'loss': 1.7458, 'grad_norm': 0.27983837777872345, 'learning_rate': 0.00019813407288824113, 'epoch': 0.09}

  9%|▉         | 382/4236 [35:05<4:57:11,  4.63s/it]


  9%|▉         | 384/4236 [35:13<4:31:13,  4.22s/it]
{'loss': 1.7174, 'grad_norm': 0.3174722853095471, 'learning_rate': 0.00019808970155512186, 'epoch': 0.09}


  9%|▉         | 386/4236 [35:23<4:56:20,  4.62s/it]

  9%|▉         | 387/4236 [35:27<4:50:04,  4.52s/it]
{'loss': 1.9247, 'grad_norm': 0.3066671631233424, 'learning_rate': 0.00019804481391905122, 'epoch': 0.09}

  9%|▉         | 388/4236 [35:33<5:20:39,  5.00s/it]


  9%|▉         | 390/4236 [35:42<5:07:55,  4.80s/it]

  9%|▉         | 391/4236 [35:46<4:53:49,  4.59s/it]

  9%|▉         | 392/4236 [35:53<5:25:44,  5.08s/it]

  9%|▉         | 393/4236 [35:58<5:31:09,  5.17s/it]

  9%|▉         | 394/4236 [36:03<5:26:52,  5.10s/it]

  9%|▉         | 395/4236 [36:09<5:40:04,  5.31s/it]

  9%|▉         | 396/4236 [36:12<5:10:46,  4.86s/it]

  9%|▉         | 397/4236 [36:17<4:56:34,  4.64s/it]

  9%|▉         | 398/4236 [36:22<5:09:47,  4.84s/it]
{'loss': 1.7542, 'grad_norm': 0.2837833455040399, 'learning_rate': 0.00019787581250903623, 'epoch': 0.09}


  9%|▉         | 400/4236 [36:38<6:33:54,  6.16s/it]

  9%|▉         | 401/4236 [36:45<6:43:11,  6.31s/it]

  9%|▉         | 402/4236 [36:48<5:49:45,  5.47s/it]

 10%|▉         | 403/4236 [36:52<5:16:54,  4.96s/it]

 10%|▉         | 404/4236 [36:56<5:10:47,  4.87s/it]

 10%|▉         | 405/4236 [37:00<4:45:53,  4.48s/it]

 10%|▉         | 406/4236 [37:05<4:52:33,  4.58s/it]
{'loss': 1.6866, 'grad_norm': 0.3306783918435041, 'learning_rate': 0.00019774855132978486, 'epoch': 0.1}


 10%|▉         | 408/4236 [37:14<4:50:10,  4.55s/it]
{'loss': 1.8398, 'grad_norm': 0.28935002546172806, 'learning_rate': 0.00019771616417266964, 'epoch': 0.1}

 10%|▉         | 409/4236 [37:19<5:08:10,  4.83s/it]

 10%|▉         | 410/4236 [37:23<4:48:49,  4.53s/it]

 10%|▉         | 411/4236 [37:31<5:56:23,  5.59s/it]

 10%|▉         | 412/4236 [37:38<6:11:52,  5.83s/it]

 10%|▉         | 413/4236 [37:41<5:31:15,  5.20s/it]


 10%|▉         | 415/4236 [37:51<5:18:47,  5.01s/it]

 10%|▉         | 416/4236 [37:55<5:02:47,  4.76s/it]
{'loss': 1.6894, 'grad_norm': 0.28093354289064265, 'learning_rate': 0.00019758433036796004, 'epoch': 0.1}


 10%|▉         | 418/4236 [38:04<4:54:44,  4.63s/it]
{'loss': 1.6721, 'grad_norm': 0.2959077085413279, 'learning_rate': 0.00019755080100823048, 'epoch': 0.1}

 10%|▉         | 419/4236 [38:10<5:17:08,  4.99s/it]

 10%|▉         | 420/4236 [38:16<5:32:31,  5.23s/it]

 10%|▉         | 421/4236 [38:20<5:10:13,  4.88s/it]


 10%|▉         | 423/4236 [38:29<5:06:38,  4.83s/it]

 10%|█         | 424/4236 [38:39<6:42:54,  6.34s/it]

 10%|█         | 425/4236 [38:45<6:35:11,  6.22s/it]

 10%|█         | 426/4236 [38:48<5:50:11,  5.51s/it]
{'loss': 1.8223, 'grad_norm': 0.2890714314917094, 'learning_rate': 0.00019741440228822533, 'epoch': 0.1}


 10%|█         | 428/4236 [38:59<5:38:17,  5.33s/it]
{'loss': 1.793, 'grad_norm': 0.3074840267579488, 'learning_rate': 0.00019737973268681115, 'epoch': 0.1}


 10%|█         | 430/4236 [39:12<6:05:06,  5.76s/it]

 10%|█         | 431/4236 [39:16<5:36:32,  5.31s/it]

 10%|█         | 432/4236 [39:20<5:14:34,  4.96s/it]
{'loss': 1.7376, 'grad_norm': 0.28203448459560865, 'learning_rate': 0.00019730971014415583, 'epoch': 0.1}


 10%|█         | 434/4236 [39:34<6:27:24,  6.11s/it]

 10%|█         | 435/4236 [39:42<7:08:14,  6.76s/it]
{'loss': 1.8044, 'grad_norm': 0.29531185237236224, 'learning_rate': 0.00019725659563757608, 'epoch': 0.1}


 10%|█         | 437/4236 [39:54<6:33:15,  6.21s/it]

 10%|█         | 438/4236 [39:58<5:54:17,  5.60s/it]

 10%|█         | 439/4236 [40:02<5:15:05,  4.98s/it]
{'loss': 1.7602, 'grad_norm': 0.2773351929495438, 'learning_rate': 0.00019718498002755137, 'epoch': 0.1}

 10%|█         | 440/4236 [40:06<4:48:23,  4.56s/it]


 10%|█         | 442/4236 [40:15<5:03:38,  4.80s/it]

 10%|█         | 443/4236 [40:19<4:42:52,  4.47s/it]

 10%|█         | 444/4236 [40:23<4:30:03,  4.27s/it]

 11%|█         | 445/4236 [40:26<4:18:11,  4.09s/it]

 11%|█         | 446/4236 [40:33<5:09:21,  4.90s/it]

 11%|█         | 447/4236 [40:43<6:33:10,  6.23s/it]
{'loss': 1.768, 'grad_norm': 0.2846876853019009, 'learning_rate': 0.0001970390212690695, 'epoch': 0.11}

 11%|█         | 448/4236 [40:50<6:53:36,  6.55s/it]

 11%|█         | 449/4236 [40:54<6:02:52,  5.75s/it]


 11%|█         | 451/4236 [41:01<4:56:21,  4.70s/it]

 11%|█         | 452/4236 [41:15<7:45:08,  7.38s/it]

 11%|█         | 453/4236 [41:20<7:09:50,  6.82s/it]
{'loss': 1.7386, 'grad_norm': 0.2974417501321709, 'learning_rate': 0.00019692716829719194, 'epoch': 0.11}


 11%|█         | 455/4236 [41:31<6:19:30,  6.02s/it]

 11%|█         | 456/4236 [41:38<6:47:57,  6.48s/it]

 11%|█         | 457/4236 [41:43<6:18:57,  6.02s/it]

 11%|█         | 458/4236 [41:50<6:34:37,  6.27s/it]
{'loss': 1.6324, 'grad_norm': 0.33126250498273685, 'learning_rate': 0.00019683239849296866, 'epoch': 0.11}

 11%|█         | 459/4236 [41:56<6:18:04,  6.01s/it]


 11%|█         | 461/4236 [42:11<6:56:23,  6.62s/it]

 11%|█         | 462/4236 [42:14<5:58:46,  5.70s/it]

 11%|█         | 463/4236 [42:18<5:25:23,  5.17s/it]
{'loss': 1.5964, 'grad_norm': 0.2790293660425243, 'learning_rate': 0.00019673621289808098, 'epoch': 0.11}

 11%|█         | 464/4236 [42:22<4:55:22,  4.70s/it]

 11%|█         | 465/4236 [42:26<4:36:58,  4.41s/it]


 11%|█         | 467/4236 [42:51<9:09:49,  8.75s/it]

 11%|█         | 468/4236 [42:55<7:49:15,  7.47s/it]
{'loss': 1.6951, 'grad_norm': 0.2941936400096985, 'learning_rate': 0.00019663861291886257, 'epoch': 0.11}


 11%|█         | 470/4236 [43:03<6:00:55,  5.75s/it]

 11%|█         | 471/4236 [43:10<6:20:05,  6.06s/it]

 11%|█         | 472/4236 [43:14<5:39:10,  5.41s/it]

 11%|█         | 473/4236 [43:18<5:16:13,  5.04s/it]

 11%|█         | 474/4236 [43:28<6:49:55,  6.54s/it]

 11%|█         | 475/4236 [43:32<6:02:12,  5.78s/it]

 11%|█         | 476/4236 [43:38<6:05:39,  5.84s/it]

 11%|█▏        | 477/4236 [43:43<5:48:31,  5.56s/it]

 11%|█▏        | 478/4236 [43:47<5:21:57,  5.14s/it]

 11%|█▏        | 479/4236 [43:53<5:28:35,  5.25s/it]

 11%|█▏        | 480/4236 [43:59<5:36:51,  5.38s/it]

 11%|█▏        | 481/4236 [44:02<5:06:39,  4.90s/it]

 11%|█▏        | 482/4236 [44:07<4:53:25,  4.69s/it]
{'loss': 1.7594, 'grad_norm': 0.2940623711325751, 'learning_rate': 0.00019635782067826938, 'epoch': 0.11}

 11%|█▏        | 483/4236 [44:12<5:02:05,  4.83s/it]


 11%|█▏        | 485/4236 [44:21<4:51:47,  4.67s/it]

 11%|█▏        | 486/4236 [44:25<4:42:21,  4.52s/it]

 11%|█▏        | 487/4236 [44:29<4:34:47,  4.40s/it]

 12%|█▏        | 488/4236 [44:33<4:31:58,  4.35s/it]

 12%|█▏        | 489/4236 [44:37<4:17:54,  4.13s/it]
{'loss': 1.8118, 'grad_norm': 0.31339176600463226, 'learning_rate': 0.00019621328050691, 'epoch': 0.12}


 12%|█▏        | 491/4236 [44:44<4:08:11,  3.98s/it]

 12%|█▏        | 492/4236 [44:51<4:52:13,  4.68s/it]
{'loss': 1.89, 'grad_norm': 0.3020442956324626, 'learning_rate': 0.00019615049043274205, 'epoch': 0.12}


 12%|█▏        | 494/4236 [45:05<6:04:26,  5.84s/it]

 12%|█▏        | 495/4236 [45:19<8:28:03,  8.15s/it]

 12%|█▏        | 496/4236 [45:27<8:29:24,  8.17s/it]

 12%|█▏        | 497/4236 [45:31<7:12:32,  6.94s/it]
{'loss': 1.727, 'grad_norm': 0.3293819002248617, 'learning_rate': 0.00019604471581630205, 'epoch': 0.12}

 12%|█▏        | 498/4236 [45:36<6:38:59,  6.40s/it]

 12%|█▏        | 499/4236 [45:40<5:54:13,  5.69s/it]


 12%|█▏        | 501/4236 [45:52<6:18:17,  6.08s/it]

 12%|█▏        | 502/4236 [45:56<5:41:47,  5.49s/it]

 12%|█▏        | 503/4236 [46:01<5:27:35,  5.27s/it]

 12%|█▏        | 504/4236 [46:05<4:56:34,  4.77s/it]

 12%|█▏        | 505/4236 [46:09<4:41:54,  4.53s/it]

 12%|█▏        | 506/4236 [46:23<7:48:00,  7.53s/it]

 12%|█▏        | 507/4236 [46:28<6:47:39,  6.56s/it]

 12%|█▏        | 508/4236 [46:31<5:52:34,  5.67s/it]
{'loss': 1.7601, 'grad_norm': 0.3092616600568812, 'learning_rate': 0.00019580707082380279, 'epoch': 0.12}

 12%|█▏        | 509/4236 [46:36<5:40:59,  5.49s/it]

 12%|█▏        | 510/4236 [46:42<5:46:40,  5.58s/it]


 12%|█▏        | 512/4236 [46:51<5:11:20,  5.02s/it]
{'loss': 1.6405, 'grad_norm': 0.3301693978666318, 'learning_rate': 0.00019571897261222695, 'epoch': 0.12}


 12%|█▏        | 514/4236 [47:01<5:08:04,  4.97s/it]

 12%|█▏        | 515/4236 [47:07<5:29:43,  5.32s/it]

 12%|█▏        | 516/4236 [47:11<4:58:18,  4.81s/it]

 12%|█▏        | 517/4236 [47:20<6:19:39,  6.13s/it]

 12%|█▏        | 518/4236 [47:25<6:07:51,  5.94s/it]
{'loss': 1.7536, 'grad_norm': 0.25578872013001336, 'learning_rate': 0.00019558514614057609, 'epoch': 0.12}


 12%|█▏        | 520/4236 [47:35<5:33:30,  5.38s/it]

 12%|█▏        | 521/4236 [47:47<7:44:19,  7.50s/it]

 12%|█▏        | 522/4236 [47:53<7:18:00,  7.08s/it]

 12%|█▏        | 523/4236 [48:08<9:30:13,  9.21s/it]

 12%|█▏        | 524/4236 [48:13<8:24:31,  8.16s/it]

 12%|█▏        | 525/4236 [48:19<7:33:57,  7.34s/it]

 12%|█▏        | 526/4236 [48:23<6:35:58,  6.40s/it]

 12%|█▏        | 527/4236 [48:27<5:59:18,  5.81s/it]

 12%|█▏        | 528/4236 [48:31<5:19:43,  5.17s/it]

 12%|█▏        | 529/4236 [48:36<5:08:33,  4.99s/it]
{'loss': 1.5492, 'grad_norm': 0.35849659779183823, 'learning_rate': 0.00019533457282013193, 'epoch': 0.12}

 13%|█▎        | 530/4236 [48:40<4:53:49,  4.76s/it]

 13%|█▎        | 531/4236 [48:46<5:23:30,  5.24s/it]


 13%|█▎        | 533/4236 [48:56<5:09:58,  5.02s/it]
{'loss': 1.8141, 'grad_norm': 0.2963066792236597, 'learning_rate': 0.00019524178164040935, 'epoch': 0.13}

 13%|█▎        | 534/4236 [49:04<6:12:08,  6.03s/it]

 13%|█▎        | 535/4236 [49:08<5:37:58,  5.48s/it]

 13%|█▎        | 536/4236 [49:12<5:14:37,  5.10s/it]

 13%|█▎        | 537/4236 [49:16<4:45:38,  4.63s/it]

 13%|█▎        | 538/4236 [49:22<5:13:20,  5.08s/it]


 13%|█▎        | 540/4236 [49:29<4:25:50,  4.32s/it]

 13%|█▎        | 541/4236 [49:33<4:10:23,  4.07s/it]

 13%|█▎        | 542/4236 [49:37<4:12:40,  4.10s/it]

 13%|█▎        | 543/4236 [49:41<4:10:23,  4.07s/it]

 13%|█▎        | 544/4236 [49:45<4:04:53,  3.98s/it]

 13%|█▎        | 545/4236 [49:53<5:28:42,  5.34s/it]

 13%|█▎        | 546/4236 [49:59<5:29:28,  5.36s/it]
{'loss': 1.726, 'grad_norm': 0.3077243456270414, 'learning_rate': 0.00019493405985666125, 'epoch': 0.13}


 13%|█▎        | 548/4236 [50:10<5:29:57,  5.37s/it]

 13%|█▎        | 549/4236 [50:17<6:05:48,  5.95s/it]

 13%|█▎        | 550/4236 [50:22<5:48:36,  5.67s/it]

 13%|█▎        | 551/4236 [50:28<5:47:00,  5.65s/it]

 13%|█▎        | 552/4236 [50:31<5:15:25,  5.14s/it]

 13%|█▎        | 553/4236 [50:35<4:48:41,  4.70s/it]

 13%|█▎        | 554/4236 [50:42<5:26:40,  5.32s/it]
{'loss': 1.6675, 'grad_norm': 0.29377040242814834, 'learning_rate': 0.0001947400268635591, 'epoch': 0.13}


 13%|█▎        | 556/4236 [50:53<5:43:43,  5.60s/it]

 13%|█▎        | 557/4236 [50:57<5:19:44,  5.21s/it]

 13%|█▎        | 558/4236 [51:12<8:04:01,  7.90s/it]
{'loss': 1.8576, 'grad_norm': 0.3081907538280207, 'learning_rate': 0.00019464168012034566, 'epoch': 0.13}


 13%|█▎        | 560/4236 [51:21<6:31:45,  6.39s/it]

 13%|█▎        | 561/4236 [51:26<5:55:59,  5.81s/it]
{'loss': 1.7242, 'grad_norm': 0.2878701363656139, 'learning_rate': 0.0001945673388328091, 'epoch': 0.13}

 13%|█▎        | 562/4236 [51:30<5:27:26,  5.35s/it]

 13%|█▎        | 563/4236 [51:34<5:08:18,  5.04s/it]

 13%|█▎        | 564/4236 [51:39<4:53:00,  4.79s/it]

 13%|█▎        | 565/4236 [51:44<5:05:11,  4.99s/it]


 13%|█▎        | 567/4236 [51:53<4:50:30,  4.75s/it]

 13%|█▎        | 568/4236 [52:02<5:59:34,  5.88s/it]

 13%|█▎        | 569/4236 [52:07<5:50:06,  5.73s/it]

 13%|█▎        | 570/4236 [52:12<5:29:28,  5.39s/it]

 13%|█▎        | 571/4236 [52:23<7:18:02,  7.17s/it]

 14%|█▎        | 572/4236 [52:27<6:28:28,  6.36s/it]

 14%|█▎        | 573/4236 [52:35<6:58:11,  6.85s/it]

 14%|█▎        | 574/4236 [52:39<6:01:47,  5.93s/it]
{'loss': 1.7046, 'grad_norm': 0.2917269899845792, 'learning_rate': 0.00019423944640613753, 'epoch': 0.14}

 14%|█▎        | 575/4236 [52:43<5:16:10,  5.18s/it]


 14%|█▎        | 577/4236 [52:51<4:44:42,  4.67s/it]
{'loss': 1.625, 'grad_norm': 0.26866031888208197, 'learning_rate': 0.00019416245496969864, 'epoch': 0.14}


 14%|█▎        | 579/4236 [53:00<4:42:37,  4.64s/it]

 14%|█▎        | 580/4236 [53:03<4:23:24,  4.32s/it]

 14%|█▎        | 581/4236 [53:10<4:58:15,  4.90s/it]

 14%|█▎        | 582/4236 [53:14<4:44:44,  4.68s/it]

 14%|█▍        | 583/4236 [53:17<4:25:17,  4.36s/it]

 14%|█▍        | 584/4236 [53:21<4:19:40,  4.27s/it]

 14%|█▍        | 585/4236 [53:25<4:12:11,  4.14s/it]

 14%|█▍        | 586/4236 [53:29<4:02:24,  3.98s/it]
{'loss': 1.6957, 'grad_norm': 0.315363144900231, 'learning_rate': 0.00019392850850496766, 'epoch': 0.14}

 14%|█▍        | 587/4236 [53:33<3:58:23,  3.92s/it]


 14%|█▍        | 589/4236 [53:40<3:51:04,  3.80s/it]

 14%|█▍        | 590/4236 [53:53<6:38:59,  6.57s/it]
{'loss': 1.7516, 'grad_norm': 0.3026876414333064, 'learning_rate': 0.00019382310336981204, 'epoch': 0.14}


 14%|█▍        | 592/4236 [54:03<5:53:44,  5.82s/it]

 14%|█▍        | 593/4236 [54:07<5:19:22,  5.26s/it]

 14%|█▍        | 594/4236 [54:12<4:59:39,  4.94s/it]

 14%|█▍        | 595/4236 [54:20<6:00:25,  5.94s/it]
{'loss': 1.5567, 'grad_norm': 0.2925207448160455, 'learning_rate': 0.0001936901124542255, 'epoch': 0.14}

 14%|█▍        | 596/4236 [54:25<5:42:34,  5.65s/it]


 14%|█▍        | 598/4236 [54:33<5:02:46,  4.99s/it]

 14%|█▍        | 599/4236 [54:44<6:38:07,  6.57s/it]

 14%|█▍        | 600/4236 [54:48<5:57:45,  5.90s/it]
 14%|█▍        | 600/4236 [54:48<5:57:45,  5.90s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 14%|█▍        | 601/4236 [55:07<10:00:36,  9.91s/it]

 14%|█▍        | 602/4236 [55:12<8:26:06,  8.36s/it]

 14%|█▍        | 603/4236 [55:16<6:59:21,  6.93s/it]

 14%|█▍        | 604/4236 [55:22<6:44:34,  6.68s/it]

 14%|█▍        | 605/4236 [55:26<5:58:42,  5.93s/it]

 14%|█▍        | 606/4236 [55:31<5:49:32,  5.78s/it]

 14%|█▍        | 607/4236 [55:37<5:56:01,  5.89s/it]
{'loss': 1.5496, 'grad_norm': 0.29624828010592785, 'learning_rate': 0.00019336534902459048, 'epoch': 0.14}

 14%|█▍        | 608/4236 [55:41<5:13:51,  5.19s/it]


 14%|█▍        | 610/4236 [55:48<4:22:19,  4.34s/it]
{'loss': 1.7108, 'grad_norm': 0.3028695078922023, 'learning_rate': 0.0001932829285024533, 'epoch': 0.14}


 14%|█▍        | 612/4236 [55:57<4:27:05,  4.42s/it]

 14%|█▍        | 613/4236 [56:04<5:15:31,  5.23s/it]

 14%|█▍        | 614/4236 [56:09<5:10:45,  5.15s/it]

 15%|█▍        | 615/4236 [56:14<5:06:39,  5.08s/it]

 15%|█▍        | 616/4236 [56:18<4:44:56,  4.72s/it]

 15%|█▍        | 617/4236 [56:22<4:38:52,  4.62s/it]
{'loss': 1.7375, 'grad_norm': 0.263830185728849, 'learning_rate': 0.00019308870525277884, 'epoch': 0.15}


 15%|█▍        | 619/4236 [56:32<4:42:47,  4.69s/it]
{'loss': 1.7803, 'grad_norm': 0.3128962034144217, 'learning_rate': 0.00019303272267155657, 'epoch': 0.15}

 15%|█▍        | 620/4236 [56:36<4:45:46,  4.74s/it]

 15%|█▍        | 621/4236 [56:41<4:33:28,  4.54s/it]


 15%|█▍        | 623/4236 [56:49<4:30:12,  4.49s/it]

 15%|█▍        | 624/4236 [56:53<4:16:53,  4.27s/it]
{'loss': 1.602, 'grad_norm': 0.35923412777424013, 'learning_rate': 0.0001928918143404543, 'epoch': 0.15}

 15%|█▍        | 625/4236 [56:57<4:06:23,  4.09s/it]

 15%|█▍        | 626/4236 [57:01<4:03:27,  4.05s/it]

 15%|█▍        | 627/4236 [57:05<4:02:08,  4.03s/it]


 15%|█▍        | 629/4236 [57:14<4:17:47,  4.29s/it]

 15%|█▍        | 630/4236 [57:17<4:08:32,  4.14s/it]

 15%|█▍        | 631/4236 [57:22<4:20:29,  4.34s/it]

 15%|█▍        | 632/4236 [57:28<4:40:04,  4.66s/it]

 15%|█▍        | 633/4236 [57:32<4:25:51,  4.43s/it]

 15%|█▍        | 634/4236 [57:35<4:11:58,  4.20s/it]
{'loss': 1.6554, 'grad_norm': 0.32166849849841184, 'learning_rate': 0.00019260592523269243, 'epoch': 0.15}


 15%|█▌        | 636/4236 [57:43<4:11:20,  4.19s/it]

 15%|█▌        | 637/4236 [57:47<4:08:00,  4.13s/it]

 15%|█▌        | 638/4236 [57:51<4:01:29,  4.03s/it]
{'loss': 1.6279, 'grad_norm': 0.3085118771886414, 'learning_rate': 0.00019249005217351846, 'epoch': 0.15}


 15%|█▌        | 640/4236 [58:02<4:34:49,  4.59s/it]

 15%|█▌        | 641/4236 [58:06<4:26:19,  4.44s/it]
{'loss': 1.7442, 'grad_norm': 0.293346027495918, 'learning_rate': 0.00019240257935285434, 'epoch': 0.15}


 15%|█▌        | 643/4236 [58:14<4:18:50,  4.32s/it]

 15%|█▌        | 644/4236 [58:28<7:02:32,  7.06s/it]

 15%|█▌        | 645/4236 [58:32<6:04:08,  6.08s/it]
{'loss': 1.7443, 'grad_norm': 0.29096707392718385, 'learning_rate': 0.00019228519243229034, 'epoch': 0.15}


 15%|█▌        | 647/4236 [58:41<5:10:44,  5.19s/it]

 15%|█▌        | 648/4236 [58:44<4:46:51,  4.80s/it]
{'loss': 1.7417, 'grad_norm': 0.294943783160055, 'learning_rate': 0.00019219658547282067, 'epoch': 0.15}

 15%|█▌        | 649/4236 [58:49<4:45:35,  4.78s/it]


 15%|█▌        | 651/4236 [58:58<4:36:34,  4.63s/it]

 15%|█▌        | 652/4236 [59:02<4:31:49,  4.55s/it]

 15%|█▌        | 653/4236 [59:06<4:23:03,  4.41s/it]

 15%|█▌        | 654/4236 [59:11<4:19:12,  4.34s/it]
{'loss': 1.5926, 'grad_norm': 0.33624789545756223, 'learning_rate': 0.000192017916170628, 'epoch': 0.15}


 15%|█▌        | 656/4236 [59:19<4:23:43,  4.42s/it]

 16%|█▌        | 657/4236 [59:24<4:23:17,  4.41s/it]

 16%|█▌        | 658/4236 [59:28<4:12:59,  4.24s/it]

 16%|█▌        | 659/4236 [59:40<6:31:05,  6.56s/it]

 16%|█▌        | 660/4236 [59:45<6:18:01,  6.34s/it]

 16%|█▌        | 661/4236 [1:00:00<8:51:09,  8.91s/it]

 16%|█▌        | 662/4236 [1:00:08<8:33:08,  8.61s/it]

 16%|█▌        | 663/4236 [1:00:20<9:19:07,  9.39s/it]

 16%|█▌        | 664/4236 [1:00:26<8:27:18,  8.52s/it]

 16%|█▌        | 665/4236 [1:00:30<7:04:03,  7.12s/it]

 16%|█▌        | 666/4236 [1:00:34<6:16:35,  6.33s/it]

 16%|█▌        | 667/4236 [1:00:40<6:04:10,  6.12s/it]
{'loss': 1.8372, 'grad_norm': 0.29569345006296055, 'learning_rate': 0.0001916241581623503, 'epoch': 0.16}


 16%|█▌        | 669/4236 [1:00:53<5:54:09,  5.96s/it]
{'loss': 1.6462, 'grad_norm': 0.34697862278079605, 'learning_rate': 0.00019156277524205127, 'epoch': 0.16}


 16%|█▌        | 671/4236 [1:01:07<6:45:17,  6.82s/it]
{'loss': 1.7806, 'grad_norm': 0.2807439263455702, 'learning_rate': 0.00019150117812260882, 'epoch': 0.16}

 16%|█▌        | 672/4236 [1:01:13<6:20:43,  6.41s/it]


 16%|█▌        | 674/4236 [1:01:25<5:54:31,  5.97s/it]
{'loss': 1.7436, 'grad_norm': 0.3309039785103169, 'learning_rate': 0.00019140838113540346, 'epoch': 0.16}


 16%|█▌        | 676/4236 [1:01:37<6:01:08,  6.09s/it]

 16%|█▌        | 677/4236 [1:01:40<5:21:12,  5.42s/it]
{'loss': 1.8308, 'grad_norm': 0.2805867749186507, 'learning_rate': 0.00019131510301290857, 'epoch': 0.16}


 16%|█▌        | 679/4236 [1:01:56<6:29:57,  6.58s/it]
{'loss': 1.8528, 'grad_norm': 0.2902123872539665, 'learning_rate': 0.00019125265054288239, 'epoch': 0.16}

 16%|█▌        | 680/4236 [1:02:01<6:03:42,  6.14s/it]

 16%|█▌        | 681/4236 [1:02:07<5:55:50,  6.01s/it]

 16%|█▌        | 682/4236 [1:02:13<6:02:40,  6.12s/it]

 16%|█▌        | 683/4236 [1:02:19<5:54:00,  5.98s/it]


 16%|█▌        | 685/4236 [1:02:29<5:22:21,  5.45s/it]

 16%|█▌        | 686/4236 [1:02:34<5:24:15,  5.48s/it]

 16%|█▌        | 687/4236 [1:02:38<4:52:12,  4.94s/it]

 16%|█▌        | 688/4236 [1:02:42<4:37:29,  4.69s/it]

 16%|█▋        | 689/4236 [1:02:46<4:25:03,  4.48s/it]

 16%|█▋        | 690/4236 [1:02:50<4:22:46,  4.45s/it]

 16%|█▋        | 691/4236 [1:02:54<4:15:46,  4.33s/it]
{'loss': 1.802, 'grad_norm': 0.32872401016510666, 'learning_rate': 0.0001908734579245508, 'epoch': 0.16}

 16%|█▋        | 692/4236 [1:02:59<4:20:52,  4.42s/it]


 16%|█▋        | 694/4236 [1:03:08<4:29:31,  4.57s/it]

 16%|█▋        | 695/4236 [1:03:18<5:51:59,  5.96s/it]

 16%|█▋        | 696/4236 [1:03:22<5:28:41,  5.57s/it]

 16%|█▋        | 697/4236 [1:03:26<4:58:11,  5.06s/it]

 16%|█▋        | 698/4236 [1:03:31<4:53:18,  4.97s/it]

 17%|█▋        | 699/4236 [1:03:35<4:35:49,  4.68s/it]
{'loss': 1.7982, 'grad_norm': 0.2931814259007602, 'learning_rate': 0.00019061640914535023, 'epoch': 0.17}


 17%|█▋        | 701/4236 [1:03:52<6:49:34,  6.95s/it]

 17%|█▋        | 702/4236 [1:03:57<6:17:07,  6.40s/it]
{'loss': 1.6253, 'grad_norm': 0.2833249477164436, 'learning_rate': 0.00019051914089575643, 'epoch': 0.17}


 17%|█▋        | 704/4236 [1:04:09<5:59:38,  6.11s/it]

 17%|█▋        | 705/4236 [1:04:13<5:28:16,  5.58s/it]

 17%|█▋        | 706/4236 [1:04:18<5:17:47,  5.40s/it]

 17%|█▋        | 707/4236 [1:04:22<5:00:57,  5.12s/it]
{'loss': 1.8895, 'grad_norm': 0.25391595609243206, 'learning_rate': 0.0001903559686121696, 'epoch': 0.17}

 17%|█▋        | 708/4236 [1:04:27<4:53:58,  5.00s/it]

 17%|█▋        | 709/4236 [1:04:31<4:38:34,  4.74s/it]

 17%|█▋        | 710/4236 [1:04:35<4:26:35,  4.54s/it]


 17%|█▋        | 712/4236 [1:04:50<6:04:18,  6.20s/it]

 17%|█▋        | 713/4236 [1:04:55<5:30:25,  5.63s/it]

 17%|█▋        | 714/4236 [1:05:02<5:58:12,  6.10s/it]
{'loss': 1.7791, 'grad_norm': 0.29781132058716636, 'learning_rate': 0.0001901253085077468, 'epoch': 0.17}


 17%|█▋        | 716/4236 [1:05:12<5:32:55,  5.67s/it]

 17%|█▋        | 717/4236 [1:05:16<5:13:10,  5.34s/it]

 17%|█▋        | 718/4236 [1:05:22<5:10:16,  5.29s/it]
{'loss': 1.7655, 'grad_norm': 0.30743755087994534, 'learning_rate': 0.00018999234270923175, 'epoch': 0.17}


 17%|█▋        | 720/4236 [1:05:38<6:23:47,  6.55s/it]
{'loss': 1.7792, 'grad_norm': 0.34805360334991786, 'learning_rate': 0.0001899255439441043, 'epoch': 0.17}


 17%|█▋        | 722/4236 [1:05:47<5:09:46,  5.29s/it]

 17%|█▋        | 723/4236 [1:05:51<4:49:17,  4.94s/it]

 17%|█▋        | 724/4236 [1:05:56<4:57:17,  5.08s/it]

 17%|█▋        | 725/4236 [1:06:00<4:31:38,  4.64s/it]

 17%|█▋        | 726/4236 [1:06:06<4:50:34,  4.97s/it]
{'loss': 1.7498, 'grad_norm': 0.3075676192782903, 'learning_rate': 0.00018972388606192125, 'epoch': 0.17}

 17%|█▋        | 727/4236 [1:06:10<4:31:07,  4.64s/it]

 17%|█▋        | 728/4236 [1:06:13<4:12:51,  4.32s/it]


 17%|█▋        | 730/4236 [1:06:28<5:57:30,  6.12s/it]
{'loss': 1.6999, 'grad_norm': 0.2976946927821253, 'learning_rate': 0.00018958839772520124, 'epoch': 0.17}


 17%|█▋        | 732/4236 [1:06:38<5:25:07,  5.57s/it]

 17%|█▋        | 733/4236 [1:06:42<4:59:21,  5.13s/it]
{'loss': 1.761, 'grad_norm': 0.3133628467548233, 'learning_rate': 0.00018948623125494715, 'epoch': 0.17}

 17%|█▋        | 734/4236 [1:06:46<4:31:41,  4.65s/it]


 17%|█▋        | 736/4236 [1:06:59<5:41:23,  5.85s/it]
{'loss': 1.9068, 'grad_norm': 0.2778894672110852, 'learning_rate': 0.0001893835937667924, 'epoch': 0.17}


 17%|█▋        | 738/4236 [1:07:09<5:15:27,  5.41s/it]

 17%|█▋        | 739/4236 [1:07:13<4:49:53,  4.97s/it]

 17%|█▋        | 740/4236 [1:07:21<5:40:28,  5.84s/it]

 17%|█▋        | 741/4236 [1:07:25<5:17:17,  5.45s/it]
{'loss': 1.5681, 'grad_norm': 0.28142800048841066, 'learning_rate': 0.00018921148604852971, 'epoch': 0.17}

 18%|█▊        | 742/4236 [1:07:30<5:01:12,  5.17s/it]


 18%|█▊        | 744/4236 [1:07:38<4:31:59,  4.67s/it]

 18%|█▊        | 745/4236 [1:07:42<4:25:05,  4.56s/it]

 18%|█▊        | 746/4236 [1:07:46<4:20:30,  4.48s/it]

 18%|█▊        | 747/4236 [1:07:50<4:04:05,  4.20s/it]

 18%|█▊        | 748/4236 [1:08:03<6:36:08,  6.81s/it]

 18%|█▊        | 749/4236 [1:08:07<5:49:38,  6.02s/it]

 18%|█▊        | 750/4236 [1:08:11<5:10:46,  5.35s/it]

 18%|█▊        | 751/4236 [1:08:15<4:47:19,  4.95s/it]

 18%|█▊        | 752/4236 [1:08:21<5:01:05,  5.19s/it]

 18%|█▊        | 753/4236 [1:08:24<4:35:47,  4.75s/it]

 18%|█▊        | 754/4236 [1:08:28<4:24:05,  4.55s/it]

 18%|█▊        | 755/4236 [1:08:32<4:13:21,  4.37s/it]

 18%|█▊        | 756/4236 [1:08:40<5:15:15,  5.44s/it]

 18%|█▊        | 757/4236 [1:08:54<7:44:52,  8.02s/it]

 18%|█▊        | 758/4236 [1:09:02<7:37:25,  7.89s/it]

 18%|█▊        | 759/4236 [1:09:07<6:47:23,  7.03s/it]

 18%|█▊        | 760/4236 [1:09:11<5:48:43,  6.02s/it]

 18%|█▊        | 761/4236 [1:09:17<5:48:20,  6.01s/it]

 18%|█▊        | 762/4236 [1:09:30<7:59:29,  8.28s/it]
[2024-05-25 03:44:08,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 763/4236 [1:09:35<7:04:54,  7.34s/it]

 18%|█▊        | 764/4236 [1:09:41<6:27:33,  6.70s/it]

 18%|█▊        | 765/4236 [1:09:44<5:37:19,  5.83s/it]

 18%|█▊        | 766/4236 [1:09:48<5:04:40,  5.27s/it]
{'loss': 1.7747, 'grad_norm': 0.29774160880598216, 'learning_rate': 0.0001883314329771785, 'epoch': 0.18}

 18%|█▊        | 767/4236 [1:09:54<5:03:53,  5.26s/it]


 18%|█▊        | 769/4236 [1:10:04<4:59:57,  5.19s/it]
{'loss': 1.6625, 'grad_norm': 0.31733914004960995, 'learning_rate': 0.00018822365049137794, 'epoch': 0.18}


 18%|█▊        | 771/4236 [1:10:15<5:15:01,  5.45s/it]
{'loss': 1.8619, 'grad_norm': 0.3244908341935887, 'learning_rate': 0.0001881515374811647, 'epoch': 0.18}

 18%|█▊        | 772/4236 [1:10:20<5:07:32,  5.33s/it]

 18%|█▊        | 773/4236 [1:10:24<4:44:55,  4.94s/it]


 18%|█▊        | 775/4236 [1:10:31<4:07:12,  4.29s/it]
{'loss': 1.787, 'grad_norm': 0.2898714160569018, 'learning_rate': 0.00018800669297292384, 'epoch': 0.18}


 18%|█▊        | 777/4236 [1:10:39<3:59:23,  4.15s/it]

 18%|█▊        | 778/4236 [1:10:43<3:54:28,  4.07s/it]

 18%|█▊        | 779/4236 [1:10:53<5:25:25,  5.65s/it]

 18%|█▊        | 780/4236 [1:10:57<4:59:43,  5.20s/it]
{'loss': 1.9106, 'grad_norm': 0.30804223983087015, 'learning_rate': 0.0001878244794220022, 'epoch': 0.18}


 18%|█▊        | 782/4236 [1:11:08<5:07:05,  5.33s/it]

 18%|█▊        | 783/4236 [1:11:12<4:43:59,  4.93s/it]
{'loss': 1.6384, 'grad_norm': 0.31214557034930285, 'learning_rate': 0.00018771453475912572, 'epoch': 0.18}


 19%|█▊        | 785/4236 [1:11:25<5:25:29,  5.66s/it]

 19%|█▊        | 786/4236 [1:11:30<5:12:22,  5.43s/it]

 19%|█▊        | 787/4236 [1:11:35<4:54:19,  5.12s/it]

 19%|█▊        | 788/4236 [1:11:38<4:31:51,  4.73s/it]

 19%|█▊        | 789/4236 [1:11:53<7:13:18,  7.54s/it]

 19%|█▊        | 790/4236 [1:12:05<8:43:12,  9.11s/it]

 19%|█▊        | 791/4236 [1:12:11<7:45:10,  8.10s/it]

 19%|█▊        | 792/4236 [1:12:17<7:07:03,  7.44s/it]

 19%|█▊        | 793/4236 [1:12:21<6:01:01,  6.29s/it]

 19%|█▊        | 794/4236 [1:12:26<5:54:00,  6.17s/it]

 19%|█▉        | 795/4236 [1:12:30<5:17:20,  5.53s/it]

 19%|█▉        | 796/4236 [1:12:35<5:05:24,  5.33s/it]

 19%|█▉        | 797/4236 [1:12:40<4:46:35,  5.00s/it]

 19%|█▉        | 798/4236 [1:12:45<4:47:00,  5.01s/it]

 19%|█▉        | 799/4236 [1:12:55<6:14:42,  6.54s/it]

 19%|█▉        | 800/4236 [1:12:58<5:25:24,  5.68s/it]

 19%|█▉        | 801/4236 [1:13:02<4:53:23,  5.12s/it]

 19%|█▉        | 802/4236 [1:13:13<6:38:11,  6.96s/it]

 19%|█▉        | 803/4236 [1:13:18<6:01:23,  6.32s/it]
{'loss': 1.6922, 'grad_norm': 0.2899445699878759, 'learning_rate': 0.00018696979972203766, 'epoch': 0.19}


 19%|█▉        | 805/4236 [1:13:35<7:05:45,  7.45s/it]

 19%|█▉        | 806/4236 [1:13:41<6:32:21,  6.86s/it]

 19%|█▉        | 807/4236 [1:13:46<6:05:55,  6.40s/it]

 19%|█▉        | 808/4236 [1:13:50<5:30:31,  5.79s/it]

 19%|█▉        | 809/4236 [1:14:01<6:59:40,  7.35s/it]

 19%|█▉        | 810/4236 [1:14:07<6:37:13,  6.96s/it]
{'loss': 1.8761, 'grad_norm': 0.2922080844995349, 'learning_rate': 0.0001867043268784856, 'epoch': 0.19}


 19%|█▉        | 812/4236 [1:14:17<5:38:50,  5.94s/it]
{'loss': 1.6314, 'grad_norm': 0.35282357174712253, 'learning_rate': 0.0001866280207858977, 'epoch': 0.19}

 19%|█▉        | 813/4236 [1:14:22<5:15:52,  5.54s/it]


 19%|█▉        | 815/4236 [1:14:31<4:54:14,  5.16s/it]

 19%|█▉        | 816/4236 [1:14:35<4:38:07,  4.88s/it]

 19%|█▉        | 817/4236 [1:14:43<5:22:39,  5.66s/it]

 19%|█▉        | 818/4236 [1:14:48<5:07:28,  5.40s/it]

 19%|█▉        | 819/4236 [1:14:53<5:00:01,  5.27s/it]

 19%|█▉        | 820/4236 [1:14:58<5:06:14,  5.38s/it]

 19%|█▉        | 821/4236 [1:15:05<5:32:02,  5.83s/it]

 19%|█▉        | 822/4236 [1:15:09<4:58:27,  5.25s/it]
{'loss': 1.7639, 'grad_norm': 0.2949485007020853, 'learning_rate': 0.0001862434540857483, 'epoch': 0.19}


 19%|█▉        | 824/4236 [1:15:21<5:19:01,  5.61s/it]

 19%|█▉        | 825/4236 [1:15:25<4:57:54,  5.24s/it]
{'loss': 1.6543, 'grad_norm': 0.30879848693681483, 'learning_rate': 0.0001861270994922125, 'epoch': 0.19}


 20%|█▉        | 827/4236 [1:15:35<4:43:17,  4.99s/it]

 20%|█▉        | 828/4236 [1:15:46<6:27:15,  6.82s/it]

 20%|█▉        | 829/4236 [1:15:51<5:58:27,  6.31s/it]

 20%|█▉        | 830/4236 [1:15:55<5:17:26,  5.59s/it]

 20%|█▉        | 831/4236 [1:15:59<4:47:58,  5.07s/it]

 20%|█▉        | 832/4236 [1:16:05<5:12:04,  5.50s/it]

 20%|█▉        | 833/4236 [1:16:09<4:45:48,  5.04s/it]

 20%|█▉        | 834/4236 [1:16:13<4:20:20,  4.59s/it]

 20%|█▉        | 835/4236 [1:16:17<4:17:48,  4.55s/it]

 20%|█▉        | 836/4236 [1:16:21<4:06:47,  4.36s/it]
{'loss': 1.6018, 'grad_norm': 0.3384718108280203, 'learning_rate': 0.00018569659211731038, 'epoch': 0.2}

 20%|█▉        | 837/4236 [1:16:26<4:21:14,  4.61s/it]

 20%|█▉        | 838/4236 [1:16:32<4:41:23,  4.97s/it]


 20%|█▉        | 840/4236 [1:16:43<4:59:43,  5.30s/it]

 20%|█▉        | 841/4236 [1:16:47<4:35:27,  4.87s/it]

 20%|█▉        | 842/4236 [1:16:51<4:20:12,  4.60s/it]
{'loss': 1.8146, 'grad_norm': 0.27152234633196753, 'learning_rate': 0.00018545921189569507, 'epoch': 0.2}


 20%|█▉        | 844/4236 [1:16:58<3:49:53,  4.07s/it]

 20%|█▉        | 845/4236 [1:17:01<3:38:10,  3.86s/it]

 20%|█▉        | 846/4236 [1:17:06<4:03:54,  4.32s/it]
{'loss': 1.844, 'grad_norm': 0.31162059958904564, 'learning_rate': 0.00018529995850283224, 'epoch': 0.2}


 20%|██        | 848/4236 [1:17:15<4:08:10,  4.39s/it]

 20%|██        | 849/4236 [1:17:21<4:28:16,  4.75s/it]

 20%|██        | 850/4236 [1:17:25<4:19:47,  4.60s/it]

 20%|██        | 851/4236 [1:17:29<4:11:16,  4.45s/it]

 20%|██        | 852/4236 [1:17:35<4:38:45,  4.94s/it]

 20%|██        | 853/4236 [1:17:39<4:18:52,  4.59s/it]

 20%|██        | 854/4236 [1:17:43<4:06:40,  4.38s/it]
{'loss': 1.6971, 'grad_norm': 0.31697174894125546, 'learning_rate': 0.00018497905863895713, 'epoch': 0.2}


 20%|██        | 856/4236 [1:17:53<4:17:10,  4.57s/it]

 20%|██        | 857/4236 [1:17:57<4:06:57,  4.39s/it]

 20%|██        | 858/4236 [1:18:02<4:21:01,  4.64s/it]

 20%|██        | 859/4236 [1:18:18<7:32:16,  8.04s/it]

 20%|██        | 860/4236 [1:18:23<6:50:34,  7.30s/it]

 20%|██        | 861/4236 [1:18:28<6:02:17,  6.44s/it]

 20%|██        | 862/4236 [1:18:36<6:25:32,  6.86s/it]

 20%|██        | 863/4236 [1:18:40<5:36:43,  5.99s/it]

 20%|██        | 864/4236 [1:18:43<5:00:01,  5.34s/it]

 20%|██        | 865/4236 [1:18:48<4:43:28,  5.05s/it]

 20%|██        | 866/4236 [1:18:52<4:28:09,  4.77s/it]
{'loss': 1.7315, 'grad_norm': 0.29503019649837425, 'learning_rate': 0.00018449174872468436, 'epoch': 0.2}


 20%|██        | 868/4236 [1:19:04<5:00:01,  5.35s/it]
{'loss': 1.6099, 'grad_norm': 0.3203535170720438, 'learning_rate': 0.00018440983749234646, 'epoch': 0.2}


 21%|██        | 870/4236 [1:19:13<4:33:04,  4.87s/it]

 21%|██        | 871/4236 [1:19:17<4:20:29,  4.64s/it]

 21%|██        | 872/4236 [1:19:22<4:26:49,  4.76s/it]
{'loss': 1.6932, 'grad_norm': 0.3206218240364966, 'learning_rate': 0.00018424542282241145, 'epoch': 0.21}


 21%|██        | 874/4236 [1:19:35<5:04:45,  5.44s/it]
{'loss': 1.7279, 'grad_norm': 0.35944463582540326, 'learning_rate': 0.00018416291976944095, 'epoch': 0.21}

 21%|██        | 875/4236 [1:19:38<4:36:59,  4.94s/it]

 21%|██        | 876/4236 [1:19:46<5:23:13,  5.77s/it]

 21%|██        | 877/4236 [1:19:51<4:59:53,  5.36s/it]

 21%|██        | 878/4236 [1:19:54<4:31:34,  4.85s/it]


 21%|██        | 880/4236 [1:20:12<5:59:40,  6.43s/it]
{'loss': 1.706, 'grad_norm': 0.31667843683815206, 'learning_rate': 0.00018391423005595926, 'epoch': 0.21}

 21%|██        | 881/4236 [1:20:16<5:26:03,  5.83s/it]


 21%|██        | 883/4236 [1:20:31<6:07:46,  6.58s/it]

 21%|██        | 884/4236 [1:20:35<5:20:37,  5.74s/it]
{'loss': 1.4119, 'grad_norm': 0.31225514553838324, 'learning_rate': 0.00018374745505856903, 'epoch': 0.21}

 21%|██        | 885/4236 [1:20:45<6:26:44,  6.92s/it]


 21%|██        | 887/4236 [1:20:54<5:27:08,  5.86s/it]

 21%|██        | 888/4236 [1:20:59<5:08:57,  5.54s/it]

 21%|██        | 889/4236 [1:21:03<4:41:24,  5.04s/it]

 21%|██        | 890/4236 [1:21:09<5:09:24,  5.55s/it]
{'loss': 1.6936, 'grad_norm': 0.3152000064715001, 'learning_rate': 0.00018349582368135762, 'epoch': 0.21}

 21%|██        | 891/4236 [1:21:14<4:59:46,  5.38s/it]


 21%|██        | 893/4236 [1:21:41<8:35:39,  9.26s/it]

 21%|██        | 894/4236 [1:21:46<7:28:40,  8.06s/it]

 21%|██        | 895/4236 [1:21:53<7:07:57,  7.69s/it]

 21%|██        | 896/4236 [1:21:58<6:25:10,  6.92s/it]

 21%|██        | 897/4236 [1:22:02<5:35:55,  6.04s/it]

 21%|██        | 898/4236 [1:22:08<5:34:51,  6.02s/it]
{'loss': 1.7184, 'grad_norm': 0.3943235722273727, 'learning_rate': 0.00018315758151779017, 'epoch': 0.21}


 21%|██        | 900/4236 [1:22:17<4:53:38,  5.28s/it]
 21%|██        | 900/4236 [1:22:17<4:53:38,  5.28s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 21%|██▏       | 901/4236 [1:22:38<9:09:12,  9.88s/it]

 21%|██▏       | 902/4236 [1:22:43<7:53:15,  8.52s/it]

 21%|██▏       | 903/4236 [1:22:49<7:16:09,  7.85s/it]

 21%|██▏       | 904/4236 [1:22:53<6:06:38,  6.60s/it]
{'loss': 1.5921, 'grad_norm': 0.293128452015619, 'learning_rate': 0.00018290185657328072, 'epoch': 0.21}


 21%|██▏       | 906/4236 [1:23:01<4:59:00,  5.39s/it]

 21%|██▏       | 907/4236 [1:23:06<4:52:32,  5.27s/it]

 21%|██▏       | 908/4236 [1:23:12<4:56:46,  5.35s/it]

 21%|██▏       | 909/4236 [1:23:20<5:44:47,  6.22s/it]

 21%|██▏       | 910/4236 [1:23:40<9:26:44, 10.22s/it]
{'loss': 1.6473, 'grad_norm': 0.29808159439110576, 'learning_rate': 0.00018264438618898615, 'epoch': 0.21}


 22%|██▏       | 912/4236 [1:23:54<7:43:46,  8.37s/it]

 22%|██▏       | 913/4236 [1:24:01<7:28:25,  8.10s/it]

 22%|██▏       | 914/4236 [1:24:09<7:26:31,  8.07s/it]

 22%|██▏       | 915/4236 [1:24:16<6:55:37,  7.51s/it]
{'loss': 1.7729, 'grad_norm': 0.30590634715050796, 'learning_rate': 0.0001824284981231274, 'epoch': 0.22}


 22%|██▏       | 917/4236 [1:24:26<5:46:09,  6.26s/it]

 22%|██▏       | 918/4236 [1:24:36<6:45:05,  7.33s/it]
{'loss': 1.6512, 'grad_norm': 0.2865247947647378, 'learning_rate': 0.00018229838658936564, 'epoch': 0.22}


 22%|██▏       | 920/4236 [1:24:53<7:03:47,  7.67s/it]

 22%|██▏       | 921/4236 [1:24:58<6:21:44,  6.91s/it]

 22%|██▏       | 922/4236 [1:25:02<5:37:04,  6.10s/it]

 22%|██▏       | 923/4236 [1:25:08<5:27:44,  5.94s/it]

 22%|██▏       | 924/4236 [1:25:12<4:57:45,  5.39s/it]

 22%|██▏       | 925/4236 [1:25:16<4:40:21,  5.08s/it]

 22%|██▏       | 926/4236 [1:25:20<4:21:24,  4.74s/it]

 22%|██▏       | 927/4236 [1:25:25<4:22:26,  4.76s/it]
{'loss': 1.675, 'grad_norm': 0.3126236450317528, 'learning_rate': 0.0001819054556342378, 'epoch': 0.22}

 22%|██▏       | 928/4236 [1:25:29<4:04:35,  4.44s/it]
[2024-05-25 04:00:20,634] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 22%|██▏       | 930/4236 [1:25:48<6:00:03,  6.53s/it]

 22%|██▏       | 931/4236 [1:25:53<5:50:48,  6.37s/it]

 22%|██▏       | 932/4236 [1:25:58<5:26:01,  5.92s/it]

 22%|██▏       | 933/4236 [1:26:02<4:50:08,  5.27s/it]
{'loss': 1.6204, 'grad_norm': 0.30411296080766453, 'learning_rate': 0.00018164134493570981, 'epoch': 0.22}


 22%|██▏       | 935/4236 [1:26:15<5:34:05,  6.07s/it]

 22%|██▏       | 936/4236 [1:26:20<5:06:46,  5.58s/it]
{'loss': 1.7791, 'grad_norm': 0.29804950037106953, 'learning_rate': 0.00018150864464976178, 'epoch': 0.22}


 22%|██▏       | 938/4236 [1:26:29<4:42:59,  5.15s/it]

 22%|██▏       | 939/4236 [1:26:36<5:16:25,  5.76s/it]

 22%|██▏       | 940/4236 [1:26:51<7:50:36,  8.57s/it]

 22%|██▏       | 941/4236 [1:26:58<7:15:14,  7.93s/it]
{'loss': 1.544, 'grad_norm': 0.34013111888127256, 'learning_rate': 0.00018128652445860145, 'epoch': 0.22}


 22%|██▏       | 943/4236 [1:27:09<6:17:51,  6.88s/it]

 22%|██▏       | 944/4236 [1:27:14<5:36:32,  6.13s/it]

 22%|██▏       | 945/4236 [1:27:20<5:43:05,  6.26s/it]
{'loss': 1.6168, 'grad_norm': 0.2795065275613846, 'learning_rate': 0.00018110797243368743, 'epoch': 0.22}


 22%|██▏       | 947/4236 [1:27:30<5:05:29,  5.57s/it]

 22%|██▏       | 948/4236 [1:27:34<4:36:27,  5.04s/it]
{'loss': 1.7882, 'grad_norm': 0.29188107666495783, 'learning_rate': 0.00018097356025221974, 'epoch': 0.22}


 22%|██▏       | 950/4236 [1:27:46<4:59:07,  5.46s/it]

 22%|██▏       | 951/4236 [1:28:00<7:15:16,  7.95s/it]
{'loss': 1.7782, 'grad_norm': 0.2990322437892581, 'learning_rate': 0.00018083872185997274, 'epoch': 0.22}

 22%|██▏       | 952/4236 [1:28:05<6:32:43,  7.18s/it]


 23%|██▎       | 954/4236 [1:28:22<7:24:07,  8.12s/it]
{'loss': 1.796, 'grad_norm': 0.27688868970328434, 'learning_rate': 0.00018070345796667903, 'epoch': 0.23}


 23%|██▎       | 956/4236 [1:28:41<8:23:32,  9.21s/it]
{'loss': 1.8315, 'grad_norm': 0.3090223741457334, 'learning_rate': 0.00018061304599984533, 'epoch': 0.23}


 23%|██▎       | 958/4236 [1:28:50<6:08:43,  6.75s/it]

 23%|██▎       | 959/4236 [1:28:54<5:20:00,  5.86s/it]

 23%|██▎       | 960/4236 [1:28:59<5:09:16,  5.66s/it]
{'loss': 1.5949, 'grad_norm': 0.292414579138371, 'learning_rate': 0.00018043165652707649, 'epoch': 0.23}


 23%|██▎       | 962/4236 [1:29:14<5:47:54,  6.38s/it]

 23%|██▎       | 963/4236 [1:29:17<5:02:20,  5.54s/it]

 23%|██▎       | 964/4236 [1:29:23<5:13:26,  5.75s/it]

 23%|██▎       | 965/4236 [1:29:27<4:43:01,  5.19s/it]

 23%|██▎       | 966/4236 [1:29:31<4:23:51,  4.84s/it]

 23%|██▎       | 967/4236 [1:29:35<4:11:20,  4.61s/it]

 23%|██▎       | 968/4236 [1:29:40<4:06:21,  4.52s/it]

 23%|██▎       | 969/4236 [1:29:46<4:34:42,  5.05s/it]

 23%|██▎       | 970/4236 [1:29:51<4:27:14,  4.91s/it]
{'loss': 1.7957, 'grad_norm': 0.29072319218352516, 'learning_rate': 0.00017997489378879467, 'epoch': 0.23}


 23%|██▎       | 972/4236 [1:30:02<4:56:09,  5.44s/it]

 23%|██▎       | 973/4236 [1:30:06<4:26:08,  4.89s/it]

 23%|██▎       | 974/4236 [1:30:10<4:15:50,  4.71s/it]

 23%|██▎       | 975/4236 [1:30:14<4:02:15,  4.46s/it]

 23%|██▎       | 976/4236 [1:30:18<3:59:12,  4.40s/it]

 23%|██▎       | 977/4236 [1:30:22<3:52:18,  4.28s/it]
{'loss': 1.7123, 'grad_norm': 0.3423256674802351, 'learning_rate': 0.00017965237530878268, 'epoch': 0.23}

 23%|██▎       | 978/4236 [1:30:27<4:05:10,  4.52s/it]


 23%|██▎       | 980/4236 [1:30:35<3:56:13,  4.35s/it]

 23%|██▎       | 981/4236 [1:30:40<4:00:01,  4.42s/it]
{'loss': 1.6475, 'grad_norm': 0.30949211122281095, 'learning_rate': 0.00017946705359146684, 'epoch': 0.23}


 23%|██▎       | 983/4236 [1:30:53<4:42:12,  5.21s/it]

 23%|██▎       | 984/4236 [1:30:57<4:19:43,  4.79s/it]
{'loss': 1.7009, 'grad_norm': 0.3609490755787405, 'learning_rate': 0.00017932757421386023, 'epoch': 0.23}

 23%|██▎       | 985/4236 [1:31:01<4:17:37,  4.75s/it]


 23%|██▎       | 987/4236 [1:31:13<4:33:43,  5.06s/it]
{'loss': 1.6406, 'grad_norm': 0.2861809791279481, 'learning_rate': 0.00017918767728925284, 'epoch': 0.23}


 23%|██▎       | 989/4236 [1:31:24<4:56:05,  5.47s/it]

 23%|██▎       | 990/4236 [1:31:38<7:18:35,  8.11s/it]
{'loss': 1.8029, 'grad_norm': 0.2796382245971516, 'learning_rate': 0.00017904736355400322, 'epoch': 0.23}


 23%|██▎       | 992/4236 [1:31:48<5:53:46,  6.54s/it]

 23%|██▎       | 993/4236 [1:31:58<6:38:32,  7.37s/it]

 23%|██▎       | 994/4236 [1:32:02<5:43:47,  6.36s/it]
{'loss': 1.7418, 'grad_norm': 0.3048974653329104, 'learning_rate': 0.0001788596314783864, 'epoch': 0.23}


 24%|██▎       | 996/4236 [1:32:21<7:12:22,  8.01s/it]
{'loss': 1.6573, 'grad_norm': 0.2977159750764625, 'learning_rate': 0.00017876548860797756, 'epoch': 0.24}

 24%|██▎       | 997/4236 [1:32:25<6:11:58,  6.89s/it]


 24%|██▎       | 999/4236 [1:32:34<5:03:12,  5.62s/it]

 24%|██▎       | 1000/4236 [1:32:38<4:44:16,  5.27s/it]

 24%|██▎       | 1001/4236 [1:32:42<4:23:58,  4.90s/it]

 24%|██▎       | 1002/4236 [1:32:46<4:09:01,  4.62s/it]

 24%|██▎       | 1003/4236 [1:33:01<6:50:38,  7.62s/it]

 24%|██▎       | 1004/4236 [1:33:05<5:50:06,  6.50s/it]

 24%|██▎       | 1005/4236 [1:33:08<5:04:38,  5.66s/it]

 24%|██▎       | 1006/4236 [1:33:12<4:35:46,  5.12s/it]
{'loss': 1.6865, 'grad_norm': 0.27903561509086083, 'learning_rate': 0.00017829201475224495, 'epoch': 0.24}


 24%|██▍       | 1008/4236 [1:33:24<4:58:41,  5.55s/it]
{'loss': 1.7383, 'grad_norm': 0.29932834994283275, 'learning_rate': 0.00017819676963104372, 'epoch': 0.24}


 24%|██▍       | 1010/4236 [1:33:34<4:43:26,  5.27s/it]

 24%|██▍       | 1011/4236 [1:33:38<4:26:36,  4.96s/it]

 24%|██▍       | 1012/4236 [1:33:44<4:41:52,  5.25s/it]
{'loss': 1.7172, 'grad_norm': 0.3284600195117805, 'learning_rate': 0.00017800573081852122, 'epoch': 0.24}


 24%|██▍       | 1014/4236 [1:33:54<4:31:48,  5.06s/it]
{'loss': 1.8533, 'grad_norm': 0.31053452672019416, 'learning_rate': 0.00017790993757411022, 'epoch': 0.24}


 24%|██▍       | 1016/4236 [1:34:09<5:35:57,  6.26s/it]

 24%|██▍       | 1017/4236 [1:34:15<5:35:47,  6.26s/it]

 24%|██▍       | 1018/4236 [1:34:21<5:28:18,  6.12s/it]

 24%|██▍       | 1019/4236 [1:34:25<4:58:41,  5.57s/it]

 24%|██▍       | 1020/4236 [1:34:30<4:47:41,  5.37s/it]

 24%|██▍       | 1021/4236 [1:34:36<5:05:59,  5.71s/it]

 24%|██▍       | 1022/4236 [1:34:40<4:36:11,  5.16s/it]

 24%|██▍       | 1023/4236 [1:34:44<4:14:46,  4.76s/it]

 24%|██▍       | 1024/4236 [1:34:49<4:12:15,  4.71s/it]

 24%|██▍       | 1025/4236 [1:34:55<4:34:14,  5.12s/it]
{'loss': 1.6316, 'grad_norm': 0.3476351221004088, 'learning_rate': 0.00017737982286028937, 'epoch': 0.24}


 24%|██▍       | 1027/4236 [1:35:04<4:26:10,  4.98s/it]

 24%|██▍       | 1028/4236 [1:35:09<4:19:31,  4.85s/it]

 24%|██▍       | 1029/4236 [1:35:17<5:11:32,  5.83s/it]

 24%|██▍       | 1030/4236 [1:35:23<5:15:04,  5.90s/it]
{'loss': 1.7183, 'grad_norm': 0.2918668490698283, 'learning_rate': 0.0001771370491681543, 'epoch': 0.24}


 24%|██▍       | 1032/4236 [1:35:32<4:42:11,  5.28s/it]

 24%|██▍       | 1033/4236 [1:35:45<6:33:31,  7.37s/it]

 24%|██▍       | 1034/4236 [1:35:50<6:01:22,  6.77s/it]

 24%|██▍       | 1035/4236 [1:35:54<5:19:59,  6.00s/it]
{'loss': 1.5587, 'grad_norm': 0.29360416035881526, 'learning_rate': 0.0001768931476519047, 'epoch': 0.24}


 24%|██▍       | 1037/4236 [1:36:04<4:50:49,  5.45s/it]

 25%|██▍       | 1038/4236 [1:36:08<4:30:35,  5.08s/it]

 25%|██▍       | 1039/4236 [1:36:14<4:42:11,  5.30s/it]

 25%|██▍       | 1040/4236 [1:36:22<5:22:24,  6.05s/it]
{'loss': 1.7869, 'grad_norm': 0.30637472085760636, 'learning_rate': 0.000176648121877635, 'epoch': 0.25}

 25%|██▍       | 1041/4236 [1:36:26<4:42:20,  5.30s/it]


 25%|██▍       | 1043/4236 [1:36:37<4:45:28,  5.36s/it]

 25%|██▍       | 1044/4236 [1:36:42<4:48:26,  5.42s/it]
{'loss': 1.9255, 'grad_norm': 0.3417672737169483, 'learning_rate': 0.00017645129419935565, 'epoch': 0.25}


 25%|██▍       | 1046/4236 [1:36:58<6:14:04,  7.04s/it]
{'loss': 1.7583, 'grad_norm': 0.3059755145177174, 'learning_rate': 0.00017635261197328466, 'epoch': 0.25}

 25%|██▍       | 1047/4236 [1:37:09<7:25:35,  8.38s/it]


 25%|██▍       | 1049/4236 [1:37:17<5:18:35,  6.00s/it]

 25%|██▍       | 1050/4236 [1:37:21<4:41:48,  5.31s/it]

 25%|██▍       | 1051/4236 [1:37:26<4:43:38,  5.34s/it]

 25%|██▍       | 1052/4236 [1:37:30<4:21:54,  4.94s/it]
{'loss': 1.6663, 'grad_norm': 0.2932068842414895, 'learning_rate': 0.00017605549451884872, 'epoch': 0.25}


 25%|██▍       | 1054/4236 [1:37:37<3:46:51,  4.28s/it]

 25%|██▍       | 1055/4236 [1:37:44<4:32:50,  5.15s/it]

 25%|██▍       | 1056/4236 [1:37:48<4:14:00,  4.79s/it]

 25%|██▍       | 1057/4236 [1:37:52<3:59:18,  4.52s/it]

 25%|██▍       | 1058/4236 [1:37:56<3:49:07,  4.33s/it]

 25%|██▌       | 1059/4236 [1:38:00<3:43:45,  4.23s/it]

 25%|██▌       | 1060/4236 [1:38:09<4:56:37,  5.60s/it]

 25%|██▌       | 1061/4236 [1:38:13<4:35:14,  5.20s/it]
{'loss': 1.7593, 'grad_norm': 0.2892898478225541, 'learning_rate': 0.00017560681787412552, 'epoch': 0.25}


 25%|██▌       | 1063/4236 [1:38:27<5:06:23,  5.79s/it]

 25%|██▌       | 1064/4236 [1:38:31<4:31:17,  5.13s/it]

 25%|██▌       | 1065/4236 [1:38:43<6:17:57,  7.15s/it]

 25%|██▌       | 1066/4236 [1:38:50<6:23:53,  7.27s/it]

 25%|██▌       | 1067/4236 [1:38:54<5:35:05,  6.34s/it]
{'loss': 1.6782, 'grad_norm': 0.3168303067669823, 'learning_rate': 0.00017530570898642692, 'epoch': 0.25}


 25%|██▌       | 1069/4236 [1:39:07<5:25:58,  6.18s/it]

 25%|██▌       | 1070/4236 [1:39:10<4:45:58,  5.42s/it]
{'loss': 1.5359, 'grad_norm': 0.3316012132545954, 'learning_rate': 0.00017515455957943773, 'epoch': 0.25}

 25%|██▌       | 1071/4236 [1:39:14<4:16:30,  4.86s/it]


 25%|██▌       | 1073/4236 [1:39:23<4:04:59,  4.65s/it]

 25%|██▌       | 1074/4236 [1:39:26<3:51:33,  4.39s/it]

 25%|██▌       | 1075/4236 [1:39:32<4:14:18,  4.83s/it]
{'loss': 1.6117, 'grad_norm': 0.30519154677845434, 'learning_rate': 0.00017490176522382282, 'epoch': 0.25}

 25%|██▌       | 1076/4236 [1:39:38<4:23:00,  4.99s/it]


 25%|██▌       | 1078/4236 [1:39:46<4:02:37,  4.61s/it]

 25%|██▌       | 1079/4236 [1:39:53<4:47:02,  5.46s/it]

 25%|██▌       | 1080/4236 [1:39:57<4:18:42,  4.92s/it]

 26%|██▌       | 1081/4236 [1:40:01<4:05:32,  4.67s/it]

 26%|██▌       | 1082/4236 [1:40:05<3:52:16,  4.42s/it]
{'loss': 1.6811, 'grad_norm': 0.3059251712235836, 'learning_rate': 0.00017454601411849663, 'epoch': 0.26}


 26%|██▌       | 1084/4236 [1:40:14<3:55:36,  4.48s/it]
{'loss': 1.7111, 'grad_norm': 0.32065153036843524, 'learning_rate': 0.00017444397812006195, 'epoch': 0.26}


 26%|██▌       | 1086/4236 [1:40:27<4:46:17,  5.45s/it]

 26%|██▌       | 1087/4236 [1:40:31<4:19:58,  4.95s/it]

 26%|██▌       | 1088/4236 [1:40:35<4:06:20,  4.70s/it]

 26%|██▌       | 1089/4236 [1:40:45<5:33:27,  6.36s/it]

 26%|██▌       | 1090/4236 [1:40:49<4:53:50,  5.60s/it]
{'loss': 1.5457, 'grad_norm': 0.2954751988285561, 'learning_rate': 0.00017413682616986185, 'epoch': 0.26}

 26%|██▌       | 1091/4236 [1:40:54<4:33:07,  5.21s/it]


 26%|██▌       | 1093/4236 [1:41:02<4:09:32,  4.76s/it]

 26%|██▌       | 1094/4236 [1:41:07<4:13:31,  4.84s/it]
{'loss': 1.688, 'grad_norm': 0.2963105210915367, 'learning_rate': 0.00017393119063680338, 'epoch': 0.26}


 26%|██▌       | 1096/4236 [1:41:17<4:08:29,  4.75s/it]
{'loss': 1.7146, 'grad_norm': 0.2982579525835241, 'learning_rate': 0.00017382811332137442, 'epoch': 0.26}

 26%|██▌       | 1097/4236 [1:41:22<4:13:33,  4.85s/it]


 26%|██▌       | 1099/4236 [1:41:29<3:45:24,  4.31s/it]

 26%|██▌       | 1100/4236 [1:41:33<3:39:05,  4.19s/it]

 26%|██▌       | 1101/4236 [1:41:39<4:05:11,  4.69s/it]

 26%|██▌       | 1102/4236 [1:41:43<4:00:42,  4.61s/it]

 26%|██▌       | 1103/4236 [1:41:47<3:49:48,  4.40s/it]

 26%|██▌       | 1104/4236 [1:41:52<3:58:17,  4.56s/it]

 26%|██▌       | 1105/4236 [1:41:57<4:01:01,  4.62s/it]

 26%|██▌       | 1106/4236 [1:42:03<4:22:35,  5.03s/it]

 26%|██▌       | 1107/4236 [1:42:07<4:08:45,  4.77s/it]

 26%|██▌       | 1108/4236 [1:42:11<3:54:04,  4.49s/it]

 26%|██▌       | 1109/4236 [1:42:16<4:05:37,  4.71s/it]

 26%|██▌       | 1110/4236 [1:42:20<3:53:29,  4.48s/it]

 26%|██▌       | 1111/4236 [1:42:24<3:47:58,  4.38s/it]
{'loss': 1.7195, 'grad_norm': 0.3594788593513568, 'learning_rate': 0.00017304954500513224, 'epoch': 0.26}

 26%|██▋       | 1112/4236 [1:42:30<4:08:23,  4.77s/it]

 26%|██▋       | 1113/4236 [1:42:34<3:56:39,  4.55s/it]


 26%|██▋       | 1115/4236 [1:42:47<4:44:42,  5.47s/it]

 26%|██▋       | 1116/4236 [1:42:53<4:44:56,  5.48s/it]

 26%|██▋       | 1117/4236 [1:42:57<4:22:49,  5.06s/it]

 26%|██▋       | 1118/4236 [1:43:05<5:03:56,  5.85s/it]
{'loss': 1.7918, 'grad_norm': 0.2706102799473266, 'learning_rate': 0.0001726829172491808, 'epoch': 0.26}


 26%|██▋       | 1120/4236 [1:43:14<4:33:39,  5.27s/it]

 26%|██▋       | 1121/4236 [1:43:21<5:00:11,  5.78s/it]

 26%|██▋       | 1122/4236 [1:43:25<4:29:08,  5.19s/it]

 27%|██▋       | 1123/4236 [1:43:29<4:17:54,  4.97s/it]

 27%|██▋       | 1124/4236 [1:43:33<4:00:04,  4.63s/it]

 27%|██▋       | 1125/4236 [1:43:37<3:41:56,  4.28s/it]
{'loss': 1.9, 'grad_norm': 0.31625334340043454, 'learning_rate': 0.00017231420660354774, 'epoch': 0.27}


 27%|██▋       | 1127/4236 [1:43:47<3:59:00,  4.61s/it]
{'loss': 1.6366, 'grad_norm': 0.3126906580358514, 'learning_rate': 0.00017220847961038914, 'epoch': 0.27}


 27%|██▋       | 1129/4236 [1:43:56<4:05:13,  4.74s/it]
{'loss': 1.744, 'grad_norm': 0.30571051252732273, 'learning_rate': 0.0001721025836949317, 'epoch': 0.27}


 27%|██▋       | 1131/4236 [1:44:07<4:15:38,  4.94s/it]

 27%|██▋       | 1132/4236 [1:44:15<5:10:26,  6.00s/it]

 27%|██▋       | 1133/4236 [1:44:20<4:44:23,  5.50s/it]

 27%|██▋       | 1134/4236 [1:44:24<4:22:22,  5.08s/it]

 27%|██▋       | 1135/4236 [1:44:31<5:04:14,  5.89s/it]
{'loss': 1.738, 'grad_norm': 0.29591969109496524, 'learning_rate': 0.0001717838848940354, 'epoch': 0.27}


 27%|██▋       | 1137/4236 [1:44:43<5:06:14,  5.93s/it]

 27%|██▋       | 1138/4236 [1:44:48<4:47:30,  5.57s/it]

 27%|██▋       | 1139/4236 [1:44:52<4:35:10,  5.33s/it]

 27%|██▋       | 1140/4236 [1:44:59<5:00:33,  5.82s/it]

 27%|██▋       | 1141/4236 [1:45:05<4:53:14,  5.68s/it]

 27%|██▋       | 1142/4236 [1:45:09<4:27:31,  5.19s/it]

 27%|██▋       | 1143/4236 [1:45:13<4:16:17,  4.97s/it]

 27%|██▋       | 1144/4236 [1:45:17<4:02:11,  4.70s/it]

 27%|██▋       | 1145/4236 [1:45:22<3:58:43,  4.63s/it]

 27%|██▋       | 1146/4236 [1:45:26<3:46:00,  4.39s/it]

 27%|██▋       | 1147/4236 [1:45:30<3:40:22,  4.28s/it]
{'loss': 1.7798, 'grad_norm': 0.2999337409728456, 'learning_rate': 0.00017114195995770738, 'epoch': 0.27}

 27%|██▋       | 1148/4236 [1:45:36<4:11:13,  4.88s/it]

 27%|██▋       | 1149/4236 [1:45:40<4:03:29,  4.73s/it]


 27%|██▋       | 1151/4236 [1:45:48<3:37:01,  4.22s/it]

 27%|██▋       | 1152/4236 [1:45:52<3:29:21,  4.07s/it]

 27%|██▋       | 1153/4236 [1:45:57<3:46:48,  4.41s/it]

 27%|██▋       | 1154/4236 [1:46:00<3:36:27,  4.21s/it]
{'loss': 1.6905, 'grad_norm': 0.30505630794718475, 'learning_rate': 0.00017076473342121076, 'epoch': 0.27}


 27%|██▋       | 1156/4236 [1:46:09<3:33:58,  4.17s/it]

 27%|██▋       | 1157/4236 [1:46:15<4:05:03,  4.78s/it]

 27%|██▋       | 1158/4236 [1:46:19<3:54:47,  4.58s/it]

 27%|██▋       | 1159/4236 [1:46:23<3:51:30,  4.51s/it]

 27%|██▋       | 1160/4236 [1:46:28<3:51:37,  4.52s/it]

 27%|██▋       | 1161/4236 [1:46:33<4:06:00,  4.80s/it]

 27%|██▋       | 1162/4236 [1:46:38<3:57:28,  4.64s/it]

 27%|██▋       | 1163/4236 [1:46:45<4:42:20,  5.51s/it]

 27%|██▋       | 1164/4236 [1:46:49<4:17:44,  5.03s/it]

 28%|██▊       | 1165/4236 [1:46:53<4:05:15,  4.79s/it]

 28%|██▊       | 1166/4236 [1:46:57<3:48:26,  4.46s/it]

 28%|██▊       | 1167/4236 [1:47:12<6:28:31,  7.60s/it]
[2024-05-25 04:21:49,852] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.7014, 'grad_norm': 0.2945683721520297, 'learning_rate': 0.00017005879789467408, 'epoch': 0.28}


 28%|██▊       | 1169/4236 [1:47:22<5:23:30,  6.33s/it]
{'loss': 1.504, 'grad_norm': 0.3057932207570262, 'learning_rate': 0.00016994957607778194, 'epoch': 0.28}

 28%|██▊       | 1170/4236 [1:47:28<5:20:12,  6.27s/it]


 28%|██▊       | 1172/4236 [1:47:40<5:09:42,  6.06s/it]

 28%|██▊       | 1173/4236 [1:47:44<4:34:34,  5.38s/it]

 28%|██▊       | 1174/4236 [1:47:47<4:07:46,  4.86s/it]

 28%|██▊       | 1175/4236 [1:48:01<6:22:14,  7.49s/it]
{'loss': 1.8073, 'grad_norm': 0.3206117375146804, 'learning_rate': 0.00016962092982372384, 'epoch': 0.28}


 28%|██▊       | 1177/4236 [1:48:10<5:07:06,  6.02s/it]

 28%|██▊       | 1178/4236 [1:48:14<4:33:55,  5.37s/it]

 28%|██▊       | 1179/4236 [1:48:21<5:06:27,  6.02s/it]

 28%|██▊       | 1180/4236 [1:48:25<4:37:26,  5.45s/it]

 28%|██▊       | 1181/4236 [1:48:29<4:08:54,  4.89s/it]

 28%|██▊       | 1182/4236 [1:48:34<4:09:44,  4.91s/it]

 28%|██▊       | 1183/4236 [1:48:38<3:52:04,  4.56s/it]

 28%|██▊       | 1184/4236 [1:48:41<3:40:57,  4.34s/it]

 28%|██▊       | 1185/4236 [1:48:45<3:31:10,  4.15s/it]

 28%|██▊       | 1186/4236 [1:48:49<3:25:26,  4.04s/it]
{'loss': 1.723, 'grad_norm': 0.3209676504010186, 'learning_rate': 0.00016901460964619872, 'epoch': 0.28}


 28%|██▊       | 1188/4236 [1:48:58<3:42:57,  4.39s/it]
{'loss': 1.7474, 'grad_norm': 0.3206772297798707, 'learning_rate': 0.000168903843639518, 'epoch': 0.28}


 28%|██▊       | 1190/4236 [1:49:09<4:14:33,  5.01s/it]

 28%|██▊       | 1191/4236 [1:49:13<4:05:28,  4.84s/it]

 28%|██▊       | 1192/4236 [1:49:17<3:55:30,  4.64s/it]
{'loss': 1.6934, 'grad_norm': 0.2862277196156555, 'learning_rate': 0.0001686818283110514, 'epoch': 0.28}


 28%|██▊       | 1194/4236 [1:49:27<3:53:48,  4.61s/it]

 28%|██▊       | 1195/4236 [1:49:31<3:49:38,  4.53s/it]
{'loss': 1.5504, 'grad_norm': 0.3121267506934976, 'learning_rate': 0.00016851489493669073, 'epoch': 0.28}

 28%|██▊       | 1196/4236 [1:49:38<4:27:13,  5.27s/it]

 28%|██▊       | 1197/4236 [1:49:42<4:11:00,  4.96s/it]


 28%|██▊       | 1199/4236 [1:49:51<4:03:13,  4.81s/it]

 28%|██▊       | 1200/4236 [1:49:56<3:57:37,  4.70s/it]
 28%|██▊       | 1200/4236 [1:49:56<3:57:37,  4.70s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 28%|██▊       | 1201/4236 [1:50:16<7:46:56,  9.23s/it]

 28%|██▊       | 1202/4236 [1:50:22<7:01:16,  8.33s/it]
{'loss': 1.6262, 'grad_norm': 0.31950168849434957, 'learning_rate': 0.00016812398278807589, 'epoch': 0.28}

 28%|██▊       | 1203/4236 [1:50:26<6:01:55,  7.16s/it]


 28%|██▊       | 1205/4236 [1:50:45<7:15:52,  8.63s/it]

 28%|██▊       | 1206/4236 [1:50:49<6:05:11,  7.23s/it]

 28%|██▊       | 1207/4236 [1:50:53<5:18:51,  6.32s/it]

 29%|██▊       | 1208/4236 [1:50:59<5:13:41,  6.22s/it]

 29%|██▊       | 1209/4236 [1:51:10<6:16:17,  7.46s/it]
{'loss': 1.7674, 'grad_norm': 0.29896154935297464, 'learning_rate': 0.00016773111839612535, 'epoch': 0.29}


 29%|██▊       | 1211/4236 [1:51:19<5:06:47,  6.09s/it]

 29%|██▊       | 1212/4236 [1:51:23<4:33:49,  5.43s/it]

 29%|██▊       | 1213/4236 [1:51:27<4:13:29,  5.03s/it]

 29%|██▊       | 1214/4236 [1:51:31<3:54:41,  4.66s/it]

 29%|██▊       | 1215/4236 [1:51:37<4:10:22,  4.97s/it]

 29%|██▊       | 1216/4236 [1:51:42<4:08:02,  4.93s/it]

 29%|██▊       | 1217/4236 [1:51:46<3:58:00,  4.73s/it]

 29%|██▉       | 1218/4236 [1:51:52<4:14:07,  5.05s/it]

 29%|██▉       | 1219/4236 [1:51:57<4:22:06,  5.21s/it]

 29%|██▉       | 1220/4236 [1:52:01<3:58:44,  4.75s/it]

 29%|██▉       | 1221/4236 [1:52:06<3:57:30,  4.73s/it]

 29%|██▉       | 1222/4236 [1:52:10<3:51:56,  4.62s/it]

 29%|██▉       | 1223/4236 [1:52:14<3:44:11,  4.46s/it]

 29%|██▉       | 1224/4236 [1:52:25<5:20:12,  6.38s/it]
{'loss': 1.7227, 'grad_norm': 0.2927197216210764, 'learning_rate': 0.00016688274467411952, 'epoch': 0.29}


 29%|██▉       | 1226/4236 [1:52:40<5:34:18,  6.66s/it]

 29%|██▉       | 1227/4236 [1:52:43<4:51:05,  5.80s/it]

 29%|██▉       | 1228/4236 [1:52:48<4:31:06,  5.41s/it]

 29%|██▉       | 1229/4236 [1:52:52<4:10:07,  4.99s/it]

 29%|██▉       | 1230/4236 [1:52:57<4:15:14,  5.09s/it]

 29%|██▉       | 1231/4236 [1:53:01<3:56:21,  4.72s/it]

 29%|██▉       | 1232/4236 [1:53:05<3:46:39,  4.53s/it]

 29%|██▉       | 1233/4236 [1:53:10<3:45:50,  4.51s/it]

 29%|██▉       | 1234/4236 [1:53:14<3:48:12,  4.56s/it]

 29%|██▉       | 1235/4236 [1:53:18<3:38:18,  4.36s/it]

 29%|██▉       | 1236/4236 [1:53:23<3:51:25,  4.63s/it]

 29%|██▉       | 1237/4236 [1:53:28<3:47:35,  4.55s/it]

 29%|██▉       | 1238/4236 [1:53:34<4:04:18,  4.89s/it]

 29%|██▉       | 1239/4236 [1:53:38<3:57:57,  4.76s/it]

 29%|██▉       | 1240/4236 [1:53:42<3:41:46,  4.44s/it]
{'loss': 1.6938, 'grad_norm': 0.33730099000929503, 'learning_rate': 0.00016596811483149076, 'epoch': 0.29}


 29%|██▉       | 1242/4236 [1:54:04<6:14:57,  7.51s/it]

 29%|██▉       | 1243/4236 [1:54:08<5:21:58,  6.45s/it]

 29%|██▉       | 1244/4236 [1:54:12<4:52:15,  5.86s/it]

 29%|██▉       | 1245/4236 [1:54:18<4:41:33,  5.65s/it]

 29%|██▉       | 1246/4236 [1:54:22<4:18:57,  5.20s/it]

 29%|██▉       | 1247/4236 [1:54:26<4:05:15,  4.92s/it]

 29%|██▉       | 1248/4236 [1:54:30<3:46:24,  4.55s/it]

 29%|██▉       | 1249/4236 [1:54:36<4:08:01,  4.98s/it]

 30%|██▉       | 1250/4236 [1:54:42<4:31:53,  5.46s/it]

 30%|██▉       | 1251/4236 [1:54:47<4:26:18,  5.35s/it]

 30%|██▉       | 1252/4236 [1:54:54<4:48:29,  5.80s/it]

 30%|██▉       | 1253/4236 [1:54:58<4:20:54,  5.25s/it]

 30%|██▉       | 1254/4236 [1:55:06<4:57:33,  5.99s/it]

 30%|██▉       | 1255/4236 [1:55:10<4:28:04,  5.40s/it]

 30%|██▉       | 1256/4236 [1:55:16<4:45:03,  5.74s/it]
{'loss': 1.6295, 'grad_norm': 0.3055391948640428, 'learning_rate': 0.00016504360838816954, 'epoch': 0.3}


 30%|██▉       | 1258/4236 [1:55:26<4:19:04,  5.22s/it]

 30%|██▉       | 1259/4236 [1:55:31<4:27:02,  5.38s/it]

 30%|██▉       | 1260/4236 [1:55:36<4:17:39,  5.19s/it]

 30%|██▉       | 1261/4236 [1:55:41<4:18:16,  5.21s/it]

 30%|██▉       | 1262/4236 [1:55:48<4:30:37,  5.46s/it]
{'loss': 1.6626, 'grad_norm': 0.28358236780845875, 'learning_rate': 0.00016469440036482702, 'epoch': 0.3}

 30%|██▉       | 1263/4236 [1:55:53<4:30:13,  5.45s/it]

 30%|██▉       | 1264/4236 [1:55:57<4:03:49,  4.92s/it]


 30%|██▉       | 1266/4236 [1:56:05<3:38:11,  4.41s/it]

 30%|██▉       | 1267/4236 [1:56:08<3:28:53,  4.22s/it]
{'loss': 1.6149, 'grad_norm': 0.3353063163920588, 'learning_rate': 0.00016440235287681063, 'epoch': 0.3}


 30%|██▉       | 1269/4236 [1:56:16<3:23:35,  4.12s/it]

 30%|██▉       | 1270/4236 [1:56:20<3:16:21,  3.97s/it]

 30%|███       | 1271/4236 [1:56:24<3:14:09,  3.93s/it]

 30%|███       | 1272/4236 [1:56:28<3:09:39,  3.84s/it]

 30%|███       | 1273/4236 [1:56:32<3:19:55,  4.05s/it]
{'loss': 1.6434, 'grad_norm': 0.34861819674058403, 'learning_rate': 0.000164050653316889, 'epoch': 0.3}


 30%|███       | 1275/4236 [1:56:41<3:29:49,  4.25s/it]
{'loss': 1.8417, 'grad_norm': 0.30041810940490826, 'learning_rate': 0.00016393312008796684, 'epoch': 0.3}

 30%|███       | 1276/4236 [1:56:45<3:23:39,  4.13s/it]


 30%|███       | 1278/4236 [1:56:55<3:53:33,  4.74s/it]

 30%|███       | 1279/4236 [1:57:00<3:55:32,  4.78s/it]
{'loss': 1.7004, 'grad_norm': 0.3179175050231105, 'learning_rate': 0.00016369760521589503, 'epoch': 0.3}


 30%|███       | 1281/4236 [1:57:14<4:37:44,  5.64s/it]

 30%|███       | 1282/4236 [1:57:17<4:09:43,  5.07s/it]

 30%|███       | 1283/4236 [1:57:22<4:05:16,  4.98s/it]

 30%|███       | 1284/4236 [1:57:28<4:12:05,  5.12s/it]

 30%|███       | 1285/4236 [1:57:32<4:01:51,  4.92s/it]

 30%|███       | 1286/4236 [1:57:37<4:07:16,  5.03s/it]

 30%|███       | 1287/4236 [1:57:42<4:05:56,  5.00s/it]

 30%|███       | 1288/4236 [1:57:50<4:50:12,  5.91s/it]

 30%|███       | 1289/4236 [1:57:55<4:37:14,  5.64s/it]
{'loss': 1.6896, 'grad_norm': 0.3135054362154657, 'learning_rate': 0.00016310621515796322, 'epoch': 0.3}


 30%|███       | 1291/4236 [1:58:03<3:56:12,  4.81s/it]
{'loss': 1.7634, 'grad_norm': 0.2913949269477434, 'learning_rate': 0.00016298749315163567, 'epoch': 0.3}


 31%|███       | 1293/4236 [1:58:12<3:52:48,  4.75s/it]
{'loss': 1.8298, 'grad_norm': 0.3264680255699709, 'learning_rate': 0.0001628686237943017, 'epoch': 0.31}


 31%|███       | 1295/4236 [1:58:21<3:33:31,  4.36s/it]

 31%|███       | 1296/4236 [1:58:27<3:58:18,  4.86s/it]

 31%|███       | 1297/4236 [1:58:30<3:40:46,  4.51s/it]

 31%|███       | 1298/4236 [1:58:39<4:36:44,  5.65s/it]

 31%|███       | 1299/4236 [1:58:47<5:18:34,  6.51s/it]

 31%|███       | 1300/4236 [1:58:56<5:46:46,  7.09s/it]
{'loss': 1.8575, 'grad_norm': 0.3187921545712047, 'learning_rate': 0.00016245142467233065, 'epoch': 0.31}


 31%|███       | 1302/4236 [1:59:06<5:00:57,  6.15s/it]

 31%|███       | 1303/4236 [1:59:10<4:27:34,  5.47s/it]

 31%|███       | 1304/4236 [1:59:15<4:27:46,  5.48s/it]

 31%|███       | 1305/4236 [1:59:23<4:51:11,  5.96s/it]

 31%|███       | 1306/4236 [1:59:34<6:16:24,  7.71s/it]

 31%|███       | 1307/4236 [1:59:39<5:36:47,  6.90s/it]

 31%|███       | 1308/4236 [1:59:43<4:56:14,  6.07s/it]
{'loss': 1.7632, 'grad_norm': 0.3013236002346689, 'learning_rate': 0.00016197243492591627, 'epoch': 0.31}


 31%|███       | 1310/4236 [1:59:58<5:27:21,  6.71s/it]

 31%|███       | 1311/4236 [2:00:03<5:04:50,  6.25s/it]
{'loss': 1.6061, 'grad_norm': 0.30462291701480154, 'learning_rate': 0.0001617922147793351, 'epoch': 0.31}


 31%|███       | 1313/4236 [2:00:11<4:17:16,  5.28s/it]

 31%|███       | 1314/4236 [2:00:16<4:06:22,  5.06s/it]

 31%|███       | 1315/4236 [2:00:21<4:00:19,  4.94s/it]
{'loss': 1.7331, 'grad_norm': 0.30725441289621114, 'learning_rate': 0.00016155141547305517, 'epoch': 0.31}


 31%|███       | 1317/4236 [2:00:30<3:54:00,  4.81s/it]

 31%|███       | 1318/4236 [2:00:40<5:05:00,  6.27s/it]

 31%|███       | 1319/4236 [2:00:44<4:30:02,  5.55s/it]

 31%|███       | 1320/4236 [2:00:47<4:04:30,  5.03s/it]

 31%|███       | 1321/4236 [2:00:52<3:52:10,  4.78s/it]

 31%|███       | 1322/4236 [2:01:01<4:56:57,  6.11s/it]

 31%|███       | 1323/4236 [2:01:06<4:42:08,  5.81s/it]

 31%|███▏      | 1324/4236 [2:01:10<4:22:31,  5.41s/it]

 31%|███▏      | 1325/4236 [2:01:18<4:54:18,  6.07s/it]

 31%|███▏      | 1326/4236 [2:01:22<4:17:39,  5.31s/it]
{'loss': 1.7037, 'grad_norm': 0.2882179563594901, 'learning_rate': 0.00016088625435469408, 'epoch': 0.31}


 31%|███▏      | 1328/4236 [2:01:39<5:20:09,  6.61s/it]

 31%|███▏      | 1329/4236 [2:01:48<5:57:13,  7.37s/it]

 31%|███▏      | 1330/4236 [2:01:52<5:09:22,  6.39s/it]

 31%|███▏      | 1331/4236 [2:02:06<6:53:12,  8.53s/it]
[2024-05-25 04:36:43,756] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 1332/4236 [2:02:20<8:12:44, 10.18s/it]

 31%|███▏      | 1333/4236 [2:02:24<6:47:46,  8.43s/it]

 31%|███▏      | 1334/4236 [2:02:28<5:48:00,  7.20s/it]
{'loss': 1.7731, 'grad_norm': 0.3223834641961583, 'learning_rate': 0.00016039979185665062, 'epoch': 0.31}


 32%|███▏      | 1336/4236 [2:02:37<4:32:45,  5.64s/it]
{'loss': 1.732, 'grad_norm': 0.3384569329513212, 'learning_rate': 0.00016027782227650693, 'epoch': 0.32}


 32%|███▏      | 1338/4236 [2:02:46<4:03:46,  5.05s/it]

 32%|███▏      | 1339/4236 [2:02:50<3:49:28,  4.75s/it]
{'loss': 1.8791, 'grad_norm': 0.3513954612369254, 'learning_rate': 0.00016009460359788736, 'epoch': 0.32}


 32%|███▏      | 1341/4236 [2:02:58<3:42:18,  4.61s/it]
{'loss': 1.6662, 'grad_norm': 0.3216482525790816, 'learning_rate': 0.0001599722820232484, 'epoch': 0.32}


 32%|███▏      | 1343/4236 [2:03:08<3:39:22,  4.55s/it]
{'loss': 1.7189, 'grad_norm': 0.32319734616181534, 'learning_rate': 0.0001598498201512954, 'epoch': 0.32}


 32%|███▏      | 1345/4236 [2:03:16<3:29:13,  4.34s/it]

 32%|███▏      | 1346/4236 [2:03:20<3:33:53,  4.44s/it]

 32%|███▏      | 1347/4236 [2:03:24<3:25:28,  4.27s/it]

 32%|███▏      | 1348/4236 [2:03:30<3:39:18,  4.56s/it]

 32%|███▏      | 1349/4236 [2:03:34<3:33:42,  4.44s/it]

 32%|███▏      | 1350/4236 [2:03:38<3:29:10,  4.35s/it]

 32%|███▏      | 1351/4236 [2:03:42<3:23:43,  4.24s/it]

 32%|███▏      | 1352/4236 [2:03:46<3:23:04,  4.22s/it]

 32%|███▏      | 1353/4236 [2:03:52<3:47:06,  4.73s/it]

 32%|███▏      | 1354/4236 [2:03:59<4:19:46,  5.41s/it]
{'loss': 1.6388, 'grad_norm': 0.31928602780867243, 'learning_rate': 0.00015917378485974022, 'epoch': 0.32}


 32%|███▏      | 1356/4236 [2:04:09<4:07:57,  5.17s/it]

 32%|███▏      | 1357/4236 [2:04:13<3:51:52,  4.83s/it]

 32%|███▏      | 1358/4236 [2:04:24<5:25:39,  6.79s/it]

 32%|███▏      | 1359/4236 [2:04:30<5:08:31,  6.43s/it]
{'loss': 1.6913, 'grad_norm': 0.35316603699180904, 'learning_rate': 0.00015886510891900202, 'epoch': 0.32}


 32%|███▏      | 1361/4236 [2:04:42<5:06:06,  6.39s/it]

 32%|███▏      | 1362/4236 [2:04:56<6:53:05,  8.62s/it]

 32%|███▏      | 1363/4236 [2:05:00<5:51:37,  7.34s/it]

 32%|███▏      | 1364/4236 [2:05:04<4:56:59,  6.20s/it]
{'loss': 1.6122, 'grad_norm': 0.29094144573012876, 'learning_rate': 0.00015855557230897373, 'epoch': 0.32}

 32%|███▏      | 1365/4236 [2:05:08<4:22:05,  5.48s/it]


 32%|███▏      | 1367/4236 [2:05:16<3:47:56,  4.77s/it]

 32%|███▏      | 1368/4236 [2:05:25<4:45:40,  5.98s/it]

 32%|███▏      | 1369/4236 [2:05:33<5:16:40,  6.63s/it]
{'loss': 1.8994, 'grad_norm': 0.2802174723715002, 'learning_rate': 0.00015824517955540345, 'epoch': 0.32}


 32%|███▏      | 1371/4236 [2:05:45<5:02:54,  6.34s/it]

 32%|███▏      | 1372/4236 [2:05:51<5:01:05,  6.31s/it]

 32%|███▏      | 1373/4236 [2:05:57<4:48:58,  6.06s/it]

 32%|███▏      | 1374/4236 [2:06:01<4:23:32,  5.53s/it]
{'loss': 1.7136, 'grad_norm': 0.3147722451094033, 'learning_rate': 0.00015793393519655696, 'epoch': 0.32}


 32%|███▏      | 1376/4236 [2:06:12<4:25:07,  5.56s/it]
{'loss': 1.5826, 'grad_norm': 0.3286908285874056, 'learning_rate': 0.00015780920002248484, 'epoch': 0.32}


 33%|███▎      | 1378/4236 [2:06:20<3:53:28,  4.90s/it]

 33%|███▎      | 1379/4236 [2:06:24<3:37:55,  4.58s/it]
{'loss': 1.7748, 'grad_norm': 0.32878469891445666, 'learning_rate': 0.00015762184378315146, 'epoch': 0.33}


 33%|███▎      | 1381/4236 [2:06:34<3:43:11,  4.69s/it]

 33%|███▎      | 1382/4236 [2:06:38<3:37:47,  4.58s/it]

 33%|███▎      | 1383/4236 [2:06:42<3:29:56,  4.42s/it]

 33%|███▎      | 1384/4236 [2:06:46<3:19:49,  4.20s/it]

 33%|███▎      | 1385/4236 [2:06:51<3:27:46,  4.37s/it]

 33%|███▎      | 1386/4236 [2:06:55<3:23:10,  4.28s/it]

 33%|███▎      | 1387/4236 [2:06:58<3:13:32,  4.08s/it]

 33%|███▎      | 1388/4236 [2:07:02<3:07:29,  3.95s/it]

 33%|███▎      | 1389/4236 [2:07:06<3:09:06,  3.99s/it]

 33%|███▎      | 1390/4236 [2:07:10<3:12:36,  4.06s/it]

 33%|███▎      | 1391/4236 [2:07:22<5:02:25,  6.38s/it]

 33%|███▎      | 1392/4236 [2:07:26<4:28:26,  5.66s/it]

 33%|███▎      | 1393/4236 [2:07:32<4:30:57,  5.72s/it]
{'loss': 1.785, 'grad_norm': 0.33392177686895025, 'learning_rate': 0.00015674352038362837, 'epoch': 0.33}


 33%|███▎      | 1395/4236 [2:07:39<3:41:01,  4.67s/it]
{'loss': 1.801, 'grad_norm': 0.34300442373815493, 'learning_rate': 0.00015661751228344094, 'epoch': 0.33}


 33%|███▎      | 1397/4236 [2:07:49<3:46:07,  4.78s/it]
{'loss': 1.7811, 'grad_norm': 0.3614216099168171, 'learning_rate': 0.00015649137173398463, 'epoch': 0.33}
[2024-05-25 04:42:41,621] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 33%|███▎      | 1399/4236 [2:08:07<5:07:34,  6.50s/it]
{'loss': 1.675, 'grad_norm': 0.3130176801982367, 'learning_rate': 0.00015636509903034883, 'epoch': 0.33}

 33%|███▎      | 1400/4236 [2:08:12<4:38:28,  5.89s/it]


 33%|███▎      | 1402/4236 [2:08:21<4:08:45,  5.27s/it]
{'loss': 1.7876, 'grad_norm': 0.30670225372535515, 'learning_rate': 0.0001561754428320771, 'epoch': 0.33}


 33%|███▎      | 1404/4236 [2:08:34<4:46:50,  6.08s/it]
{'loss': 1.7255, 'grad_norm': 0.3509498071042971, 'learning_rate': 0.0001560488410360355, 'epoch': 0.33}


 33%|███▎      | 1406/4236 [2:08:49<5:02:58,  6.42s/it]

 33%|███▎      | 1407/4236 [2:08:52<4:24:41,  5.61s/it]
{'loss': 1.691, 'grad_norm': 0.29288023586313683, 'learning_rate': 0.00015585869258660307, 'epoch': 0.33}


 33%|███▎      | 1409/4236 [2:09:05<4:36:14,  5.86s/it]

 33%|███▎      | 1410/4236 [2:09:11<4:36:07,  5.86s/it]

 33%|███▎      | 1411/4236 [2:09:15<4:07:32,  5.26s/it]

 33%|███▎      | 1412/4236 [2:09:20<4:12:52,  5.37s/it]

 33%|███▎      | 1413/4236 [2:09:27<4:25:29,  5.64s/it]

 33%|███▎      | 1414/4236 [2:09:30<3:56:27,  5.03s/it]

 33%|███▎      | 1415/4236 [2:09:37<4:16:42,  5.46s/it]
{'loss': 1.7647, 'grad_norm': 0.3252632948124424, 'learning_rate': 0.00015535019536322157, 'epoch': 0.33}

 33%|███▎      | 1416/4236 [2:09:42<4:12:50,  5.38s/it]


 33%|███▎      | 1418/4236 [2:09:51<3:51:43,  4.93s/it]

 33%|███▎      | 1419/4236 [2:09:55<3:39:06,  4.67s/it]

 34%|███▎      | 1420/4236 [2:09:59<3:29:39,  4.47s/it]

 34%|███▎      | 1421/4236 [2:10:04<3:40:24,  4.70s/it]

 34%|███▎      | 1422/4236 [2:10:08<3:32:02,  4.52s/it]

 34%|███▎      | 1423/4236 [2:10:19<5:05:25,  6.51s/it]

 34%|███▎      | 1424/4236 [2:10:24<4:45:47,  6.10s/it]

 34%|███▎      | 1425/4236 [2:10:29<4:25:36,  5.67s/it]

 34%|███▎      | 1426/4236 [2:10:33<4:01:52,  5.16s/it]

 34%|███▎      | 1427/4236 [2:10:37<3:46:49,  4.84s/it]

 34%|███▎      | 1428/4236 [2:10:42<3:47:14,  4.86s/it]

 34%|███▎      | 1429/4236 [2:10:47<3:43:15,  4.77s/it]

 34%|███▍      | 1430/4236 [2:10:50<3:25:34,  4.40s/it]

 34%|███▍      | 1431/4236 [2:10:54<3:20:27,  4.29s/it]
{'loss': 1.7808, 'grad_norm': 0.35503234413143564, 'learning_rate': 0.0001543270047868921, 'epoch': 0.34}

 34%|███▍      | 1432/4236 [2:11:04<4:33:40,  5.86s/it]

 34%|███▍      | 1433/4236 [2:11:08<4:11:44,  5.39s/it]


 34%|███▍      | 1435/4236 [2:11:16<3:42:29,  4.77s/it]
{'loss': 1.7764, 'grad_norm': 0.3194973837115017, 'learning_rate': 0.00015406993023824627, 'epoch': 0.34}


 34%|███▍      | 1437/4236 [2:11:29<4:28:59,  5.77s/it]

 34%|███▍      | 1438/4236 [2:11:32<3:59:29,  5.14s/it]

 34%|███▍      | 1439/4236 [2:11:36<3:39:58,  4.72s/it]
{'loss': 1.7896, 'grad_norm': 0.3142531321936712, 'learning_rate': 0.0001538123497317617, 'epoch': 0.34}


 34%|███▍      | 1441/4236 [2:11:45<3:30:51,  4.53s/it]

 34%|███▍      | 1442/4236 [2:11:49<3:26:44,  4.44s/it]

 34%|███▍      | 1443/4236 [2:11:53<3:27:55,  4.47s/it]
{'loss': 1.7116, 'grad_norm': 0.3345792358810092, 'learning_rate': 0.00015355426567774066, 'epoch': 0.34}


 34%|███▍      | 1445/4236 [2:12:02<3:27:27,  4.46s/it]

 34%|███▍      | 1446/4236 [2:12:11<4:26:02,  5.72s/it]
{'loss': 1.8699, 'grad_norm': 0.29725594116319953, 'learning_rate': 0.00015336037363675953, 'epoch': 0.34}


 34%|███▍      | 1448/4236 [2:12:20<4:02:10,  5.21s/it]

 34%|███▍      | 1449/4236 [2:12:31<5:20:45,  6.91s/it]
{'loss': 1.8263, 'grad_norm': 0.31507995715607623, 'learning_rate': 0.00015316620072920465, 'epoch': 0.34}


 34%|███▍      | 1451/4236 [2:12:39<4:11:09,  5.41s/it]

 34%|███▍      | 1452/4236 [2:12:45<4:06:54,  5.32s/it]

 34%|███▍      | 1453/4236 [2:12:49<3:52:42,  5.02s/it]

 34%|███▍      | 1454/4236 [2:12:53<3:46:27,  4.88s/it]

 34%|███▍      | 1455/4236 [2:12:59<3:53:25,  5.04s/it]

 34%|███▍      | 1456/4236 [2:13:04<3:51:36,  5.00s/it]
{'loss': 1.5723, 'grad_norm': 0.34097523928352513, 'learning_rate': 0.00015271204409655055, 'epoch': 0.34}


 34%|███▍      | 1458/4236 [2:13:15<4:07:56,  5.36s/it]
{'loss': 1.7738, 'grad_norm': 0.32761969640358357, 'learning_rate': 0.00015258200703490262, 'epoch': 0.34}

 34%|███▍      | 1459/4236 [2:13:20<3:58:45,  5.16s/it]


 34%|███▍      | 1461/4236 [2:13:29<3:40:22,  4.76s/it]
{'loss': 1.7281, 'grad_norm': 0.31966098395185044, 'learning_rate': 0.0001523867208962012, 'epoch': 0.34}


 35%|███▍      | 1463/4236 [2:13:41<4:12:24,  5.46s/it]

 35%|███▍      | 1464/4236 [2:13:45<3:51:50,  5.02s/it]

 35%|███▍      | 1465/4236 [2:13:49<3:42:31,  4.82s/it]

 35%|███▍      | 1466/4236 [2:13:53<3:30:52,  4.57s/it]
{'loss': 1.6025, 'grad_norm': 0.320579077457995, 'learning_rate': 0.00015206063174735176, 'epoch': 0.35}

 35%|███▍      | 1467/4236 [2:13:58<3:39:11,  4.75s/it]


 35%|███▍      | 1469/4236 [2:14:08<3:43:56,  4.86s/it]

 35%|███▍      | 1470/4236 [2:14:15<4:14:52,  5.53s/it]

 35%|███▍      | 1471/4236 [2:14:20<3:56:37,  5.13s/it]

 35%|███▍      | 1472/4236 [2:14:24<3:38:56,  4.75s/it]
{'loss': 1.6227, 'grad_norm': 0.3392907205972271, 'learning_rate': 0.00015166832043050755, 'epoch': 0.35}

 35%|███▍      | 1473/4236 [2:14:28<3:38:13,  4.74s/it]


 35%|███▍      | 1475/4236 [2:14:39<3:54:56,  5.11s/it]

 35%|███▍      | 1476/4236 [2:14:44<3:55:35,  5.12s/it]
{'loss': 1.7912, 'grad_norm': 0.29582070604741817, 'learning_rate': 0.00015140617468633577, 'epoch': 0.35}


 35%|███▍      | 1478/4236 [2:14:53<3:40:49,  4.80s/it]
{'loss': 1.8091, 'grad_norm': 0.2813093920307508, 'learning_rate': 0.00015127492127382677, 'epoch': 0.35}

 35%|███▍      | 1479/4236 [2:15:08<6:00:37,  7.85s/it]


 35%|███▍      | 1481/4236 [2:15:16<4:23:29,  5.74s/it]

 35%|███▍      | 1482/4236 [2:15:21<4:21:32,  5.70s/it]

 35%|███▌      | 1483/4236 [2:15:30<5:00:00,  6.54s/it]

 35%|███▌      | 1484/4236 [2:15:34<4:24:07,  5.76s/it]
{'loss': 1.721, 'grad_norm': 0.32169387524598153, 'learning_rate': 0.00015088044256004958, 'epoch': 0.35}

 35%|███▌      | 1485/4236 [2:15:38<4:09:33,  5.44s/it]


 35%|███▌      | 1487/4236 [2:15:49<4:05:03,  5.35s/it]

 35%|███▌      | 1488/4236 [2:15:55<4:16:46,  5.61s/it]

 35%|███▌      | 1489/4236 [2:15:59<3:52:12,  5.07s/it]
{'loss': 1.6757, 'grad_norm': 0.30436743081647, 'learning_rate': 0.00015055089162820462, 'epoch': 0.35}


 35%|███▌      | 1491/4236 [2:16:11<4:12:59,  5.53s/it]

 35%|███▌      | 1492/4236 [2:16:15<3:50:33,  5.04s/it]

 35%|███▌      | 1493/4236 [2:16:19<3:42:31,  4.87s/it]

 35%|███▌      | 1494/4236 [2:16:24<3:39:46,  4.81s/it]

 35%|███▌      | 1495/4236 [2:16:33<4:45:34,  6.25s/it]

 35%|███▌      | 1496/4236 [2:16:45<6:00:12,  7.89s/it]

 35%|███▌      | 1497/4236 [2:16:51<5:36:41,  7.38s/it]
{'loss': 1.7697, 'grad_norm': 0.3423073886037348, 'learning_rate': 0.00015002207480368246, 'epoch': 0.35}

 35%|███▌      | 1498/4236 [2:16:56<5:02:25,  6.63s/it]

 35%|███▌      | 1499/4236 [2:17:00<4:24:18,  5.79s/it]

 35%|███▌      | 1500/4236 [2:17:04<4:00:35,  5.28s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 35%|███▌      | 1501/4236 [2:17:24<7:14:44,  9.54s/it]

 35%|███▌      | 1502/4236 [2:17:31<6:39:30,  8.77s/it]

 35%|███▌      | 1503/4236 [2:17:35<5:46:35,  7.61s/it]

 36%|███▌      | 1504/4236 [2:17:39<4:56:38,  6.51s/it]

 36%|███▌      | 1505/4236 [2:17:52<6:16:08,  8.26s/it]

 36%|███▌      | 1506/4236 [2:17:57<5:28:59,  7.23s/it]
{'loss': 1.7016, 'grad_norm': 0.3375355942022404, 'learning_rate': 0.0001494249188616723, 'epoch': 0.36}


 36%|███▌      | 1508/4236 [2:18:05<4:24:51,  5.83s/it]

 36%|███▌      | 1509/4236 [2:18:09<3:59:15,  5.26s/it]
{'loss': 1.4853, 'grad_norm': 0.3240714842288216, 'learning_rate': 0.00014922534517866843, 'epoch': 0.36}


 36%|███▌      | 1511/4236 [2:18:21<4:16:24,  5.65s/it]

 36%|███▌      | 1512/4236 [2:18:25<3:56:40,  5.21s/it]
{'loss': 1.7151, 'grad_norm': 0.2849117806538277, 'learning_rate': 0.00014902551239414218, 'epoch': 0.36}

 36%|███▌      | 1513/4236 [2:18:30<3:51:58,  5.11s/it]

 36%|███▌      | 1514/4236 [2:18:34<3:39:39,  4.84s/it]

 36%|███▌      | 1515/4236 [2:18:38<3:29:11,  4.61s/it]


 36%|███▌      | 1517/4236 [2:18:56<5:20:42,  7.08s/it]

 36%|███▌      | 1518/4236 [2:19:01<5:02:15,  6.67s/it]

 36%|███▌      | 1519/4236 [2:19:06<4:32:23,  6.02s/it]

 36%|███▌      | 1520/4236 [2:19:10<4:06:05,  5.44s/it]

 36%|███▌      | 1521/4236 [2:19:14<3:43:44,  4.94s/it]

 36%|███▌      | 1522/4236 [2:19:18<3:29:02,  4.62s/it]

 36%|███▌      | 1523/4236 [2:19:21<3:15:38,  4.33s/it]
{'loss': 1.6828, 'grad_norm': 0.3095941336059398, 'learning_rate': 0.00014829059243954274, 'epoch': 0.36}


 36%|███▌      | 1525/4236 [2:19:37<4:24:17,  5.85s/it]

 36%|███▌      | 1526/4236 [2:19:51<6:11:56,  8.23s/it]

 36%|███▌      | 1527/4236 [2:19:55<5:18:22,  7.05s/it]

 36%|███▌      | 1528/4236 [2:19:59<4:32:49,  6.04s/it]

 36%|███▌      | 1529/4236 [2:20:03<4:06:44,  5.47s/it]

 36%|███▌      | 1530/4236 [2:20:07<3:45:43,  5.00s/it]

 36%|███▌      | 1531/4236 [2:20:13<4:04:43,  5.43s/it]

 36%|███▌      | 1532/4236 [2:20:18<3:49:46,  5.10s/it]

 36%|███▌      | 1533/4236 [2:20:21<3:29:24,  4.65s/it]

 36%|███▌      | 1534/4236 [2:20:26<3:31:57,  4.71s/it]

 36%|███▌      | 1535/4236 [2:20:30<3:18:19,  4.41s/it]

 36%|███▋      | 1536/4236 [2:20:34<3:17:10,  4.38s/it]

 36%|███▋      | 1537/4236 [2:20:38<3:13:13,  4.30s/it]
{'loss': 1.6412, 'grad_norm': 0.32174523429017016, 'learning_rate': 0.00014735030424532514, 'epoch': 0.36}


 36%|███▋      | 1539/4236 [2:20:48<3:21:54,  4.49s/it]

 36%|███▋      | 1540/4236 [2:20:53<3:24:51,  4.56s/it]

 36%|███▋      | 1541/4236 [2:20:57<3:24:07,  4.54s/it]

 36%|███▋      | 1542/4236 [2:21:01<3:12:12,  4.28s/it]

 36%|███▋      | 1543/4236 [2:21:05<3:08:48,  4.21s/it]

 36%|███▋      | 1544/4236 [2:21:10<3:20:38,  4.47s/it]
{'loss': 1.8504, 'grad_norm': 0.304630888608122, 'learning_rate': 0.00014687811801041323, 'epoch': 0.36}


 36%|███▋      | 1546/4236 [2:21:19<3:17:22,  4.40s/it]
{'loss': 1.552, 'grad_norm': 0.30154849557319613, 'learning_rate': 0.00014674296031863323, 'epoch': 0.36}


 37%|███▋      | 1548/4236 [2:21:27<3:14:07,  4.33s/it]

 37%|███▋      | 1549/4236 [2:21:32<3:21:45,  4.51s/it]

 37%|███▋      | 1550/4236 [2:21:36<3:12:33,  4.30s/it]

 37%|███▋      | 1551/4236 [2:21:41<3:24:55,  4.58s/it]

 37%|███▋      | 1552/4236 [2:21:45<3:13:08,  4.32s/it]
{'loss': 1.6436, 'grad_norm': 0.3374679444699397, 'learning_rate': 0.00014633683241503464, 'epoch': 0.37}

 37%|███▋      | 1553/4236 [2:21:49<3:05:15,  4.14s/it]

 37%|███▋      | 1554/4236 [2:21:53<3:03:34,  4.11s/it]


 37%|███▋      | 1556/4236 [2:22:01<3:07:59,  4.21s/it]

 37%|███▋      | 1557/4236 [2:22:06<3:20:09,  4.48s/it]

 37%|███▋      | 1558/4236 [2:22:10<3:11:50,  4.30s/it]
{'loss': 1.5565, 'grad_norm': 0.2917553686534758, 'learning_rate': 0.00014592972892229778, 'epoch': 0.37}


 37%|███▋      | 1560/4236 [2:22:23<4:10:37,  5.62s/it]

 37%|███▋      | 1561/4236 [2:22:28<3:58:58,  5.36s/it]

 37%|███▋      | 1562/4236 [2:22:36<4:34:18,  6.15s/it]

 37%|███▋      | 1563/4236 [2:22:50<6:13:55,  8.39s/it]

 37%|███▋      | 1564/4236 [2:23:03<7:24:41,  9.99s/it]

 37%|███▋      | 1565/4236 [2:23:08<6:14:48,  8.42s/it]
{'loss': 1.7376, 'grad_norm': 0.31102033223710507, 'learning_rate': 0.00014545355324728032, 'epoch': 0.37}

 37%|███▋      | 1566/4236 [2:23:15<5:52:51,  7.93s/it]

 37%|███▋      | 1567/4236 [2:23:19<4:57:19,  6.68s/it]


 37%|███▋      | 1569/4236 [2:23:30<4:39:40,  6.29s/it]

 37%|███▋      | 1570/4236 [2:23:35<4:22:12,  5.90s/it]

 37%|███▋      | 1571/4236 [2:23:39<3:51:13,  5.21s/it]

 37%|███▋      | 1572/4236 [2:23:43<3:39:47,  4.95s/it]

 37%|███▋      | 1573/4236 [2:23:47<3:22:14,  4.56s/it]

 37%|███▋      | 1574/4236 [2:23:58<4:53:06,  6.61s/it]

 37%|███▋      | 1575/4236 [2:24:03<4:27:43,  6.04s/it]

 37%|███▋      | 1576/4236 [2:24:08<4:16:33,  5.79s/it]

 37%|███▋      | 1577/4236 [2:24:12<3:48:13,  5.15s/it]

 37%|███▋      | 1578/4236 [2:24:17<3:52:20,  5.24s/it]

 37%|███▋      | 1579/4236 [2:24:21<3:32:15,  4.79s/it]

 37%|███▋      | 1580/4236 [2:24:25<3:22:09,  4.57s/it]

 37%|███▋      | 1581/4236 [2:24:29<3:14:28,  4.40s/it]
{'loss': 1.7989, 'grad_norm': 0.32624128848643935, 'learning_rate': 0.00014436028243841316, 'epoch': 0.37}

 37%|███▋      | 1582/4236 [2:24:37<3:57:15,  5.36s/it]


 37%|███▋      | 1584/4236 [2:24:44<3:21:08,  4.55s/it]

 37%|███▋      | 1585/4236 [2:24:49<3:27:17,  4.69s/it]

 37%|███▋      | 1586/4236 [2:24:53<3:14:54,  4.41s/it]
{'loss': 1.6918, 'grad_norm': 0.35050098984939476, 'learning_rate': 0.0001440172655595992, 'epoch': 0.37}

 37%|███▋      | 1587/4236 [2:25:07<5:16:51,  7.18s/it]

 37%|███▋      | 1588/4236 [2:25:11<4:32:49,  6.18s/it]


 38%|███▊      | 1590/4236 [2:25:19<3:51:37,  5.25s/it]
{'loss': 1.6487, 'grad_norm': 0.31724067998777006, 'learning_rate': 0.00014374238843909844, 'epoch': 0.38}


 38%|███▊      | 1592/4236 [2:25:26<3:13:40,  4.39s/it]

 38%|███▊      | 1593/4236 [2:25:36<4:22:46,  5.97s/it]

 38%|███▊      | 1594/4236 [2:25:40<3:51:19,  5.25s/it]

 38%|███▊      | 1595/4236 [2:25:44<3:38:08,  4.96s/it]

 38%|███▊      | 1596/4236 [2:25:50<3:54:55,  5.34s/it]

 38%|███▊      | 1597/4236 [2:25:55<3:49:52,  5.23s/it]

 38%|███▊      | 1598/4236 [2:26:10<5:52:26,  8.02s/it]

 38%|███▊      | 1599/4236 [2:26:21<6:38:48,  9.07s/it]

 38%|███▊      | 1600/4236 [2:26:33<7:08:41,  9.76s/it]

 38%|███▊      | 1601/4236 [2:26:50<8:52:41, 12.13s/it]

 38%|███▊      | 1602/4236 [2:26:54<7:06:23,  9.71s/it]
{'loss': 1.7087, 'grad_norm': 0.3298177991193223, 'learning_rate': 0.0001429153114763868, 'epoch': 0.38}


 38%|███▊      | 1604/4236 [2:27:02<4:58:20,  6.80s/it]

 38%|███▊      | 1605/4236 [2:27:06<4:19:26,  5.92s/it]

 38%|███▊      | 1606/4236 [2:27:10<3:57:05,  5.41s/it]

 38%|███▊      | 1607/4236 [2:27:16<3:51:52,  5.29s/it]

 38%|███▊      | 1608/4236 [2:27:20<3:44:31,  5.13s/it]

 38%|███▊      | 1609/4236 [2:27:24<3:31:01,  4.82s/it]

 38%|███▊      | 1610/4236 [2:27:30<3:35:44,  4.93s/it]

 38%|███▊      | 1611/4236 [2:27:34<3:23:47,  4.66s/it]
{'loss': 1.6491, 'grad_norm': 0.2897318692565237, 'learning_rate': 0.00014229262812361622, 'epoch': 0.38}

 38%|███▊      | 1612/4236 [2:27:39<3:33:41,  4.89s/it]


 38%|███▊      | 1614/4236 [2:27:49<3:28:00,  4.76s/it]

 38%|███▊      | 1615/4236 [2:27:54<3:29:28,  4.80s/it]

 38%|███▊      | 1616/4236 [2:27:58<3:21:49,  4.62s/it]

 38%|███▊      | 1617/4236 [2:28:02<3:21:42,  4.62s/it]

 38%|███▊      | 1618/4236 [2:28:07<3:25:26,  4.71s/it]

 38%|███▊      | 1619/4236 [2:28:12<3:22:16,  4.64s/it]

 38%|███▊      | 1620/4236 [2:28:20<4:03:20,  5.58s/it]

 38%|███▊      | 1621/4236 [2:28:24<3:49:35,  5.27s/it]

 38%|███▊      | 1622/4236 [2:28:30<3:53:25,  5.36s/it]

 38%|███▊      | 1623/4236 [2:28:34<3:41:34,  5.09s/it]
{'loss': 1.5576, 'grad_norm': 0.3263266488615686, 'learning_rate': 0.0001414592722276709, 'epoch': 0.38}

 38%|███▊      | 1624/4236 [2:28:39<3:39:16,  5.04s/it]

 38%|███▊      | 1625/4236 [2:28:43<3:24:38,  4.70s/it]

 38%|███▊      | 1626/4236 [2:28:51<4:10:08,  5.75s/it]


 38%|███▊      | 1628/4236 [2:29:03<4:18:24,  5.94s/it]

 38%|███▊      | 1629/4236 [2:29:09<4:18:14,  5.94s/it]
{'loss': 1.774, 'grad_norm': 0.29589025530404545, 'learning_rate': 0.0001410412805452757, 'epoch': 0.38}


 39%|███▊      | 1631/4236 [2:29:20<4:11:21,  5.79s/it]

 39%|███▊      | 1632/4236 [2:29:26<4:19:27,  5.98s/it]

 39%|███▊      | 1633/4236 [2:29:32<4:08:53,  5.74s/it]

 39%|███▊      | 1634/4236 [2:29:36<3:51:44,  5.34s/it]
{'loss': 1.753, 'grad_norm': 0.36060449255712085, 'learning_rate': 0.00014069229369664514, 'epoch': 0.39}


 39%|███▊      | 1636/4236 [2:29:50<4:13:26,  5.85s/it]

 39%|███▊      | 1637/4236 [2:30:04<6:07:39,  8.49s/it]
[2024-05-25 05:04:42,261] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▊      | 1638/4236 [2:30:08<5:07:49,  7.11s/it]

 39%|███▊      | 1639/4236 [2:30:12<4:23:27,  6.09s/it]

 39%|███▊      | 1640/4236 [2:30:17<4:07:00,  5.71s/it]

 39%|███▊      | 1641/4236 [2:30:20<3:40:34,  5.10s/it]
{'loss': 1.6612, 'grad_norm': 0.3295612029055382, 'learning_rate': 0.00014020271371404682, 'epoch': 0.39}


 39%|███▉      | 1643/4236 [2:30:31<3:43:10,  5.16s/it]
{'loss': 1.5699, 'grad_norm': 0.35075222717257565, 'learning_rate': 0.00014006262149486256, 'epoch': 0.39}


 39%|███▉      | 1645/4236 [2:30:40<3:26:45,  4.79s/it]

 39%|███▉      | 1646/4236 [2:30:45<3:33:40,  4.95s/it]
{'loss': 1.5149, 'grad_norm': 0.3238570230086739, 'learning_rate': 0.00013985230754118733, 'epoch': 0.39}


 39%|███▉      | 1648/4236 [2:30:56<3:45:49,  5.24s/it]

 39%|███▉      | 1649/4236 [2:31:00<3:30:22,  4.88s/it]

 39%|███▉      | 1650/4236 [2:31:06<3:36:23,  5.02s/it]
{'loss': 1.5618, 'grad_norm': 0.32435968864966025, 'learning_rate': 0.00013957156282543603, 'epoch': 0.39}


 39%|███▉      | 1652/4236 [2:31:21<4:40:02,  6.50s/it]

 39%|███▉      | 1653/4236 [2:31:24<4:03:03,  5.65s/it]

 39%|███▉      | 1654/4236 [2:31:28<3:39:20,  5.10s/it]
{'loss': 1.6295, 'grad_norm': 0.2903946215137491, 'learning_rate': 0.0001392904478199079, 'epoch': 0.39}


 39%|███▉      | 1656/4236 [2:31:40<3:57:56,  5.53s/it]

 39%|███▉      | 1657/4236 [2:31:44<3:45:07,  5.24s/it]

 39%|███▉      | 1658/4236 [2:31:48<3:30:35,  4.90s/it]

 39%|███▉      | 1659/4236 [2:31:53<3:24:36,  4.76s/it]

 39%|███▉      | 1660/4236 [2:31:59<3:37:13,  5.06s/it]
{'loss': 1.5694, 'grad_norm': 0.3071595953133145, 'learning_rate': 0.00013886808677350039, 'epoch': 0.39}


 39%|███▉      | 1662/4236 [2:32:10<3:51:54,  5.41s/it]
{'loss': 1.6891, 'grad_norm': 0.30138134669185596, 'learning_rate': 0.00013872711746506413, 'epoch': 0.39}


 39%|███▉      | 1664/4236 [2:32:20<3:33:37,  4.98s/it]
{'loss': 1.6725, 'grad_norm': 0.33875911714708373, 'learning_rate': 0.00013858605755959902, 'epoch': 0.39}

 39%|███▉      | 1665/4236 [2:32:23<3:16:26,  4.58s/it]

 39%|███▉      | 1666/4236 [2:32:29<3:33:23,  4.98s/it]


 39%|███▉      | 1668/4236 [2:32:52<6:12:15,  8.70s/it]

 39%|███▉      | 1669/4236 [2:32:56<5:10:24,  7.26s/it]

 39%|███▉      | 1670/4236 [2:33:01<4:40:17,  6.55s/it]

 39%|███▉      | 1671/4236 [2:33:05<4:09:54,  5.85s/it]

 39%|███▉      | 1672/4236 [2:33:19<5:52:54,  8.26s/it]

 39%|███▉      | 1673/4236 [2:33:24<5:14:55,  7.37s/it]
{'loss': 1.8146, 'grad_norm': 0.32305370603852007, 'learning_rate': 0.0001379501756996088, 'epoch': 0.39}


 40%|███▉      | 1675/4236 [2:33:36<4:59:35,  7.02s/it]
{'loss': 1.5937, 'grad_norm': 0.28865782030621734, 'learning_rate': 0.00013780862341472182, 'epoch': 0.4}


 40%|███▉      | 1677/4236 [2:33:54<5:26:47,  7.66s/it]

 40%|███▉      | 1678/4236 [2:34:00<4:58:50,  7.01s/it]

 40%|███▉      | 1679/4236 [2:34:09<5:25:19,  7.63s/it]

 40%|███▉      | 1680/4236 [2:34:13<4:35:43,  6.47s/it]

 40%|███▉      | 1681/4236 [2:34:18<4:15:20,  6.00s/it]
{'loss': 1.6082, 'grad_norm': 0.30715495409221644, 'learning_rate': 0.00013738343719567464, 'epoch': 0.4}


 40%|███▉      | 1683/4236 [2:34:26<3:34:53,  5.05s/it]
{'loss': 1.6393, 'grad_norm': 0.3174267738289576, 'learning_rate': 0.0001372415331063831, 'epoch': 0.4}

 40%|███▉      | 1684/4236 [2:34:29<3:17:24,  4.64s/it]


 40%|███▉      | 1686/4236 [2:34:38<3:14:34,  4.58s/it]

 40%|███▉      | 1687/4236 [2:34:44<3:23:31,  4.79s/it]
{'loss': 1.6703, 'grad_norm': 0.3082138783866967, 'learning_rate': 0.00013695746389487466, 'epoch': 0.4}


 40%|███▉      | 1689/4236 [2:34:53<3:15:10,  4.60s/it]
{'loss': 1.7125, 'grad_norm': 0.34014182327755016, 'learning_rate': 0.0001368152994372005, 'epoch': 0.4}

 40%|███▉      | 1690/4236 [2:34:57<3:13:09,  4.55s/it]


 40%|███▉      | 1692/4236 [2:35:06<3:11:25,  4.51s/it]
{'loss': 1.6289, 'grad_norm': 0.3312885339972189, 'learning_rate': 0.00013660189137108578, 'epoch': 0.4}


 40%|███▉      | 1694/4236 [2:35:15<3:07:34,  4.43s/it]

 40%|████      | 1695/4236 [2:35:19<3:01:38,  4.29s/it]

 40%|████      | 1696/4236 [2:35:24<3:06:13,  4.40s/it]

 40%|████      | 1697/4236 [2:35:28<3:01:12,  4.28s/it]

 40%|████      | 1698/4236 [2:35:33<3:12:41,  4.56s/it]

 40%|████      | 1699/4236 [2:35:42<4:15:41,  6.05s/it]

 40%|████      | 1700/4236 [2:35:47<3:58:19,  5.64s/it]

 40%|████      | 1701/4236 [2:35:51<3:36:40,  5.13s/it]

 40%|████      | 1702/4236 [2:35:56<3:38:15,  5.17s/it]

 40%|████      | 1703/4236 [2:36:00<3:22:18,  4.79s/it]

 40%|████      | 1704/4236 [2:36:04<3:14:08,  4.60s/it]

 40%|████      | 1705/4236 [2:36:09<3:13:06,  4.58s/it]

 40%|████      | 1706/4236 [2:36:14<3:16:32,  4.66s/it]

 40%|████      | 1707/4236 [2:36:18<3:13:49,  4.60s/it]

 40%|████      | 1708/4236 [2:36:22<3:02:58,  4.34s/it]

 40%|████      | 1709/4236 [2:36:26<3:01:22,  4.31s/it]

 40%|████      | 1710/4236 [2:36:40<5:03:51,  7.22s/it]

 40%|████      | 1711/4236 [2:36:44<4:20:56,  6.20s/it]

 40%|████      | 1712/4236 [2:36:48<3:55:17,  5.59s/it]

 40%|████      | 1713/4236 [2:36:53<3:46:52,  5.40s/it]

 40%|████      | 1714/4236 [2:36:57<3:29:12,  4.98s/it]

 40%|████      | 1715/4236 [2:37:04<3:56:24,  5.63s/it]

 41%|████      | 1716/4236 [2:37:17<5:21:44,  7.66s/it]
{'loss': 1.7312, 'grad_norm': 0.3680630461740297, 'learning_rate': 0.00013488778575915258, 'epoch': 0.41}


 41%|████      | 1718/4236 [2:37:26<4:19:41,  6.19s/it]

 41%|████      | 1719/4236 [2:37:30<3:49:04,  5.46s/it]

 41%|████      | 1720/4236 [2:37:35<3:37:32,  5.19s/it]
{'loss': 1.9136, 'grad_norm': 0.3697032297556354, 'learning_rate': 0.00013460094325387048, 'epoch': 0.41}


 41%|████      | 1722/4236 [2:37:45<3:31:59,  5.06s/it]

 41%|████      | 1723/4236 [2:37:48<3:14:53,  4.65s/it]

 41%|████      | 1724/4236 [2:37:54<3:29:17,  5.00s/it]

 41%|████      | 1725/4236 [2:37:58<3:15:18,  4.67s/it]

 41%|████      | 1726/4236 [2:38:02<3:06:30,  4.46s/it]

 41%|████      | 1727/4236 [2:38:10<3:57:54,  5.69s/it]

 41%|████      | 1728/4236 [2:38:15<3:43:55,  5.36s/it]
{'loss': 1.8499, 'grad_norm': 0.34098526452406824, 'learning_rate': 0.00013402628959842104, 'epoch': 0.41}

 41%|████      | 1729/4236 [2:38:22<4:00:02,  5.75s/it]


 41%|████      | 1731/4236 [2:38:32<3:56:20,  5.66s/it]
{'loss': 1.7564, 'grad_norm': 0.29287190241841243, 'learning_rate': 0.00013381046497154816, 'epoch': 0.41}


 41%|████      | 1733/4236 [2:38:43<3:49:51,  5.51s/it]
{'loss': 1.7463, 'grad_norm': 0.32153465262815256, 'learning_rate': 0.00013366648294787383, 'epoch': 0.41}


 41%|████      | 1735/4236 [2:38:52<3:28:59,  5.01s/it]

 41%|████      | 1736/4236 [2:38:58<3:43:08,  5.36s/it]
{'loss': 1.8992, 'grad_norm': 0.2905068514628345, 'learning_rate': 0.00013345036234579138, 'epoch': 0.41}


 41%|████      | 1738/4236 [2:39:14<4:46:38,  6.88s/it]

 41%|████      | 1739/4236 [2:39:19<4:15:56,  6.15s/it]

 41%|████      | 1740/4236 [2:39:33<5:56:51,  8.58s/it]
[2024-05-25 05:14:10,810] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████      | 1741/4236 [2:39:39<5:31:25,  7.97s/it]

 41%|████      | 1742/4236 [2:39:46<5:19:31,  7.69s/it]

 41%|████      | 1743/4236 [2:39:50<4:31:20,  6.53s/it]

 41%|████      | 1744/4236 [2:39:54<4:00:24,  5.79s/it]

 41%|████      | 1745/4236 [2:40:01<4:14:06,  6.12s/it]

 41%|████      | 1746/4236 [2:40:05<3:43:33,  5.39s/it]

 41%|████      | 1747/4236 [2:40:11<3:58:47,  5.76s/it]

 41%|████▏     | 1748/4236 [2:40:17<3:58:25,  5.75s/it]

 41%|████▏     | 1749/4236 [2:40:21<3:39:14,  5.29s/it]

 41%|████▏     | 1750/4236 [2:40:31<4:30:29,  6.53s/it]

 41%|████▏     | 1751/4236 [2:40:34<3:54:34,  5.66s/it]

 41%|████▏     | 1752/4236 [2:40:40<3:53:57,  5.65s/it]
{'loss': 1.6616, 'grad_norm': 0.3045780613065236, 'learning_rate': 0.00013229477332322016, 'epoch': 0.41}


 41%|████▏     | 1754/4236 [2:40:47<3:13:00,  4.67s/it]

 41%|████▏     | 1755/4236 [2:40:52<3:11:40,  4.64s/it]

 41%|████▏     | 1756/4236 [2:40:57<3:13:36,  4.68s/it]
{'loss': 1.6404, 'grad_norm': 0.3241265515123213, 'learning_rate': 0.0001320051138084542, 'epoch': 0.41}

 41%|████▏     | 1757/4236 [2:41:02<3:17:21,  4.78s/it]


 42%|████▏     | 1759/4236 [2:41:13<3:42:46,  5.40s/it]

 42%|████▏     | 1760/4236 [2:41:23<4:32:58,  6.62s/it]
{'loss': 1.6023, 'grad_norm': 0.3112275042937948, 'learning_rate': 0.0001317151548067434, 'epoch': 0.42}


 42%|████▏     | 1762/4236 [2:41:32<3:51:15,  5.61s/it]

 42%|████▏     | 1763/4236 [2:41:37<3:36:46,  5.26s/it]
{'loss': 1.7364, 'grad_norm': 0.28235074097656926, 'learning_rate': 0.00013149749064925756, 'epoch': 0.42}

 42%|████▏     | 1764/4236 [2:41:44<3:58:49,  5.80s/it]

 42%|████▏     | 1765/4236 [2:41:50<4:03:59,  5.92s/it]


 42%|████▏     | 1767/4236 [2:41:58<3:28:49,  5.07s/it]

 42%|████▏     | 1768/4236 [2:42:03<3:18:25,  4.82s/it]

 42%|████▏     | 1769/4236 [2:42:07<3:12:02,  4.67s/it]

 42%|████▏     | 1770/4236 [2:42:12<3:12:37,  4.69s/it]

 42%|████▏     | 1771/4236 [2:42:17<3:23:35,  4.96s/it]

 42%|████▏     | 1772/4236 [2:42:21<3:13:49,  4.72s/it]

 42%|████▏     | 1773/4236 [2:42:41<6:11:54,  9.06s/it]
[2024-05-25 05:17:18,598] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 1774/4236 [2:42:48<5:46:37,  8.45s/it]

 42%|████▏     | 1775/4236 [2:42:52<5:00:22,  7.32s/it]

 42%|████▏     | 1776/4236 [2:42:58<4:46:39,  6.99s/it]

 42%|████▏     | 1777/4236 [2:43:03<4:17:12,  6.28s/it]

 42%|████▏     | 1778/4236 [2:43:07<3:42:40,  5.44s/it]

 42%|████▏     | 1779/4236 [2:43:11<3:27:30,  5.07s/it]

 42%|████▏     | 1780/4236 [2:43:15<3:20:37,  4.90s/it]
{'loss': 1.4755, 'grad_norm': 0.310906715759356, 'learning_rate': 0.00013026096255559056, 'epoch': 0.42}


 42%|████▏     | 1782/4236 [2:43:27<3:41:37,  5.42s/it]

 42%|████▏     | 1783/4236 [2:43:33<3:45:59,  5.53s/it]

 42%|████▏     | 1784/4236 [2:43:39<3:51:30,  5.67s/it]

 42%|████▏     | 1785/4236 [2:43:43<3:39:01,  5.36s/it]

 42%|████▏     | 1786/4236 [2:43:47<3:26:06,  5.05s/it]

 42%|████▏     | 1787/4236 [2:43:54<3:41:06,  5.42s/it]

 42%|████▏     | 1788/4236 [2:43:59<3:35:48,  5.29s/it]

 42%|████▏     | 1789/4236 [2:44:04<3:39:19,  5.38s/it]

 42%|████▏     | 1790/4236 [2:44:08<3:21:16,  4.94s/it]

 42%|████▏     | 1791/4236 [2:44:12<3:12:48,  4.73s/it]
{'loss': 1.7403, 'grad_norm': 0.299776302123453, 'learning_rate': 0.00012945811756631255, 'epoch': 0.42}


 42%|████▏     | 1793/4236 [2:44:23<3:23:31,  5.00s/it]

 42%|████▏     | 1794/4236 [2:44:27<3:20:52,  4.94s/it]

 42%|████▏     | 1795/4236 [2:44:31<3:08:02,  4.62s/it]

 42%|████▏     | 1796/4236 [2:44:37<3:20:09,  4.92s/it]

 42%|████▏     | 1797/4236 [2:44:49<4:42:56,  6.96s/it]

 42%|████▏     | 1798/4236 [2:44:53<4:13:53,  6.25s/it]
{'loss': 1.722, 'grad_norm': 0.3303077934237802, 'learning_rate': 0.00012894612723767646, 'epoch': 0.42}

 42%|████▏     | 1799/4236 [2:44:58<3:58:19,  5.87s/it]

 42%|████▏     | 1800/4236 [2:45:10<5:13:55,  7.73s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 43%|████▎     | 1801/4236 [2:45:29<7:23:03, 10.92s/it]

 43%|████▎     | 1802/4236 [2:45:33<6:03:07,  8.95s/it]

 43%|████▎     | 1803/4236 [2:45:37<5:02:40,  7.46s/it]

 43%|████▎     | 1804/4236 [2:45:41<4:20:31,  6.43s/it]

 43%|████▎     | 1805/4236 [2:45:49<4:44:59,  7.03s/it]

 43%|████▎     | 1806/4236 [2:45:53<4:03:09,  6.00s/it]

 43%|████▎     | 1807/4236 [2:46:01<4:32:59,  6.74s/it]

 43%|████▎     | 1808/4236 [2:46:06<4:00:11,  5.94s/it]

 43%|████▎     | 1809/4236 [2:46:09<3:36:25,  5.35s/it]

 43%|████▎     | 1810/4236 [2:46:19<4:22:53,  6.50s/it]

 43%|████▎     | 1811/4236 [2:46:23<4:00:48,  5.96s/it]

 43%|████▎     | 1812/4236 [2:46:27<3:34:12,  5.30s/it]

 43%|████▎     | 1813/4236 [2:46:33<3:45:52,  5.59s/it]

 43%|████▎     | 1814/4236 [2:46:38<3:38:34,  5.41s/it]
{'loss': 1.7497, 'grad_norm': 0.35867164157965825, 'learning_rate': 0.00012777277236825576, 'epoch': 0.43}

 43%|████▎     | 1815/4236 [2:46:42<3:21:42,  5.00s/it]

 43%|████▎     | 1816/4236 [2:46:46<3:06:00,  4.61s/it]


 43%|████▎     | 1818/4236 [2:46:55<2:59:17,  4.45s/it]

 43%|████▎     | 1819/4236 [2:47:00<3:07:34,  4.66s/it]

 43%|████▎     | 1820/4236 [2:47:04<3:00:42,  4.49s/it]
{'loss': 1.7579, 'grad_norm': 0.31040594401847493, 'learning_rate': 0.0001273316828320369, 'epoch': 0.43}

 43%|████▎     | 1821/4236 [2:47:08<2:57:10,  4.40s/it]


 43%|████▎     | 1823/4236 [2:47:17<2:59:13,  4.46s/it]

 43%|████▎     | 1824/4236 [2:47:22<2:59:11,  4.46s/it]

 43%|████▎     | 1825/4236 [2:47:27<3:14:30,  4.84s/it]

 43%|████▎     | 1826/4236 [2:47:34<3:37:54,  5.43s/it]

 43%|████▎     | 1827/4236 [2:47:38<3:19:01,  4.96s/it]

 43%|████▎     | 1828/4236 [2:47:42<3:05:36,  4.62s/it]

 43%|████▎     | 1829/4236 [2:47:46<3:00:02,  4.49s/it]

 43%|████▎     | 1830/4236 [2:47:50<2:53:04,  4.32s/it]

 43%|████▎     | 1831/4236 [2:47:54<2:51:58,  4.29s/it]

 43%|████▎     | 1832/4236 [2:47:59<3:03:55,  4.59s/it]

 43%|████▎     | 1833/4236 [2:48:03<2:57:51,  4.44s/it]

 43%|████▎     | 1834/4236 [2:48:07<2:51:47,  4.29s/it]

 43%|████▎     | 1835/4236 [2:48:12<2:56:24,  4.41s/it]

 43%|████▎     | 1836/4236 [2:48:16<2:47:12,  4.18s/it]
{'loss': 1.8153, 'grad_norm': 0.32865519575122454, 'learning_rate': 0.00012615265602340257, 'epoch': 0.43}


 43%|████▎     | 1838/4236 [2:48:25<2:51:32,  4.29s/it]

 43%|████▎     | 1839/4236 [2:48:30<3:02:32,  4.57s/it]

 43%|████▎     | 1840/4236 [2:48:34<2:54:35,  4.37s/it]

 43%|████▎     | 1841/4236 [2:48:38<2:51:50,  4.30s/it]
{'loss': 1.6015, 'grad_norm': 0.3462642655639818, 'learning_rate': 0.00012578339884646166, 'epoch': 0.43}

 43%|████▎     | 1842/4236 [2:48:52<4:51:39,  7.31s/it]


 44%|████▎     | 1844/4236 [2:49:18<7:08:52, 10.76s/it]
{'loss': 1.7003, 'grad_norm': 0.3097129743130219, 'learning_rate': 0.0001255616632440475, 'epoch': 0.44}


 44%|████▎     | 1846/4236 [2:49:28<5:08:56,  7.76s/it]

 44%|████▎     | 1847/4236 [2:49:35<4:59:57,  7.53s/it]

 44%|████▎     | 1848/4236 [2:49:40<4:27:44,  6.73s/it]

 44%|████▎     | 1849/4236 [2:49:50<5:01:50,  7.59s/it]

 44%|████▎     | 1850/4236 [2:49:54<4:18:26,  6.50s/it]
{'loss': 1.7193, 'grad_norm': 0.3068954261985969, 'learning_rate': 0.00012511778956950323, 'epoch': 0.44}


 44%|████▎     | 1852/4236 [2:50:05<4:01:55,  6.09s/it]

 44%|████▎     | 1853/4236 [2:50:10<3:41:34,  5.58s/it]
{'loss': 1.7969, 'grad_norm': 0.2798797954371527, 'learning_rate': 0.00012489565383373762, 'epoch': 0.44}


 44%|████▍     | 1855/4236 [2:50:23<3:54:42,  5.91s/it]
{'loss': 1.8269, 'grad_norm': 0.3156306497869262, 'learning_rate': 0.0001247474904708822, 'epoch': 0.44}


 44%|████▍     | 1857/4236 [2:50:30<3:08:45,  4.76s/it]

 44%|████▍     | 1858/4236 [2:50:35<3:09:57,  4.79s/it]

 44%|████▍     | 1859/4236 [2:50:50<5:11:10,  7.85s/it]

 44%|████▍     | 1860/4236 [2:50:54<4:24:53,  6.69s/it]

 44%|████▍     | 1861/4236 [2:51:02<4:40:48,  7.09s/it]

 44%|████▍     | 1862/4236 [2:51:06<4:03:06,  6.14s/it]
{'loss': 1.8034, 'grad_norm': 0.32324705914026736, 'learning_rate': 0.00012422846506526694, 'epoch': 0.44}

 44%|████▍     | 1863/4236 [2:51:14<4:29:28,  6.81s/it]


 44%|████▍     | 1865/4236 [2:51:25<3:52:54,  5.89s/it]

 44%|████▍     | 1866/4236 [2:51:29<3:38:14,  5.53s/it]

 44%|████▍     | 1867/4236 [2:51:36<3:46:58,  5.75s/it]

 44%|████▍     | 1868/4236 [2:51:40<3:25:34,  5.21s/it]
{'loss': 1.6187, 'grad_norm': 0.30514981428210347, 'learning_rate': 0.00012378303295967145, 'epoch': 0.44}


 44%|████▍     | 1870/4236 [2:51:49<3:10:41,  4.84s/it]

 44%|████▍     | 1871/4236 [2:52:02<4:47:24,  7.29s/it]

 44%|████▍     | 1872/4236 [2:52:08<4:33:56,  6.95s/it]

 44%|████▍     | 1873/4236 [2:52:12<3:55:57,  5.99s/it]

 44%|████▍     | 1874/4236 [2:52:16<3:30:14,  5.34s/it]

 44%|████▍     | 1875/4236 [2:52:20<3:16:57,  5.01s/it]

 44%|████▍     | 1876/4236 [2:52:24<3:02:25,  4.64s/it]

 44%|████▍     | 1877/4236 [2:52:27<2:51:20,  4.36s/it]

 44%|████▍     | 1878/4236 [2:52:31<2:47:04,  4.25s/it]

 44%|████▍     | 1879/4236 [2:52:35<2:41:19,  4.11s/it]

 44%|████▍     | 1880/4236 [2:52:39<2:41:02,  4.10s/it]

 44%|████▍     | 1881/4236 [2:52:43<2:40:02,  4.08s/it]

 44%|████▍     | 1882/4236 [2:52:51<3:29:26,  5.34s/it]
{'loss': 1.7296, 'grad_norm': 0.30683562445366136, 'learning_rate': 0.0001227417603066519, 'epoch': 0.44}


 44%|████▍     | 1884/4236 [2:53:01<3:22:58,  5.18s/it]

 44%|████▍     | 1885/4236 [2:53:06<3:09:28,  4.84s/it]

 45%|████▍     | 1886/4236 [2:53:10<3:02:34,  4.66s/it]

 45%|████▍     | 1887/4236 [2:53:14<3:00:46,  4.62s/it]

 45%|████▍     | 1888/4236 [2:53:20<3:16:58,  5.03s/it]

 45%|████▍     | 1889/4236 [2:53:24<3:04:03,  4.71s/it]

 45%|████▍     | 1890/4236 [2:53:28<2:54:01,  4.45s/it]

 45%|████▍     | 1891/4236 [2:53:35<3:23:49,  5.22s/it]

 45%|████▍     | 1892/4236 [2:53:39<3:08:01,  4.81s/it]

 45%|████▍     | 1893/4236 [2:53:47<3:45:41,  5.78s/it]
{'loss': 1.6384, 'grad_norm': 0.3065158082715816, 'learning_rate': 0.0001219217827361398, 'epoch': 0.45}

 45%|████▍     | 1894/4236 [2:53:57<4:30:53,  6.94s/it]

 45%|████▍     | 1895/4236 [2:54:03<4:21:42,  6.71s/it]


 45%|████▍     | 1897/4236 [2:54:11<3:35:33,  5.53s/it]
{'loss': 1.6641, 'grad_norm': 0.33849757836932903, 'learning_rate': 0.0001216232213975746, 'epoch': 0.45}


 45%|████▍     | 1899/4236 [2:54:21<3:21:43,  5.18s/it]

 45%|████▍     | 1900/4236 [2:54:25<3:06:38,  4.79s/it]
{'loss': 1.7027, 'grad_norm': 0.3188682195139932, 'learning_rate': 0.00012139916745603508, 'epoch': 0.45}


 45%|████▍     | 1902/4236 [2:54:34<3:06:10,  4.79s/it]

 45%|████▍     | 1903/4236 [2:54:38<2:56:16,  4.53s/it]

 45%|████▍     | 1904/4236 [2:54:44<3:05:31,  4.77s/it]

 45%|████▍     | 1905/4236 [2:54:48<2:56:43,  4.55s/it]
{'loss': 1.8421, 'grad_norm': 0.3228667090047714, 'learning_rate': 0.00012102549450007395, 'epoch': 0.45}


 45%|████▌     | 1907/4236 [2:54:57<3:01:26,  4.67s/it]

 45%|████▌     | 1908/4236 [2:55:01<2:55:01,  4.51s/it]

 45%|████▌     | 1909/4236 [2:55:05<2:47:58,  4.33s/it]

 45%|████▌     | 1910/4236 [2:55:10<2:47:56,  4.33s/it]

 45%|████▌     | 1911/4236 [2:55:14<2:48:29,  4.35s/it]

 45%|████▌     | 1912/4236 [2:55:18<2:48:33,  4.35s/it]

 45%|████▌     | 1913/4236 [2:55:22<2:44:40,  4.25s/it]

 45%|████▌     | 1914/4236 [2:55:28<3:01:09,  4.68s/it]

 45%|████▌     | 1915/4236 [2:55:36<3:39:19,  5.67s/it]

 45%|████▌     | 1916/4236 [2:55:41<3:24:23,  5.29s/it]

 45%|████▌     | 1917/4236 [2:55:45<3:20:36,  5.19s/it]

 45%|████▌     | 1918/4236 [2:55:49<3:04:05,  4.77s/it]

 45%|████▌     | 1919/4236 [2:55:53<2:55:21,  4.54s/it]

 45%|████▌     | 1920/4236 [2:55:58<2:55:04,  4.54s/it]

 45%|████▌     | 1921/4236 [2:56:07<3:44:55,  5.83s/it]

 45%|████▌     | 1922/4236 [2:56:12<3:44:20,  5.82s/it]

 45%|████▌     | 1923/4236 [2:56:16<3:23:26,  5.28s/it]
{'loss': 1.9081, 'grad_norm': 0.318344430945092, 'learning_rate': 0.00011967776571612286, 'epoch': 0.45}


 45%|████▌     | 1925/4236 [2:56:25<3:03:35,  4.77s/it]
{'loss': 1.7514, 'grad_norm': 0.30086611691033316, 'learning_rate': 0.00011952778323145725, 'epoch': 0.45}


 45%|████▌     | 1927/4236 [2:56:38<3:44:10,  5.83s/it]
{'loss': 1.8046, 'grad_norm': 0.3011259021217438, 'learning_rate': 0.00011937775506409548, 'epoch': 0.45}


 46%|████▌     | 1929/4236 [2:56:47<3:21:33,  5.24s/it]

 46%|████▌     | 1930/4236 [2:56:56<4:03:19,  6.33s/it]

 46%|████▌     | 1931/4236 [2:57:00<3:34:38,  5.59s/it]
{'loss': 1.6928, 'grad_norm': 0.3511038623029361, 'learning_rate': 0.0001190775630852746, 'epoch': 0.46}

 46%|████▌     | 1932/4236 [2:57:05<3:21:44,  5.25s/it]


 46%|████▌     | 1934/4236 [2:57:13<3:02:14,  4.75s/it]
{'loss': 1.6753, 'grad_norm': 0.32082270230304494, 'learning_rate': 0.00011885230179519962, 'epoch': 0.46}


 46%|████▌     | 1936/4236 [2:57:21<2:41:27,  4.21s/it]

 46%|████▌     | 1937/4236 [2:57:25<2:47:05,  4.36s/it]

 46%|████▌     | 1938/4236 [2:57:30<2:54:27,  4.55s/it]
{'loss': 1.7268, 'grad_norm': 0.3171522029972822, 'learning_rate': 0.00011855179925474364, 'epoch': 0.46}


 46%|████▌     | 1940/4236 [2:57:40<2:54:09,  4.55s/it]

 46%|████▌     | 1941/4236 [2:57:54<4:46:05,  7.48s/it]

 46%|████▌     | 1942/4236 [2:58:04<5:19:30,  8.36s/it]

 46%|████▌     | 1943/4236 [2:58:08<4:30:05,  7.07s/it]
{'loss': 1.7351, 'grad_norm': 0.3398676761744001, 'learning_rate': 0.00011817592728675677, 'epoch': 0.46}


 46%|████▌     | 1945/4236 [2:58:23<4:47:06,  7.52s/it]

 46%|████▌     | 1946/4236 [2:58:27<4:00:34,  6.30s/it]

 46%|████▌     | 1947/4236 [2:58:31<3:34:26,  5.62s/it]
{'loss': 1.7485, 'grad_norm': 0.3312955588972107, 'learning_rate': 0.00011787503810771465, 'epoch': 0.46}


 46%|████▌     | 1949/4236 [2:58:41<3:31:25,  5.55s/it]

 46%|████▌     | 1950/4236 [2:58:46<3:22:08,  5.31s/it]

 46%|████▌     | 1951/4236 [2:58:51<3:13:17,  5.08s/it]

 46%|████▌     | 1952/4236 [2:58:54<2:58:46,  4.70s/it]

 46%|████▌     | 1953/4236 [2:59:04<3:50:24,  6.06s/it]

 46%|████▌     | 1954/4236 [2:59:09<3:39:33,  5.77s/it]

 46%|████▌     | 1955/4236 [2:59:12<3:15:41,  5.15s/it]
{'loss': 1.7926, 'grad_norm': 0.29710156663530074, 'learning_rate': 0.00011727276077127108, 'epoch': 0.46}

 46%|████▌     | 1956/4236 [2:59:17<3:10:21,  5.01s/it]

 46%|████▌     | 1957/4236 [2:59:27<4:04:29,  6.44s/it]


 46%|████▌     | 1959/4236 [2:59:36<3:32:58,  5.61s/it]
{'loss': 1.6911, 'grad_norm': 0.2852756273234922, 'learning_rate': 0.00011697137824966273, 'epoch': 0.46}


 46%|████▋     | 1961/4236 [2:59:48<3:36:08,  5.70s/it]

 46%|████▋     | 1962/4236 [2:59:51<3:14:09,  5.12s/it]

 46%|████▋     | 1963/4236 [2:59:59<3:37:33,  5.74s/it]

 46%|████▋     | 1964/4236 [3:00:02<3:16:40,  5.19s/it]

 46%|████▋     | 1965/4236 [3:00:10<3:41:16,  5.85s/it]

 46%|████▋     | 1966/4236 [3:00:15<3:33:11,  5.63s/it]
{'loss': 1.5993, 'grad_norm': 0.31849907608805145, 'learning_rate': 0.00011644357839958553, 'epoch': 0.46}

 46%|████▋     | 1967/4236 [3:00:19<3:16:53,  5.21s/it]

 46%|████▋     | 1968/4236 [3:00:23<3:04:17,  4.88s/it]


 47%|████▋     | 1970/4236 [3:00:34<3:19:27,  5.28s/it]

 47%|████▋     | 1971/4236 [3:00:39<3:19:50,  5.29s/it]
{'loss': 1.7169, 'grad_norm': 0.3189924933092304, 'learning_rate': 0.00011606628911781123, 'epoch': 0.47}


 47%|████▋     | 1973/4236 [3:00:49<3:05:45,  4.93s/it]

 47%|████▋     | 1974/4236 [3:00:53<2:52:22,  4.57s/it]

 47%|████▋     | 1975/4236 [3:01:00<3:26:57,  5.49s/it]

 47%|████▋     | 1976/4236 [3:01:05<3:14:36,  5.17s/it]

 47%|████▋     | 1977/4236 [3:01:09<3:02:17,  4.84s/it]

 47%|████▋     | 1978/4236 [3:01:14<3:03:22,  4.87s/it]
{'loss': 1.5608, 'grad_norm': 0.28816936152889777, 'learning_rate': 0.00011553769071772889, 'epoch': 0.47}


 47%|████▋     | 1980/4236 [3:01:21<2:39:38,  4.25s/it]

 47%|████▋     | 1981/4236 [3:01:25<2:41:43,  4.30s/it]

 47%|████▋     | 1982/4236 [3:01:30<2:42:25,  4.32s/it]

 47%|████▋     | 1983/4236 [3:01:44<4:30:34,  7.21s/it]
{'loss': 1.7131, 'grad_norm': 0.29564015154039386, 'learning_rate': 0.0001151598469357976, 'epoch': 0.47}

 47%|████▋     | 1984/4236 [3:01:53<4:57:02,  7.91s/it]


 47%|████▋     | 1986/4236 [3:02:12<5:38:59,  9.04s/it]

 47%|████▋     | 1987/4236 [3:02:21<5:31:37,  8.85s/it]

 47%|████▋     | 1988/4236 [3:02:25<4:38:26,  7.43s/it]
{'loss': 1.6042, 'grad_norm': 0.3179047019089332, 'learning_rate': 0.000114781781501092, 'epoch': 0.47}


 47%|████▋     | 1990/4236 [3:02:38<4:28:04,  7.16s/it]

 47%|████▋     | 1991/4236 [3:02:43<4:01:57,  6.47s/it]
{'loss': 1.5728, 'grad_norm': 0.32839699134115524, 'learning_rate': 0.00011455483814633238, 'epoch': 0.47}

 47%|████▋     | 1992/4236 [3:02:47<3:38:48,  5.85s/it]


 47%|████▋     | 1994/4236 [3:02:58<3:23:33,  5.45s/it]

 47%|████▋     | 1995/4236 [3:03:03<3:18:44,  5.32s/it]

 47%|████▋     | 1996/4236 [3:03:08<3:15:21,  5.23s/it]
{'loss': 1.8788, 'grad_norm': 0.29800198897468466, 'learning_rate': 0.00011417642956614474, 'epoch': 0.47}


 47%|████▋     | 1998/4236 [3:03:21<3:48:02,  6.11s/it]

 47%|████▋     | 1999/4236 [3:03:26<3:35:53,  5.79s/it]

 47%|████▋     | 2000/4236 [3:03:30<3:15:25,  5.24s/it]

 47%|████▋     | 2001/4236 [3:03:40<4:15:19,  6.85s/it]

 47%|████▋     | 2002/4236 [3:03:45<3:47:49,  6.12s/it]

 47%|████▋     | 2003/4236 [3:03:49<3:28:52,  5.61s/it]

 47%|████▋     | 2004/4236 [3:03:56<3:43:45,  6.01s/it]
{'loss': 1.6253, 'grad_norm': 0.32217239331127434, 'learning_rate': 0.00011357054701028836, 'epoch': 0.47}


 47%|████▋     | 2006/4236 [3:04:06<3:28:18,  5.60s/it]

 47%|████▋     | 2007/4236 [3:04:10<3:10:24,  5.13s/it]

 47%|████▋     | 2008/4236 [3:04:15<3:01:42,  4.89s/it]

 47%|████▋     | 2009/4236 [3:04:21<3:16:10,  5.29s/it]
{'loss': 1.7276, 'grad_norm': 0.32288289892336736, 'learning_rate': 0.00011319161103282398, 'epoch': 0.47}

 47%|████▋     | 2010/4236 [3:04:27<3:28:42,  5.63s/it]


 47%|████▋     | 2012/4236 [3:04:36<3:06:43,  5.04s/it]

 48%|████▊     | 2013/4236 [3:04:40<2:54:16,  4.70s/it]

 48%|████▊     | 2014/4236 [3:04:45<2:49:54,  4.59s/it]

 48%|████▊     | 2015/4236 [3:04:48<2:40:50,  4.35s/it]

 48%|████▊     | 2016/4236 [3:04:53<2:46:19,  4.50s/it]
{'loss': 1.7693, 'grad_norm': 0.30575334716518676, 'learning_rate': 0.0001126607778757822, 'epoch': 0.48}


 48%|████▊     | 2018/4236 [3:05:02<2:41:43,  4.37s/it]

 48%|████▊     | 2019/4236 [3:05:06<2:36:18,  4.23s/it]

 48%|████▊     | 2020/4236 [3:05:10<2:42:58,  4.41s/it]

 48%|████▊     | 2021/4236 [3:05:15<2:46:49,  4.52s/it]

 48%|████▊     | 2022/4236 [3:05:19<2:36:44,  4.25s/it]

 48%|████▊     | 2023/4236 [3:05:26<3:13:56,  5.26s/it]

 48%|████▊     | 2024/4236 [3:05:30<2:59:48,  4.88s/it]

 48%|████▊     | 2025/4236 [3:05:34<2:46:06,  4.51s/it]

 48%|████▊     | 2026/4236 [3:05:38<2:42:29,  4.41s/it]
{'loss': 1.8787, 'grad_norm': 0.30880545416521216, 'learning_rate': 0.00011190181918019049, 'epoch': 0.48}


 48%|████▊     | 2028/4236 [3:05:47<2:39:58,  4.35s/it]

 48%|████▊     | 2029/4236 [3:05:50<2:33:26,  4.17s/it]

 48%|████▊     | 2030/4236 [3:05:56<2:50:25,  4.64s/it]

 48%|████▊     | 2031/4236 [3:06:06<3:52:37,  6.33s/it]

 48%|████▊     | 2032/4236 [3:06:10<3:28:15,  5.67s/it]

 48%|████▊     | 2033/4236 [3:06:19<4:02:06,  6.59s/it]
{'loss': 1.7785, 'grad_norm': 0.36558734834862344, 'learning_rate': 0.0001113701312916225, 'epoch': 0.48}


 48%|████▊     | 2035/4236 [3:06:27<3:13:02,  5.26s/it]

 48%|████▊     | 2036/4236 [3:06:35<3:41:38,  6.04s/it]

 48%|████▊     | 2037/4236 [3:06:39<3:15:25,  5.33s/it]

 48%|████▊     | 2038/4236 [3:06:44<3:19:27,  5.44s/it]

 48%|████▊     | 2039/4236 [3:06:55<4:16:16,  7.00s/it]

 48%|████▊     | 2040/4236 [3:07:00<3:55:44,  6.44s/it]

 48%|████▊     | 2041/4236 [3:07:07<3:57:05,  6.48s/it]

 48%|████▊     | 2042/4236 [3:07:12<3:43:13,  6.10s/it]

 48%|████▊     | 2043/4236 [3:07:19<3:53:08,  6.38s/it]

 48%|████▊     | 2044/4236 [3:07:33<5:12:05,  8.54s/it]

 48%|████▊     | 2045/4236 [3:07:36<4:17:16,  7.05s/it]

 48%|████▊     | 2046/4236 [3:07:41<3:56:29,  6.48s/it]

 48%|████▊     | 2047/4236 [3:07:47<3:45:34,  6.18s/it]

 48%|████▊     | 2048/4236 [3:07:51<3:22:00,  5.54s/it]

 48%|████▊     | 2049/4236 [3:07:55<3:00:56,  4.96s/it]

 48%|████▊     | 2050/4236 [3:07:59<2:50:06,  4.67s/it]

 48%|████▊     | 2051/4236 [3:08:02<2:40:03,  4.40s/it]

 48%|████▊     | 2052/4236 [3:08:07<2:42:25,  4.46s/it]

 48%|████▊     | 2053/4236 [3:08:11<2:43:16,  4.49s/it]

 48%|████▊     | 2054/4236 [3:08:15<2:35:39,  4.28s/it]

 49%|████▊     | 2055/4236 [3:08:19<2:31:49,  4.18s/it]

 49%|████▊     | 2056/4236 [3:08:24<2:41:22,  4.44s/it]

 49%|████▊     | 2057/4236 [3:08:28<2:37:42,  4.34s/it]

 49%|████▊     | 2058/4236 [3:08:32<2:34:18,  4.25s/it]

 49%|████▊     | 2059/4236 [3:08:37<2:39:34,  4.40s/it]

 49%|████▊     | 2060/4236 [3:08:41<2:32:27,  4.20s/it]

 49%|████▊     | 2061/4236 [3:08:45<2:29:03,  4.11s/it]
{'loss': 1.5915, 'grad_norm': 0.30126453116122415, 'learning_rate': 0.00010924027387921818, 'epoch': 0.49}

 49%|████▊     | 2062/4236 [3:08:50<2:39:44,  4.41s/it]


 49%|████▊     | 2064/4236 [3:09:03<3:25:42,  5.68s/it]

 49%|████▊     | 2065/4236 [3:09:07<3:07:56,  5.19s/it]
{'loss': 1.8516, 'grad_norm': 0.33416555688805594, 'learning_rate': 0.00010893563988239772, 'epoch': 0.49}


 49%|████▉     | 2067/4236 [3:09:17<2:57:33,  4.91s/it]
{'loss': 1.7721, 'grad_norm': 0.28214674777510657, 'learning_rate': 0.0001087832913501701, 'epoch': 0.49}

 49%|████▉     | 2068/4236 [3:09:22<2:58:02,  4.93s/it]


 49%|████▉     | 2070/4236 [3:09:32<3:05:15,  5.13s/it]

 49%|████▉     | 2071/4236 [3:09:36<2:52:36,  4.78s/it]

 49%|████▉     | 2072/4236 [3:09:43<3:08:36,  5.23s/it]

 49%|████▉     | 2073/4236 [3:09:49<3:24:13,  5.67s/it]

 49%|████▉     | 2074/4236 [3:09:57<3:46:50,  6.30s/it]

 49%|████▉     | 2075/4236 [3:10:01<3:24:27,  5.68s/it]

 49%|████▉     | 2076/4236 [3:10:06<3:15:20,  5.43s/it]

 49%|████▉     | 2077/4236 [3:10:11<3:04:09,  5.12s/it]

 49%|████▉     | 2078/4236 [3:10:15<3:00:28,  5.02s/it]

 49%|████▉     | 2079/4236 [3:10:19<2:46:45,  4.64s/it]

 49%|████▉     | 2080/4236 [3:10:23<2:40:43,  4.47s/it]

 49%|████▉     | 2081/4236 [3:10:27<2:29:43,  4.17s/it]

 49%|████▉     | 2082/4236 [3:10:31<2:28:15,  4.13s/it]

 49%|████▉     | 2083/4236 [3:10:35<2:28:18,  4.13s/it]
{'loss': 1.9136, 'grad_norm': 0.31104669314772104, 'learning_rate': 0.00010756379333475904, 'epoch': 0.49}


 49%|████▉     | 2085/4236 [3:10:48<3:21:14,  5.61s/it]

 49%|████▉     | 2086/4236 [3:10:54<3:26:03,  5.75s/it]

 49%|████▉     | 2087/4236 [3:10:58<3:05:36,  5.18s/it]
{'loss': 1.709, 'grad_norm': 0.36568931061910825, 'learning_rate': 0.00010725873475203331, 'epoch': 0.49}


 49%|████▉     | 2089/4236 [3:11:08<2:59:21,  5.01s/it]

 49%|████▉     | 2090/4236 [3:11:13<3:02:11,  5.09s/it]

 49%|████▉     | 2091/4236 [3:11:19<3:03:00,  5.12s/it]
{'loss': 1.6938, 'grad_norm': 0.302446278612098, 'learning_rate': 0.00010695360824590303, 'epoch': 0.49}


 49%|████▉     | 2093/4236 [3:11:30<3:03:31,  5.14s/it]

 49%|████▉     | 2094/4236 [3:11:39<3:52:39,  6.52s/it]

 49%|████▉     | 2095/4236 [3:11:43<3:24:45,  5.74s/it]

 49%|████▉     | 2096/4236 [3:11:47<3:03:42,  5.15s/it]

 50%|████▉     | 2097/4236 [3:11:54<3:27:10,  5.81s/it]
{'loss': 1.8682, 'grad_norm': 0.325723844412933, 'learning_rate': 0.00010649579737626887, 'epoch': 0.5}

 50%|████▉     | 2098/4236 [3:11:58<3:05:30,  5.21s/it]


 50%|████▉     | 2100/4236 [3:12:12<3:52:13,  6.52s/it]
 50%|████▉     | 2100/4236 [3:12:12<3:52:13,  6.52s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|████▉     | 2101/4236 [3:12:33<6:25:38, 10.84s/it]

 50%|████▉     | 2102/4236 [3:12:38<5:22:16,  9.06s/it]

 50%|████▉     | 2103/4236 [3:12:47<5:23:29,  9.10s/it]

 50%|████▉     | 2104/4236 [3:12:53<4:44:49,  8.02s/it]

 50%|████▉     | 2105/4236 [3:13:00<4:30:56,  7.63s/it]
{'loss': 1.7051, 'grad_norm': 0.3203079409979455, 'learning_rate': 0.00010588517180519006, 'epoch': 0.5}


 50%|████▉     | 2107/4236 [3:13:09<3:39:54,  6.20s/it]

 50%|████▉     | 2108/4236 [3:13:13<3:10:36,  5.37s/it]

 50%|████▉     | 2109/4236 [3:13:16<2:53:18,  4.89s/it]

 50%|████▉     | 2110/4236 [3:13:23<3:16:09,  5.54s/it]

 50%|████▉     | 2111/4236 [3:13:27<2:57:43,  5.02s/it]

 50%|████▉     | 2112/4236 [3:13:31<2:48:51,  4.77s/it]

 50%|████▉     | 2113/4236 [3:13:35<2:38:40,  4.48s/it]

 50%|████▉     | 2114/4236 [3:13:40<2:43:51,  4.63s/it]
{'loss': 1.6766, 'grad_norm': 0.2898868516582382, 'learning_rate': 0.00010519795587197767, 'epoch': 0.5}

 50%|████▉     | 2115/4236 [3:13:44<2:35:01,  4.39s/it]


 50%|████▉     | 2117/4236 [3:13:52<2:29:26,  4.23s/it]

 50%|█████     | 2118/4236 [3:13:59<2:51:30,  4.86s/it]

 50%|█████     | 2119/4236 [3:14:02<2:40:23,  4.55s/it]

 50%|█████     | 2120/4236 [3:14:11<3:21:18,  5.71s/it]

 50%|█████     | 2121/4236 [3:14:16<3:19:31,  5.66s/it]

 50%|█████     | 2122/4236 [3:14:21<3:11:08,  5.43s/it]

 50%|█████     | 2123/4236 [3:14:25<2:54:56,  4.97s/it]

 50%|█████     | 2124/4236 [3:14:31<3:03:53,  5.22s/it]

 50%|█████     | 2125/4236 [3:14:36<2:56:32,  5.02s/it]

 50%|█████     | 2126/4236 [3:14:39<2:42:54,  4.63s/it]

 50%|█████     | 2127/4236 [3:14:45<2:50:28,  4.85s/it]

 50%|█████     | 2128/4236 [3:14:53<3:29:39,  5.97s/it]

 50%|█████     | 2129/4236 [3:14:57<3:06:10,  5.30s/it]

 50%|█████     | 2130/4236 [3:15:03<3:12:34,  5.49s/it]

 50%|█████     | 2131/4236 [3:15:10<3:29:40,  5.98s/it]

 50%|█████     | 2132/4236 [3:15:14<3:05:29,  5.29s/it]
{'loss': 1.6159, 'grad_norm': 0.3384213174228121, 'learning_rate': 0.00010382281785856724, 'epoch': 0.5}


 50%|█████     | 2134/4236 [3:15:23<2:52:06,  4.91s/it]
{'loss': 1.725, 'grad_norm': 0.31196426740787564, 'learning_rate': 0.00010366997526445928, 'epoch': 0.5}

 50%|█████     | 2135/4236 [3:15:28<2:57:53,  5.08s/it]


 50%|█████     | 2137/4236 [3:15:36<2:33:29,  4.39s/it]

 50%|█████     | 2138/4236 [3:15:40<2:28:07,  4.24s/it]

 50%|█████     | 2139/4236 [3:15:44<2:25:18,  4.16s/it]

 51%|█████     | 2140/4236 [3:15:49<2:39:04,  4.55s/it]

 51%|█████     | 2141/4236 [3:15:57<3:18:29,  5.68s/it]

 51%|█████     | 2142/4236 [3:16:02<3:02:26,  5.23s/it]

 51%|█████     | 2143/4236 [3:16:06<2:50:45,  4.90s/it]

 51%|█████     | 2144/4236 [3:16:10<2:42:03,  4.65s/it]

 51%|█████     | 2145/4236 [3:16:14<2:35:10,  4.45s/it]

 51%|█████     | 2146/4236 [3:16:18<2:33:49,  4.42s/it]

 51%|█████     | 2147/4236 [3:16:29<3:39:29,  6.30s/it]

 51%|█████     | 2148/4236 [3:16:33<3:21:00,  5.78s/it]

 51%|█████     | 2149/4236 [3:16:37<2:58:27,  5.13s/it]
{'loss': 1.7296, 'grad_norm': 0.3170562053093354, 'learning_rate': 0.0001025234068449557, 'epoch': 0.51}


 51%|█████     | 2151/4236 [3:16:47<2:52:18,  4.96s/it]

 51%|█████     | 2152/4236 [3:16:51<2:43:53,  4.72s/it]

 51%|█████     | 2153/4236 [3:16:56<2:47:14,  4.82s/it]
{'loss': 1.7326, 'grad_norm': 0.3509090106482203, 'learning_rate': 0.00010221759295692157, 'epoch': 0.51}


 51%|█████     | 2155/4236 [3:17:05<2:41:29,  4.66s/it]

 51%|█████     | 2156/4236 [3:17:09<2:34:20,  4.45s/it]

 51%|█████     | 2157/4236 [3:17:13<2:27:10,  4.25s/it]

 51%|█████     | 2158/4236 [3:17:19<2:44:17,  4.74s/it]

 51%|█████     | 2159/4236 [3:17:25<2:59:00,  5.17s/it]

 51%|█████     | 2160/4236 [3:17:30<3:00:18,  5.21s/it]
{'loss': 1.6413, 'grad_norm': 0.29529656436311164, 'learning_rate': 0.00010168237044218452, 'epoch': 0.51}


 51%|█████     | 2162/4236 [3:17:41<3:05:37,  5.37s/it]
{'loss': 1.8327, 'grad_norm': 0.2986107295068256, 'learning_rate': 0.00010152944019761914, 'epoch': 0.51}


 51%|█████     | 2164/4236 [3:17:50<2:52:18,  4.99s/it]

 51%|█████     | 2165/4236 [3:17:56<3:02:44,  5.29s/it]

 51%|█████     | 2166/4236 [3:18:01<2:59:49,  5.21s/it]

 51%|█████     | 2167/4236 [3:18:13<4:13:26,  7.35s/it]

 51%|█████     | 2168/4236 [3:18:18<3:48:00,  6.62s/it]
{'loss': 1.6879, 'grad_norm': 0.3836998080547197, 'learning_rate': 0.00010107062942745275, 'epoch': 0.51}


 51%|█████     | 2170/4236 [3:18:27<3:13:17,  5.61s/it]

 51%|█████▏    | 2171/4236 [3:18:31<2:55:04,  5.09s/it]

 51%|█████▏    | 2172/4236 [3:18:35<2:46:54,  4.85s/it]

 51%|█████▏    | 2173/4236 [3:18:40<2:43:48,  4.76s/it]

 51%|█████▏    | 2174/4236 [3:18:55<4:35:13,  8.01s/it]

 51%|█████▏    | 2175/4236 [3:19:01<4:06:17,  7.17s/it]

 51%|█████▏    | 2176/4236 [3:19:07<4:00:16,  7.00s/it]

 51%|█████▏    | 2177/4236 [3:19:12<3:36:11,  6.30s/it]

 51%|█████▏    | 2178/4236 [3:19:16<3:08:33,  5.50s/it]

 51%|█████▏    | 2179/4236 [3:19:25<3:44:44,  6.56s/it]
{'loss': 1.8367, 'grad_norm': 0.28209804923964776, 'learning_rate': 0.00010022942477343613, 'epoch': 0.51}


 51%|█████▏    | 2181/4236 [3:19:33<3:04:43,  5.39s/it]
{'loss': 1.6047, 'grad_norm': 0.3168629539918834, 'learning_rate': 0.00010007647498411313, 'epoch': 0.51}


 52%|█████▏    | 2183/4236 [3:19:53<4:29:12,  7.87s/it]

 52%|█████▏    | 2184/4236 [3:19:56<3:42:25,  6.50s/it]
{'loss': 1.6396, 'grad_norm': 0.35369695061751844, 'learning_rate': 9.98470500764996e-05, 'epoch': 0.52}


 52%|█████▏    | 2186/4236 [3:20:15<4:19:52,  7.61s/it]

 52%|█████▏    | 2187/4236 [3:20:19<3:41:00,  6.47s/it]

 52%|█████▏    | 2188/4236 [3:20:23<3:13:01,  5.66s/it]

 52%|█████▏    | 2189/4236 [3:20:27<3:00:50,  5.30s/it]

 52%|█████▏    | 2190/4236 [3:20:32<2:58:45,  5.24s/it]

 52%|█████▏    | 2191/4236 [3:20:36<2:43:29,  4.80s/it]

 52%|█████▏    | 2192/4236 [3:20:43<3:09:46,  5.57s/it]

 52%|█████▏    | 2193/4236 [3:20:47<2:52:30,  5.07s/it]

 52%|█████▏    | 2194/4236 [3:20:51<2:36:56,  4.61s/it]
{'loss': 1.6754, 'grad_norm': 0.3363971929577092, 'learning_rate': 9.908231298217289e-05, 'epoch': 0.52}


 52%|█████▏    | 2196/4236 [3:21:00<2:35:20,  4.57s/it]

 52%|█████▏    | 2197/4236 [3:21:06<2:57:27,  5.22s/it]

 52%|█████▏    | 2198/4236 [3:21:11<2:54:56,  5.15s/it]

 52%|█████▏    | 2199/4236 [3:21:26<4:28:19,  7.90s/it]

 52%|█████▏    | 2200/4236 [3:21:32<4:09:08,  7.34s/it]

 52%|█████▏    | 2201/4236 [3:21:41<4:30:29,  7.98s/it]

 52%|█████▏    | 2202/4236 [3:21:46<3:54:09,  6.91s/it]

 52%|█████▏    | 2203/4236 [3:21:55<4:24:51,  7.82s/it]

 52%|█████▏    | 2204/4236 [3:22:07<5:00:05,  8.86s/it]

 52%|█████▏    | 2205/4236 [3:22:12<4:17:52,  7.62s/it]

 52%|█████▏    | 2206/4236 [3:22:15<3:36:46,  6.41s/it]
{'loss': 1.7139, 'grad_norm': 0.3321637406359375, 'learning_rate': 9.816470324893581e-05, 'epoch': 0.52}


 52%|█████▏    | 2208/4236 [3:22:28<3:43:12,  6.60s/it]

 52%|█████▏    | 2209/4236 [3:22:33<3:25:03,  6.07s/it]

 52%|█████▏    | 2210/4236 [3:22:38<3:13:58,  5.74s/it]

 52%|█████▏    | 2211/4236 [3:22:42<2:58:27,  5.29s/it]

 52%|█████▏    | 2212/4236 [3:22:47<2:55:11,  5.19s/it]

 52%|█████▏    | 2213/4236 [3:22:54<3:10:07,  5.64s/it]

 52%|█████▏    | 2214/4236 [3:23:01<3:22:45,  6.02s/it]

 52%|█████▏    | 2215/4236 [3:23:06<3:11:55,  5.70s/it]

 52%|█████▏    | 2216/4236 [3:23:20<4:33:06,  8.11s/it]

 52%|█████▏    | 2217/4236 [3:23:24<3:51:58,  6.89s/it]
{'loss': 1.5963, 'grad_norm': 0.3053909708495064, 'learning_rate': 9.732369488694832e-05, 'epoch': 0.52}


 52%|█████▏    | 2219/4236 [3:23:35<3:27:41,  6.18s/it]

 52%|█████▏    | 2220/4236 [3:23:41<3:23:35,  6.06s/it]

 52%|█████▏    | 2221/4236 [3:23:47<3:22:55,  6.04s/it]

 52%|█████▏    | 2222/4236 [3:23:52<3:11:36,  5.71s/it]

 52%|█████▏    | 2223/4236 [3:23:58<3:12:32,  5.74s/it]

 53%|█████▎    | 2224/4236 [3:24:03<3:09:44,  5.66s/it]

 53%|█████▎    | 2225/4236 [3:24:07<2:51:06,  5.10s/it]

 53%|█████▎    | 2226/4236 [3:24:12<2:55:16,  5.23s/it]

 53%|█████▎    | 2227/4236 [3:24:16<2:41:24,  4.82s/it]

 53%|█████▎    | 2228/4236 [3:24:21<2:35:39,  4.65s/it]

 53%|█████▎    | 2229/4236 [3:24:24<2:26:19,  4.37s/it]

 53%|█████▎    | 2230/4236 [3:24:28<2:21:58,  4.25s/it]

 53%|█████▎    | 2231/4236 [3:24:33<2:21:49,  4.24s/it]

 53%|█████▎    | 2232/4236 [3:24:37<2:26:34,  4.39s/it]

 53%|█████▎    | 2233/4236 [3:24:42<2:29:39,  4.48s/it]

 53%|█████▎    | 2234/4236 [3:24:46<2:26:30,  4.39s/it]

 53%|█████▎    | 2235/4236 [3:25:02<4:19:11,  7.77s/it]

 53%|█████▎    | 2236/4236 [3:25:17<5:29:16,  9.88s/it]

 53%|█████▎    | 2237/4236 [3:25:20<4:26:28,  8.00s/it]

 53%|█████▎    | 2238/4236 [3:25:24<3:49:04,  6.88s/it]

 53%|█████▎    | 2239/4236 [3:25:31<3:46:23,  6.80s/it]
{'loss': 1.7761, 'grad_norm': 0.3185002068484755, 'learning_rate': 9.564230583430866e-05, 'epoch': 0.53}

 53%|█████▎    | 2240/4236 [3:25:41<4:16:30,  7.71s/it]


 53%|█████▎    | 2242/4236 [3:25:48<3:07:48,  5.65s/it]

 53%|█████▎    | 2243/4236 [3:25:55<3:21:01,  6.05s/it]

 53%|█████▎    | 2244/4236 [3:26:02<3:24:06,  6.15s/it]

 53%|█████▎    | 2245/4236 [3:26:05<2:57:43,  5.36s/it]

 53%|█████▎    | 2246/4236 [3:26:09<2:39:09,  4.80s/it]
{'loss': 1.6501, 'grad_norm': 0.31436828203947764, 'learning_rate': 9.510755440654484e-05, 'epoch': 0.53}


 53%|█████▎    | 2248/4236 [3:26:17<2:28:55,  4.49s/it]
{'loss': 1.5846, 'grad_norm': 0.28711102397634486, 'learning_rate': 9.49547933659829e-05, 'epoch': 0.53}


 53%|█████▎    | 2250/4236 [3:26:25<2:15:22,  4.09s/it]

 53%|█████▎    | 2251/4236 [3:26:31<2:42:26,  4.91s/it]

 53%|█████▎    | 2252/4236 [3:26:35<2:33:14,  4.63s/it]

 53%|█████▎    | 2253/4236 [3:26:41<2:38:38,  4.80s/it]

 53%|█████▎    | 2254/4236 [3:26:44<2:29:11,  4.52s/it]

 53%|█████▎    | 2255/4236 [3:26:48<2:21:33,  4.29s/it]

 53%|█████▎    | 2256/4236 [3:26:52<2:17:34,  4.17s/it]

 53%|█████▎    | 2257/4236 [3:26:57<2:26:34,  4.44s/it]

 53%|█████▎    | 2258/4236 [3:27:01<2:25:13,  4.41s/it]

 53%|█████▎    | 2259/4236 [3:27:06<2:23:27,  4.35s/it]

 53%|█████▎    | 2260/4236 [3:27:12<2:40:14,  4.87s/it]

 53%|█████▎    | 2261/4236 [3:27:18<2:56:13,  5.35s/it]

 53%|█████▎    | 2262/4236 [3:27:22<2:44:05,  4.99s/it]

 53%|█████▎    | 2263/4236 [3:27:28<2:51:43,  5.22s/it]

 53%|█████▎    | 2264/4236 [3:27:34<2:59:33,  5.46s/it]

 53%|█████▎    | 2265/4236 [3:27:38<2:44:43,  5.01s/it]

 53%|█████▎    | 2266/4236 [3:27:44<2:54:59,  5.33s/it]

 54%|█████▎    | 2267/4236 [3:27:48<2:42:36,  4.96s/it]
{'loss': 1.7712, 'grad_norm': 0.3476586312464032, 'learning_rate': 9.350420262373117e-05, 'epoch': 0.54}


 54%|█████▎    | 2269/4236 [3:28:02<3:08:49,  5.76s/it]

 54%|█████▎    | 2270/4236 [3:28:06<2:48:56,  5.16s/it]

 54%|█████▎    | 2271/4236 [3:28:09<2:34:01,  4.70s/it]

 54%|█████▎    | 2272/4236 [3:28:15<2:40:44,  4.91s/it]
{'loss': 1.6012, 'grad_norm': 0.31112291560910554, 'learning_rate': 9.312268365907989e-05, 'epoch': 0.54}


 54%|█████▎    | 2274/4236 [3:28:24<2:38:18,  4.84s/it]

 54%|█████▎    | 2275/4236 [3:28:30<2:47:05,  5.11s/it]

 54%|█████▎    | 2276/4236 [3:28:36<2:57:15,  5.43s/it]

 54%|█████▍    | 2277/4236 [3:28:41<2:57:02,  5.42s/it]

 54%|█████▍    | 2278/4236 [3:28:45<2:42:58,  4.99s/it]
{'loss': 1.7349, 'grad_norm': 0.3042286654916187, 'learning_rate': 9.26649941229481e-05, 'epoch': 0.54}


 54%|█████▍    | 2280/4236 [3:28:54<2:29:47,  4.59s/it]

 54%|█████▍    | 2281/4236 [3:29:00<2:46:17,  5.10s/it]

 54%|█████▍    | 2282/4236 [3:29:08<3:13:16,  5.93s/it]

 54%|█████▍    | 2283/4236 [3:29:12<2:52:49,  5.31s/it]

 54%|█████▍    | 2284/4236 [3:29:16<2:38:24,  4.87s/it]

 54%|█████▍    | 2285/4236 [3:29:20<2:38:43,  4.88s/it]
{'loss': 1.6096, 'grad_norm': 0.3520273731291283, 'learning_rate': 9.213121886049692e-05, 'epoch': 0.54}


 54%|█████▍    | 2287/4236 [3:29:28<2:22:02,  4.37s/it]

 54%|█████▍    | 2288/4236 [3:29:33<2:27:31,  4.54s/it]

 54%|█████▍    | 2289/4236 [3:29:38<2:29:17,  4.60s/it]

 54%|█████▍    | 2290/4236 [3:29:43<2:28:46,  4.59s/it]

 54%|█████▍    | 2291/4236 [3:29:52<3:14:55,  6.01s/it]
{'loss': 1.7293, 'grad_norm': 0.29903661259798237, 'learning_rate': 9.167387610484712e-05, 'epoch': 0.54}


 54%|█████▍    | 2293/4236 [3:30:08<3:39:31,  6.78s/it]

 54%|█████▍    | 2294/4236 [3:30:13<3:24:42,  6.32s/it]

 54%|█████▍    | 2295/4236 [3:30:20<3:27:27,  6.41s/it]

 54%|█████▍    | 2296/4236 [3:30:24<3:01:52,  5.63s/it]

 54%|█████▍    | 2297/4236 [3:30:29<2:54:44,  5.41s/it]

 54%|█████▍    | 2298/4236 [3:30:44<4:28:32,  8.31s/it]

 54%|█████▍    | 2299/4236 [3:30:51<4:15:11,  7.90s/it]
{'loss': 1.7856, 'grad_norm': 0.33134606765694796, 'learning_rate': 9.106436011760229e-05, 'epoch': 0.54}


 54%|█████▍    | 2301/4236 [3:31:00<3:21:47,  6.26s/it]

 54%|█████▍    | 2302/4236 [3:31:06<3:16:15,  6.09s/it]
{'loss': 1.616, 'grad_norm': 0.3491990264659747, 'learning_rate': 9.083587662517499e-05, 'epoch': 0.54}


 54%|█████▍    | 2304/4236 [3:31:14<2:41:32,  5.02s/it]

 54%|█████▍    | 2305/4236 [3:31:18<2:32:34,  4.74s/it]

 54%|█████▍    | 2306/4236 [3:31:22<2:25:13,  4.51s/it]

 54%|█████▍    | 2307/4236 [3:31:31<3:08:50,  5.87s/it]

 54%|█████▍    | 2308/4236 [3:31:36<3:00:21,  5.61s/it]

 55%|█████▍    | 2309/4236 [3:31:55<5:07:25,  9.57s/it]

 55%|█████▍    | 2310/4236 [3:32:00<4:30:37,  8.43s/it]
{'loss': 1.526, 'grad_norm': 0.2861213831756738, 'learning_rate': 9.022682639855882e-05, 'epoch': 0.55}


 55%|█████▍    | 2312/4236 [3:32:09<3:19:58,  6.24s/it]

 55%|█████▍    | 2313/4236 [3:32:13<2:57:02,  5.52s/it]

 55%|█████▍    | 2314/4236 [3:32:17<2:42:56,  5.09s/it]

 55%|█████▍    | 2315/4236 [3:32:22<2:39:41,  4.99s/it]

 55%|█████▍    | 2316/4236 [3:32:31<3:18:45,  6.21s/it]

 55%|█████▍    | 2317/4236 [3:32:34<2:55:15,  5.48s/it]

 55%|█████▍    | 2318/4236 [3:32:38<2:42:14,  5.08s/it]

 55%|█████▍    | 2319/4236 [3:32:53<4:08:37,  7.78s/it]
{'loss': 1.7188, 'grad_norm': 0.3006132064614113, 'learning_rate': 8.954208328325112e-05, 'epoch': 0.55}


 55%|█████▍    | 2321/4236 [3:33:02<3:13:53,  6.08s/it]

 55%|█████▍    | 2322/4236 [3:33:07<3:03:13,  5.74s/it]

 55%|█████▍    | 2323/4236 [3:33:12<2:59:27,  5.63s/it]

 55%|█████▍    | 2324/4236 [3:33:16<2:44:04,  5.15s/it]
{'loss': 1.5668, 'grad_norm': 0.31700077380772995, 'learning_rate': 8.91618824331948e-05, 'epoch': 0.55}


 55%|█████▍    | 2326/4236 [3:33:23<2:15:28,  4.26s/it]

 55%|█████▍    | 2327/4236 [3:33:27<2:16:36,  4.29s/it]

 55%|█████▍    | 2328/4236 [3:33:31<2:12:13,  4.16s/it]

 55%|█████▍    | 2329/4236 [3:33:34<2:05:20,  3.94s/it]

 55%|█████▌    | 2330/4236 [3:33:39<2:12:07,  4.16s/it]

 55%|█████▌    | 2331/4236 [3:33:43<2:07:31,  4.02s/it]

 55%|█████▌    | 2332/4236 [3:33:46<2:02:12,  3.85s/it]

 55%|█████▌    | 2333/4236 [3:33:52<2:17:33,  4.34s/it]

 55%|█████▌    | 2334/4236 [3:33:56<2:20:15,  4.42s/it]

 55%|█████▌    | 2335/4236 [3:34:08<3:32:11,  6.70s/it]

 55%|█████▌    | 2336/4236 [3:34:13<3:14:04,  6.13s/it]

 55%|█████▌    | 2337/4236 [3:34:19<3:09:19,  5.98s/it]

 55%|█████▌    | 2338/4236 [3:34:23<2:55:00,  5.53s/it]

 55%|█████▌    | 2339/4236 [3:34:27<2:37:06,  4.97s/it]
{'loss': 1.6447, 'grad_norm': 0.36844200275123046, 'learning_rate': 8.80222528945914e-05, 'epoch': 0.55}

 55%|█████▌    | 2340/4236 [3:34:32<2:33:47,  4.87s/it]


 55%|█████▌    | 2342/4236 [3:34:39<2:15:46,  4.30s/it]

 55%|█████▌    | 2343/4236 [3:34:44<2:18:56,  4.40s/it]

 55%|█████▌    | 2344/4236 [3:34:54<3:15:08,  6.19s/it]

 55%|█████▌    | 2345/4236 [3:35:02<3:32:30,  6.74s/it]

 55%|█████▌    | 2346/4236 [3:35:07<3:10:43,  6.05s/it]

 55%|█████▌    | 2347/4236 [3:35:10<2:47:59,  5.34s/it]

 55%|█████▌    | 2348/4236 [3:35:15<2:42:04,  5.15s/it]
{'loss': 1.7326, 'grad_norm': 0.3453632568725709, 'learning_rate': 8.733922212421785e-05, 'epoch': 0.55}


 55%|█████▌    | 2350/4236 [3:35:23<2:25:39,  4.63s/it]

 56%|█████▌    | 2351/4236 [3:35:35<3:29:12,  6.66s/it]

 56%|█████▌    | 2352/4236 [3:35:39<3:06:14,  5.93s/it]

 56%|█████▌    | 2353/4236 [3:35:42<2:43:24,  5.21s/it]

 56%|█████▌    | 2354/4236 [3:35:46<2:31:10,  4.82s/it]

 56%|█████▌    | 2355/4236 [3:35:54<3:01:01,  5.77s/it]

 56%|█████▌    | 2356/4236 [3:35:58<2:41:57,  5.17s/it]

 56%|█████▌    | 2357/4236 [3:36:07<3:14:06,  6.20s/it]

 56%|█████▌    | 2358/4236 [3:36:12<3:04:51,  5.91s/it]

 56%|█████▌    | 2359/4236 [3:36:19<3:14:19,  6.21s/it]

 56%|█████▌    | 2360/4236 [3:36:23<2:53:45,  5.56s/it]

 56%|█████▌    | 2361/4236 [3:36:35<3:50:57,  7.39s/it]

 56%|█████▌    | 2362/4236 [3:36:47<4:40:14,  8.97s/it]

 56%|█████▌    | 2363/4236 [3:36:55<4:32:36,  8.73s/it]

 56%|█████▌    | 2364/4236 [3:37:03<4:19:02,  8.30s/it]
{'loss': 1.6634, 'grad_norm': 0.3366239385600167, 'learning_rate': 8.612644680109319e-05, 'epoch': 0.56}

 56%|█████▌    | 2365/4236 [3:37:08<3:48:54,  7.34s/it]

 56%|█████▌    | 2366/4236 [3:37:14<3:34:18,  6.88s/it]


 56%|█████▌    | 2368/4236 [3:37:24<3:07:35,  6.03s/it]

 56%|█████▌    | 2369/4236 [3:37:29<2:52:36,  5.55s/it]

 56%|█████▌    | 2370/4236 [3:37:36<3:07:27,  6.03s/it]
{'loss': 1.6147, 'grad_norm': 0.2879289825150324, 'learning_rate': 8.56721818189763e-05, 'epoch': 0.56}


 56%|█████▌    | 2372/4236 [3:37:53<3:58:51,  7.69s/it]
[2024-05-25 06:12:30,816] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▌    | 2373/4236 [3:38:02<4:12:06,  8.12s/it]

 56%|█████▌    | 2374/4236 [3:38:06<3:36:33,  6.98s/it]

 56%|█████▌    | 2375/4236 [3:38:12<3:25:24,  6.62s/it]

 56%|█████▌    | 2376/4236 [3:38:19<3:33:12,  6.88s/it]

 56%|█████▌    | 2377/4236 [3:38:23<3:03:34,  5.93s/it]

 56%|█████▌    | 2378/4236 [3:38:27<2:41:13,  5.21s/it]

 56%|█████▌    | 2379/4236 [3:38:31<2:33:03,  4.95s/it]
{'loss': 1.7386, 'grad_norm': 0.35072887682614484, 'learning_rate': 8.499135294923184e-05, 'epoch': 0.56}


 56%|█████▌    | 2381/4236 [3:38:39<2:19:33,  4.51s/it]

 56%|█████▌    | 2382/4236 [3:38:53<3:39:43,  7.11s/it]
[2024-05-25 06:13:30,598] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▋    | 2383/4236 [3:39:11<5:26:17, 10.57s/it]

 56%|█████▋    | 2384/4236 [3:39:16<4:32:45,  8.84s/it]
{'loss': 1.637, 'grad_norm': 0.3744062500004401, 'learning_rate': 8.461341984307114e-05, 'epoch': 0.56}


 56%|█████▋    | 2386/4236 [3:39:24<3:14:25,  6.31s/it]

 56%|█████▋    | 2387/4236 [3:39:29<3:03:22,  5.95s/it]
{'loss': 1.7962, 'grad_norm': 0.2850877711794325, 'learning_rate': 8.438676761042913e-05, 'epoch': 0.56}


 56%|█████▋    | 2389/4236 [3:39:42<3:14:24,  6.32s/it]

 56%|█████▋    | 2390/4236 [3:39:47<2:57:26,  5.77s/it]

 56%|█████▋    | 2391/4236 [3:39:53<3:00:26,  5.87s/it]

 56%|█████▋    | 2392/4236 [3:40:01<3:18:10,  6.45s/it]

 56%|█████▋    | 2393/4236 [3:40:04<2:51:56,  5.60s/it]

 57%|█████▋    | 2394/4236 [3:40:11<2:58:23,  5.81s/it]
{'loss': 1.8917, 'grad_norm': 0.3120314157630813, 'learning_rate': 8.385823405678797e-05, 'epoch': 0.57}


 57%|█████▋    | 2396/4236 [3:40:20<2:44:01,  5.35s/it]

 57%|█████▋    | 2397/4236 [3:40:35<4:05:20,  8.00s/it]

 57%|█████▋    | 2398/4236 [3:40:38<3:25:28,  6.71s/it]

 57%|█████▋    | 2399/4236 [3:40:42<2:59:05,  5.85s/it]

 57%|█████▋    | 2400/4236 [3:40:46<2:43:33,  5.35s/it]
 57%|█████▋    | 2400/4236 [3:40:46<2:43:33,  5.35s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.7525, 'grad_norm': 0.3303996858063835, 'learning_rate': 8.333016308113629e-05, 'epoch': 0.57}

 57%|█████▋    | 2402/4236 [3:41:10<4:00:10,  7.86s/it]

 57%|█████▋    | 2403/4236 [3:41:14<3:23:29,  6.66s/it]

 57%|█████▋    | 2404/4236 [3:41:19<3:15:54,  6.42s/it]

 57%|█████▋    | 2405/4236 [3:41:25<3:07:44,  6.15s/it]

 57%|█████▋    | 2406/4236 [3:41:29<2:47:13,  5.48s/it]

 57%|█████▋    | 2407/4236 [3:41:32<2:30:00,  4.92s/it]
{'loss': 1.7091, 'grad_norm': 0.330783235063828, 'learning_rate': 8.287791046208796e-05, 'epoch': 0.57}

 57%|█████▋    | 2408/4236 [3:41:46<3:49:59,  7.55s/it]

 57%|█████▋    | 2409/4236 [3:41:50<3:18:40,  6.52s/it]


 57%|█████▋    | 2411/4236 [3:41:58<2:33:13,  5.04s/it]
{'loss': 1.6068, 'grad_norm': 0.31207229832683436, 'learning_rate': 8.257660840273579e-05, 'epoch': 0.57}

 57%|█████▋    | 2412/4236 [3:42:04<2:46:13,  5.47s/it]


 57%|█████▋    | 2414/4236 [3:42:13<2:31:36,  4.99s/it]

 57%|█████▋    | 2415/4236 [3:42:17<2:25:09,  4.78s/it]

 57%|█████▋    | 2416/4236 [3:42:22<2:19:56,  4.61s/it]

 57%|█████▋    | 2417/4236 [3:42:25<2:11:00,  4.32s/it]

 57%|█████▋    | 2418/4236 [3:42:29<2:05:02,  4.13s/it]

 57%|█████▋    | 2419/4236 [3:42:33<2:05:13,  4.14s/it]

 57%|█████▋    | 2420/4236 [3:42:37<2:05:53,  4.16s/it]
{'loss': 1.8276, 'grad_norm': 0.31056553311530544, 'learning_rate': 8.189927917289182e-05, 'epoch': 0.57}

 57%|█████▋    | 2421/4236 [3:42:42<2:14:11,  4.44s/it]

 57%|█████▋    | 2422/4236 [3:42:46<2:09:06,  4.27s/it]

 57%|█████▋    | 2423/4236 [3:42:52<2:22:36,  4.72s/it]


 57%|█████▋    | 2425/4236 [3:43:01<2:23:10,  4.74s/it]

 57%|█████▋    | 2426/4236 [3:43:05<2:16:41,  4.53s/it]

 57%|█████▋    | 2427/4236 [3:43:10<2:15:03,  4.48s/it]
{'loss': 1.6785, 'grad_norm': 0.30412789422905806, 'learning_rate': 8.13730587254664e-05, 'epoch': 0.57}


 57%|█████▋    | 2429/4236 [3:43:21<2:29:31,  4.96s/it]
{'loss': 1.6497, 'grad_norm': 0.31530143546989453, 'learning_rate': 8.122280741130176e-05, 'epoch': 0.57}

 57%|█████▋    | 2430/4236 [3:43:30<3:05:45,  6.17s/it]

 57%|█████▋    | 2431/4236 [3:43:34<2:46:38,  5.54s/it]

 57%|█████▋    | 2432/4236 [3:43:38<2:31:22,  5.03s/it]


 57%|█████▋    | 2434/4236 [3:43:49<2:40:50,  5.36s/it]

 57%|█████▋    | 2435/4236 [3:43:54<2:29:17,  4.97s/it]

 58%|█████▊    | 2436/4236 [3:44:04<3:14:09,  6.47s/it]
{'loss': 1.6673, 'grad_norm': 0.33952972672621323, 'learning_rate': 8.069727604092213e-05, 'epoch': 0.58}


 58%|█████▊    | 2438/4236 [3:44:12<2:37:17,  5.25s/it]

 58%|█████▊    | 2439/4236 [3:44:15<2:20:43,  4.70s/it]

 58%|█████▊    | 2440/4236 [3:44:23<2:46:19,  5.56s/it]

 58%|█████▊    | 2441/4236 [3:44:28<2:40:51,  5.38s/it]
{'loss': 1.7967, 'grad_norm': 0.3208591843728648, 'learning_rate': 8.032223428387719e-05, 'epoch': 0.58}


 58%|█████▊    | 2443/4236 [3:44:39<2:44:55,  5.52s/it]

 58%|█████▊    | 2444/4236 [3:44:44<2:34:13,  5.16s/it]
{'loss': 1.5524, 'grad_norm': 0.33671362682146333, 'learning_rate': 8.009734697942053e-05, 'epoch': 0.58}


 58%|█████▊    | 2446/4236 [3:44:55<2:39:27,  5.35s/it]

 58%|█████▊    | 2447/4236 [3:45:00<2:31:55,  5.10s/it]

 58%|█████▊    | 2448/4236 [3:45:05<2:34:31,  5.19s/it]

 58%|█████▊    | 2449/4236 [3:45:10<2:28:45,  4.99s/it]

 58%|█████▊    | 2450/4236 [3:45:13<2:18:25,  4.65s/it]

 58%|█████▊    | 2451/4236 [3:45:19<2:29:36,  5.03s/it]

 58%|█████▊    | 2452/4236 [3:45:30<3:16:20,  6.60s/it]

 58%|█████▊    | 2453/4236 [3:45:33<2:50:40,  5.74s/it]

 58%|█████▊    | 2454/4236 [3:45:37<2:35:07,  5.22s/it]
{'loss': 1.5762, 'grad_norm': 0.3137980940251658, 'learning_rate': 7.934848587054695e-05, 'epoch': 0.58}


 58%|█████▊    | 2456/4236 [3:45:46<2:20:35,  4.74s/it]

 58%|█████▊    | 2457/4236 [3:45:57<3:17:17,  6.65s/it]

 58%|█████▊    | 2458/4236 [3:46:01<2:55:18,  5.92s/it]

 58%|█████▊    | 2459/4236 [3:46:06<2:40:00,  5.40s/it]
{'loss': 1.6899, 'grad_norm': 0.33615150748091627, 'learning_rate': 7.897450549992609e-05, 'epoch': 0.58}

 58%|█████▊    | 2460/4236 [3:46:11<2:36:36,  5.29s/it]


 58%|█████▊    | 2462/4236 [3:46:20<2:25:00,  4.90s/it]

 58%|█████▊    | 2463/4236 [3:46:25<2:31:13,  5.12s/it]

 58%|█████▊    | 2464/4236 [3:46:30<2:24:53,  4.91s/it]
{'loss': 1.7228, 'grad_norm': 0.34876627459035586, 'learning_rate': 7.86008325439649e-05, 'epoch': 0.58}

 58%|█████▊    | 2465/4236 [3:46:38<2:57:15,  6.01s/it]


 58%|█████▊    | 2467/4236 [3:46:47<2:34:55,  5.25s/it]
{'loss': 1.577, 'grad_norm': 0.3058983015012477, 'learning_rate': 7.837677860242542e-05, 'epoch': 0.58}


 58%|█████▊    | 2469/4236 [3:46:59<2:49:06,  5.74s/it]

 58%|█████▊    | 2470/4236 [3:47:03<2:32:35,  5.18s/it]

 58%|█████▊    | 2471/4236 [3:47:08<2:28:22,  5.04s/it]
{'loss': 1.727, 'grad_norm': 0.2779411836287564, 'learning_rate': 7.807821726386022e-05, 'epoch': 0.58}


 58%|█████▊    | 2473/4236 [3:47:18<2:29:20,  5.08s/it]
{'loss': 1.6992, 'grad_norm': 0.33894053484713393, 'learning_rate': 7.792901334471369e-05, 'epoch': 0.58}

 58%|█████▊    | 2474/4236 [3:47:22<2:22:44,  4.86s/it]

 58%|█████▊    | 2475/4236 [3:47:26<2:14:18,  4.58s/it]

 58%|█████▊    | 2476/4236 [3:47:30<2:10:33,  4.45s/it]


 58%|█████▊    | 2478/4236 [3:47:41<2:22:47,  4.87s/it]

 59%|█████▊    | 2479/4236 [3:47:46<2:19:04,  4.75s/it]

 59%|█████▊    | 2480/4236 [3:47:49<2:09:19,  4.42s/it]

 59%|█████▊    | 2481/4236 [3:47:58<2:43:28,  5.59s/it]
{'loss': 1.7213, 'grad_norm': 0.30220234852855965, 'learning_rate': 7.733271747862265e-05, 'epoch': 0.59}


 59%|█████▊    | 2483/4236 [3:48:07<2:31:44,  5.19s/it]
{'loss': 1.6796, 'grad_norm': 0.3320169374860282, 'learning_rate': 7.718377520841938e-05, 'epoch': 0.59}


 59%|█████▊    | 2485/4236 [3:48:17<2:27:06,  5.04s/it]

 59%|█████▊    | 2486/4236 [3:48:21<2:19:23,  4.78s/it]

 59%|█████▊    | 2487/4236 [3:48:25<2:10:11,  4.47s/it]
{'loss': 1.5567, 'grad_norm': 0.32421974927734565, 'learning_rate': 7.688605114304487e-05, 'epoch': 0.59}


 59%|█████▉    | 2489/4236 [3:48:39<2:45:57,  5.70s/it]

 59%|█████▉    | 2490/4236 [3:48:44<2:36:42,  5.38s/it]
{'loss': 1.8447, 'grad_norm': 0.2756312398628157, 'learning_rate': 7.66628998808129e-05, 'epoch': 0.59}

 59%|█████▉    | 2491/4236 [3:48:49<2:33:51,  5.29s/it]

 59%|█████▉    | 2492/4236 [3:48:54<2:35:57,  5.37s/it]

 59%|█████▉    | 2493/4236 [3:49:00<2:41:22,  5.56s/it]

 59%|█████▉    | 2494/4236 [3:49:09<3:05:28,  6.39s/it]

 59%|█████▉    | 2495/4236 [3:49:15<3:01:04,  6.24s/it]


 59%|█████▉    | 2497/4236 [3:49:23<2:33:37,  5.30s/it]
{'loss': 1.7142, 'grad_norm': 0.32938173288397343, 'learning_rate': 7.614269332940409e-05, 'epoch': 0.59}

 59%|█████▉    | 2498/4236 [3:49:27<2:20:24,  4.85s/it]


 59%|█████▉    | 2500/4236 [3:49:48<3:56:18,  8.17s/it]

 59%|█████▉    | 2501/4236 [3:49:54<3:33:55,  7.40s/it]

 59%|█████▉    | 2502/4236 [3:50:00<3:20:45,  6.95s/it]

 59%|█████▉    | 2503/4236 [3:50:03<2:53:09,  6.00s/it]

 59%|█████▉    | 2504/4236 [3:50:10<2:55:01,  6.06s/it]
{'loss': 1.6891, 'grad_norm': 0.3341948271785995, 'learning_rate': 7.562317046186182e-05, 'epoch': 0.59}


 59%|█████▉    | 2506/4236 [3:50:20<2:38:32,  5.50s/it]

 59%|█████▉    | 2507/4236 [3:50:24<2:31:21,  5.25s/it]

 59%|█████▉    | 2508/4236 [3:50:32<2:50:18,  5.91s/it]

 59%|█████▉    | 2509/4236 [3:50:36<2:33:20,  5.33s/it]

 59%|█████▉    | 2510/4236 [3:50:40<2:22:42,  4.96s/it]

 59%|█████▉    | 2511/4236 [3:50:44<2:16:32,  4.75s/it]

 59%|█████▉    | 2512/4236 [3:50:58<3:32:48,  7.41s/it]

 59%|█████▉    | 2513/4236 [3:51:02<3:02:50,  6.37s/it]

 59%|█████▉    | 2514/4236 [3:51:05<2:40:27,  5.59s/it]
{'loss': 1.6644, 'grad_norm': 0.3066547445810311, 'learning_rate': 7.488221043049679e-05, 'epoch': 0.59}

 59%|█████▉    | 2515/4236 [3:51:09<2:22:08,  4.96s/it]


 59%|█████▉    | 2517/4236 [3:51:24<2:49:46,  5.93s/it]

 59%|█████▉    | 2518/4236 [3:51:28<2:35:23,  5.43s/it]

 59%|█████▉    | 2519/4236 [3:51:32<2:21:36,  4.95s/it]

 59%|█████▉    | 2520/4236 [3:51:36<2:11:52,  4.61s/it]

 60%|█████▉    | 2521/4236 [3:51:39<2:05:02,  4.37s/it]
{'loss': 1.7755, 'grad_norm': 0.34403028508258393, 'learning_rate': 7.4364409881292e-05, 'epoch': 0.6}

 60%|█████▉    | 2522/4236 [3:51:43<1:59:29,  4.18s/it]


 60%|█████▉    | 2524/4236 [3:51:54<2:19:52,  4.90s/it]
{'loss': 1.7289, 'grad_norm': 0.2864861452273696, 'learning_rate': 7.414271938689e-05, 'epoch': 0.6}

 60%|█████▉    | 2525/4236 [3:51:59<2:16:16,  4.78s/it]


 60%|█████▉    | 2527/4236 [3:52:06<2:02:31,  4.30s/it]

 60%|█████▉    | 2528/4236 [3:52:10<1:55:36,  4.06s/it]

 60%|█████▉    | 2529/4236 [3:52:14<1:56:15,  4.09s/it]
{'loss': 1.8727, 'grad_norm': 0.3153619610163125, 'learning_rate': 7.377353825403462e-05, 'epoch': 0.6}


 60%|█████▉    | 2531/4236 [3:52:32<2:53:49,  6.12s/it]

 60%|█████▉    | 2532/4236 [3:52:36<2:37:06,  5.53s/it]

 60%|█████▉    | 2533/4236 [3:52:41<2:29:50,  5.28s/it]
{'loss': 1.6782, 'grad_norm': 0.305070422022358, 'learning_rate': 7.347846917876544e-05, 'epoch': 0.6}

 60%|█████▉    | 2534/4236 [3:52:47<2:38:03,  5.57s/it]

 60%|█████▉    | 2535/4236 [3:52:51<2:29:25,  5.27s/it]

 60%|█████▉    | 2536/4236 [3:52:55<2:17:07,  4.84s/it]


 60%|█████▉    | 2538/4236 [3:53:04<2:08:50,  4.55s/it]
{'loss': 1.7144, 'grad_norm': 0.3001928009102896, 'learning_rate': 7.310998215337663e-05, 'epoch': 0.6}


 60%|█████▉    | 2540/4236 [3:53:14<2:21:05,  4.99s/it]

 60%|█████▉    | 2541/4236 [3:53:18<2:10:51,  4.63s/it]
{'loss': 1.6714, 'grad_norm': 0.30651853395901585, 'learning_rate': 7.288907831042279e-05, 'epoch': 0.6}


 60%|██████    | 2543/4236 [3:53:28<2:11:11,  4.65s/it]

 60%|██████    | 2544/4236 [3:53:32<2:05:16,  4.44s/it]

 60%|██████    | 2545/4236 [3:53:36<2:00:49,  4.29s/it]

 60%|██████    | 2546/4236 [3:53:42<2:23:13,  5.09s/it]

 60%|██████    | 2547/4236 [3:53:46<2:13:09,  4.73s/it]
{'loss': 1.7157, 'grad_norm': 0.33660111548624716, 'learning_rate': 7.24476998879914e-05, 'epoch': 0.6}


 60%|██████    | 2549/4236 [3:53:56<2:17:00,  4.87s/it]
{'loss': 1.6803, 'grad_norm': 0.3317650278275419, 'learning_rate': 7.230070219842955e-05, 'epoch': 0.6}


 60%|██████    | 2551/4236 [3:54:15<3:03:40,  6.54s/it]

 60%|██████    | 2552/4236 [3:54:18<2:39:05,  5.67s/it]

 60%|██████    | 2553/4236 [3:54:22<2:24:48,  5.16s/it]

 60%|██████    | 2554/4236 [3:54:28<2:28:08,  5.28s/it]

 60%|██████    | 2555/4236 [3:54:32<2:22:38,  5.09s/it]

 60%|██████    | 2556/4236 [3:54:37<2:15:18,  4.83s/it]

 60%|██████    | 2557/4236 [3:54:41<2:08:25,  4.59s/it]
{'loss': 1.7633, 'grad_norm': 0.31149001928277154, 'learning_rate': 7.171336286558534e-05, 'epoch': 0.6}

 60%|██████    | 2558/4236 [3:54:47<2:23:40,  5.14s/it]

 60%|██████    | 2559/4236 [3:54:51<2:14:20,  4.81s/it]

 60%|██████    | 2560/4236 [3:54:55<2:09:58,  4.65s/it]

 60%|██████    | 2561/4236 [3:54:59<2:03:15,  4.42s/it]

 60%|██████    | 2562/4236 [3:55:03<1:58:53,  4.26s/it]


 61%|██████    | 2564/4236 [3:55:12<2:04:04,  4.45s/it]

 61%|██████    | 2565/4236 [3:55:21<2:37:14,  5.65s/it]

 61%|██████    | 2566/4236 [3:55:27<2:39:14,  5.72s/it]

 61%|██████    | 2567/4236 [3:55:30<2:23:16,  5.15s/it]
{'loss': 1.6953, 'grad_norm': 0.30452974943399735, 'learning_rate': 7.098068015768648e-05, 'epoch': 0.61}

 61%|██████    | 2568/4236 [3:55:35<2:19:29,  5.02s/it]


 61%|██████    | 2570/4236 [3:55:44<2:10:48,  4.71s/it]

 61%|██████    | 2571/4236 [3:55:52<2:44:42,  5.94s/it]

 61%|██████    | 2572/4236 [3:55:58<2:38:30,  5.72s/it]

 61%|██████    | 2573/4236 [3:56:04<2:43:50,  5.91s/it]
{'loss': 1.7027, 'grad_norm': 0.2841671990960442, 'learning_rate': 7.054188243368747e-05, 'epoch': 0.61}

 61%|██████    | 2574/4236 [3:56:09<2:37:04,  5.67s/it]


 61%|██████    | 2576/4236 [3:56:26<3:24:07,  7.38s/it]
{'loss': 1.7598, 'grad_norm': 0.32519464298603723, 'learning_rate': 7.032271557719847e-05, 'epoch': 0.61}


 61%|██████    | 2578/4236 [3:56:35<2:37:38,  5.70s/it]
{'loss': 1.8555, 'grad_norm': 0.291377276975731, 'learning_rate': 7.017669105100106e-05, 'epoch': 0.61}


 61%|██████    | 2580/4236 [3:56:43<2:13:05,  4.82s/it]
{'loss': 1.7409, 'grad_norm': 0.3146113270181284, 'learning_rate': 7.003073629253639e-05, 'epoch': 0.61}

 61%|██████    | 2581/4236 [3:56:47<2:10:46,  4.74s/it]

 61%|██████    | 2582/4236 [3:56:51<2:02:59,  4.46s/it]

 61%|██████    | 2583/4236 [3:56:55<2:01:40,  4.42s/it]


 61%|██████    | 2585/4236 [3:57:06<2:13:24,  4.85s/it]
{'loss': 1.6944, 'grad_norm': 0.33200951859818906, 'learning_rate': 6.966615687051516e-05, 'epoch': 0.61}


 61%|██████    | 2587/4236 [3:57:18<2:27:18,  5.36s/it]

 61%|██████    | 2588/4236 [3:57:22<2:17:46,  5.02s/it]
{'loss': 1.579, 'grad_norm': 0.3407238265662633, 'learning_rate': 6.944762176238149e-05, 'epoch': 0.61}


 61%|██████    | 2590/4236 [3:57:34<2:26:08,  5.33s/it]
{'loss': 1.643, 'grad_norm': 0.30143623763681393, 'learning_rate': 6.93020209609194e-05, 'epoch': 0.61}

 61%|██████    | 2591/4236 [3:57:40<2:31:02,  5.51s/it]


 61%|██████    | 2593/4236 [3:57:55<2:52:02,  6.28s/it]

 61%|██████    | 2594/4236 [3:58:05<3:23:29,  7.44s/it]

 61%|██████▏   | 2595/4236 [3:58:08<2:52:13,  6.30s/it]
{'loss': 1.6522, 'grad_norm': 0.3248652594541839, 'learning_rate': 6.89383338877959e-05, 'epoch': 0.61}

 61%|██████▏   | 2596/4236 [3:58:13<2:38:59,  5.82s/it]


 61%|██████▏   | 2598/4236 [3:58:24<2:37:27,  5.77s/it]

 61%|██████▏   | 2599/4236 [3:58:28<2:25:12,  5.32s/it]

 61%|██████▏   | 2600/4236 [3:58:33<2:17:11,  5.03s/it]

 61%|██████▏   | 2601/4236 [3:58:37<2:10:11,  4.78s/it]

 61%|██████▏   | 2602/4236 [3:58:41<2:00:57,  4.44s/it]

 61%|██████▏   | 2603/4236 [3:58:44<1:55:50,  4.26s/it]

 61%|██████▏   | 2604/4236 [3:58:59<3:18:52,  7.31s/it]
[2024-05-25 06:33:36,814] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.6438, 'grad_norm': 0.29059291962012995, 'learning_rate': 6.82848451932566e-05, 'epoch': 0.61}


 62%|██████▏   | 2606/4236 [3:59:08<2:40:37,  5.91s/it]
{'loss': 1.756, 'grad_norm': 0.320596103118671, 'learning_rate': 6.813982842604794e-05, 'epoch': 0.62}

 62%|██████▏   | 2607/4236 [3:59:13<2:36:36,  5.77s/it]

 62%|██████▏   | 2608/4236 [3:59:17<2:21:39,  5.22s/it]

 62%|██████▏   | 2609/4236 [3:59:21<2:12:08,  4.87s/it]

 62%|██████▏   | 2610/4236 [3:59:26<2:07:43,  4.71s/it]


 62%|██████▏   | 2612/4236 [3:59:34<2:01:55,  4.50s/it]

 62%|██████▏   | 2613/4236 [3:59:41<2:18:08,  5.11s/it]
{'loss': 1.6843, 'grad_norm': 0.3351632056781219, 'learning_rate': 6.76328589106199e-05, 'epoch': 0.62}

 62%|██████▏   | 2614/4236 [3:59:45<2:13:04,  4.92s/it]

 62%|██████▏   | 2615/4236 [3:59:53<2:38:07,  5.85s/it]

 62%|██████▏   | 2616/4236 [3:59:57<2:24:50,  5.36s/it]


 62%|██████▏   | 2618/4236 [4:00:08<2:26:43,  5.44s/it]

 62%|██████▏   | 2619/4236 [4:00:13<2:22:30,  5.29s/it]
{'loss': 1.6896, 'grad_norm': 0.36369480462668224, 'learning_rate': 6.719905131742395e-05, 'epoch': 0.62}


 62%|██████▏   | 2621/4236 [4:00:25<2:29:08,  5.54s/it]

 62%|██████▏   | 2622/4236 [4:00:29<2:15:44,  5.05s/it]

 62%|██████▏   | 2623/4236 [4:00:33<2:07:53,  4.76s/it]
{'loss': 1.6519, 'grad_norm': 0.30559667503174903, 'learning_rate': 6.691022935915243e-05, 'epoch': 0.62}

 62%|██████▏   | 2624/4236 [4:00:38<2:09:22,  4.82s/it]

 62%|██████▏   | 2625/4236 [4:00:42<2:01:27,  4.52s/it]

 62%|██████▏   | 2626/4236 [4:00:46<1:57:52,  4.39s/it]


 62%|██████▏   | 2628/4236 [4:00:56<2:09:32,  4.83s/it]
{'loss': 1.7428, 'grad_norm': 0.35115197263881504, 'learning_rate': 6.654963765420865e-05, 'epoch': 0.62}

 62%|██████▏   | 2629/4236 [4:01:05<2:41:49,  6.04s/it]


 62%|██████▏   | 2631/4236 [4:01:14<2:20:42,  5.26s/it]

 62%|██████▏   | 2632/4236 [4:01:19<2:15:46,  5.08s/it]

 62%|██████▏   | 2633/4236 [4:01:23<2:09:50,  4.86s/it]

 62%|██████▏   | 2634/4236 [4:01:27<2:01:04,  4.53s/it]
{'loss': 1.5585, 'grad_norm': 0.3443633336624356, 'learning_rate': 6.611757365625637e-05, 'epoch': 0.62}


 62%|██████▏   | 2636/4236 [4:01:48<3:19:22,  7.48s/it]

 62%|██████▏   | 2637/4236 [4:01:52<2:51:59,  6.45s/it]

 62%|██████▏   | 2638/4236 [4:02:03<3:27:29,  7.79s/it]

 62%|██████▏   | 2639/4236 [4:02:08<3:05:51,  6.98s/it]
{'loss': 1.6975, 'grad_norm': 0.28343446597881844, 'learning_rate': 6.575806487478961e-05, 'epoch': 0.62}

 62%|██████▏   | 2640/4236 [4:02:12<2:38:57,  5.98s/it]


 62%|██████▏   | 2642/4236 [4:02:29<3:23:27,  7.66s/it]

 62%|██████▏   | 2643/4236 [4:02:33<2:54:49,  6.58s/it]

 62%|██████▏   | 2644/4236 [4:02:38<2:43:47,  6.17s/it]
{'loss': 1.6181, 'grad_norm': 0.29738588423414986, 'learning_rate': 6.539905674612957e-05, 'epoch': 0.62}

 62%|██████▏   | 2645/4236 [4:02:44<2:41:17,  6.08s/it]


 62%|██████▏   | 2647/4236 [4:03:01<3:04:31,  6.97s/it]

 63%|██████▎   | 2648/4236 [4:03:06<2:51:36,  6.48s/it]
{'loss': 1.5516, 'grad_norm': 0.3105213759462372, 'learning_rate': 6.511221424084748e-05, 'epoch': 0.63}


 63%|██████▎   | 2650/4236 [4:03:23<3:22:34,  7.66s/it]

 63%|██████▎   | 2651/4236 [4:03:26<2:49:00,  6.40s/it]
{'loss': 1.7426, 'grad_norm': 0.30536898429664827, 'learning_rate': 6.489729645554959e-05, 'epoch': 0.63}

 63%|██████▎   | 2652/4236 [4:03:30<2:25:11,  5.50s/it]


 63%|██████▎   | 2654/4236 [4:03:45<3:02:18,  6.91s/it]
{'loss': 1.7438, 'grad_norm': 0.3001838884805598, 'learning_rate': 6.468256343612576e-05, 'epoch': 0.63}

 63%|██████▎   | 2655/4236 [4:03:50<2:46:37,  6.32s/it]


 63%|██████▎   | 2657/4236 [4:03:59<2:25:52,  5.54s/it]

 63%|██████▎   | 2658/4236 [4:04:10<3:09:54,  7.22s/it]
{'loss': 1.4899, 'grad_norm': 0.33855077063593647, 'learning_rate': 6.439654211054498e-05, 'epoch': 0.63}


 63%|██████▎   | 2660/4236 [4:04:20<2:38:47,  6.05s/it]

 63%|██████▎   | 2661/4236 [4:04:25<2:25:59,  5.56s/it]

 63%|██████▎   | 2662/4236 [4:04:28<2:10:26,  4.97s/it]
{'loss': 1.7354, 'grad_norm': 0.32996736009410094, 'learning_rate': 6.411085394331166e-05, 'epoch': 0.63}


 63%|██████▎   | 2664/4236 [4:04:41<2:33:14,  5.85s/it]

 63%|██████▎   | 2665/4236 [4:04:47<2:36:06,  5.96s/it]

 63%|██████▎   | 2666/4236 [4:04:51<2:17:57,  5.27s/it]

 63%|██████▎   | 2667/4236 [4:04:54<2:06:01,  4.82s/it]
{'loss': 1.64, 'grad_norm': 0.3184886597242373, 'learning_rate': 6.37542163105316e-05, 'epoch': 0.63}

 63%|██████▎   | 2668/4236 [4:04:59<2:07:40,  4.89s/it]

 63%|██████▎   | 2669/4236 [4:05:04<2:01:39,  4.66s/it]

 63%|██████▎   | 2670/4236 [4:05:08<1:58:13,  4.53s/it]

 63%|██████▎   | 2671/4236 [4:05:12<1:56:16,  4.46s/it]


 63%|██████▎   | 2673/4236 [4:05:22<2:08:26,  4.93s/it]
{'loss': 1.7872, 'grad_norm': 0.27428947055950825, 'learning_rate': 6.332695114505413e-05, 'epoch': 0.63}

 63%|██████▎   | 2674/4236 [4:05:28<2:10:08,  5.00s/it]

 63%|██████▎   | 2675/4236 [4:05:32<2:04:49,  4.80s/it]

 63%|██████▎   | 2676/4236 [4:05:36<1:57:07,  4.50s/it]


 63%|██████▎   | 2678/4236 [4:05:45<1:57:18,  4.52s/it]

 63%|██████▎   | 2679/4236 [4:05:51<2:09:51,  5.00s/it]

 63%|██████▎   | 2680/4236 [4:05:57<2:16:19,  5.26s/it]

 63%|██████▎   | 2681/4236 [4:06:04<2:31:21,  5.84s/it]
{'loss': 1.5963, 'grad_norm': 0.32779502378659103, 'learning_rate': 6.275846689361693e-05, 'epoch': 0.63}

 63%|██████▎   | 2682/4236 [4:06:12<2:44:52,  6.37s/it]


 63%|██████▎   | 2684/4236 [4:06:22<2:31:17,  5.85s/it]

 63%|██████▎   | 2685/4236 [4:06:27<2:20:50,  5.45s/it]

 63%|██████▎   | 2686/4236 [4:06:31<2:11:22,  5.09s/it]

 63%|██████▎   | 2687/4236 [4:06:35<2:02:20,  4.74s/it]

 63%|██████▎   | 2688/4236 [4:06:45<2:41:26,  6.26s/it]
{'loss': 1.7297, 'grad_norm': 0.28654655952274866, 'learning_rate': 6.22621859165507e-05, 'epoch': 0.63}

 63%|██████▎   | 2689/4236 [4:06:50<2:32:48,  5.93s/it]

 64%|██████▎   | 2690/4236 [4:06:56<2:30:48,  5.85s/it]


 64%|██████▎   | 2692/4236 [4:07:05<2:15:14,  5.26s/it]

 64%|██████▎   | 2693/4236 [4:07:11<2:14:39,  5.24s/it]

 64%|██████▎   | 2694/4236 [4:07:14<2:03:31,  4.81s/it]

 64%|██████▎   | 2695/4236 [4:07:19<2:02:27,  4.77s/it]
{'loss': 1.7168, 'grad_norm': 0.3089062977621978, 'learning_rate': 6.176698639997806e-05, 'epoch': 0.64}

 64%|██████▎   | 2696/4236 [4:07:26<2:17:05,  5.34s/it]

 64%|██████▎   | 2697/4236 [4:07:32<2:20:53,  5.49s/it]

 64%|██████▎   | 2698/4236 [4:07:44<3:10:47,  7.44s/it]


 64%|██████▎   | 2700/4236 [4:07:52<2:31:45,  5.93s/it]
 64%|██████▎   | 2700/4236 [4:07:52<2:31:45,  5.93s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.7071, 'grad_norm': 0.32329151965852826, 'learning_rate': 6.134340118365936e-05, 'epoch': 0.64}
 64%|██████▍   | 2701/4236 [4:08:12<4:15:03,  9.97s/it]


 64%|██████▍   | 2703/4236 [4:08:23<3:19:54,  7.82s/it]
{'loss': 1.5425, 'grad_norm': 0.2944246917261756, 'learning_rate': 6.120238653547301e-05, 'epoch': 0.64}


 64%|██████▍   | 2705/4236 [4:08:39<3:13:43,  7.59s/it]
{'loss': 1.6348, 'grad_norm': 0.32073983375354087, 'learning_rate': 6.10614626492316e-05, 'epoch': 0.64}


 64%|██████▍   | 2707/4236 [4:08:49<2:41:31,  6.34s/it]

 64%|██████▍   | 2708/4236 [4:08:59<3:12:10,  7.55s/it]

 64%|██████▍   | 2709/4236 [4:09:05<2:54:44,  6.87s/it]

 64%|██████▍   | 2710/4236 [4:09:08<2:31:21,  5.95s/it]

 64%|██████▍   | 2711/4236 [4:09:12<2:16:05,  5.35s/it]
{'loss': 1.9237, 'grad_norm': 0.31055069321675466, 'learning_rate': 6.063923885784138e-05, 'epoch': 0.64}

 64%|██████▍   | 2712/4236 [4:09:18<2:15:22,  5.33s/it]

 64%|██████▍   | 2713/4236 [4:09:22<2:08:43,  5.07s/it]

 64%|██████▍   | 2714/4236 [4:09:36<3:16:47,  7.76s/it]

 64%|██████▍   | 2715/4236 [4:09:40<2:48:47,  6.66s/it]


 64%|██████▍   | 2717/4236 [4:09:53<2:54:14,  6.88s/it]

 64%|██████▍   | 2718/4236 [4:09:57<2:31:16,  5.98s/it]

 64%|██████▍   | 2719/4236 [4:10:02<2:17:38,  5.44s/it]

 64%|██████▍   | 2720/4236 [4:10:06<2:06:50,  5.02s/it]
{'loss': 1.8396, 'grad_norm': 0.311997141022892, 'learning_rate': 6.000745978069589e-05, 'epoch': 0.64}


 64%|██████▍   | 2722/4236 [4:10:17<2:13:52,  5.31s/it]

 64%|██████▍   | 2723/4236 [4:10:23<2:22:25,  5.65s/it]

 64%|██████▍   | 2724/4236 [4:10:30<2:25:52,  5.79s/it]

 64%|██████▍   | 2725/4236 [4:10:35<2:20:20,  5.57s/it]

 64%|██████▍   | 2726/4236 [4:10:39<2:09:01,  5.13s/it]
{'loss': 1.7763, 'grad_norm': 0.3343911108843274, 'learning_rate': 5.958732440139403e-05, 'epoch': 0.64}

 64%|██████▍   | 2727/4236 [4:10:42<1:56:38,  4.64s/it]

 64%|██████▍   | 2728/4236 [4:10:47<1:54:07,  4.54s/it]

 64%|██████▍   | 2729/4236 [4:10:50<1:49:47,  4.37s/it]


 64%|██████▍   | 2731/4236 [4:11:01<2:07:36,  5.09s/it]

 64%|██████▍   | 2732/4236 [4:11:15<3:12:54,  7.70s/it]

 65%|██████▍   | 2733/4236 [4:11:19<2:43:03,  6.51s/it]
{'loss': 1.6644, 'grad_norm': 0.2997584361218756, 'learning_rate': 5.9098242471764295e-05, 'epoch': 0.65}

 65%|██████▍   | 2734/4236 [4:11:23<2:20:47,  5.62s/it]


 65%|██████▍   | 2736/4236 [4:11:31<2:03:13,  4.93s/it]

 65%|██████▍   | 2737/4236 [4:11:37<2:14:05,  5.37s/it]
{'loss': 1.5194, 'grad_norm': 0.3285497954689502, 'learning_rate': 5.8819292448395226e-05, 'epoch': 0.65}

 65%|██████▍   | 2738/4236 [4:11:42<2:10:14,  5.22s/it]


 65%|██████▍   | 2740/4236 [4:11:51<1:58:22,  4.75s/it]
{'loss': 1.5514, 'grad_norm': 0.31277541203507353, 'learning_rate': 5.8610332672422375e-05, 'epoch': 0.65}


 65%|██████▍   | 2742/4236 [4:11:59<1:49:43,  4.41s/it]

 65%|██████▍   | 2743/4236 [4:12:03<1:49:33,  4.40s/it]

 65%|██████▍   | 2744/4236 [4:12:09<1:57:25,  4.72s/it]

 65%|██████▍   | 2745/4236 [4:12:13<1:52:51,  4.54s/it]
{'loss': 1.691, 'grad_norm': 0.34554540960166824, 'learning_rate': 5.826255105022712e-05, 'epoch': 0.65}


 65%|██████▍   | 2747/4236 [4:12:35<3:13:27,  7.80s/it]

 65%|██████▍   | 2748/4236 [4:12:48<3:46:54,  9.15s/it]
{'loss': 1.6694, 'grad_norm': 0.3293567956275808, 'learning_rate': 5.805417466946242e-05, 'epoch': 0.65}


 65%|██████▍   | 2750/4236 [4:12:57<2:49:33,  6.85s/it]

 65%|██████▍   | 2751/4236 [4:13:01<2:27:55,  5.98s/it]

 65%|██████▍   | 2752/4236 [4:13:05<2:12:47,  5.37s/it]

 65%|██████▍   | 2753/4236 [4:13:09<2:04:54,  5.05s/it]

 65%|██████▌   | 2754/4236 [4:13:14<2:00:47,  4.89s/it]

 65%|██████▌   | 2755/4236 [4:13:28<3:08:17,  7.63s/it]

 65%|██████▌   | 2756/4236 [4:13:32<2:40:54,  6.52s/it]

 65%|██████▌   | 2757/4236 [4:13:36<2:23:24,  5.82s/it]

 65%|██████▌   | 2758/4236 [4:13:40<2:09:06,  5.24s/it]
{'loss': 1.6301, 'grad_norm': 0.2870718704753573, 'learning_rate': 5.736118744564059e-05, 'epoch': 0.65}

 65%|██████▌   | 2759/4236 [4:13:44<2:01:16,  4.93s/it]


 65%|██████▌   | 2761/4236 [4:13:53<1:56:44,  4.75s/it]

 65%|██████▌   | 2762/4236 [4:13:59<2:06:30,  5.15s/it]

 65%|██████▌   | 2763/4236 [4:14:03<1:56:54,  4.76s/it]

 65%|██████▌   | 2764/4236 [4:14:07<1:50:25,  4.50s/it]

 65%|██████▌   | 2765/4236 [4:14:13<2:01:49,  4.97s/it]
{'loss': 1.5538, 'grad_norm': 0.3048234865404537, 'learning_rate': 5.6877577726833174e-05, 'epoch': 0.65}

 65%|██████▌   | 2766/4236 [4:14:17<1:52:04,  4.57s/it]

 65%|██████▌   | 2767/4236 [4:14:20<1:44:26,  4.27s/it]
[2024-05-25 06:49:12,645] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 65%|██████▌   | 2769/4236 [4:14:39<2:37:35,  6.45s/it]
{'loss': 1.8243, 'grad_norm': 0.33148079739829295, 'learning_rate': 5.6601783263399556e-05, 'epoch': 0.65}


 65%|██████▌   | 2771/4236 [4:14:49<2:19:39,  5.72s/it]

 65%|██████▌   | 2772/4236 [4:14:53<2:07:23,  5.22s/it]

 65%|██████▌   | 2773/4236 [4:14:57<2:00:11,  4.93s/it]

 65%|██████▌   | 2774/4236 [4:15:06<2:23:37,  5.89s/it]
{'loss': 1.6669, 'grad_norm': 0.30945919083664714, 'learning_rate': 5.625761156090159e-05, 'epoch': 0.65}


 66%|██████▌   | 2776/4236 [4:15:16<2:08:45,  5.29s/it]
{'loss': 1.6765, 'grad_norm': 0.3567653375524556, 'learning_rate': 5.6120121675031566e-05, 'epoch': 0.66}

 66%|██████▌   | 2777/4236 [4:15:21<2:07:25,  5.24s/it]

 66%|██████▌   | 2778/4236 [4:15:25<1:56:54,  4.81s/it]

 66%|██████▌   | 2779/4236 [4:15:28<1:49:39,  4.52s/it]

 66%|██████▌   | 2780/4236 [4:15:32<1:46:45,  4.40s/it]


 66%|██████▌   | 2782/4236 [4:15:46<2:21:28,  5.84s/it]

 66%|██████▌   | 2783/4236 [4:15:50<2:09:40,  5.36s/it]

 66%|██████▌   | 2784/4236 [4:15:56<2:11:21,  5.43s/it]

 66%|██████▌   | 2785/4236 [4:16:01<2:13:53,  5.54s/it]

 66%|██████▌   | 2786/4236 [4:16:05<2:00:10,  4.97s/it]

 66%|██████▌   | 2787/4236 [4:16:19<3:07:11,  7.75s/it]

 66%|██████▌   | 2788/4236 [4:16:23<2:40:12,  6.64s/it]

 66%|██████▌   | 2789/4236 [4:16:27<2:19:31,  5.79s/it]
{'loss': 1.6608, 'grad_norm': 0.33763386527662487, 'learning_rate': 5.522895389123606e-05, 'epoch': 0.66}

 66%|██████▌   | 2790/4236 [4:16:31<2:04:29,  5.17s/it]


 66%|██████▌   | 2792/4236 [4:16:48<2:55:17,  7.28s/it]

 66%|██████▌   | 2793/4236 [4:16:54<2:46:34,  6.93s/it]

 66%|██████▌   | 2794/4236 [4:16:57<2:22:10,  5.92s/it]

 66%|██████▌   | 2795/4236 [4:17:07<2:51:02,  7.12s/it]

 66%|██████▌   | 2796/4236 [4:17:12<2:33:15,  6.39s/it]

 66%|██████▌   | 2797/4236 [4:17:16<2:13:34,  5.57s/it]

 66%|██████▌   | 2798/4236 [4:17:21<2:11:37,  5.49s/it]
{'loss': 1.7444, 'grad_norm': 0.3248597049505175, 'learning_rate': 5.4614578500302806e-05, 'epoch': 0.66}


 66%|██████▌   | 2800/4236 [4:17:31<2:03:48,  5.17s/it]

 66%|██████▌   | 2801/4236 [4:17:35<1:57:39,  4.92s/it]

 66%|██████▌   | 2802/4236 [4:17:39<1:51:47,  4.68s/it]

 66%|██████▌   | 2803/4236 [4:17:54<2:58:49,  7.49s/it]

 66%|██████▌   | 2804/4236 [4:17:59<2:47:19,  7.01s/it]

 66%|██████▌   | 2805/4236 [4:18:04<2:30:46,  6.32s/it]
{'loss': 1.5571, 'grad_norm': 0.31822159781178333, 'learning_rate': 5.4138215907226744e-05, 'epoch': 0.66}

 66%|██████▌   | 2806/4236 [4:18:09<2:19:28,  5.85s/it]


 66%|██████▋   | 2808/4236 [4:18:25<2:41:00,  6.77s/it]

 66%|██████▋   | 2809/4236 [4:18:30<2:22:47,  6.00s/it]
{'loss': 1.8132, 'grad_norm': 0.29071226688877594, 'learning_rate': 5.386659791791695e-05, 'epoch': 0.66}

 66%|██████▋   | 2810/4236 [4:18:35<2:18:49,  5.84s/it]


 66%|██████▋   | 2812/4236 [4:18:53<2:47:52,  7.07s/it]
{'loss': 1.7155, 'grad_norm': 0.33060833088884245, 'learning_rate': 5.3663167584965366e-05, 'epoch': 0.66}

 66%|██████▋   | 2813/4236 [4:18:58<2:35:20,  6.55s/it]

 66%|██████▋   | 2814/4236 [4:19:03<2:19:57,  5.91s/it]


 66%|██████▋   | 2816/4236 [4:19:11<2:00:24,  5.09s/it]

 67%|██████▋   | 2817/4236 [4:19:16<1:54:36,  4.85s/it]

 67%|██████▋   | 2818/4236 [4:19:19<1:46:50,  4.52s/it]

 67%|██████▋   | 2819/4236 [4:19:25<1:57:09,  4.96s/it]
{'loss': 1.6921, 'grad_norm': 0.3101995539536591, 'learning_rate': 5.318944714707861e-05, 'epoch': 0.67}


 67%|██████▋   | 2821/4236 [4:19:33<1:45:34,  4.48s/it]

 67%|██████▋   | 2822/4236 [4:19:38<1:46:49,  4.53s/it]
{'loss': 1.6758, 'grad_norm': 0.2924035037037772, 'learning_rate': 5.298683396303594e-05, 'epoch': 0.67}

 67%|██████▋   | 2823/4236 [4:19:43<1:49:56,  4.67s/it]

 67%|██████▋   | 2824/4236 [4:19:49<1:57:36,  5.00s/it]

 67%|██████▋   | 2825/4236 [4:19:52<1:48:36,  4.62s/it]


 67%|██████▋   | 2827/4236 [4:20:02<1:50:12,  4.69s/it]

 67%|██████▋   | 2828/4236 [4:20:10<2:13:48,  5.70s/it]

 67%|██████▋   | 2829/4236 [4:20:16<2:15:58,  5.80s/it]

 67%|██████▋   | 2830/4236 [4:20:20<2:06:40,  5.41s/it]

 67%|██████▋   | 2831/4236 [4:20:24<1:54:13,  4.88s/it]

 67%|██████▋   | 2832/4236 [4:20:28<1:49:48,  4.69s/it]

 67%|██████▋   | 2833/4236 [4:20:32<1:44:24,  4.46s/it]
{'loss': 1.6636, 'grad_norm': 0.2911483243213535, 'learning_rate': 5.2246044188012266e-05, 'epoch': 0.67}

 67%|██████▋   | 2834/4236 [4:20:40<2:10:31,  5.59s/it]


 67%|██████▋   | 2836/4236 [4:20:49<1:57:46,  5.05s/it]
{'loss': 1.5099, 'grad_norm': 0.29599836656302214, 'learning_rate': 5.204459490896818e-05, 'epoch': 0.67}

 67%|██████▋   | 2837/4236 [4:20:53<1:48:13,  4.64s/it]


 67%|██████▋   | 2839/4236 [4:21:02<1:45:03,  4.51s/it]
{'loss': 1.7406, 'grad_norm': 0.2990506176045475, 'learning_rate': 5.184339804701574e-05, 'epoch': 0.67}


 67%|██████▋   | 2841/4236 [4:21:12<1:48:47,  4.68s/it]

 67%|██████▋   | 2842/4236 [4:21:15<1:42:10,  4.40s/it]

 67%|██████▋   | 2843/4236 [4:21:20<1:44:25,  4.50s/it]

 67%|██████▋   | 2844/4236 [4:21:26<1:51:43,  4.82s/it]
{'loss': 1.7733, 'grad_norm': 0.33093489136716886, 'learning_rate': 5.150863374636707e-05, 'epoch': 0.67}

 67%|██████▋   | 2845/4236 [4:21:31<1:56:44,  5.04s/it]

 67%|██████▋   | 2846/4236 [4:21:35<1:47:44,  4.65s/it]

 67%|██████▋   | 2847/4236 [4:21:39<1:41:53,  4.40s/it]


 67%|██████▋   | 2849/4236 [4:21:55<2:29:52,  6.48s/it]
{'loss': 1.6902, 'grad_norm': 0.2923043942550667, 'learning_rate': 5.117457844007065e-05, 'epoch': 0.67}

 67%|██████▋   | 2850/4236 [4:22:03<2:36:45,  6.79s/it]


 67%|██████▋   | 2852/4236 [4:22:14<2:17:02,  5.94s/it]

 67%|██████▋   | 2853/4236 [4:22:18<2:06:34,  5.49s/it]

 67%|██████▋   | 2854/4236 [4:22:22<1:53:35,  4.93s/it]

 67%|██████▋   | 2855/4236 [4:22:25<1:44:46,  4.55s/it]
{'loss': 1.6528, 'grad_norm': 0.30202878380187814, 'learning_rate': 5.077465482133161e-05, 'epoch': 0.67}
[2024-05-25 06:57:16,695] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 2856/4236 [4:22:39<2:45:21,  7.19s/it]


 67%|██████▋   | 2858/4236 [4:22:47<2:12:37,  5.77s/it]

 67%|██████▋   | 2859/4236 [4:22:52<2:04:57,  5.44s/it]
{'loss': 1.7397, 'grad_norm': 0.3549942682796186, 'learning_rate': 5.0508614337044945e-05, 'epoch': 0.67}

 68%|██████▊   | 2860/4236 [4:22:59<2:14:06,  5.85s/it]


 68%|██████▊   | 2862/4236 [4:23:08<1:59:03,  5.20s/it]
{'loss': 1.517, 'grad_norm': 0.3014334095397114, 'learning_rate': 5.03093877565685e-05, 'epoch': 0.68}

 68%|██████▊   | 2863/4236 [4:23:13<1:56:51,  5.11s/it]

 68%|██████▊   | 2864/4236 [4:23:17<1:48:09,  4.73s/it]


 68%|██████▊   | 2866/4236 [4:23:27<1:57:30,  5.15s/it]

 68%|██████▊   | 2867/4236 [4:23:31<1:49:23,  4.79s/it]

 68%|██████▊   | 2868/4236 [4:23:36<1:50:44,  4.86s/it]

 68%|██████▊   | 2869/4236 [4:23:40<1:43:18,  4.53s/it]

 68%|██████▊   | 2870/4236 [4:23:50<2:18:58,  6.10s/it]

 68%|██████▊   | 2871/4236 [4:23:55<2:08:34,  5.65s/it]

 68%|██████▊   | 2872/4236 [4:24:06<2:48:36,  7.42s/it]
{'loss': 1.6441, 'grad_norm': 0.2972130832196851, 'learning_rate': 4.964719401097444e-05, 'epoch': 0.68}


 68%|██████▊   | 2874/4236 [4:24:16<2:19:58,  6.17s/it]

 68%|██████▊   | 2875/4236 [4:24:22<2:16:47,  6.03s/it]

 68%|██████▊   | 2876/4236 [4:24:26<2:03:10,  5.43s/it]

 68%|██████▊   | 2877/4236 [4:24:46<3:42:57,  9.84s/it]
{'loss': 1.6585, 'grad_norm': 0.2915555662560798, 'learning_rate': 4.931719903617405e-05, 'epoch': 0.68}

 68%|██████▊   | 2878/4236 [4:24:55<3:35:59,  9.54s/it]


 68%|██████▊   | 2880/4236 [4:25:15<3:48:01, 10.09s/it]
{'loss': 1.7326, 'grad_norm': 0.3490030827556317, 'learning_rate': 4.9119557439950416e-05, 'epoch': 0.68}


 68%|██████▊   | 2882/4236 [4:25:25<2:55:42,  7.79s/it]
{'loss': 1.6399, 'grad_norm': 0.30627078706027433, 'learning_rate': 4.898794509678708e-05, 'epoch': 0.68}


 68%|██████▊   | 2884/4236 [4:25:37<2:26:45,  6.51s/it]

 68%|██████▊   | 2885/4236 [4:25:40<2:07:26,  5.66s/it]

 68%|██████▊   | 2886/4236 [4:25:44<1:54:22,  5.08s/it]

 68%|██████▊   | 2887/4236 [4:25:48<1:49:37,  4.88s/it]

 68%|██████▊   | 2888/4236 [4:25:52<1:43:05,  4.59s/it]
{'loss': 1.7298, 'grad_norm': 0.3193504435099546, 'learning_rate': 4.859382531366427e-05, 'epoch': 0.68}


 68%|██████▊   | 2890/4236 [4:26:03<1:49:44,  4.89s/it]

 68%|██████▊   | 2891/4236 [4:26:06<1:41:35,  4.53s/it]

 68%|██████▊   | 2892/4236 [4:26:14<2:05:06,  5.59s/it]
{'loss': 1.6782, 'grad_norm': 0.3171284368160539, 'learning_rate': 4.8331679569492484e-05, 'epoch': 0.68}

 68%|██████▊   | 2893/4236 [4:26:19<2:00:33,  5.39s/it]


 68%|██████▊   | 2895/4236 [4:26:30<1:57:01,  5.24s/it]

 68%|██████▊   | 2896/4236 [4:26:34<1:53:34,  5.09s/it]
{'loss': 1.7407, 'grad_norm': 0.31113050124614383, 'learning_rate': 4.807001731016374e-05, 'epoch': 0.68}

 68%|██████▊   | 2897/4236 [4:26:41<2:02:53,  5.51s/it]


 68%|██████▊   | 2899/4236 [4:26:52<2:00:09,  5.39s/it]
{'loss': 1.7878, 'grad_norm': 0.3249239409871834, 'learning_rate': 4.787408937569032e-05, 'epoch': 0.68}

 68%|██████▊   | 2900/4236 [4:26:55<1:49:21,  4.91s/it]


 69%|██████▊   | 2902/4236 [4:27:04<1:44:33,  4.70s/it]

 69%|██████▊   | 2903/4236 [4:27:08<1:38:26,  4.43s/it]

 69%|██████▊   | 2904/4236 [4:27:13<1:38:53,  4.45s/it]

 69%|██████▊   | 2905/4236 [4:27:16<1:33:50,  4.23s/it]

 69%|██████▊   | 2906/4236 [4:27:23<1:49:12,  4.93s/it]
{'loss': 1.6618, 'grad_norm': 0.3327215885080895, 'learning_rate': 4.741799296509741e-05, 'epoch': 0.69}


 69%|██████▊   | 2908/4236 [4:27:35<1:56:03,  5.24s/it]

 69%|██████▊   | 2909/4236 [4:27:40<1:55:45,  5.23s/it]

 69%|██████▊   | 2910/4236 [4:27:48<2:15:36,  6.14s/it]
{'loss': 1.7144, 'grad_norm': 0.31204520968340327, 'learning_rate': 4.715804215473809e-05, 'epoch': 0.69}


 69%|██████▊   | 2912/4236 [4:27:58<2:04:36,  5.65s/it]
{'loss': 1.7206, 'grad_norm': 0.31010950383330455, 'learning_rate': 4.702825202287944e-05, 'epoch': 0.69}


 69%|██████▉   | 2914/4236 [4:28:10<2:03:49,  5.62s/it]

 69%|██████▉   | 2915/4236 [4:28:15<1:58:10,  5.37s/it]

 69%|██████▉   | 2916/4236 [4:28:19<1:49:08,  4.96s/it]

 69%|██████▉   | 2917/4236 [4:28:22<1:40:18,  4.56s/it]

 69%|██████▉   | 2918/4236 [4:28:28<1:47:52,  4.91s/it]

 69%|██████▉   | 2919/4236 [4:28:36<2:09:42,  5.91s/it]

 69%|██████▉   | 2920/4236 [4:28:41<2:00:14,  5.48s/it]
{'loss': 1.5909, 'grad_norm': 0.3045940161804193, 'learning_rate': 4.651033373216092e-05, 'epoch': 0.69}


 69%|██████▉   | 2922/4236 [4:28:50<1:48:49,  4.97s/it]
{'loss': 1.8427, 'grad_norm': 0.30183067639860306, 'learning_rate': 4.638116623316332e-05, 'epoch': 0.69}

 69%|██████▉   | 2923/4236 [4:28:59<2:16:55,  6.26s/it]


 69%|██████▉   | 2925/4236 [4:29:08<1:58:36,  5.43s/it]

 69%|██████▉   | 2926/4236 [4:29:12<1:49:35,  5.02s/it]
{'loss': 1.6708, 'grad_norm': 0.30907024396458205, 'learning_rate': 4.6123207839802496e-05, 'epoch': 0.69}


 69%|██████▉   | 2928/4236 [4:29:22<1:47:19,  4.92s/it]

 69%|██████▉   | 2929/4236 [4:29:26<1:39:39,  4.57s/it]
{'loss': 1.6845, 'grad_norm': 0.3281356071626031, 'learning_rate': 4.593006976175375e-05, 'epoch': 0.69}

 69%|██████▉   | 2930/4236 [4:29:30<1:34:06,  4.32s/it]


 69%|██████▉   | 2932/4236 [4:29:39<1:39:39,  4.59s/it]

 69%|██████▉   | 2933/4236 [4:29:46<1:58:04,  5.44s/it]

 69%|██████▉   | 2934/4236 [4:29:50<1:44:33,  4.82s/it]

 69%|██████▉   | 2935/4236 [4:29:55<1:46:12,  4.90s/it]

 69%|██████▉   | 2936/4236 [4:30:00<1:50:19,  5.09s/it]

 69%|██████▉   | 2937/4236 [4:30:05<1:43:43,  4.79s/it]
{'loss': 1.7243, 'grad_norm': 0.3220347181754406, 'learning_rate': 4.541642902786863e-05, 'epoch': 0.69}

 69%|██████▉   | 2938/4236 [4:30:10<1:46:22,  4.92s/it]


 69%|██████▉   | 2940/4236 [4:30:18<1:38:46,  4.57s/it]

 69%|██████▉   | 2941/4236 [4:30:22<1:35:27,  4.42s/it]

 69%|██████▉   | 2942/4236 [4:30:26<1:31:50,  4.26s/it]

 69%|██████▉   | 2943/4236 [4:30:33<1:48:54,  5.05s/it]

 69%|██████▉   | 2944/4236 [4:30:37<1:41:35,  4.72s/it]

 70%|██████▉   | 2945/4236 [4:30:43<1:49:37,  5.10s/it]

 70%|██████▉   | 2946/4236 [4:30:47<1:41:07,  4.70s/it]
{'loss': 1.6941, 'grad_norm': 0.3188843096865939, 'learning_rate': 4.4841026242497094e-05, 'epoch': 0.7}


 70%|██████▉   | 2948/4236 [4:30:55<1:32:32,  4.31s/it]

 70%|██████▉   | 2949/4236 [4:31:05<2:09:28,  6.04s/it]

 70%|██████▉   | 2950/4236 [4:31:11<2:09:24,  6.04s/it]
{'loss': 1.733, 'grad_norm': 0.3086619887331475, 'learning_rate': 4.4586128794136295e-05, 'epoch': 0.7}

 70%|██████▉   | 2951/4236 [4:31:15<1:59:40,  5.59s/it]

 70%|██████▉   | 2952/4236 [4:31:20<1:50:49,  5.18s/it]


 70%|██████▉   | 2954/4236 [4:31:30<1:50:41,  5.18s/it]

 70%|██████▉   | 2955/4236 [4:31:34<1:43:19,  4.84s/it]

 70%|██████▉   | 2956/4236 [4:31:39<1:41:03,  4.74s/it]

 70%|██████▉   | 2957/4236 [4:31:43<1:35:57,  4.50s/it]

 70%|██████▉   | 2958/4236 [4:31:56<2:34:59,  7.28s/it]
[2024-05-25 07:06:34,381] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 2959/4236 [4:32:01<2:17:11,  6.45s/it]

 70%|██████▉   | 2960/4236 [4:32:05<2:00:21,  5.66s/it]

 70%|██████▉   | 2961/4236 [4:32:10<1:58:29,  5.58s/it]

 70%|██████▉   | 2962/4236 [4:32:15<1:54:39,  5.40s/it]
{'loss': 1.8101, 'grad_norm': 0.3212965405779958, 'learning_rate': 4.382455716792291e-05, 'epoch': 0.7}

 70%|██████▉   | 2963/4236 [4:32:24<2:14:02,  6.32s/it]


 70%|██████▉   | 2965/4236 [4:32:34<2:07:44,  6.03s/it]
{'loss': 1.5614, 'grad_norm': 0.3389696825654998, 'learning_rate': 4.363490096965122e-05, 'epoch': 0.7}

 70%|███████   | 2966/4236 [4:32:38<1:51:13,  5.26s/it]

 70%|███████   | 2967/4236 [4:32:44<1:54:59,  5.44s/it]


 70%|███████   | 2969/4236 [4:33:00<2:31:35,  7.18s/it]

 70%|███████   | 2970/4236 [4:33:08<2:39:18,  7.55s/it]

 70%|███████   | 2971/4236 [4:33:12<2:16:50,  6.49s/it]

 70%|███████   | 2972/4236 [4:33:17<2:05:08,  5.94s/it]
{'loss': 1.6617, 'grad_norm': 0.2979285435175776, 'learning_rate': 4.3193525326884435e-05, 'epoch': 0.7}


 70%|███████   | 2974/4236 [4:33:26<1:50:30,  5.25s/it]

 70%|███████   | 2975/4236 [4:33:30<1:42:44,  4.89s/it]
{'loss': 1.6638, 'grad_norm': 0.3448564196063466, 'learning_rate': 4.300486194261057e-05, 'epoch': 0.7}


 70%|███████   | 2977/4236 [4:33:40<1:43:01,  4.91s/it]
{'loss': 1.7289, 'grad_norm': 0.30045870722886936, 'learning_rate': 4.287925295766366e-05, 'epoch': 0.7}


 70%|███████   | 2979/4236 [4:34:00<2:39:05,  7.59s/it]

 70%|███████   | 2980/4236 [4:34:05<2:20:08,  6.69s/it]

 70%|███████   | 2981/4236 [4:34:09<2:02:46,  5.87s/it]

 70%|███████   | 2982/4236 [4:34:13<1:51:43,  5.35s/it]

 70%|███████   | 2983/4236 [4:34:17<1:45:02,  5.03s/it]

 70%|███████   | 2984/4236 [4:34:21<1:37:32,  4.67s/it]

 70%|███████   | 2985/4236 [4:34:25<1:33:33,  4.49s/it]
{'loss': 1.7065, 'grad_norm': 0.31077764129736724, 'learning_rate': 4.237815621684855e-05, 'epoch': 0.7}


 71%|███████   | 2987/4236 [4:34:35<1:35:42,  4.60s/it]

 71%|███████   | 2988/4236 [4:34:41<1:44:37,  5.03s/it]
{'loss': 1.6846, 'grad_norm': 0.3039614534411347, 'learning_rate': 4.219079997751515e-05, 'epoch': 0.71}

 71%|███████   | 2989/4236 [4:34:46<1:45:18,  5.07s/it]


 71%|███████   | 2991/4236 [4:34:59<2:02:38,  5.91s/it]

 71%|███████   | 2992/4236 [4:35:03<1:49:58,  5.30s/it]

 71%|███████   | 2993/4236 [4:35:06<1:39:08,  4.79s/it]

 71%|███████   | 2994/4236 [4:35:11<1:40:07,  4.84s/it]
{'loss': 1.7354, 'grad_norm': 0.3288791651044594, 'learning_rate': 4.181700133338783e-05, 'epoch': 0.71}

 71%|███████   | 2995/4236 [4:35:16<1:36:33,  4.67s/it]


 71%|███████   | 2997/4236 [4:35:24<1:33:23,  4.52s/it]
{'loss': 1.6801, 'grad_norm': 0.2760452898203589, 'learning_rate': 4.1630560896112806e-05, 'epoch': 0.71}

 71%|███████   | 2998/4236 [4:35:28<1:27:56,  4.26s/it]


 71%|███████   | 3000/4236 [4:35:47<2:33:51,  7.47s/it]
[2024-05-25 07:10:24,919] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 71%|███████   | 3000/4236 [4:35:47<2:33:51,  7.47s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.5909, 'grad_norm': 0.32696534618598844, 'learning_rate': 4.1382451732532665e-05, 'epoch': 0.71}

 71%|███████   | 3002/4236 [4:36:11<3:12:17,  9.35s/it]
{'loss': 1.5953, 'grad_norm': 0.3071050720934102, 'learning_rate': 4.132051005606694e-05, 'epoch': 0.71}


 71%|███████   | 3004/4236 [4:36:21<2:23:03,  6.97s/it]

 71%|███████   | 3005/4236 [4:36:25<2:06:10,  6.15s/it]

 71%|███████   | 3006/4236 [4:36:37<2:37:41,  7.69s/it]
{'loss': 1.832, 'grad_norm': 0.301837652566602, 'learning_rate': 4.107308689470426e-05, 'epoch': 0.71}

 71%|███████   | 3007/4236 [4:36:46<2:47:08,  8.16s/it]

 71%|███████   | 3008/4236 [4:36:50<2:22:26,  6.96s/it]

 71%|███████   | 3009/4236 [4:36:54<2:04:02,  6.07s/it]


 71%|███████   | 3011/4236 [4:37:03<1:45:43,  5.18s/it]

 71%|███████   | 3012/4236 [4:37:07<1:39:39,  4.88s/it]
{'loss': 1.7377, 'grad_norm': 0.3440703402783138, 'learning_rate': 4.070298676273886e-05, 'epoch': 0.71}

 71%|███████   | 3013/4236 [4:37:12<1:39:36,  4.89s/it]

 71%|███████   | 3014/4236 [4:37:16<1:36:10,  4.72s/it]


 71%|███████   | 3016/4236 [4:37:25<1:31:58,  4.52s/it]

 71%|███████   | 3017/4236 [4:37:29<1:29:39,  4.41s/it]
{'loss': 1.7443, 'grad_norm': 0.2784717972305296, 'learning_rate': 4.039552333829131e-05, 'epoch': 0.71}

 71%|███████   | 3018/4236 [4:37:46<2:46:31,  8.20s/it]


 71%|███████▏  | 3020/4236 [4:37:58<2:26:20,  7.22s/it]

 71%|███████▏  | 3021/4236 [4:38:03<2:08:10,  6.33s/it]
{'loss': 1.8322, 'grad_norm': 0.2847701592559317, 'learning_rate': 4.0150179848704614e-05, 'epoch': 0.71}

 71%|███████▏  | 3022/4236 [4:38:08<2:02:07,  6.04s/it]

 71%|███████▏  | 3023/4236 [4:38:12<1:48:34,  5.37s/it]

 71%|███████▏  | 3024/4236 [4:38:18<1:52:09,  5.55s/it]


 71%|███████▏  | 3026/4236 [4:38:29<1:50:23,  5.47s/it]

 71%|███████▏  | 3027/4236 [4:38:41<2:33:54,  7.64s/it]
{'loss': 1.692, 'grad_norm': 0.30422062464579613, 'learning_rate': 3.9783215410943174e-05, 'epoch': 0.71}


 72%|███████▏  | 3029/4236 [4:38:51<2:04:11,  6.17s/it]

 72%|███████▏  | 3030/4236 [4:38:59<2:14:50,  6.71s/it]

 72%|███████▏  | 3031/4236 [4:39:06<2:12:05,  6.58s/it]

 72%|███████▏  | 3032/4236 [4:39:11<2:04:36,  6.21s/it]

 72%|███████▏  | 3033/4236 [4:39:15<1:49:46,  5.48s/it]
{'loss': 1.5319, 'grad_norm': 0.30582080637673326, 'learning_rate': 3.941751879486746e-05, 'epoch': 0.72}


 72%|███████▏  | 3035/4236 [4:39:27<1:52:19,  5.61s/it]

 72%|███████▏  | 3036/4236 [4:39:32<1:48:21,  5.42s/it]

 72%|███████▏  | 3037/4236 [4:39:39<1:57:31,  5.88s/it]
{'loss': 1.5971, 'grad_norm': 0.3228104299143485, 'learning_rate': 3.917442919951508e-05, 'epoch': 0.72}


 72%|███████▏  | 3039/4236 [4:39:49<1:46:57,  5.36s/it]
{'loss': 1.7613, 'grad_norm': 0.3140346162383471, 'learning_rate': 3.9053097699960305e-05, 'epoch': 0.72}


 72%|███████▏  | 3041/4236 [4:39:57<1:38:05,  4.93s/it]

 72%|███████▏  | 3042/4236 [4:40:03<1:44:48,  5.27s/it]

 72%|███████▏  | 3043/4236 [4:40:07<1:35:34,  4.81s/it]

 72%|███████▏  | 3044/4236 [4:40:13<1:40:35,  5.06s/it]

 72%|███████▏  | 3045/4236 [4:40:18<1:39:22,  5.01s/it]
{'loss': 1.7708, 'grad_norm': 0.3020336198994435, 'learning_rate': 3.8689959798849395e-05, 'epoch': 0.72}


 72%|███████▏  | 3047/4236 [4:40:26<1:29:27,  4.51s/it]
{'loss': 1.6089, 'grad_norm': 0.34777124074833277, 'learning_rate': 3.856920030823436e-05, 'epoch': 0.72}


 72%|███████▏  | 3049/4236 [4:40:34<1:23:05,  4.20s/it]

 72%|███████▏  | 3050/4236 [4:40:37<1:19:06,  4.00s/it]

 72%|███████▏  | 3051/4236 [4:40:41<1:18:26,  3.97s/it]
{'loss': 1.6833, 'grad_norm': 0.3363884106418509, 'learning_rate': 3.832811273714569e-05, 'epoch': 0.72}


 72%|███████▏  | 3053/4236 [4:40:51<1:29:24,  4.54s/it]

 72%|███████▏  | 3054/4236 [4:40:55<1:23:56,  4.26s/it]

 72%|███████▏  | 3055/4236 [4:40:59<1:22:10,  4.18s/it]

 72%|███████▏  | 3056/4236 [4:41:06<1:37:11,  4.94s/it]

 72%|███████▏  | 3057/4236 [4:41:10<1:31:29,  4.66s/it]

 72%|███████▏  | 3058/4236 [4:41:13<1:24:07,  4.28s/it]
{'loss': 1.7174, 'grad_norm': 0.3213898652142163, 'learning_rate': 3.790759947167995e-05, 'epoch': 0.72}

 72%|███████▏  | 3059/4236 [4:41:19<1:31:32,  4.67s/it]

 72%|███████▏  | 3060/4236 [4:41:22<1:27:22,  4.46s/it]

 72%|███████▏  | 3061/4236 [4:41:26<1:21:54,  4.18s/it]


 72%|███████▏  | 3063/4236 [4:41:37<1:33:08,  4.76s/it]

 72%|███████▏  | 3064/4236 [4:41:41<1:29:44,  4.59s/it]

 72%|███████▏  | 3065/4236 [4:41:45<1:27:15,  4.47s/it]

 72%|███████▏  | 3066/4236 [4:41:49<1:23:51,  4.30s/it]

 72%|███████▏  | 3067/4236 [4:41:53<1:19:54,  4.10s/it]

 72%|███████▏  | 3068/4236 [4:41:57<1:17:54,  4.00s/it]

 72%|███████▏  | 3069/4236 [4:42:01<1:22:30,  4.24s/it]

 72%|███████▏  | 3070/4236 [4:42:07<1:30:51,  4.68s/it]

 72%|███████▏  | 3071/4236 [4:42:12<1:29:29,  4.61s/it]

 73%|███████▎  | 3072/4236 [4:42:16<1:26:33,  4.46s/it]

 73%|███████▎  | 3073/4236 [4:42:19<1:21:38,  4.21s/it]

 73%|███████▎  | 3074/4236 [4:42:23<1:19:30,  4.11s/it]
{'loss': 1.6705, 'grad_norm': 0.3120238762802601, 'learning_rate': 3.69531274089583e-05, 'epoch': 0.73}

 73%|███████▎  | 3075/4236 [4:42:30<1:35:33,  4.94s/it]

 73%|███████▎  | 3076/4236 [4:42:34<1:31:51,  4.75s/it]

 73%|███████▎  | 3077/4236 [4:42:38<1:25:12,  4.41s/it]


 73%|███████▎  | 3079/4236 [4:42:56<2:01:03,  6.28s/it]
{'loss': 1.7762, 'grad_norm': 0.31816876842658376, 'learning_rate': 3.665678399299388e-05, 'epoch': 0.73}


 73%|███████▎  | 3081/4236 [4:43:05<1:42:55,  5.35s/it]
{'loss': 1.6382, 'grad_norm': 0.338515196147064, 'learning_rate': 3.653850570471106e-05, 'epoch': 0.73}


 73%|███████▎  | 3083/4236 [4:43:20<1:59:47,  6.23s/it]

 73%|███████▎  | 3084/4236 [4:43:34<2:44:48,  8.58s/it]
{'loss': 1.726, 'grad_norm': 0.30035380690424845, 'learning_rate': 3.6361366720915976e-05, 'epoch': 0.73}


 73%|███████▎  | 3086/4236 [4:43:46<2:19:43,  7.29s/it]
{'loss': 1.7195, 'grad_norm': 0.2961230032435256, 'learning_rate': 3.62434601003546e-05, 'epoch': 0.73}

 73%|███████▎  | 3087/4236 [4:43:50<2:02:23,  6.39s/it]


 73%|███████▎  | 3089/4236 [4:44:01<1:51:02,  5.81s/it]

 73%|███████▎  | 3090/4236 [4:44:05<1:42:53,  5.39s/it]
{'loss': 1.5959, 'grad_norm': 0.3226498984558385, 'learning_rate': 3.6008094584982185e-05, 'epoch': 0.73}


 73%|███████▎  | 3092/4236 [4:44:20<1:58:06,  6.19s/it]

 73%|███████▎  | 3093/4236 [4:44:26<1:54:40,  6.02s/it]

 73%|███████▎  | 3094/4236 [4:44:32<1:56:51,  6.14s/it]

 73%|███████▎  | 3095/4236 [4:44:36<1:44:31,  5.50s/it]
{'loss': 1.8175, 'grad_norm': 0.2930003106646575, 'learning_rate': 3.5714730014261596e-05, 'epoch': 0.73}


 73%|███████▎  | 3097/4236 [4:44:46<1:37:23,  5.13s/it]

 73%|███████▎  | 3098/4236 [4:44:51<1:39:21,  5.24s/it]

 73%|███████▎  | 3099/4236 [4:44:57<1:43:21,  5.45s/it]

 73%|███████▎  | 3100/4236 [4:45:01<1:37:31,  5.15s/it]

 73%|███████▎  | 3101/4236 [4:45:05<1:29:40,  4.74s/it]

 73%|███████▎  | 3102/4236 [4:45:10<1:27:59,  4.66s/it]

 73%|███████▎  | 3103/4236 [4:45:14<1:25:05,  4.51s/it]
{'loss': 1.6892, 'grad_norm': 0.3182992572614359, 'learning_rate': 3.524730350912484e-05, 'epoch': 0.73}


 73%|███████▎  | 3105/4236 [4:45:24<1:28:24,  4.69s/it]

 73%|███████▎  | 3106/4236 [4:45:29<1:29:16,  4.74s/it]

 73%|███████▎  | 3107/4236 [4:45:33<1:26:03,  4.57s/it]
{'loss': 1.5697, 'grad_norm': 0.31609925937301564, 'learning_rate': 3.501449804676772e-05, 'epoch': 0.73}

 73%|███████▎  | 3108/4236 [4:45:37<1:21:15,  4.32s/it]

 73%|███████▎  | 3109/4236 [4:45:41<1:18:42,  4.19s/it]


 73%|███████▎  | 3111/4236 [4:45:49<1:18:54,  4.21s/it]
{'loss': 1.5627, 'grad_norm': 0.33747042233430113, 'learning_rate': 3.4782300684402134e-05, 'epoch': 0.73}


 73%|███████▎  | 3113/4236 [4:45:59<1:28:51,  4.75s/it]
{'loss': 1.7247, 'grad_norm': 0.2984301417889702, 'learning_rate': 3.466643071993282e-05, 'epoch': 0.73}

 74%|███████▎  | 3114/4236 [4:46:05<1:33:19,  4.99s/it]

 74%|███████▎  | 3115/4236 [4:46:08<1:27:08,  4.66s/it]


 74%|███████▎  | 3117/4236 [4:46:18<1:30:23,  4.85s/it]

 74%|███████▎  | 3118/4236 [4:46:23<1:30:19,  4.85s/it]
{'loss': 1.6981, 'grad_norm': 0.30586267402220846, 'learning_rate': 3.4377425072962465e-05, 'epoch': 0.74}


 74%|███████▎  | 3120/4236 [4:46:33<1:34:59,  5.11s/it]

 74%|███████▎  | 3121/4236 [4:46:38<1:33:29,  5.03s/it]
{'loss': 1.6626, 'grad_norm': 0.3308945995159679, 'learning_rate': 3.420448196076282e-05, 'epoch': 0.74}


 74%|███████▎  | 3123/4236 [4:46:48<1:31:18,  4.92s/it]
{'loss': 1.4779, 'grad_norm': 0.3130382612750663, 'learning_rate': 3.40893788964834e-05, 'epoch': 0.74}


 74%|███████▍  | 3125/4236 [4:46:56<1:23:24,  4.50s/it]

 74%|███████▍  | 3126/4236 [4:47:02<1:31:41,  4.96s/it]

 74%|███████▍  | 3127/4236 [4:47:06<1:26:32,  4.68s/it]

 74%|███████▍  | 3128/4236 [4:47:11<1:27:57,  4.76s/it]

 74%|███████▍  | 3129/4236 [4:47:18<1:40:08,  5.43s/it]

 74%|███████▍  | 3130/4236 [4:47:22<1:30:18,  4.90s/it]
{'loss': 1.756, 'grad_norm': 0.31034172128041626, 'learning_rate': 3.36877341759205e-05, 'epoch': 0.74}


 74%|███████▍  | 3132/4236 [4:47:40<2:04:35,  6.77s/it]

 74%|███████▍  | 3133/4236 [4:47:44<1:46:25,  5.79s/it]
{'loss': 1.6049, 'grad_norm': 0.33872440181425767, 'learning_rate': 3.3516181787051384e-05, 'epoch': 0.74}


 74%|███████▍  | 3135/4236 [4:47:53<1:36:29,  5.26s/it]

 74%|███████▍  | 3136/4236 [4:47:59<1:40:26,  5.48s/it]
{'loss': 1.6117, 'grad_norm': 0.30717728569228425, 'learning_rate': 3.334497934105111e-05, 'epoch': 0.74}


 74%|███████▍  | 3138/4236 [4:48:12<1:47:31,  5.88s/it]

 74%|███████▍  | 3139/4236 [4:48:18<1:46:48,  5.84s/it]

 74%|███████▍  | 3140/4236 [4:48:22<1:35:56,  5.25s/it]
{'loss': 1.6506, 'grad_norm': 0.3718345219640325, 'learning_rate': 3.311725532588049e-05, 'epoch': 0.74}

 74%|███████▍  | 3141/4236 [4:48:26<1:32:55,  5.09s/it]

 74%|███████▍  | 3142/4236 [4:48:31<1:28:11,  4.84s/it]

 74%|███████▍  | 3143/4236 [4:48:36<1:32:36,  5.08s/it]


 74%|███████▍  | 3145/4236 [4:48:50<1:51:55,  6.16s/it]
{'loss': 1.8166, 'grad_norm': 0.3192459545298209, 'learning_rate': 3.283348066240192e-05, 'epoch': 0.74}

 74%|███████▍  | 3146/4236 [4:48:55<1:44:25,  5.75s/it]


 74%|███████▍  | 3148/4236 [4:49:06<1:39:47,  5.50s/it]

 74%|███████▍  | 3149/4236 [4:49:09<1:29:48,  4.96s/it]
{'loss': 1.7406, 'grad_norm': 0.3065425569254301, 'learning_rate': 3.2607167805034755e-05, 'epoch': 0.74}


 74%|███████▍  | 3151/4236 [4:49:21<1:41:08,  5.59s/it]

 74%|███████▍  | 3152/4236 [4:49:25<1:31:26,  5.06s/it]

 74%|███████▍  | 3153/4236 [4:49:30<1:28:20,  4.89s/it]

 74%|███████▍  | 3154/4236 [4:49:34<1:24:06,  4.66s/it]

 74%|███████▍  | 3155/4236 [4:49:40<1:32:10,  5.12s/it]

 75%|███████▍  | 3156/4236 [4:49:46<1:37:38,  5.42s/it]

 75%|███████▍  | 3157/4236 [4:49:50<1:28:21,  4.91s/it]
{'loss': 1.6421, 'grad_norm': 0.32627549623573143, 'learning_rate': 3.2156436081663356e-05, 'epoch': 0.75}


 75%|███████▍  | 3159/4236 [4:49:58<1:21:05,  4.52s/it]
{'loss': 1.696, 'grad_norm': 0.33311570238166355, 'learning_rate': 3.20441492706012e-05, 'epoch': 0.75}


 75%|███████▍  | 3161/4236 [4:50:09<1:27:06,  4.86s/it]
{'loss': 1.6145, 'grad_norm': 0.29336118574037534, 'learning_rate': 3.19320214333685e-05, 'epoch': 0.75}

 75%|███████▍  | 3162/4236 [4:50:13<1:20:37,  4.50s/it]


 75%|███████▍  | 3164/4236 [4:50:22<1:19:26,  4.45s/it]

 75%|███████▍  | 3165/4236 [4:50:25<1:15:09,  4.21s/it]

 75%|███████▍  | 3166/4236 [4:50:36<1:52:12,  6.29s/it]
{'loss': 1.6015, 'grad_norm': 0.31604790592047727, 'learning_rate': 3.165239907127275e-05, 'epoch': 0.75}

 75%|███████▍  | 3167/4236 [4:50:45<2:03:46,  6.95s/it]

 75%|███████▍  | 3168/4236 [4:50:55<2:21:08,  7.93s/it]

 75%|███████▍  | 3169/4236 [4:50:59<2:00:16,  6.76s/it]

 75%|███████▍  | 3170/4236 [4:51:03<1:44:31,  5.88s/it]


 75%|███████▍  | 3172/4236 [4:51:14<1:40:57,  5.69s/it]

 75%|███████▍  | 3173/4236 [4:51:18<1:36:01,  5.42s/it]

 75%|███████▍  | 3174/4236 [4:51:22<1:27:46,  4.96s/it]
{'loss': 1.7706, 'grad_norm': 0.3488626002748388, 'learning_rate': 3.120708355869767e-05, 'epoch': 0.75}


 75%|███████▍  | 3176/4236 [4:51:32<1:27:37,  4.96s/it]

 75%|███████▌  | 3177/4236 [4:51:38<1:31:05,  5.16s/it]

 75%|███████▌  | 3178/4236 [4:51:42<1:27:37,  4.97s/it]
{'loss': 1.6475, 'grad_norm': 0.33164451695654185, 'learning_rate': 3.098539035380128e-05, 'epoch': 0.75}

 75%|███████▌  | 3179/4236 [4:51:47<1:27:02,  4.94s/it]

 75%|███████▌  | 3180/4236 [4:51:51<1:22:24,  4.68s/it]


 75%|███████▌  | 3182/4236 [4:52:01<1:24:20,  4.80s/it]

 75%|███████▌  | 3183/4236 [4:52:06<1:24:53,  4.84s/it]

 75%|███████▌  | 3184/4236 [4:52:10<1:21:01,  4.62s/it]

 75%|███████▌  | 3185/4236 [4:52:14<1:17:17,  4.41s/it]

 75%|███████▌  | 3186/4236 [4:52:18<1:15:30,  4.31s/it]

 75%|███████▌  | 3187/4236 [4:52:23<1:19:57,  4.57s/it]

 75%|███████▌  | 3188/4236 [4:52:29<1:22:53,  4.75s/it]
{'loss': 1.7829, 'grad_norm': 0.32035692513913255, 'learning_rate': 3.0433987249480678e-05, 'epoch': 0.75}

 75%|███████▌  | 3189/4236 [4:52:33<1:21:02,  4.64s/it]


 75%|███████▌  | 3191/4236 [4:52:41<1:13:00,  4.19s/it]
{'loss': 1.6389, 'grad_norm': 0.3362704563534491, 'learning_rate': 3.0269358213777276e-05, 'epoch': 0.75}

 75%|███████▌  | 3192/4236 [4:52:45<1:14:08,  4.26s/it]

 75%|███████▌  | 3193/4236 [4:52:51<1:21:28,  4.69s/it]


 75%|███████▌  | 3195/4236 [4:53:06<1:50:23,  6.36s/it]
{'loss': 1.8929, 'grad_norm': 0.34785431247401416, 'learning_rate': 3.0050423922218074e-05, 'epoch': 0.75}

 75%|███████▌  | 3196/4236 [4:53:09<1:36:29,  5.57s/it]


 75%|███████▌  | 3198/4236 [4:53:17<1:19:33,  4.60s/it]

 76%|███████▌  | 3199/4236 [4:53:21<1:20:11,  4.64s/it]

 76%|███████▌  | 3200/4236 [4:53:26<1:20:17,  4.65s/it]
{'loss': 1.7495, 'grad_norm': 0.3244848356246155, 'learning_rate': 2.9777676759853767e-05, 'epoch': 0.76}


 76%|███████▌  | 3202/4236 [4:53:48<2:19:24,  8.09s/it]

 76%|███████▌  | 3203/4236 [4:53:52<1:58:40,  6.89s/it]

 76%|███████▌  | 3204/4236 [4:53:56<1:45:55,  6.16s/it]
{'loss': 1.6367, 'grad_norm': 0.28824754284905707, 'learning_rate': 2.956021808004824e-05, 'epoch': 0.76}


 76%|███████▌  | 3206/4236 [4:54:05<1:28:22,  5.15s/it]

 76%|███████▌  | 3207/4236 [4:54:12<1:38:25,  5.74s/it]

 76%|███████▌  | 3208/4236 [4:54:16<1:29:26,  5.22s/it]

 76%|███████▌  | 3209/4236 [4:54:19<1:21:30,  4.76s/it]

 76%|███████▌  | 3210/4236 [4:54:27<1:33:36,  5.47s/it]

 76%|███████▌  | 3211/4236 [4:54:32<1:31:36,  5.36s/it]

 76%|███████▌  | 3212/4236 [4:54:40<1:48:45,  6.37s/it]

 76%|███████▌  | 3213/4236 [4:54:44<1:35:47,  5.62s/it]

 76%|███████▌  | 3214/4236 [4:54:48<1:25:44,  5.03s/it]

 76%|███████▌  | 3215/4236 [4:54:52<1:21:38,  4.80s/it]

 76%|███████▌  | 3216/4236 [4:54:58<1:24:54,  4.99s/it]
{'loss': 1.6465, 'grad_norm': 0.3283449752424728, 'learning_rate': 2.8911804978796665e-05, 'epoch': 0.76}


 76%|███████▌  | 3218/4236 [4:55:08<1:26:45,  5.11s/it]
{'loss': 1.7112, 'grad_norm': 0.28884980874460586, 'learning_rate': 2.8804316712623535e-05, 'epoch': 0.76}


 76%|███████▌  | 3220/4236 [4:55:20<1:32:36,  5.47s/it]

 76%|███████▌  | 3221/4236 [4:55:26<1:35:47,  5.66s/it]

 76%|███████▌  | 3222/4236 [4:55:30<1:29:57,  5.32s/it]

 76%|███████▌  | 3223/4236 [4:55:35<1:24:45,  5.02s/it]

 76%|███████▌  | 3224/4236 [4:55:38<1:18:08,  4.63s/it]
{'loss': 1.6939, 'grad_norm': 0.3302433507535266, 'learning_rate': 2.8482852235939672e-05, 'epoch': 0.76}

 76%|███████▌  | 3225/4236 [4:55:45<1:26:55,  5.16s/it]

 76%|███████▌  | 3226/4236 [4:55:49<1:21:34,  4.85s/it]


 76%|███████▌  | 3228/4236 [4:55:57<1:12:45,  4.33s/it]
{'loss': 1.6226, 'grad_norm': 0.27664766003623587, 'learning_rate': 2.8269378692116676e-05, 'epoch': 0.76}

 76%|███████▌  | 3229/4236 [4:56:01<1:13:50,  4.40s/it]

 76%|███████▋  | 3230/4236 [4:56:12<1:43:19,  6.16s/it]


 76%|███████▋  | 3232/4236 [4:56:19<1:21:51,  4.89s/it]

 76%|███████▋  | 3233/4236 [4:56:26<1:31:38,  5.48s/it]

 76%|███████▋  | 3234/4236 [4:56:30<1:25:51,  5.14s/it]

 76%|███████▋  | 3235/4236 [4:56:35<1:23:56,  5.03s/it]
{'loss': 1.6356, 'grad_norm': 0.281612991326515, 'learning_rate': 2.7897416305068323e-05, 'epoch': 0.76}

 76%|███████▋  | 3236/4236 [4:56:41<1:29:30,  5.37s/it]

 76%|███████▋  | 3237/4236 [4:56:55<2:14:04,  8.05s/it]

 76%|███████▋  | 3238/4236 [4:56:59<1:53:08,  6.80s/it]

 76%|███████▋  | 3239/4236 [4:57:03<1:37:51,  5.89s/it]


 77%|███████▋  | 3241/4236 [4:57:11<1:20:36,  4.86s/it]
{'loss': 1.745, 'grad_norm': 0.3207656500481228, 'learning_rate': 2.758023557292695e-05, 'epoch': 0.77}


 77%|███████▋  | 3243/4236 [4:57:21<1:22:49,  5.00s/it]
{'loss': 1.6872, 'grad_norm': 0.2988587991362713, 'learning_rate': 2.7474847165973648e-05, 'epoch': 0.77}


 77%|███████▋  | 3245/4236 [4:57:30<1:19:08,  4.79s/it]

 77%|███████▋  | 3246/4236 [4:57:36<1:23:47,  5.08s/it]

 77%|███████▋  | 3247/4236 [4:57:49<2:03:59,  7.52s/it]

 77%|███████▋  | 3248/4236 [4:57:54<1:52:38,  6.84s/it]

 77%|███████▋  | 3249/4236 [4:57:59<1:40:13,  6.09s/it]

 77%|███████▋  | 3250/4236 [4:58:03<1:30:51,  5.53s/it]

 77%|███████▋  | 3251/4236 [4:58:07<1:22:19,  5.01s/it]

 77%|███████▋  | 3252/4236 [4:58:14<1:34:55,  5.79s/it]

 77%|███████▋  | 3253/4236 [4:58:19<1:28:43,  5.42s/it]

 77%|███████▋  | 3254/4236 [4:58:23<1:22:27,  5.04s/it]

 77%|███████▋  | 3255/4236 [4:58:37<2:06:36,  7.74s/it]
{'loss': 1.7894, 'grad_norm': 0.3354867510936834, 'learning_rate': 2.6846088250825264e-05, 'epoch': 0.77}


 77%|███████▋  | 3257/4236 [4:58:46<1:40:06,  6.14s/it]

 77%|███████▋  | 3258/4236 [4:58:52<1:38:53,  6.07s/it]

 77%|███████▋  | 3259/4236 [4:58:56<1:31:11,  5.60s/it]

 77%|███████▋  | 3260/4236 [4:59:02<1:32:57,  5.71s/it]
{'loss': 1.8099, 'grad_norm': 0.3160695756229921, 'learning_rate': 2.6585920635126836e-05, 'epoch': 0.77}


 77%|███████▋  | 3262/4236 [4:59:12<1:24:53,  5.23s/it]

 77%|███████▋  | 3263/4236 [4:59:16<1:18:28,  4.84s/it]
{'loss': 1.692, 'grad_norm': 0.2998033577597643, 'learning_rate': 2.6430335050307264e-05, 'epoch': 0.77}


 77%|███████▋  | 3265/4236 [4:59:24<1:11:08,  4.40s/it]

 77%|███████▋  | 3266/4236 [4:59:28<1:10:38,  4.37s/it]

 77%|███████▋  | 3267/4236 [4:59:32<1:07:37,  4.19s/it]
{'loss': 1.7331, 'grad_norm': 0.2986112202038588, 'learning_rate': 2.6223490118134408e-05, 'epoch': 0.77}


 77%|███████▋  | 3269/4236 [4:59:43<1:17:15,  4.79s/it]
{'loss': 1.5886, 'grad_norm': 0.33272135126364605, 'learning_rate': 2.6120326416928088e-05, 'epoch': 0.77}


 77%|███████▋  | 3271/4236 [5:00:01<1:47:05,  6.66s/it]
{'loss': 1.6597, 'grad_norm': 0.29508114136008184, 'learning_rate': 2.6017335547560452e-05, 'epoch': 0.77}

 77%|███████▋  | 3272/4236 [5:00:06<1:37:38,  6.08s/it]


 77%|███████▋  | 3274/4236 [5:00:15<1:27:26,  5.45s/it]

 77%|███████▋  | 3275/4236 [5:00:20<1:24:48,  5.29s/it]

 77%|███████▋  | 3276/4236 [5:00:25<1:21:04,  5.07s/it]

 77%|███████▋  | 3277/4236 [5:00:29<1:18:18,  4.90s/it]

 77%|███████▋  | 3278/4236 [5:00:33<1:13:11,  4.58s/it]

 77%|███████▋  | 3279/4236 [5:00:45<1:47:30,  6.74s/it]
{'loss': 1.5932, 'grad_norm': 0.31366981936824867, 'learning_rate': 2.5607105201073324e-05, 'epoch': 0.77}


 77%|███████▋  | 3281/4236 [5:00:53<1:25:45,  5.39s/it]

 77%|███████▋  | 3282/4236 [5:00:59<1:27:03,  5.48s/it]

 78%|███████▊  | 3283/4236 [5:01:05<1:30:09,  5.68s/it]

 78%|███████▊  | 3284/4236 [5:01:09<1:21:30,  5.14s/it]

 78%|███████▊  | 3285/4236 [5:01:15<1:26:19,  5.45s/it]

 78%|███████▊  | 3286/4236 [5:01:28<2:04:46,  7.88s/it]
{'loss': 1.7371, 'grad_norm': 0.26752260519324506, 'learning_rate': 2.5250437294891637e-05, 'epoch': 0.78}


 78%|███████▊  | 3288/4236 [5:01:43<1:56:42,  7.39s/it]

 78%|███████▊  | 3289/4236 [5:01:47<1:40:32,  6.37s/it]

 78%|███████▊  | 3290/4236 [5:01:53<1:38:49,  6.27s/it]

 78%|███████▊  | 3291/4236 [5:02:00<1:44:06,  6.61s/it]

 78%|███████▊  | 3292/4236 [5:02:05<1:32:14,  5.86s/it]

 78%|███████▊  | 3293/4236 [5:02:09<1:25:59,  5.47s/it]

 78%|███████▊  | 3294/4236 [5:02:13<1:18:04,  4.97s/it]

 78%|███████▊  | 3295/4236 [5:02:17<1:12:21,  4.61s/it]
{'loss': 1.6211, 'grad_norm': 0.32257267076566576, 'learning_rate': 2.4795013292011882e-05, 'epoch': 0.78}


 78%|███████▊  | 3297/4236 [5:02:32<1:38:52,  6.32s/it]

 78%|███████▊  | 3298/4236 [5:02:37<1:28:54,  5.69s/it]

 78%|███████▊  | 3299/4236 [5:02:40<1:20:16,  5.14s/it]

 78%|███████▊  | 3300/4236 [5:02:51<1:43:53,  6.66s/it]
 78%|███████▊  | 3300/4236 [5:02:51<1:43:53,  6.66s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 78%|███████▊  | 3301/4236 [5:03:13<2:59:08, 11.50s/it]
{'loss': 1.7107, 'grad_norm': 0.31446697304264104, 'learning_rate': 2.4493375195496292e-05, 'epoch': 0.78}

 78%|███████▊  | 3302/4236 [5:03:18<2:26:23,  9.40s/it]

 78%|███████▊  | 3303/4236 [5:03:22<2:00:45,  7.77s/it]


 78%|███████▊  | 3305/4236 [5:03:31<1:34:00,  6.06s/it]

 78%|███████▊  | 3306/4236 [5:03:35<1:24:09,  5.43s/it]

 78%|███████▊  | 3307/4236 [5:03:39<1:21:05,  5.24s/it]
{'loss': 1.8077, 'grad_norm': 0.3113083163174635, 'learning_rate': 2.419332683740656e-05, 'epoch': 0.78}


 78%|███████▊  | 3309/4236 [5:03:48<1:16:40,  4.96s/it]

 78%|███████▊  | 3310/4236 [5:03:59<1:40:33,  6.52s/it]

 78%|███████▊  | 3311/4236 [5:04:02<1:28:05,  5.71s/it]
{'loss': 1.9194, 'grad_norm': 0.3241484649937724, 'learning_rate': 2.3994180907732856e-05, 'epoch': 0.78}


 78%|███████▊  | 3313/4236 [5:04:15<1:29:42,  5.83s/it]

 78%|███████▊  | 3314/4236 [5:04:23<1:39:55,  6.50s/it]

 78%|███████▊  | 3315/4236 [5:04:27<1:30:26,  5.89s/it]

 78%|███████▊  | 3316/4236 [5:04:33<1:27:51,  5.73s/it]
{'loss': 1.6134, 'grad_norm': 0.33777262267108865, 'learning_rate': 2.374624886974106e-05, 'epoch': 0.78}


 78%|███████▊  | 3318/4236 [5:04:50<1:48:02,  7.06s/it]
{'loss': 1.5951, 'grad_norm': 0.3113864510519723, 'learning_rate': 2.3647388026715355e-05, 'epoch': 0.78}


 78%|███████▊  | 3320/4236 [5:05:00<1:33:30,  6.12s/it]

 78%|███████▊  | 3321/4236 [5:05:05<1:24:58,  5.57s/it]

 78%|███████▊  | 3322/4236 [5:05:09<1:21:12,  5.33s/it]
{'loss': 1.5635, 'grad_norm': 0.33271999606914765, 'learning_rate': 2.345020242238235e-05, 'epoch': 0.78}


 78%|███████▊  | 3324/4236 [5:05:17<1:09:36,  4.58s/it]

 78%|███████▊  | 3325/4236 [5:05:27<1:35:44,  6.31s/it]

 79%|███████▊  | 3326/4236 [5:05:42<2:15:23,  8.93s/it]

 79%|███████▊  | 3327/4236 [5:05:46<1:52:08,  7.40s/it]

 79%|███████▊  | 3328/4236 [5:06:01<2:26:50,  9.70s/it]

 79%|███████▊  | 3329/4236 [5:06:07<2:07:07,  8.41s/it]

 79%|███████▊  | 3330/4236 [5:06:12<1:54:19,  7.57s/it]
{'loss': 1.5482, 'grad_norm': 0.3176860121154462, 'learning_rate': 2.3057981989851728e-05, 'epoch': 0.79}

 79%|███████▊  | 3331/4236 [5:06:16<1:36:32,  6.40s/it]

 79%|███████▊  | 3332/4236 [5:06:20<1:26:04,  5.71s/it]


 79%|███████▊  | 3334/4236 [5:06:44<2:22:00,  9.45s/it]
[2024-05-25 07:41:21,556] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▊  | 3335/4236 [5:06:53<2:19:56,  9.32s/it]

 79%|███████▉  | 3336/4236 [5:06:56<1:55:21,  7.69s/it]

 79%|███████▉  | 3337/4236 [5:07:01<1:40:49,  6.73s/it]

 79%|███████▉  | 3338/4236 [5:07:04<1:26:05,  5.75s/it]
{'loss': 1.5503, 'grad_norm': 0.31299770732130155, 'learning_rate': 2.266864148158958e-05, 'epoch': 0.79}


 79%|███████▉  | 3340/4236 [5:07:15<1:21:32,  5.46s/it]

 79%|███████▉  | 3341/4236 [5:07:19<1:12:40,  4.87s/it]

 79%|███████▉  | 3342/4236 [5:07:22<1:06:28,  4.46s/it]

 79%|███████▉  | 3343/4236 [5:07:26<1:04:11,  4.31s/it]
{'loss': 1.7944, 'grad_norm': 0.3294503848143019, 'learning_rate': 2.2426772605254752e-05, 'epoch': 0.79}


 79%|███████▉  | 3345/4236 [5:07:39<1:18:56,  5.32s/it]

 79%|███████▉  | 3346/4236 [5:07:43<1:16:32,  5.16s/it]

 79%|███████▉  | 3347/4236 [5:07:50<1:24:25,  5.70s/it]
{'loss': 1.7311, 'grad_norm': 0.3363465018907867, 'learning_rate': 2.2234093960074233e-05, 'epoch': 0.79}

 79%|███████▉  | 3348/4236 [5:07:56<1:23:27,  5.64s/it]


 79%|███████▉  | 3350/4236 [5:08:14<1:45:26,  7.14s/it]

 79%|███████▉  | 3351/4236 [5:08:17<1:30:58,  6.17s/it]
{'loss': 1.7773, 'grad_norm': 0.29357320564979916, 'learning_rate': 2.2042143007153994e-05, 'epoch': 0.79}


 79%|███████▉  | 3353/4236 [5:08:36<1:58:09,  8.03s/it]

 79%|███████▉  | 3354/4236 [5:08:41<1:45:39,  7.19s/it]

 79%|███████▉  | 3355/4236 [5:08:51<2:00:51,  8.23s/it]

 79%|███████▉  | 3356/4236 [5:08:57<1:47:02,  7.30s/it]
{'loss': 1.6791, 'grad_norm': 0.291792625208721, 'learning_rate': 2.1803230368956297e-05, 'epoch': 0.79}


 79%|███████▉  | 3358/4236 [5:09:07<1:30:11,  6.16s/it]
{'loss': 1.6248, 'grad_norm': 0.30879079824804073, 'learning_rate': 2.170798524775508e-05, 'epoch': 0.79}


 79%|███████▉  | 3360/4236 [5:09:18<1:22:04,  5.62s/it]

 79%|███████▉  | 3361/4236 [5:09:22<1:14:14,  5.09s/it]

 79%|███████▉  | 3362/4236 [5:09:26<1:09:58,  4.80s/it]
{'loss': 1.6853, 'grad_norm': 0.3264099254118629, 'learning_rate': 2.1518044689539775e-05, 'epoch': 0.79}


 79%|███████▉  | 3364/4236 [5:09:36<1:10:05,  4.82s/it]
{'loss': 1.7804, 'grad_norm': 0.30748173648442023, 'learning_rate': 2.1423349696866802e-05, 'epoch': 0.79}


 79%|███████▉  | 3366/4236 [5:09:47<1:14:07,  5.11s/it]

 79%|███████▉  | 3367/4236 [5:09:51<1:09:20,  4.79s/it]

 80%|███████▉  | 3368/4236 [5:09:59<1:21:34,  5.64s/it]

 80%|███████▉  | 3369/4236 [5:10:03<1:14:50,  5.18s/it]
{'loss': 1.7628, 'grad_norm': 0.28004509398143995, 'learning_rate': 2.1187416910347725e-05, 'epoch': 0.8}

 80%|███████▉  | 3370/4236 [5:10:06<1:08:51,  4.77s/it]


 80%|███████▉  | 3372/4236 [5:10:15<1:04:31,  4.48s/it]

 80%|███████▉  | 3373/4236 [5:10:19<1:04:46,  4.50s/it]

 80%|███████▉  | 3374/4236 [5:10:25<1:10:08,  4.88s/it]

 80%|███████▉  | 3375/4236 [5:10:36<1:35:51,  6.68s/it]

 80%|███████▉  | 3376/4236 [5:10:47<1:54:56,  8.02s/it]
{'loss': 1.8582, 'grad_norm': 0.30285565103271256, 'learning_rate': 2.0859047679963427e-05, 'epoch': 0.8}


 80%|███████▉  | 3378/4236 [5:10:58<1:34:52,  6.63s/it]
{'loss': 1.753, 'grad_norm': 0.3098500756771336, 'learning_rate': 2.076564405384258e-05, 'epoch': 0.8}

 80%|███████▉  | 3379/4236 [5:11:02<1:25:01,  5.95s/it]


 80%|███████▉  | 3381/4236 [5:11:15<1:24:55,  5.96s/it]
{'loss': 1.4565, 'grad_norm': 0.3051040152161887, 'learning_rate': 2.0625886229859582e-05, 'epoch': 0.8}


 80%|███████▉  | 3383/4236 [5:11:23<1:13:39,  5.18s/it]

 80%|███████▉  | 3384/4236 [5:11:28<1:13:40,  5.19s/it]

 80%|███████▉  | 3385/4236 [5:11:33<1:10:22,  4.96s/it]

 80%|███████▉  | 3386/4236 [5:11:37<1:07:18,  4.75s/it]
{'loss': 1.6619, 'grad_norm': 0.3486084332500299, 'learning_rate': 2.039388531208818e-05, 'epoch': 0.8}


 80%|███████▉  | 3388/4236 [5:11:50<1:16:12,  5.39s/it]

 80%|████████  | 3389/4236 [5:11:54<1:11:05,  5.04s/it]

 80%|████████  | 3390/4236 [5:12:04<1:32:17,  6.55s/it]

 80%|████████  | 3391/4236 [5:12:09<1:25:36,  6.08s/it]
{'loss': 1.7322, 'grad_norm': 0.3201552507895446, 'learning_rate': 2.0163048318744493e-05, 'epoch': 0.8}

 80%|████████  | 3392/4236 [5:12:12<1:14:51,  5.32s/it]


 80%|████████  | 3394/4236 [5:12:21<1:06:08,  4.71s/it]

 80%|████████  | 3395/4236 [5:12:25<1:05:42,  4.69s/it]

 80%|████████  | 3396/4236 [5:12:29<1:02:48,  4.49s/it]

 80%|████████  | 3397/4236 [5:12:37<1:15:19,  5.39s/it]

 80%|████████  | 3398/4236 [5:12:53<2:00:23,  8.62s/it]

 80%|████████  | 3399/4236 [5:13:06<2:17:34,  9.86s/it]
{'loss': 1.5694, 'grad_norm': 0.3081596322077302, 'learning_rate': 1.9796138510009697e-05, 'epoch': 0.8}


 80%|████████  | 3401/4236 [5:13:19<1:55:35,  8.31s/it]

 80%|████████  | 3402/4236 [5:13:23<1:40:00,  7.19s/it]

 80%|████████  | 3403/4236 [5:13:29<1:35:40,  6.89s/it]

 80%|████████  | 3404/4236 [5:13:37<1:39:10,  7.15s/it]

 80%|████████  | 3405/4236 [5:13:41<1:25:56,  6.21s/it]

 80%|████████  | 3406/4236 [5:13:45<1:16:00,  5.49s/it]
{'loss': 1.5324, 'grad_norm': 0.33783301467823895, 'learning_rate': 1.947755455067156e-05, 'epoch': 0.8}


 80%|████████  | 3408/4236 [5:13:58<1:21:36,  5.91s/it]

 80%|████████  | 3409/4236 [5:14:02<1:12:30,  5.26s/it]
{'loss': 1.6313, 'grad_norm': 0.3104216028528075, 'learning_rate': 1.93417244305479e-05, 'epoch': 0.8}


 81%|████████  | 3411/4236 [5:14:10<1:04:18,  4.68s/it]

 81%|████████  | 3412/4236 [5:14:14<1:00:13,  4.39s/it]

 81%|████████  | 3413/4236 [5:14:24<1:23:38,  6.10s/it]

 81%|████████  | 3414/4236 [5:14:34<1:40:28,  7.33s/it]

 81%|████████  | 3415/4236 [5:14:38<1:25:37,  6.26s/it]

 81%|████████  | 3416/4236 [5:14:42<1:16:05,  5.57s/it]
{'loss': 1.6521, 'grad_norm': 0.32525005858290473, 'learning_rate': 1.9026439747780277e-05, 'epoch': 0.81}

 81%|████████  | 3417/4236 [5:14:47<1:13:13,  5.36s/it]


 81%|████████  | 3419/4236 [5:15:00<1:19:01,  5.80s/it]
{'loss': 1.8285, 'grad_norm': 0.29814579777128397, 'learning_rate': 1.8892027566312597e-05, 'epoch': 0.81}


 81%|████████  | 3421/4236 [5:15:10<1:14:01,  5.45s/it]

 81%|████████  | 3422/4236 [5:15:14<1:08:26,  5.04s/it]

 81%|████████  | 3423/4236 [5:15:27<1:42:23,  7.56s/it]

 81%|████████  | 3424/4236 [5:15:32<1:28:34,  6.54s/it]
{'loss': 1.7213, 'grad_norm': 0.30203594778665954, 'learning_rate': 1.8668956319490128e-05, 'epoch': 0.81}


 81%|████████  | 3426/4236 [5:15:41<1:16:54,  5.70s/it]

 81%|████████  | 3427/4236 [5:15:45<1:10:55,  5.26s/it]

 81%|████████  | 3428/4236 [5:15:50<1:08:25,  5.08s/it]

 81%|████████  | 3429/4236 [5:15:55<1:07:11,  5.00s/it]

 81%|████████  | 3430/4236 [5:15:59<1:05:05,  4.85s/it]

 81%|████████  | 3431/4236 [5:16:17<1:57:36,  8.77s/it]

 81%|████████  | 3432/4236 [5:16:21<1:37:48,  7.30s/it]

 81%|████████  | 3433/4236 [5:16:25<1:23:17,  6.22s/it]

 81%|████████  | 3434/4236 [5:16:33<1:31:01,  6.81s/it]

 81%|████████  | 3435/4236 [5:16:38<1:22:57,  6.21s/it]
{'loss': 1.7687, 'grad_norm': 0.34631959664516887, 'learning_rate': 1.8182389932318476e-05, 'epoch': 0.81}

 81%|████████  | 3436/4236 [5:16:43<1:17:01,  5.78s/it]


 81%|████████  | 3438/4236 [5:16:50<1:02:19,  4.69s/it]
{'loss': 1.7196, 'grad_norm': 0.30580050511573503, 'learning_rate': 1.8050693422322595e-05, 'epoch': 0.81}

 81%|████████  | 3439/4236 [5:16:55<1:02:36,  4.71s/it]


 81%|████████  | 3441/4236 [5:17:03<58:42,  4.43s/it]
{'loss': 1.7272, 'grad_norm': 0.31360990234261193, 'learning_rate': 1.7919428259024518e-05, 'epoch': 0.81}


 81%|████████▏ | 3443/4236 [5:17:22<1:36:43,  7.32s/it]

 81%|████████▏ | 3444/4236 [5:17:35<2:01:05,  9.17s/it]

 81%|████████▏ | 3445/4236 [5:17:39<1:39:22,  7.54s/it]
{'loss': 1.6299, 'grad_norm': 0.292617269757583, 'learning_rate': 1.7745080218909027e-05, 'epoch': 0.81}

 81%|████████▏ | 3446/4236 [5:17:44<1:30:57,  6.91s/it]


 81%|████████▏ | 3448/4236 [5:17:53<1:14:48,  5.70s/it]

 81%|████████▏ | 3449/4236 [5:17:57<1:06:33,  5.07s/it]
{'loss': 1.7211, 'grad_norm': 0.281570340088414, 'learning_rate': 1.757150187687261e-05, 'epoch': 0.81}


 81%|████████▏ | 3451/4236 [5:18:18<1:47:14,  8.20s/it]

 81%|████████▏ | 3452/4236 [5:18:22<1:30:38,  6.94s/it]

 82%|████████▏ | 3453/4236 [5:18:30<1:33:10,  7.14s/it]

 82%|████████▏ | 3454/4236 [5:18:34<1:20:39,  6.19s/it]

 82%|████████▏ | 3455/4236 [5:18:38<1:10:53,  5.45s/it]
{'loss': 1.6202, 'grad_norm': 0.3399661644799853, 'learning_rate': 1.7312581098799575e-05, 'epoch': 0.82}

 82%|████████▏ | 3456/4236 [5:18:51<1:41:25,  7.80s/it]

 82%|████████▏ | 3457/4236 [5:18:55<1:25:32,  6.59s/it]


 82%|████████▏ | 3459/4236 [5:19:05<1:15:38,  5.84s/it]
{'loss': 1.8234, 'grad_norm': 0.29636849066674986, 'learning_rate': 1.714093409228118e-05, 'epoch': 0.82}

 82%|████████▏ | 3460/4236 [5:19:09<1:06:49,  5.17s/it]

 82%|████████▏ | 3461/4236 [5:19:19<1:25:08,  6.59s/it]


 82%|████████▏ | 3463/4236 [5:19:27<1:11:01,  5.51s/it]

 82%|████████▏ | 3464/4236 [5:19:31<1:04:08,  4.98s/it]

 82%|████████▏ | 3465/4236 [5:19:38<1:11:45,  5.58s/it]
{'loss': 1.755, 'grad_norm': 0.30204249004911105, 'learning_rate': 1.6884917866250693e-05, 'epoch': 0.82}

 82%|████████▏ | 3466/4236 [5:19:43<1:07:33,  5.26s/it]


 82%|████████▏ | 3468/4236 [5:19:54<1:10:04,  5.47s/it]

 82%|████████▏ | 3469/4236 [5:20:00<1:10:19,  5.50s/it]

 82%|████████▏ | 3470/4236 [5:20:04<1:04:01,  5.01s/it]
{'loss': 1.703, 'grad_norm': 0.37287415128956625, 'learning_rate': 1.667290753497386e-05, 'epoch': 0.82}

 82%|████████▏ | 3471/4236 [5:20:11<1:13:05,  5.73s/it]


 82%|████████▏ | 3473/4236 [5:20:20<1:04:37,  5.08s/it]

 82%|████████▏ | 3474/4236 [5:20:28<1:18:15,  6.16s/it]

 82%|████████▏ | 3475/4236 [5:20:34<1:16:53,  6.06s/it]
{'loss': 1.7776, 'grad_norm': 0.30587770953584825, 'learning_rate': 1.6462115532700962e-05, 'epoch': 0.82}


 82%|████████▏ | 3477/4236 [5:20:52<1:30:05,  7.12s/it]
{'loss': 1.6446, 'grad_norm': 0.3471173569374581, 'learning_rate': 1.637814055488024e-05, 'epoch': 0.82}


 82%|████████▏ | 3479/4236 [5:21:02<1:15:08,  5.96s/it]

 82%|████████▏ | 3480/4236 [5:21:06<1:05:59,  5.24s/it]

 82%|████████▏ | 3481/4236 [5:21:10<1:01:18,  4.87s/it]

 82%|████████▏ | 3482/4236 [5:21:13<57:26,  4.57s/it]

 82%|████████▏ | 3483/4236 [5:21:30<1:44:02,  8.29s/it]

 82%|████████▏ | 3484/4236 [5:21:35<1:31:32,  7.30s/it]

 82%|████████▏ | 3485/4236 [5:21:40<1:19:57,  6.39s/it]

 82%|████████▏ | 3486/4236 [5:21:46<1:17:42,  6.22s/it]

 82%|████████▏ | 3487/4236 [5:22:00<1:50:01,  8.81s/it]
{'loss': 1.7959, 'grad_norm': 0.29775128309164267, 'learning_rate': 1.5961203914853883e-05, 'epoch': 0.82}

 82%|████████▏ | 3488/4236 [5:22:05<1:33:56,  7.54s/it]


 82%|████████▏ | 3490/4236 [5:22:14<1:14:41,  6.01s/it]

 82%|████████▏ | 3491/4236 [5:22:19<1:10:18,  5.66s/it]
{'loss': 1.6031, 'grad_norm': 0.4153180750219414, 'learning_rate': 1.57958040809816e-05, 'epoch': 0.82}


 82%|████████▏ | 3493/4236 [5:22:32<1:18:54,  6.37s/it]
{'loss': 1.5718, 'grad_norm': 0.30020755660410675, 'learning_rate': 1.571339954449126e-05, 'epoch': 0.82}

 82%|████████▏ | 3494/4236 [5:22:37<1:13:20,  5.93s/it]


 83%|████████▎ | 3496/4236 [5:22:45<1:03:01,  5.11s/it]

 83%|████████▎ | 3497/4236 [5:22:50<1:01:00,  4.95s/it]

 83%|████████▎ | 3498/4236 [5:22:54<56:55,  4.63s/it]

 83%|████████▎ | 3499/4236 [5:22:58<53:21,  4.34s/it]

 83%|████████▎ | 3500/4236 [5:23:02<51:49,  4.22s/it]

 83%|████████▎ | 3501/4236 [5:23:05<50:46,  4.14s/it]

 83%|████████▎ | 3502/4236 [5:23:12<59:50,  4.89s/it]
{'loss': 1.7493, 'grad_norm': 0.2754754410017403, 'learning_rate': 1.5345021974001716e-05, 'epoch': 0.83}


 83%|████████▎ | 3504/4236 [5:23:22<58:19,  4.78s/it]

 83%|████████▎ | 3505/4236 [5:23:26<55:35,  4.56s/it]

 83%|████████▎ | 3506/4236 [5:23:32<1:02:48,  5.16s/it]
{'loss': 1.6439, 'grad_norm': 0.34128588809847277, 'learning_rate': 1.5182584829243818e-05, 'epoch': 0.83}


 83%|████████▎ | 3508/4236 [5:23:44<1:09:52,  5.76s/it]

 83%|████████▎ | 3509/4236 [5:23:51<1:14:36,  6.16s/it]

 83%|████████▎ | 3510/4236 [5:23:58<1:16:49,  6.35s/it]
{'loss': 1.6473, 'grad_norm': 0.3303040841790602, 'learning_rate': 1.5020941361042884e-05, 'epoch': 0.83}


 83%|████████▎ | 3512/4236 [5:24:11<1:17:41,  6.44s/it]
{'loss': 1.6002, 'grad_norm': 0.31021015782876216, 'learning_rate': 1.4940417728621236e-05, 'epoch': 0.83}

 83%|████████▎ | 3513/4236 [5:24:15<1:07:07,  5.57s/it]

 83%|████████▎ | 3514/4236 [5:24:19<1:02:40,  5.21s/it]


 83%|████████▎ | 3516/4236 [5:24:27<53:15,  4.44s/it]

 83%|████████▎ | 3517/4236 [5:24:32<56:39,  4.73s/it]
{'loss': 1.7732, 'grad_norm': 0.32801390967920063, 'learning_rate': 1.4739979621250022e-05, 'epoch': 0.83}

 83%|████████▎ | 3518/4236 [5:24:47<1:33:56,  7.85s/it]

 83%|████████▎ | 3519/4236 [5:24:51<1:20:01,  6.70s/it]

 83%|████████▎ | 3520/4236 [5:24:57<1:16:50,  6.44s/it]


 83%|████████▎ | 3522/4236 [5:25:06<1:06:10,  5.56s/it]

 83%|████████▎ | 3523/4236 [5:25:10<1:00:17,  5.07s/it]
{'loss': 1.7758, 'grad_norm': 0.3212671731588662, 'learning_rate': 1.4501099648378469e-05, 'epoch': 0.83}

 83%|████████▎ | 3524/4236 [5:25:15<58:53,  4.96s/it]


 83%|████████▎ | 3526/4236 [5:25:24<57:41,  4.88s/it]
{'loss': 1.7205, 'grad_norm': 0.30148265897486975, 'learning_rate': 1.4382334393593767e-05, 'epoch': 0.83}


 83%|████████▎ | 3528/4236 [5:25:32<51:35,  4.37s/it]
{'loss': 1.6636, 'grad_norm': 0.3102734868233228, 'learning_rate': 1.4303407882689634e-05, 'epoch': 0.83}


 83%|████████▎ | 3530/4236 [5:25:44<59:25,  5.05s/it]

 83%|████████▎ | 3531/4236 [5:25:48<54:42,  4.66s/it]
{'loss': 1.5479, 'grad_norm': 0.3275419927893587, 'learning_rate': 1.4185394066357483e-05, 'epoch': 0.83}

 83%|████████▎ | 3532/4236 [5:25:53<57:40,  4.92s/it]

 83%|████████▎ | 3533/4236 [5:25:57<53:26,  4.56s/it]

 83%|████████▎ | 3534/4236 [5:26:01<51:58,  4.44s/it]


 83%|████████▎ | 3536/4236 [5:26:12<58:41,  5.03s/it]

 83%|████████▎ | 3537/4236 [5:26:18<1:00:30,  5.19s/it]
{'loss': 1.6525, 'grad_norm': 0.34251909215799453, 'learning_rate': 1.3950722128516491e-05, 'epoch': 0.83}

 84%|████████▎ | 3538/4236 [5:26:27<1:15:40,  6.51s/it]


 84%|████████▎ | 3540/4236 [5:26:36<1:04:27,  5.56s/it]

 84%|████████▎ | 3541/4236 [5:26:40<59:39,  5.15s/it]

 84%|████████▎ | 3542/4236 [5:26:46<1:01:27,  5.31s/it]

 84%|████████▎ | 3543/4236 [5:26:51<58:48,  5.09s/it]

 84%|████████▎ | 3544/4236 [5:26:55<54:24,  4.72s/it]

 84%|████████▎ | 3545/4236 [5:27:08<1:25:34,  7.43s/it]

 84%|████████▎ | 3546/4236 [5:27:13<1:15:05,  6.53s/it]

 84%|████████▎ | 3547/4236 [5:27:20<1:18:10,  6.81s/it]

 84%|████████▍ | 3548/4236 [5:27:25<1:10:59,  6.19s/it]

 84%|████████▍ | 3549/4236 [5:27:30<1:05:55,  5.76s/it]

 84%|████████▍ | 3550/4236 [5:27:34<1:02:07,  5.43s/it]

 84%|████████▍ | 3551/4236 [5:27:39<58:38,  5.14s/it]

 84%|████████▍ | 3552/4236 [5:27:44<59:27,  5.22s/it]

 84%|████████▍ | 3553/4236 [5:27:48<54:17,  4.77s/it]

 84%|████████▍ | 3554/4236 [5:27:54<57:51,  5.09s/it]

 84%|████████▍ | 3555/4236 [5:27:58<53:52,  4.75s/it]

 84%|████████▍ | 3556/4236 [5:28:02<52:17,  4.61s/it]
{'loss': 1.709, 'grad_norm': 0.31444394639695483, 'learning_rate': 1.3219569862365089e-05, 'epoch': 0.84}


 84%|████████▍ | 3558/4236 [5:28:11<50:28,  4.47s/it]
{'loss': 1.5678, 'grad_norm': 0.3385241062851489, 'learning_rate': 1.314366961468797e-05, 'epoch': 0.84}


 84%|████████▍ | 3560/4236 [5:28:21<54:01,  4.80s/it]
{'loss': 1.7267, 'grad_norm': 0.3336911523439709, 'learning_rate': 1.3067972556041752e-05, 'epoch': 0.84}

 84%|████████▍ | 3561/4236 [5:28:25<52:16,  4.65s/it]


 84%|████████▍ | 3563/4236 [5:28:35<54:35,  4.87s/it]
{'loss': 1.6676, 'grad_norm': 0.325593828995592, 'learning_rate': 1.2954808334745183e-05, 'epoch': 0.84}

 84%|████████▍ | 3564/4236 [5:28:39<51:55,  4.64s/it]

 84%|████████▍ | 3565/4236 [5:28:44<51:07,  4.57s/it]


 84%|████████▍ | 3567/4236 [5:29:02<1:22:01,  7.36s/it]
{'loss': 1.5741, 'grad_norm': 0.3327771332299247, 'learning_rate': 1.2804635516745567e-05, 'epoch': 0.84}


 84%|████████▍ | 3569/4236 [5:29:14<1:11:27,  6.43s/it]

 84%|████████▍ | 3570/4236 [5:29:22<1:17:46,  7.01s/it]

 84%|████████▍ | 3571/4236 [5:29:27<1:08:38,  6.19s/it]

 84%|████████▍ | 3572/4236 [5:29:34<1:12:30,  6.55s/it]

 84%|████████▍ | 3573/4236 [5:29:40<1:11:39,  6.48s/it]
{'loss': 1.6229, 'grad_norm': 0.37166793011215776, 'learning_rate': 1.2580906592077402e-05, 'epoch': 0.84}

 84%|████████▍ | 3574/4236 [5:29:46<1:07:48,  6.15s/it]


 84%|████████▍ | 3576/4236 [5:29:55<1:00:42,  5.52s/it]

 84%|████████▍ | 3577/4236 [5:30:06<1:18:49,  7.18s/it]
{'loss': 1.665, 'grad_norm': 0.30459429679048033, 'learning_rate': 1.2432776212569274e-05, 'epoch': 0.84}


 84%|████████▍ | 3579/4236 [5:30:15<1:02:20,  5.69s/it]
{'loss': 1.7994, 'grad_norm': 0.3228530392023315, 'learning_rate': 1.2359018214411633e-05, 'epoch': 0.84}

 85%|████████▍ | 3580/4236 [5:30:20<58:42,  5.37s/it]


 85%|████████▍ | 3582/4236 [5:30:27<49:06,  4.51s/it]

 85%|████████▍ | 3583/4236 [5:30:32<50:59,  4.69s/it]
{'loss': 1.9172, 'grad_norm': 0.3200495560023197, 'learning_rate': 1.2212117464024763e-05, 'epoch': 0.85}


 85%|████████▍ | 3585/4236 [5:30:52<1:15:24,  6.95s/it]
{'loss': 1.6606, 'grad_norm': 0.3638321940415944, 'learning_rate': 1.2138975055450663e-05, 'epoch': 0.85}

 85%|████████▍ | 3586/4236 [5:30:57<1:09:04,  6.38s/it]


 85%|████████▍ | 3588/4236 [5:31:07<1:01:08,  5.66s/it]

 85%|████████▍ | 3589/4236 [5:31:13<1:02:06,  5.76s/it]

 85%|████████▍ | 3590/4236 [5:31:17<56:45,  5.27s/it]

 85%|████████▍ | 3591/4236 [5:31:21<52:06,  4.85s/it]

 85%|████████▍ | 3592/4236 [5:31:25<49:25,  4.60s/it]

 85%|████████▍ | 3593/4236 [5:31:29<47:55,  4.47s/it]
{'loss': 1.5556, 'grad_norm': 0.3034930671770201, 'learning_rate': 1.1848462518835313e-05, 'epoch': 0.85}


 85%|████████▍ | 3595/4236 [5:31:39<50:52,  4.76s/it]
{'loss': 1.7375, 'grad_norm': 0.32520937846261744, 'learning_rate': 1.1776349508622075e-05, 'epoch': 0.85}


 85%|████████▍ | 3597/4236 [5:31:47<46:13,  4.34s/it]
{'loss': 1.668, 'grad_norm': 0.33165342037573353, 'learning_rate': 1.1704442886106393e-05, 'epoch': 0.85}

 85%|████████▍ | 3598/4236 [5:31:51<46:15,  4.35s/it]

 85%|████████▍ | 3599/4236 [5:31:55<44:35,  4.20s/it]

 85%|████████▍ | 3600/4236 [5:32:00<45:02,  4.25s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 85%|████████▌ | 3601/4236 [5:32:21<1:39:38,  9.42s/it]

 85%|████████▌ | 3602/4236 [5:32:25<1:22:04,  7.77s/it]
{'loss': 1.5774, 'grad_norm': 0.3353821944875266, 'learning_rate': 1.1525580378723156e-05, 'epoch': 0.85}

 85%|████████▌ | 3603/4236 [5:32:30<1:12:21,  6.86s/it]

 85%|████████▌ | 3604/4236 [5:32:34<1:02:52,  5.97s/it]


 85%|████████▌ | 3606/4236 [5:32:51<1:12:40,  6.92s/it]

 85%|████████▌ | 3607/4236 [5:33:07<1:41:58,  9.73s/it]
[2024-05-25 08:07:44,952] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 3608/4236 [5:33:11<1:23:56,  8.02s/it]

 85%|████████▌ | 3609/4236 [5:33:16<1:15:39,  7.24s/it]

 85%|████████▌ | 3610/4236 [5:33:21<1:05:49,  6.31s/it]

 85%|████████▌ | 3611/4236 [5:33:25<1:00:01,  5.76s/it]

 85%|████████▌ | 3612/4236 [5:33:29<55:31,  5.34s/it]
{'loss': 1.5776, 'grad_norm': 0.3044396818774668, 'learning_rate': 1.1171738725086833e-05, 'epoch': 0.85}

 85%|████████▌ | 3613/4236 [5:33:40<1:10:52,  6.83s/it]


 85%|████████▌ | 3615/4236 [5:33:47<54:32,  5.27s/it]

 85%|████████▌ | 3616/4236 [5:33:51<50:02,  4.84s/it]

 85%|████████▌ | 3617/4236 [5:33:57<51:50,  5.03s/it]

 85%|████████▌ | 3618/4236 [5:34:01<49:09,  4.77s/it]

 85%|████████▌ | 3619/4236 [5:34:05<46:32,  4.53s/it]

 85%|████████▌ | 3620/4236 [5:34:11<53:09,  5.18s/it]

 85%|████████▌ | 3621/4236 [5:34:23<1:14:13,  7.24s/it]
{'loss': 1.6158, 'grad_norm': 0.35203255669791467, 'learning_rate': 1.0857722402527847e-05, 'epoch': 0.85}


 86%|████████▌ | 3623/4236 [5:34:35<1:06:52,  6.55s/it]

 86%|████████▌ | 3624/4236 [5:34:39<58:09,  5.70s/it]
{'loss': 1.7188, 'grad_norm': 0.3322334952520362, 'learning_rate': 1.0753987977820213e-05, 'epoch': 0.86}

 86%|████████▌ | 3625/4236 [5:34:44<54:23,  5.34s/it]

 86%|████████▌ | 3626/4236 [5:34:50<56:34,  5.57s/it]


 86%|████████▌ | 3628/4236 [5:35:01<55:18,  5.46s/it]

 86%|████████▌ | 3629/4236 [5:35:05<51:23,  5.08s/it]

 86%|████████▌ | 3630/4236 [5:35:13<59:33,  5.90s/it]
{'loss': 1.7225, 'grad_norm': 0.30088421529961873, 'learning_rate': 1.054792893240969e-05, 'epoch': 0.86}

 86%|████████▌ | 3631/4236 [5:35:18<57:17,  5.68s/it]

 86%|████████▌ | 3632/4236 [5:35:22<51:13,  5.09s/it]


 86%|████████▌ | 3634/4236 [5:35:33<55:07,  5.49s/it]
{'loss': 1.6574, 'grad_norm': 0.3350989171898088, 'learning_rate': 1.0411602274798772e-05, 'epoch': 0.86}


 86%|████████▌ | 3636/4236 [5:35:44<54:02,  5.40s/it]

 86%|████████▌ | 3637/4236 [5:35:48<50:00,  5.01s/it]
{'loss': 1.5963, 'grad_norm': 0.3280279139274174, 'learning_rate': 1.0309907360206727e-05, 'epoch': 0.86}


 86%|████████▌ | 3639/4236 [5:35:58<50:24,  5.07s/it]

 86%|████████▌ | 3640/4236 [5:36:03<48:16,  4.86s/it]

 86%|████████▌ | 3641/4236 [5:36:07<48:00,  4.84s/it]
{'loss': 1.5896, 'grad_norm': 0.3103479912668386, 'learning_rate': 1.0175048596596682e-05, 'epoch': 0.86}

 86%|████████▌ | 3642/4236 [5:36:12<46:15,  4.67s/it]


 86%|████████▌ | 3644/4236 [5:36:23<51:17,  5.20s/it]

 86%|████████▌ | 3645/4236 [5:36:28<48:31,  4.93s/it]

 86%|████████▌ | 3646/4236 [5:36:33<49:53,  5.07s/it]

 86%|████████▌ | 3647/4236 [5:36:38<50:44,  5.17s/it]

 86%|████████▌ | 3648/4236 [5:36:43<48:42,  4.97s/it]
{'loss': 1.6544, 'grad_norm': 0.3048831031452657, 'learning_rate': 9.941069050962626e-06, 'epoch': 0.86}

 86%|████████▌ | 3649/4236 [5:36:49<50:40,  5.18s/it]


 86%|████████▌ | 3651/4236 [5:36:57<46:35,  4.78s/it]
{'loss': 1.6538, 'grad_norm': 0.32087764526887946, 'learning_rate': 9.841581766854401e-06, 'epoch': 0.86}

 86%|████████▌ | 3652/4236 [5:37:02<47:12,  4.85s/it]

 86%|████████▌ | 3653/4236 [5:37:06<45:05,  4.64s/it]

 86%|████████▋ | 3654/4236 [5:37:10<42:56,  4.43s/it]

 86%|████████▋ | 3655/4236 [5:37:14<41:15,  4.26s/it]

 86%|████████▋ | 3656/4236 [5:37:18<39:43,  4.11s/it]


 86%|████████▋ | 3658/4236 [5:37:27<42:38,  4.43s/it]
{'loss': 1.7149, 'grad_norm': 0.30259683046901836, 'learning_rate': 9.611291166610336e-06, 'epoch': 0.86}


 86%|████████▋ | 3660/4236 [5:37:37<45:38,  4.75s/it]

 86%|████████▋ | 3661/4236 [5:37:41<42:53,  4.48s/it]

 86%|████████▋ | 3662/4236 [5:37:46<42:37,  4.46s/it]
{'loss': 1.7418, 'grad_norm': 0.26967454470463165, 'learning_rate': 9.480859104243578e-06, 'epoch': 0.86}

 86%|████████▋ | 3663/4236 [5:37:50<43:52,  4.59s/it]

 86%|████████▋ | 3664/4236 [5:37:56<46:31,  4.88s/it]

 87%|████████▋ | 3665/4236 [5:38:02<49:15,  5.18s/it]


 87%|████████▋ | 3667/4236 [5:38:14<52:38,  5.55s/it]

 87%|████████▋ | 3668/4236 [5:38:19<52:13,  5.52s/it]

 87%|████████▋ | 3669/4236 [5:38:23<48:21,  5.12s/it]

 87%|████████▋ | 3670/4236 [5:38:27<45:15,  4.80s/it]
{'loss': 1.7139, 'grad_norm': 0.2952412008545745, 'learning_rate': 9.222537282979592e-06, 'epoch': 0.87}

 87%|████████▋ | 3671/4236 [5:38:35<52:01,  5.52s/it]


 87%|████████▋ | 3673/4236 [5:38:45<50:40,  5.40s/it]

 87%|████████▋ | 3674/4236 [5:38:53<59:32,  6.36s/it]
{'loss': 1.6738, 'grad_norm': 0.3539613891327988, 'learning_rate': 9.094649941321499e-06, 'epoch': 0.87}


 87%|████████▋ | 3676/4236 [5:39:06<58:19,  6.25s/it]
{'loss': 1.8397, 'grad_norm': 0.3089051762294896, 'learning_rate': 9.031025187597519e-06, 'epoch': 0.87}


 87%|████████▋ | 3678/4236 [5:39:15<50:51,  5.47s/it]
{'loss': 1.7534, 'grad_norm': 0.32491624641526373, 'learning_rate': 8.967613243898466e-06, 'epoch': 0.87}

 87%|████████▋ | 3679/4236 [5:39:20<48:12,  5.19s/it]

 87%|████████▋ | 3680/4236 [5:39:24<44:56,  4.85s/it]

 87%|████████▋ | 3681/4236 [5:39:28<43:44,  4.73s/it]

 87%|████████▋ | 3682/4236 [5:39:34<47:04,  5.10s/it]

 87%|████████▋ | 3683/4236 [5:39:38<43:33,  4.73s/it]


 87%|████████▋ | 3685/4236 [5:39:50<49:45,  5.42s/it]
{'loss': 1.6194, 'grad_norm': 0.2906553732606382, 'learning_rate': 8.747349457117616e-06, 'epoch': 0.87}


 87%|████████▋ | 3687/4236 [5:40:06<1:02:34,  6.84s/it]

 87%|████████▋ | 3688/4236 [5:40:12<1:00:26,  6.62s/it]
{'loss': 1.5011, 'grad_norm': 0.3021606692315401, 'learning_rate': 8.653750850371666e-06, 'epoch': 0.87}


 87%|████████▋ | 3690/4236 [5:40:21<52:26,  5.76s/it]
{'loss': 1.6586, 'grad_norm': 0.31272105362174746, 'learning_rate': 8.59161886459654e-06, 'epoch': 0.87}

 87%|████████▋ | 3691/4236 [5:40:27<51:24,  5.66s/it]

 87%|████████▋ | 3692/4236 [5:40:32<49:44,  5.49s/it]

 87%|████████▋ | 3693/4236 [5:40:36<46:30,  5.14s/it]

 87%|████████▋ | 3694/4236 [5:40:40<43:03,  4.77s/it]

 87%|████████▋ | 3695/4236 [5:40:44<40:24,  4.48s/it]

 87%|████████▋ | 3696/4236 [5:40:48<39:56,  4.44s/it]


 87%|████████▋ | 3698/4236 [5:41:06<1:03:12,  7.05s/it]
{'loss': 1.6684, 'grad_norm': 0.29756946903920123, 'learning_rate': 8.34523074707273e-06, 'epoch': 0.87}

 87%|████████▋ | 3699/4236 [5:41:12<1:00:40,  6.78s/it]


 87%|████████▋ | 3701/4236 [5:41:22<51:45,  5.80s/it]
{'loss': 1.5941, 'grad_norm': 0.3256794557927476, 'learning_rate': 8.253719168369768e-06, 'epoch': 0.87}

 87%|████████▋ | 3702/4236 [5:41:28<54:33,  6.13s/it]

 87%|████████▋ | 3703/4236 [5:41:32<47:57,  5.40s/it]

 87%|████████▋ | 3704/4236 [5:41:38<49:36,  5.60s/it]


 87%|████████▋ | 3706/4236 [5:41:51<52:17,  5.92s/it]

 88%|████████▊ | 3707/4236 [5:41:55<46:55,  5.32s/it]

 88%|████████▊ | 3708/4236 [5:42:00<45:23,  5.16s/it]
{'loss': 1.7086, 'grad_norm': 0.31135844481724695, 'learning_rate': 8.042070977478533e-06, 'epoch': 0.88}


 88%|████████▊ | 3710/4236 [5:42:10<44:05,  5.03s/it]

 88%|████████▊ | 3711/4236 [5:42:13<40:47,  4.66s/it]
{'loss': 1.7753, 'grad_norm': 0.32460944641246664, 'learning_rate': 7.952170970524985e-06, 'epoch': 0.88}

 88%|████████▊ | 3712/4236 [5:42:24<57:12,  6.55s/it]

 88%|████████▊ | 3713/4236 [5:42:28<50:12,  5.76s/it]

 88%|████████▊ | 3714/4236 [5:42:33<47:05,  5.41s/it]


 88%|████████▊ | 3716/4236 [5:42:46<52:11,  6.02s/it]

 88%|████████▊ | 3717/4236 [5:42:50<47:19,  5.47s/it]

 88%|████████▊ | 3718/4236 [5:42:56<47:48,  5.54s/it]

 88%|████████▊ | 3719/4236 [5:43:00<43:43,  5.07s/it]

 88%|████████▊ | 3720/4236 [5:43:06<46:42,  5.43s/it]

 88%|████████▊ | 3721/4236 [5:43:10<43:09,  5.03s/it]

 88%|████████▊ | 3722/4236 [5:43:16<45:24,  5.30s/it]

 88%|████████▊ | 3723/4236 [5:43:20<41:49,  4.89s/it]
{'loss': 1.6289, 'grad_norm': 0.29775662683330384, 'learning_rate': 7.597420647145692e-06, 'epoch': 0.88}


 88%|████████▊ | 3725/4236 [5:43:34<49:26,  5.80s/it]
{'loss': 1.7176, 'grad_norm': 0.29329096358467793, 'learning_rate': 7.539051363970218e-06, 'epoch': 0.88}

 88%|████████▊ | 3726/4236 [5:43:38<45:18,  5.33s/it]

 88%|████████▊ | 3727/4236 [5:43:43<44:03,  5.19s/it]

 88%|████████▊ | 3728/4236 [5:43:49<46:24,  5.48s/it]


 88%|████████▊ | 3730/4236 [5:44:00<46:21,  5.50s/it]

 88%|████████▊ | 3731/4236 [5:44:04<43:18,  5.15s/it]
{'loss': 1.7419, 'grad_norm': 0.34202515376443027, 'learning_rate': 7.365241859918659e-06, 'epoch': 0.88}


 88%|████████▊ | 3733/4236 [5:44:14<41:17,  4.92s/it]
{'loss': 1.7426, 'grad_norm': 0.28682453204131186, 'learning_rate': 7.3077385921760166e-06, 'epoch': 0.88}

 88%|████████▊ | 3734/4236 [5:44:19<41:55,  5.01s/it]

 88%|████████▊ | 3735/4236 [5:44:23<38:46,  4.64s/it]

 88%|████████▊ | 3736/4236 [5:44:26<36:24,  4.37s/it]


 88%|████████▊ | 3738/4236 [5:44:35<36:28,  4.39s/it]
{'loss': 1.6501, 'grad_norm': 0.30734476998877536, 'learning_rate': 7.164929397086817e-06, 'epoch': 0.88}

 88%|████████▊ | 3739/4236 [5:44:41<38:31,  4.65s/it]


 88%|████████▊ | 3741/4236 [5:44:49<37:48,  4.58s/it]

 88%|████████▊ | 3742/4236 [5:44:53<35:56,  4.37s/it]

 88%|████████▊ | 3743/4236 [5:44:57<35:15,  4.29s/it]

 88%|████████▊ | 3744/4236 [5:45:02<36:17,  4.43s/it]

 88%|████████▊ | 3745/4236 [5:45:06<34:24,  4.20s/it]
{'loss': 1.73, 'grad_norm': 0.3020678699620551, 'learning_rate': 6.967277328443456e-06, 'epoch': 0.88}

 88%|████████▊ | 3746/4236 [5:45:13<41:33,  5.09s/it]

 88%|████████▊ | 3747/4236 [5:45:17<37:43,  4.63s/it]

 88%|████████▊ | 3748/4236 [5:45:23<40:55,  5.03s/it]

 89%|████████▊ | 3749/4236 [5:45:30<47:58,  5.91s/it]


 89%|████████▊ | 3751/4236 [5:45:50<1:05:21,  8.09s/it]
{'loss': 1.6421, 'grad_norm': 0.2996795291424673, 'learning_rate': 6.799983021802692e-06, 'epoch': 0.89}


 89%|████████▊ | 3753/4236 [5:46:00<52:30,  6.52s/it]

 89%|████████▊ | 3754/4236 [5:46:04<47:26,  5.91s/it]
{'loss': 1.4451, 'grad_norm': 0.29829700825988703, 'learning_rate': 6.717071497546701e-06, 'epoch': 0.89}

 89%|████████▊ | 3755/4236 [5:46:17<1:03:07,  7.87s/it]

 89%|████████▊ | 3756/4236 [5:46:23<59:33,  7.44s/it]


 89%|████████▊ | 3758/4236 [5:46:37<56:51,  7.14s/it]
{'loss': 1.6029, 'grad_norm': 0.29993934605950384, 'learning_rate': 6.607286654669376e-06, 'epoch': 0.89}


 89%|████████▉ | 3760/4236 [5:46:47<47:16,  5.96s/it]
{'loss': 1.657, 'grad_norm': 0.31516802320156395, 'learning_rate': 6.552721889218194e-06, 'epoch': 0.89}
[2024-05-25 08:21:38,824] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 3761/4236 [5:47:01<1:04:42,  8.17s/it]


 89%|████████▉ | 3763/4236 [5:47:12<52:45,  6.69s/it]
{'loss': 1.5383, 'grad_norm': 0.30333885420212703, 'learning_rate': 6.471284670212907e-06, 'epoch': 0.89}

 89%|████████▉ | 3764/4236 [5:47:25<1:07:49,  8.62s/it]

 89%|████████▉ | 3765/4236 [5:47:29<56:28,  7.19s/it]

 89%|████████▉ | 3766/4236 [5:47:32<48:36,  6.21s/it]

 89%|████████▉ | 3767/4236 [5:47:41<53:54,  6.90s/it]


 89%|████████▉ | 3769/4236 [5:47:54<52:06,  6.69s/it]

 89%|████████▉ | 3770/4236 [5:47:58<45:51,  5.90s/it]
{'loss': 1.6387, 'grad_norm': 0.29408668856416487, 'learning_rate': 6.283179712372489e-06, 'epoch': 0.89}


 89%|████████▉ | 3772/4236 [5:48:14<52:40,  6.81s/it]

 89%|████████▉ | 3773/4236 [5:48:18<45:36,  5.91s/it]
{'loss': 1.7625, 'grad_norm': 0.3008536617777559, 'learning_rate': 6.203385131909067e-06, 'epoch': 0.89}

 89%|████████▉ | 3774/4236 [5:48:21<40:28,  5.26s/it]

 89%|████████▉ | 3775/4236 [5:48:29<45:59,  5.99s/it]


 89%|████████▉ | 3777/4236 [5:48:40<44:52,  5.87s/it]

 89%|████████▉ | 3778/4236 [5:48:44<40:38,  5.32s/it]

 89%|████████▉ | 3779/4236 [5:48:48<36:40,  4.81s/it]
{'loss': 1.8685, 'grad_norm': 0.3155844210472455, 'learning_rate': 6.045277506232394e-06, 'epoch': 0.89}

 89%|████████▉ | 3780/4236 [5:48:56<43:11,  5.68s/it]

 89%|████████▉ | 3781/4236 [5:49:05<51:43,  6.82s/it]


 89%|████████▉ | 3783/4236 [5:49:16<46:28,  6.15s/it]

 89%|████████▉ | 3784/4236 [5:49:20<41:32,  5.51s/it]
{'loss': 1.5849, 'grad_norm': 0.31973340373113135, 'learning_rate': 5.9150320983166526e-06, 'epoch': 0.89}

 89%|████████▉ | 3785/4236 [5:49:23<37:35,  5.00s/it]


 89%|████████▉ | 3787/4236 [5:49:32<34:53,  4.66s/it]

 89%|████████▉ | 3788/4236 [5:49:36<33:33,  4.49s/it]
{'loss': 1.7395, 'grad_norm': 0.3189879630899974, 'learning_rate': 5.811826127913855e-06, 'epoch': 0.89}

 89%|████████▉ | 3789/4236 [5:49:45<42:27,  5.70s/it]

 89%|████████▉ | 3790/4236 [5:49:51<43:23,  5.84s/it]


 90%|████████▉ | 3792/4236 [5:50:18<1:18:58, 10.67s/it]
{'loss': 1.8852, 'grad_norm': 0.31236719427146187, 'learning_rate': 5.709501520676852e-06, 'epoch': 0.9}


 90%|████████▉ | 3794/4236 [5:50:28<56:02,  7.61s/it]
{'loss': 1.7153, 'grad_norm': 0.31078779228066683, 'learning_rate': 5.658670027786561e-06, 'epoch': 0.9}


 90%|████████▉ | 3796/4236 [5:50:38<46:40,  6.37s/it]
{'loss': 1.5938, 'grad_norm': 0.3059982971837695, 'learning_rate': 5.608059234105234e-06, 'epoch': 0.9}

 90%|████████▉ | 3797/4236 [5:50:43<44:03,  6.02s/it]

 90%|████████▉ | 3798/4236 [5:50:48<39:50,  5.46s/it]

 90%|████████▉ | 3799/4236 [5:50:53<39:18,  5.40s/it]

 90%|████████▉ | 3800/4236 [5:50:57<36:59,  5.09s/it]

 90%|████████▉ | 3801/4236 [5:51:01<33:50,  4.67s/it]

 90%|████████▉ | 3802/4236 [5:51:16<55:26,  7.66s/it]

 90%|████████▉ | 3803/4236 [5:51:21<50:51,  7.05s/it]

 90%|████████▉ | 3804/4236 [5:51:25<44:19,  6.16s/it]

 90%|████████▉ | 3805/4236 [5:51:29<39:32,  5.51s/it]

 90%|████████▉ | 3806/4236 [5:51:33<36:48,  5.14s/it]


 90%|████████▉ | 3808/4236 [5:51:43<34:51,  4.89s/it]

 90%|████████▉ | 3809/4236 [5:51:49<37:04,  5.21s/it]
{'loss': 1.6465, 'grad_norm': 0.38552402407255354, 'learning_rate': 5.284476746133904e-06, 'epoch': 0.9}

 90%|████████▉ | 3810/4236 [5:51:53<34:51,  4.91s/it]

 90%|████████▉ | 3811/4236 [5:51:57<33:06,  4.67s/it]


 90%|█████████ | 3813/4236 [5:52:06<32:35,  4.62s/it]
{'loss': 1.6559, 'grad_norm': 0.34444113809103205, 'learning_rate': 5.186794812433915e-06, 'epoch': 0.9}

 90%|█████████ | 3814/4236 [5:52:11<34:18,  4.88s/it]


 90%|█████████ | 3816/4236 [5:52:19<29:32,  4.22s/it]
{'loss': 1.6377, 'grad_norm': 0.31594193191194875, 'learning_rate': 5.114115545278874e-06, 'epoch': 0.9}


 90%|█████████ | 3818/4236 [5:52:38<52:08,  7.48s/it]

 90%|█████████ | 3819/4236 [5:52:42<44:47,  6.44s/it]

 90%|█████████ | 3820/4236 [5:52:48<43:34,  6.28s/it]

 90%|█████████ | 3821/4236 [5:52:54<43:01,  6.22s/it]
{'loss': 1.6994, 'grad_norm': 0.34738822388870544, 'learning_rate': 4.99409348642359e-06, 'epoch': 0.9}


 90%|█████████ | 3823/4236 [5:53:16<1:03:44,  9.26s/it]

 90%|█████████ | 3824/4236 [5:53:24<1:00:11,  8.77s/it]
{'loss': 1.5887, 'grad_norm': 0.32406089709582797, 'learning_rate': 4.922746900659125e-06, 'epoch': 0.9}


 90%|█████████ | 3826/4236 [5:53:32<43:40,  6.39s/it]

 90%|█████████ | 3827/4236 [5:53:38<42:30,  6.24s/it]

 90%|█████████ | 3828/4236 [5:53:43<38:44,  5.70s/it]
{'loss': 1.7587, 'grad_norm': 0.36310127738620124, 'learning_rate': 4.828396656795964e-06, 'epoch': 0.9}

 90%|█████████ | 3829/4236 [5:53:49<39:56,  5.89s/it]


 90%|█████████ | 3831/4236 [5:54:00<38:52,  5.76s/it]

 90%|█████████ | 3832/4236 [5:54:04<34:23,  5.11s/it]

 90%|█████████ | 3833/4236 [5:54:08<32:45,  4.88s/it]

 91%|█████████ | 3834/4236 [5:54:13<31:46,  4.74s/it]
{'loss': 1.7296, 'grad_norm': 0.2941033712350513, 'learning_rate': 4.6885413750836216e-06, 'epoch': 0.91}


 91%|█████████ | 3836/4236 [5:54:26<38:17,  5.74s/it]
{'loss': 1.7298, 'grad_norm': 0.3193588109214735, 'learning_rate': 4.642368740353431e-06, 'epoch': 0.91}


 91%|█████████ | 3838/4236 [5:54:40<43:38,  6.58s/it]

 91%|█████████ | 3839/4236 [5:54:44<38:07,  5.76s/it]

 91%|█████████ | 3840/4236 [5:54:48<34:14,  5.19s/it]
{'loss': 1.8012, 'grad_norm': 0.30835110749030525, 'learning_rate': 4.550692808525003e-06, 'epoch': 0.91}

 91%|█████████ | 3841/4236 [5:54:53<34:35,  5.25s/it]


 91%|█████████ | 3843/4236 [5:55:06<38:30,  5.88s/it]
{'loss': 1.6542, 'grad_norm': 0.3033010899816213, 'learning_rate': 4.482521952050356e-06, 'epoch': 0.91}

 91%|█████████ | 3844/4236 [5:55:13<41:00,  6.28s/it]


 91%|█████████ | 3846/4236 [5:55:22<34:57,  5.38s/it]
{'loss': 1.7264, 'grad_norm': 0.3130652529823807, 'learning_rate': 4.4148538594239174e-06, 'epoch': 0.91}


 91%|█████████ | 3848/4236 [5:55:40<44:34,  6.89s/it]

 91%|█████████ | 3849/4236 [5:55:45<40:03,  6.21s/it]
{'loss': 1.5489, 'grad_norm': 0.29641245386210796, 'learning_rate': 4.34768888682211e-06, 'epoch': 0.91}

 91%|█████████ | 3850/4236 [5:55:50<37:20,  5.80s/it]


 91%|█████████ | 3852/4236 [5:55:59<32:41,  5.11s/it]

 91%|█████████ | 3853/4236 [5:56:03<30:10,  4.73s/it]

 91%|█████████ | 3854/4236 [5:56:06<27:40,  4.35s/it]

 91%|█████████ | 3855/4236 [5:56:11<28:09,  4.43s/it]
{'loss': 1.5814, 'grad_norm': 0.2998076596347595, 'learning_rate': 4.214869713154956e-06, 'epoch': 0.91}

 91%|█████████ | 3856/4236 [5:56:15<28:03,  4.43s/it]

 91%|█████████ | 3857/4236 [5:56:29<46:25,  7.35s/it]

 91%|█████████ | 3858/4236 [5:56:36<44:32,  7.07s/it]

 91%|█████████ | 3859/4236 [5:56:39<38:01,  6.05s/it]


 91%|█████████ | 3861/4236 [5:56:48<33:08,  5.30s/it]

 91%|█████████ | 3862/4236 [5:56:52<30:29,  4.89s/it]

 91%|█████████ | 3863/4236 [5:57:01<37:15,  5.99s/it]

 91%|█████████ | 3864/4236 [5:57:05<33:20,  5.38s/it]

 91%|█████████ | 3865/4236 [5:57:09<30:22,  4.91s/it]
{'loss': 1.7643, 'grad_norm': 0.3540332156225177, 'learning_rate': 3.997987314268259e-06, 'epoch': 0.91}


 91%|█████████▏| 3867/4236 [5:57:17<27:25,  4.46s/it]

 91%|█████████▏| 3868/4236 [5:57:21<26:29,  4.32s/it]
{'loss': 1.7538, 'grad_norm': 0.321872324694248, 'learning_rate': 3.934016868711265e-06, 'epoch': 0.91}

 91%|█████████▏| 3869/4236 [5:57:26<27:26,  4.49s/it]


 91%|█████████▏| 3871/4236 [5:57:36<29:12,  4.80s/it]

 91%|█████████▏| 3872/4236 [5:57:40<27:41,  4.56s/it]
{'loss': 1.5044, 'grad_norm': 0.33102769284072636, 'learning_rate': 3.849509567257959e-06, 'epoch': 0.91}

 91%|█████████▏| 3873/4236 [5:57:44<25:48,  4.26s/it]

 91%|█████████▏| 3874/4236 [5:57:49<27:52,  4.62s/it]


 92%|█████████▏| 3876/4236 [5:57:58<27:47,  4.63s/it]

 92%|█████████▏| 3877/4236 [5:58:03<27:04,  4.53s/it]

 92%|█████████▏| 3878/4236 [5:58:09<30:05,  5.04s/it]

 92%|█████████▏| 3879/4236 [5:58:13<28:02,  4.71s/it]
{'loss': 1.7355, 'grad_norm': 0.29260577143746413, 'learning_rate': 3.7037872251210916e-06, 'epoch': 0.92}

 92%|█████████▏| 3880/4236 [5:58:30<49:26,  8.33s/it]


 92%|█████████▏| 3882/4236 [5:58:42<42:11,  7.15s/it]

 92%|█████████▏| 3883/4236 [5:58:47<37:40,  6.40s/it]

 92%|█████████▏| 3884/4236 [5:58:59<46:35,  7.94s/it]
{'loss': 1.5468, 'grad_norm': 0.3034091412709837, 'learning_rate': 3.6013891368177344e-06, 'epoch': 0.92}

 92%|█████████▏| 3885/4236 [5:59:09<51:07,  8.74s/it]


 92%|█████████▏| 3887/4236 [5:59:21<41:32,  7.14s/it]

 92%|█████████▏| 3888/4236 [5:59:27<38:41,  6.67s/it]

 92%|█████████▏| 3889/4236 [5:59:31<34:35,  5.98s/it]

 92%|█████████▏| 3890/4236 [5:59:35<31:06,  5.39s/it]
{'loss': 1.5972, 'grad_norm': 0.32904963451554464, 'learning_rate': 3.4803720328264733e-06, 'epoch': 0.92}


 92%|█████████▏| 3892/4236 [5:59:44<28:40,  5.00s/it]
{'loss': 1.4686, 'grad_norm': 0.2803885095430048, 'learning_rate': 3.4404844629715494e-06, 'epoch': 0.92}


 92%|█████████▏| 3894/4236 [5:59:53<26:16,  4.61s/it]

 92%|█████████▏| 3895/4236 [5:59:56<24:32,  4.32s/it]

 92%|█████████▏| 3896/4236 [6:00:00<23:53,  4.22s/it]
{'loss': 1.8348, 'grad_norm': 0.3657957897414101, 'learning_rate': 3.361387081137457e-06, 'epoch': 0.92}

 92%|█████████▏| 3897/4236 [6:00:06<25:27,  4.51s/it]

 92%|█████████▏| 3898/4236 [6:00:10<25:40,  4.56s/it]

 92%|█████████▏| 3899/4236 [6:00:14<24:36,  4.38s/it]

 92%|█████████▏| 3900/4236 [6:00:23<32:22,  5.78s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.7582, 'grad_norm': 0.3250319114963687, 'learning_rate': 3.263787101919036e-06, 'epoch': 0.92}
 92%|█████████▏| 3901/4236 [6:00:44<57:04, 10.22s/it]

 92%|█████████▏| 3902/4236 [6:00:48<46:14,  8.31s/it]

 92%|█████████▏| 3903/4236 [6:00:53<41:41,  7.51s/it]

 92%|█████████▏| 3904/4236 [6:00:58<36:26,  6.59s/it]


 92%|█████████▏| 3906/4236 [6:01:06<29:59,  5.45s/it]
{'loss': 1.5245, 'grad_norm': 0.3190114140414361, 'learning_rate': 3.167601507031348e-06, 'epoch': 0.92}

 92%|█████████▏| 3907/4236 [6:01:10<27:06,  4.94s/it]

 92%|█████████▏| 3908/4236 [6:01:14<25:22,  4.64s/it]

 92%|█████████▏| 3909/4236 [6:01:20<27:18,  5.01s/it]

 92%|█████████▏| 3910/4236 [6:01:24<25:24,  4.68s/it]

 92%|█████████▏| 3911/4236 [6:01:28<24:42,  4.56s/it]


 92%|█████████▏| 3913/4236 [6:01:41<30:40,  5.70s/it]
{'loss': 1.7404, 'grad_norm': 0.3062307420457945, 'learning_rate': 3.0353205135826733e-06, 'epoch': 0.92}


 92%|█████████▏| 3915/4236 [6:01:53<31:25,  5.88s/it]

 92%|█████████▏| 3916/4236 [6:02:01<33:37,  6.31s/it]
{'loss': 1.622, 'grad_norm': 0.31326075103379086, 'learning_rate': 2.9794790748826186e-06, 'epoch': 0.92}

 92%|█████████▏| 3917/4236 [6:02:04<29:32,  5.56s/it]


 93%|█████████▎| 3919/4236 [6:02:13<25:28,  4.82s/it]

 93%|█████████▎| 3920/4236 [6:02:17<25:22,  4.82s/it]

 93%|█████████▎| 3921/4236 [6:02:21<24:00,  4.57s/it]

 93%|█████████▎| 3922/4236 [6:02:25<22:42,  4.34s/it]

 93%|█████████▎| 3923/4236 [6:02:29<21:53,  4.20s/it]
{'loss': 1.746, 'grad_norm': 0.3003859194274486, 'learning_rate': 2.851168846812713e-06, 'epoch': 0.93}


 93%|█████████▎| 3925/4236 [6:02:41<27:05,  5.23s/it]
{'loss': 1.6005, 'grad_norm': 0.2915235914009537, 'learning_rate': 2.8150199724486625e-06, 'epoch': 0.93}


 93%|█████████▎| 3927/4236 [6:02:55<32:41,  6.35s/it]

 93%|█████████▎| 3928/4236 [6:02:59<29:05,  5.67s/it]
{'loss': 1.698, 'grad_norm': 0.32339191541047607, 'learning_rate': 2.7612229713524508e-06, 'epoch': 0.93}


 93%|█████████▎| 3930/4236 [6:03:09<27:39,  5.42s/it]
{'loss': 1.6427, 'grad_norm': 0.32910642554270414, 'learning_rate': 2.7256426332767373e-06, 'epoch': 0.93}

 93%|█████████▎| 3931/4236 [6:03:14<25:55,  5.10s/it]

 93%|█████████▎| 3932/4236 [6:03:18<23:51,  4.71s/it]

 93%|█████████▎| 3933/4236 [6:03:24<25:44,  5.10s/it]

 93%|█████████▎| 3934/4236 [6:03:28<25:08,  5.00s/it]


 93%|█████████▎| 3936/4236 [6:03:37<22:50,  4.57s/it]

 93%|█████████▎| 3937/4236 [6:03:49<33:58,  6.82s/it]
{'loss': 1.7521, 'grad_norm': 0.3068543320544863, 'learning_rate': 2.6029040315058485e-06, 'epoch': 0.93}


 93%|█████████▎| 3939/4236 [6:04:01<31:57,  6.46s/it]
{'loss': 1.6059, 'grad_norm': 0.3380957356837387, 'learning_rate': 2.5683483641168128e-06, 'epoch': 0.93}

 93%|█████████▎| 3940/4236 [6:04:06<30:10,  6.12s/it]


 93%|█████████▎| 3942/4236 [6:04:15<25:51,  5.28s/it]

 93%|█████████▎| 3943/4236 [6:04:19<23:25,  4.80s/it]

 93%|█████████▎| 3944/4236 [6:04:23<23:13,  4.77s/it]
{'loss': 1.6699, 'grad_norm': 0.33093845074113104, 'learning_rate': 2.482956558846017e-06, 'epoch': 0.93}

 93%|█████████▎| 3945/4236 [6:04:28<23:06,  4.76s/it]


 93%|█████████▎| 3947/4236 [6:04:43<29:53,  6.21s/it]
{'loss': 1.9399, 'grad_norm': 0.3004314437421122, 'learning_rate': 2.432405781071534e-06, 'epoch': 0.93}

 93%|█████████▎| 3948/4236 [6:04:50<31:10,  6.49s/it]


 93%|█████████▎| 3950/4236 [6:05:01<28:33,  5.99s/it]

 93%|█████████▎| 3951/4236 [6:05:05<25:26,  5.36s/it]
{'loss': 1.753, 'grad_norm': 0.3269669169282282, 'learning_rate': 2.3658036526572726e-06, 'epoch': 0.93}

 93%|█████████▎| 3952/4236 [6:05:09<22:54,  4.84s/it]

 93%|█████████▎| 3953/4236 [6:05:12<21:13,  4.50s/it]

 93%|█████████▎| 3954/4236 [6:05:18<23:07,  4.92s/it]


 93%|█████████▎| 3956/4236 [6:05:29<24:03,  5.16s/it]

 93%|█████████▎| 3957/4236 [6:05:33<22:42,  4.88s/it]

 93%|█████████▎| 3958/4236 [6:05:38<21:25,  4.62s/it]
{'loss': 1.751, 'grad_norm': 0.309056405453815, 'learning_rate': 2.2514486702151395e-06, 'epoch': 0.93}

 93%|█████████▎| 3959/4236 [6:05:42<21:31,  4.66s/it]


 94%|█████████▎| 3961/4236 [6:06:00<32:48,  7.16s/it]
[2024-05-25 08:40:37,609] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 94%|█████████▎| 3962/4236 [6:06:03<27:45,  6.08s/it]

 94%|█████████▎| 3963/4236 [6:06:12<30:52,  6.79s/it]

 94%|█████████▎| 3964/4236 [6:06:15<26:30,  5.85s/it]

 94%|█████████▎| 3965/4236 [6:06:19<24:14,  5.37s/it]
{'loss': 1.8796, 'grad_norm': 0.3342290971658632, 'learning_rate': 2.1398948886144577e-06, 'epoch': 0.94}

 94%|█████████▎| 3966/4236 [6:06:24<23:28,  5.22s/it]

 94%|█████████▎| 3967/4236 [6:06:28<21:53,  4.88s/it]

 94%|█████████▎| 3968/4236 [6:06:32<20:19,  4.55s/it]

 94%|█████████▎| 3969/4236 [6:06:36<18:49,  4.23s/it]

 94%|█████████▎| 3970/4236 [6:06:41<19:48,  4.47s/it]


 94%|█████████▍| 3972/4236 [6:06:51<21:31,  4.89s/it]

 94%|█████████▍| 3973/4236 [6:06:55<19:59,  4.56s/it]
{'loss': 1.7433, 'grad_norm': 0.28756077906417765, 'learning_rate': 2.0158389915421184e-06, 'epoch': 0.94}


 94%|█████████▍| 3975/4236 [6:07:03<18:19,  4.21s/it]
{'loss': 1.7078, 'grad_norm': 0.3089373441145041, 'learning_rate': 1.9853978900709503e-06, 'epoch': 0.94}

 94%|█████████▍| 3976/4236 [6:07:11<22:50,  5.27s/it]


 94%|█████████▍| 3978/4236 [6:07:21<22:46,  5.29s/it]
{'loss': 1.4893, 'grad_norm': 0.33484134415311106, 'learning_rate': 1.9401661831259355e-06, 'epoch': 0.94}

 94%|█████████▍| 3979/4236 [6:07:26<21:49,  5.10s/it]

 94%|█████████▍| 3980/4236 [6:07:31<21:24,  5.02s/it]


 94%|█████████▍| 3982/4236 [6:07:41<21:42,  5.13s/it]
{'loss': 1.7962, 'grad_norm': 0.31898087922414786, 'learning_rate': 1.8806601746646502e-06, 'epoch': 0.94}

 94%|█████████▍| 3983/4236 [6:07:46<21:30,  5.10s/it]


 94%|█████████▍| 3985/4236 [6:07:55<19:27,  4.65s/it]

 94%|█████████▍| 3986/4236 [6:07:59<18:46,  4.50s/it]
{'loss': 1.5562, 'grad_norm': 0.3122418893827673, 'learning_rate': 1.822072315143053e-06, 'epoch': 0.94}


 94%|█████████▍| 3988/4236 [6:08:14<25:18,  6.12s/it]

 94%|█████████▍| 3989/4236 [6:08:18<22:37,  5.50s/it]
{'loss': 1.8341, 'grad_norm': 0.3299520035333393, 'learning_rate': 1.778734285863859e-06, 'epoch': 0.94}

 94%|█████████▍| 3990/4236 [6:08:28<28:44,  7.01s/it]


 94%|█████████▍| 3992/4236 [6:08:39<25:40,  6.31s/it]

 94%|█████████▍| 3993/4236 [6:08:44<23:24,  5.78s/it]

 94%|█████████▍| 3994/4236 [6:08:48<21:05,  5.23s/it]

 94%|█████████▍| 3995/4236 [6:08:54<21:57,  5.47s/it]

 94%|█████████▍| 3996/4236 [6:08:59<22:16,  5.57s/it]

 94%|█████████▍| 3997/4236 [6:09:05<22:11,  5.57s/it]
{'loss': 1.6945, 'grad_norm': 0.34352722284359793, 'learning_rate': 1.6656943520394265e-06, 'epoch': 0.94}

 94%|█████████▍| 3998/4236 [6:09:11<22:34,  5.69s/it]

 94%|█████████▍| 3999/4236 [6:09:17<22:44,  5.76s/it]


 94%|█████████▍| 4001/4236 [6:09:25<19:40,  5.02s/it]
{'loss': 1.6675, 'grad_norm': 0.34016194707271774, 'learning_rate': 1.6105543635510202e-06, 'epoch': 0.94}


 94%|█████████▍| 4003/4236 [6:09:35<19:08,  4.93s/it]
{'loss': 1.8688, 'grad_norm': 0.3450803878351647, 'learning_rate': 1.5833295910627988e-06, 'epoch': 0.94}


 95%|█████████▍| 4005/4236 [6:09:51<27:05,  7.04s/it]

 95%|█████████▍| 4006/4236 [6:09:55<23:07,  6.03s/it]

 95%|█████████▍| 4007/4236 [6:10:04<25:55,  6.79s/it]

 95%|█████████▍| 4008/4236 [6:10:08<22:32,  5.93s/it]
{'loss': 1.7017, 'grad_norm': 0.3202243454927746, 'learning_rate': 1.5162750669274972e-06, 'epoch': 0.95}

 95%|█████████▍| 4009/4236 [6:10:12<21:08,  5.59s/it]

 95%|█████████▍| 4010/4236 [6:10:17<19:37,  5.21s/it]


 95%|█████████▍| 4012/4236 [6:10:35<25:32,  6.84s/it]

 95%|█████████▍| 4013/4236 [6:10:40<22:44,  6.12s/it]

 95%|█████████▍| 4014/4236 [6:10:43<19:55,  5.39s/it]

 95%|█████████▍| 4015/4236 [6:10:50<20:59,  5.70s/it]

 95%|█████████▍| 4016/4236 [6:10:54<19:12,  5.24s/it]
{'loss': 1.6436, 'grad_norm': 0.3066818434442339, 'learning_rate': 1.411983292619501e-06, 'epoch': 0.95}


 95%|█████████▍| 4018/4236 [6:11:07<22:06,  6.08s/it]

 95%|█████████▍| 4019/4236 [6:11:23<33:06,  9.15s/it]

 95%|█████████▍| 4020/4236 [6:11:27<27:24,  7.62s/it]

 95%|█████████▍| 4021/4236 [6:11:33<25:42,  7.18s/it]

 95%|█████████▍| 4022/4236 [6:11:38<22:17,  6.25s/it]
{'loss': 1.6082, 'grad_norm': 0.2963892536144894, 'learning_rate': 1.3361859011829293e-06, 'epoch': 0.95}


 95%|█████████▍| 4024/4236 [6:11:46<17:55,  5.07s/it]
{'loss': 1.7279, 'grad_norm': 0.3394083354545769, 'learning_rate': 1.3113816479602303e-06, 'epoch': 0.95}


 95%|█████████▌| 4026/4236 [6:11:55<17:11,  4.91s/it]
{'loss': 1.5557, 'grad_norm': 0.3290382213365738, 'learning_rate': 1.286808263859407e-06, 'epoch': 0.95}


 95%|█████████▌| 4028/4236 [6:12:15<24:14,  6.99s/it]

 95%|█████████▌| 4029/4236 [6:12:20<21:32,  6.24s/it]
{'loss': 1.5659, 'grad_norm': 0.2977030489433682, 'learning_rate': 1.250381192915051e-06, 'epoch': 0.95}

 95%|█████████▌| 4030/4236 [6:12:24<19:55,  5.80s/it]


 95%|█████████▌| 4032/4236 [6:12:38<21:06,  6.21s/it]

 95%|█████████▌| 4033/4236 [6:12:41<18:25,  5.45s/it]

 95%|█████████▌| 4034/4236 [6:12:54<25:33,  7.59s/it]
{'loss': 1.7513, 'grad_norm': 0.34996988577075144, 'learning_rate': 1.1908245602950009e-06, 'epoch': 0.95}

 95%|█████████▌| 4035/4236 [6:12:58<22:24,  6.69s/it]

 95%|█████████▌| 4036/4236 [6:13:11<27:55,  8.38s/it]


 95%|█████████▌| 4038/4236 [6:13:22<23:24,  7.09s/it]

 95%|█████████▌| 4039/4236 [6:13:26<20:08,  6.13s/it]

 95%|█████████▌| 4040/4236 [6:13:32<19:40,  6.02s/it]
{'loss': 1.7117, 'grad_norm': 0.30484240222622244, 'learning_rate': 1.1212636713235581e-06, 'epoch': 0.95}

 95%|█████████▌| 4041/4236 [6:13:37<18:52,  5.81s/it]


 95%|█████████▌| 4043/4236 [6:13:46<16:33,  5.15s/it]
{'loss': 1.7726, 'grad_norm': 0.3262791508462791, 'learning_rate': 1.087263820135731e-06, 'epoch': 0.95}

 95%|█████████▌| 4044/4236 [6:13:51<16:01,  5.01s/it]


 96%|█████████▌| 4046/4236 [6:14:03<18:12,  5.75s/it]

 96%|█████████▌| 4047/4236 [6:14:08<17:08,  5.44s/it]
{'loss': 1.6616, 'grad_norm': 0.2794719125787854, 'learning_rate': 1.0427405925148792e-06, 'epoch': 0.96}

 96%|█████████▌| 4048/4236 [6:14:12<16:04,  5.13s/it]

 96%|█████████▌| 4049/4236 [6:14:17<15:14,  4.89s/it]

 96%|█████████▌| 4050/4236 [6:14:21<14:11,  4.58s/it]

 96%|█████████▌| 4051/4236 [6:14:27<15:36,  5.06s/it]


 96%|█████████▌| 4053/4236 [6:14:36<14:43,  4.83s/it]

 96%|█████████▌| 4054/4236 [6:14:40<14:11,  4.68s/it]

 96%|█████████▌| 4055/4236 [6:14:44<13:27,  4.46s/it]

 96%|█████████▌| 4056/4236 [6:14:48<12:49,  4.28s/it]
{'loss': 1.6135, 'grad_norm': 0.3160998422245982, 'learning_rate': 9.459496010617463e-07, 'epoch': 0.96}

 96%|█████████▌| 4057/4236 [6:14:53<13:34,  4.55s/it]

 96%|█████████▌| 4058/4236 [6:14:58<14:04,  4.74s/it]

 96%|█████████▌| 4059/4236 [6:15:03<13:32,  4.59s/it]


 96%|█████████▌| 4061/4236 [6:15:14<15:16,  5.24s/it]

 96%|█████████▌| 4062/4236 [6:15:18<14:09,  4.88s/it]
{'loss': 1.6454, 'grad_norm': 0.31715619323893735, 'learning_rate': 8.840288847790757e-07, 'epoch': 0.96}


 96%|█████████▌| 4064/4236 [6:15:30<15:04,  5.26s/it]

 96%|█████████▌| 4065/4236 [6:15:34<13:57,  4.89s/it]
{'loss': 1.6998, 'grad_norm': 0.3091640710235707, 'learning_rate': 8.538510030472125e-07, 'epoch': 0.96}

 96%|█████████▌| 4066/4236 [6:15:38<13:54,  4.91s/it]

 96%|█████████▌| 4067/4236 [6:15:43<13:08,  4.67s/it]


 96%|█████████▌| 4069/4236 [6:15:58<17:27,  6.27s/it]

 96%|█████████▌| 4070/4236 [6:16:02<15:40,  5.67s/it]
{'loss': 1.6025, 'grad_norm': 0.3148232939130156, 'learning_rate': 8.047143076844399e-07, 'epoch': 0.96}

 96%|█████████▌| 4071/4236 [6:16:06<14:36,  5.31s/it]


 96%|█████████▌| 4073/4236 [6:16:16<13:23,  4.93s/it]

 96%|█████████▌| 4074/4236 [6:16:19<12:27,  4.62s/it]
{'loss': 1.6078, 'grad_norm': 0.32422298043500036, 'learning_rate': 7.664491613706859e-07, 'epoch': 0.96}


 96%|█████████▌| 4076/4236 [6:16:32<15:23,  5.77s/it]

 96%|█████████▌| 4077/4236 [6:16:36<13:43,  5.18s/it]
{'loss': 1.6814, 'grad_norm': 0.34018591989334623, 'learning_rate': 7.383596598144471e-07, 'epoch': 0.96}

 96%|█████████▋| 4078/4236 [6:16:41<13:14,  5.03s/it]


 96%|█████████▋| 4080/4236 [6:16:52<13:59,  5.38s/it]
{'loss': 1.55, 'grad_norm': 0.33886587117920614, 'learning_rate': 7.10792629802659e-07, 'epoch': 0.96}

 96%|█████████▋| 4081/4236 [6:16:55<12:26,  4.81s/it]

 96%|█████████▋| 4082/4236 [6:16:59<11:27,  4.46s/it]

 96%|█████████▋| 4083/4236 [6:17:12<18:30,  7.26s/it]

 96%|█████████▋| 4084/4236 [6:17:17<16:28,  6.51s/it]

 96%|█████████▋| 4085/4236 [6:17:21<14:15,  5.67s/it]

 96%|█████████▋| 4086/4236 [6:17:27<14:23,  5.76s/it]


 97%|█████████▋| 4088/4236 [6:17:34<11:34,  4.69s/it]

 97%|█████████▋| 4089/4236 [6:17:40<11:59,  4.90s/it]

 97%|█████████▋| 4090/4236 [6:17:44<11:25,  4.69s/it]
{'loss': 1.7818, 'grad_norm': 0.326836027070823, 'learning_rate': 6.226777777862514e-07, 'epoch': 0.97}


 97%|█████████▋| 4092/4236 [6:17:52<10:36,  4.42s/it]
{'loss': 1.5788, 'grad_norm': 0.3470277441944936, 'learning_rate': 6.057520859582111e-07, 'epoch': 0.97}


 97%|█████████▋| 4094/4236 [6:18:04<11:51,  5.01s/it]
{'loss': 1.6663, 'grad_norm': 0.3631414463086518, 'learning_rate': 5.890589139801561e-07, 'epoch': 0.97}

 97%|█████████▋| 4095/4236 [6:18:11<13:18,  5.67s/it]

 97%|█████████▋| 4096/4236 [6:18:15<12:03,  5.17s/it]

 97%|█████████▋| 4097/4236 [6:18:29<17:45,  7.66s/it]

 97%|█████████▋| 4098/4236 [6:18:33<15:18,  6.66s/it]


 97%|█████████▋| 4100/4236 [6:18:42<12:33,  5.54s/it]
{'loss': 1.6497, 'grad_norm': 0.33670852422111325, 'learning_rate': 5.403749049405438e-07, 'epoch': 0.97}


 97%|█████████▋| 4102/4236 [6:18:52<11:37,  5.21s/it]
{'loss': 1.7206, 'grad_norm': 0.33093225076129057, 'learning_rate': 5.246121974365026e-07, 'epoch': 0.97}

 97%|█████████▋| 4103/4236 [6:19:01<13:32,  6.11s/it]


 97%|█████████▋| 4105/4236 [6:19:10<11:31,  5.28s/it]
{'loss': 1.5942, 'grad_norm': 0.34930456575536045, 'learning_rate': 5.014044781789795e-07, 'epoch': 0.97}


 97%|█████████▋| 4107/4236 [6:19:28<14:58,  6.96s/it]

 97%|█████████▋| 4108/4236 [6:19:32<12:52,  6.04s/it]

 97%|█████████▋| 4109/4236 [6:19:39<12:58,  6.13s/it]

 97%|█████████▋| 4110/4236 [6:19:44<12:22,  5.90s/it]
{'loss': 1.5031, 'grad_norm': 0.28916235829027653, 'learning_rate': 4.638888246633166e-07, 'epoch': 0.97}


 97%|█████████▋| 4112/4236 [6:19:52<10:10,  4.92s/it]
{'loss': 1.6874, 'grad_norm': 0.3384726357104849, 'learning_rate': 4.4929002334880287e-07, 'epoch': 0.97}

 97%|█████████▋| 4113/4236 [6:19:57<09:57,  4.86s/it]

 97%|█████████▋| 4114/4236 [6:20:03<10:51,  5.34s/it]


 97%|█████████▋| 4116/4236 [6:20:10<08:53,  4.45s/it]

 97%|█████████▋| 4117/4236 [6:20:24<14:11,  7.15s/it]

 97%|█████████▋| 4118/4236 [6:20:28<12:14,  6.23s/it]

 97%|█████████▋| 4119/4236 [6:20:36<13:09,  6.75s/it]

 97%|█████████▋| 4120/4236 [6:20:40<11:41,  6.04s/it]
{'loss': 1.6278, 'grad_norm': 0.36878861346062414, 'learning_rate': 3.9322401016331776e-07, 'epoch': 0.97}

 97%|█████████▋| 4121/4236 [6:20:46<11:05,  5.79s/it]

 97%|█████████▋| 4122/4236 [6:20:50<10:00,  5.26s/it]


 97%|█████████▋| 4124/4236 [6:21:00<09:39,  5.17s/it]

 97%|█████████▋| 4125/4236 [6:21:04<08:57,  4.84s/it]
{'loss': 1.6343, 'grad_norm': 0.2839845613437525, 'learning_rate': 3.600758823722683e-07, 'epoch': 0.97}


 97%|█████████▋| 4127/4236 [6:21:16<09:53,  5.44s/it]

 97%|█████████▋| 4128/4236 [6:21:20<08:56,  4.97s/it]
{'loss': 1.577, 'grad_norm': 0.3201034420620886, 'learning_rate': 3.408862585817407e-07, 'epoch': 0.97}

 97%|█████████▋| 4129/4236 [6:21:26<09:17,  5.21s/it]


 98%|█████████▊| 4131/4236 [6:21:36<09:12,  5.26s/it]
{'loss': 1.7746, 'grad_norm': 0.29123631095685903, 'learning_rate': 3.222211984685641e-07, 'epoch': 0.98}


 98%|█████████▊| 4133/4236 [6:21:48<09:29,  5.53s/it]
{'loss': 1.7276, 'grad_norm': 0.3158447402828787, 'learning_rate': 3.1006929795467375e-07, 'epoch': 0.98}


 98%|█████████▊| 4135/4236 [6:21:58<08:48,  5.23s/it]

 98%|█████████▊| 4136/4236 [6:22:02<08:04,  4.85s/it]

 98%|█████████▊| 4137/4236 [6:22:06<07:53,  4.78s/it]

 98%|█████████▊| 4138/4236 [6:22:10<07:20,  4.49s/it]
{'loss': 1.6337, 'grad_norm': 0.3030734306671944, 'learning_rate': 2.807099080934017e-07, 'epoch': 0.98}

 98%|█████████▊| 4139/4236 [6:22:15<07:31,  4.65s/it]

 98%|█████████▊| 4140/4236 [6:22:20<07:21,  4.60s/it]


 98%|█████████▊| 4142/4236 [6:22:30<07:42,  4.92s/it]

 98%|█████████▊| 4143/4236 [6:22:35<07:22,  4.76s/it]
{'loss': 1.6866, 'grad_norm': 0.31442552478592656, 'learning_rate': 2.5280851826283705e-07, 'epoch': 0.98}

 98%|█████████▊| 4144/4236 [6:22:39<07:15,  4.74s/it]

 98%|█████████▊| 4145/4236 [6:22:43<06:46,  4.47s/it]


 98%|█████████▊| 4147/4236 [6:22:54<07:08,  4.82s/it]

 98%|█████████▊| 4148/4236 [6:22:59<07:03,  4.81s/it]

 98%|█████████▊| 4149/4236 [6:23:05<07:33,  5.22s/it]

 98%|█████████▊| 4150/4236 [6:23:11<07:45,  5.41s/it]

 98%|█████████▊| 4151/4236 [6:23:24<11:09,  7.88s/it]

 98%|█████████▊| 4152/4236 [6:23:36<12:46,  9.13s/it]

 98%|█████████▊| 4153/4236 [6:23:42<11:18,  8.18s/it]
{'loss': 1.7592, 'grad_norm': 0.37671667493255623, 'learning_rate': 2.013813491600569e-07, 'epoch': 0.98}

 98%|█████████▊| 4154/4236 [6:23:48<09:58,  7.30s/it]


 98%|█████████▊| 4156/4236 [6:23:58<08:31,  6.39s/it]

 98%|█████████▊| 4157/4236 [6:24:02<07:25,  5.64s/it]
{'loss': 1.8081, 'grad_norm': 0.3375282205921177, 'learning_rate': 1.824445776682504e-07, 'epoch': 0.98}

 98%|█████████▊| 4158/4236 [6:24:07<07:02,  5.42s/it]


 98%|█████████▊| 4160/4236 [6:24:15<05:50,  4.62s/it]
{'loss': 1.6752, 'grad_norm': 0.3477510768381353, 'learning_rate': 1.68854953400599e-07, 'epoch': 0.98}

 98%|█████████▊| 4161/4236 [6:24:19<05:44,  4.59s/it]

 98%|█████████▊| 4162/4236 [6:24:24<05:38,  4.58s/it]

 98%|█████████▊| 4163/4236 [6:24:28<05:20,  4.39s/it]


 98%|█████████▊| 4165/4236 [6:24:37<05:16,  4.46s/it]

 98%|█████████▊| 4166/4236 [6:24:41<05:06,  4.38s/it]

 98%|█████████▊| 4167/4236 [6:24:45<04:54,  4.27s/it]
{'loss': 1.6344, 'grad_norm': 0.3560968472675244, 'learning_rate': 1.391894398097615e-07, 'epoch': 0.98}

 98%|█████████▊| 4168/4236 [6:24:50<05:05,  4.49s/it]

 98%|█████████▊| 4169/4236 [6:24:54<04:53,  4.37s/it]


 98%|█████████▊| 4171/4236 [6:25:12<06:57,  6.42s/it]
{'loss': 1.7127, 'grad_norm': 0.3259149851513553, 'learning_rate': 1.2352252601147697e-07, 'epoch': 0.98}


 99%|█████████▊| 4173/4236 [6:25:38<09:29,  9.04s/it]
{'loss': 1.627, 'grad_norm': 0.35940827121445684, 'learning_rate': 1.1603953203079831e-07, 'epoch': 0.99}


 99%|█████████▊| 4175/4236 [6:25:55<08:31,  8.38s/it]
{'loss': 1.7554, 'grad_norm': 0.3092242871947101, 'learning_rate': 1.0879020351861612e-07, 'epoch': 0.99}


 99%|█████████▊| 4177/4236 [6:26:03<06:00,  6.11s/it]
{'loss': 1.8344, 'grad_norm': 0.3321600354671783, 'learning_rate': 1.0177455743376474e-07, 'epoch': 0.99}


 99%|█████████▊| 4179/4236 [6:26:14<05:44,  6.04s/it]

 99%|█████████▊| 4180/4236 [6:26:19<05:15,  5.64s/it]

 99%|█████████▊| 4181/4236 [6:26:27<05:48,  6.33s/it]

 99%|█████████▊| 4182/4236 [6:26:40<07:37,  8.47s/it]

 99%|█████████▊| 4183/4236 [6:26:44<06:19,  7.16s/it]
{'loss': 1.7191, 'grad_norm': 0.3267977915659739, 'learning_rate': 8.212987513159798e-08, 'epoch': 0.99}

 99%|█████████▉| 4184/4236 [6:26:49<05:37,  6.48s/it]

 99%|█████████▉| 4185/4236 [6:26:54<05:03,  5.94s/it]


 99%|█████████▉| 4187/4236 [6:27:08<05:32,  6.78s/it]

 99%|█████████▉| 4188/4236 [6:27:12<04:43,  5.91s/it]

 99%|█████████▉| 4189/4236 [6:27:16<04:12,  5.38s/it]
{'loss': 1.4437, 'grad_norm': 0.2934287440918737, 'learning_rate': 6.458889270980484e-08, 'epoch': 0.99}

 99%|█████████▉| 4190/4236 [6:27:21<04:02,  5.27s/it]

 99%|█████████▉| 4191/4236 [6:27:25<03:36,  4.82s/it]


 99%|█████████▉| 4193/4236 [6:27:43<05:15,  7.33s/it]
[2024-05-25 09:02:21,040] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.7324, 'grad_norm': 0.3060903277584483, 'learning_rate': 5.406381078409961e-08, 'epoch': 0.99}

 99%|█████████▉| 4194/4236 [6:27:47<04:26,  6.35s/it]


 99%|█████████▉| 4196/4236 [6:27:56<03:36,  5.42s/it]
{'loss': 1.7844, 'grad_norm': 0.3662695917052631, 'learning_rate': 4.6783746361866996e-08, 'epoch': 0.99}

 99%|█████████▉| 4197/4236 [6:28:00<03:13,  4.96s/it]


 99%|█████████▉| 4199/4236 [6:28:08<02:49,  4.58s/it]
{'loss': 1.6754, 'grad_norm': 0.3004086206313892, 'learning_rate': 4.002979364895465e-08, 'epoch': 0.99}

 99%|█████████▉| 4200/4236 [6:28:17<03:31,  5.88s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.6523, 'grad_norm': 0.3261799350643874, 'learning_rate': 3.581946045947415e-08, 'epoch': 0.99}
 99%|█████████▉| 4201/4236 [6:28:38<06:01, 10.32s/it]


 99%|█████████▉| 4203/4236 [6:28:57<05:13,  9.49s/it]

 99%|█████████▉| 4204/4236 [6:29:03<04:29,  8.42s/it]
{'loss': 1.9117, 'grad_norm': 0.3193170435794372, 'learning_rate': 2.994243822774801e-08, 'epoch': 0.99}

 99%|█████████▉| 4205/4236 [6:29:08<03:47,  7.35s/it]


 99%|█████████▉| 4207/4236 [6:29:23<03:42,  7.66s/it]
{'loss': 1.6504, 'grad_norm': 0.337177556924237, 'learning_rate': 2.4591616350910784e-08, 'epoch': 0.99}

 99%|█████████▉| 4208/4236 [6:29:27<03:09,  6.78s/it]


 99%|█████████▉| 4210/4236 [6:29:35<02:16,  5.25s/it]

 99%|█████████▉| 4211/4236 [6:29:49<03:14,  7.77s/it]
{'loss': 1.7301, 'grad_norm': 0.2756349388613064, 'learning_rate': 1.8275769347575467e-08, 'epoch': 0.99}

 99%|█████████▉| 4212/4236 [6:29:56<03:03,  7.66s/it]

 99%|█████████▉| 4213/4236 [6:30:00<02:31,  6.59s/it]


100%|█████████▉| 4215/4236 [6:30:13<02:14,  6.38s/it]
{'loss': 1.7012, 'grad_norm': 0.3169388383928963, 'learning_rate': 1.2895498493203928e-08, 'epoch': 1.0}

100%|█████████▉| 4216/4236 [6:30:18<01:56,  5.81s/it]


100%|█████████▉| 4218/4236 [6:30:30<01:53,  6.32s/it]
{'loss': 1.6229, 'grad_norm': 0.3294369762224112, 'learning_rate': 9.474297815437184e-09, 'epoch': 1.0}

100%|█████████▉| 4219/4236 [6:30:36<01:41,  5.95s/it]


100%|█████████▉| 4221/4236 [6:30:46<01:23,  5.59s/it]
{'loss': 1.6497, 'grad_norm': 0.2976267192465114, 'learning_rate': 6.579405228257507e-09, 'epoch': 1.0}

100%|█████████▉| 4222/4236 [6:31:00<01:52,  8.06s/it]


100%|█████████▉| 4224/4236 [6:31:19<01:51,  9.29s/it]

100%|█████████▉| 4225/4236 [6:31:25<01:30,  8.19s/it]
{'loss': 1.761, 'grad_norm': 0.3356573988838725, 'learning_rate': 3.5382758569624254e-09, 'epoch': 1.0}


100%|█████████▉| 4227/4236 [6:31:33<00:55,  6.14s/it]
{'loss': 1.5878, 'grad_norm': 0.30813013237171977, 'learning_rate': 2.3686025052538363e-09, 'epoch': 1.0}


100%|█████████▉| 4229/4236 [6:31:49<00:49,  7.05s/it]

100%|█████████▉| 4230/4236 [6:31:53<00:36,  6.11s/it]

100%|█████████▉| 4231/4236 [6:31:57<00:27,  5.51s/it]

100%|█████████▉| 4232/4236 [6:32:03<00:22,  5.54s/it]

100%|█████████▉| 4233/4236 [6:32:06<00:15,  5.03s/it]
{'loss': 1.7064, 'grad_norm': 0.32692083297901064, 'learning_rate': 2.631789796425643e-10, 'epoch': 1.0}


100%|█████████▉| 4235/4236 [6:32:17<00:05,  5.45s/it]
{'loss': 1.7364, 'grad_norm': 0.3216676304253855, 'learning_rate': 2.9242120247463536e-11, 'epoch': 1.0}
{'loss': 1.7046, 'grad_norm': 0.31480906086356464, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 4236/4236 [6:32:22<00:00,  5.56s/it]
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(