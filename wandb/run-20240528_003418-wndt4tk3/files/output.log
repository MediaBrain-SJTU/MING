/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3886 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3886 [00:12<13:45:59, 12.76s/it]

  0%|          | 2/3886 [00:18<9:35:24,  8.89s/it]
{'loss': 2.3306, 'grad_norm': 1.2683524793796404, 'learning_rate': 3.4188034188034193e-06, 'epoch': 0.0}


  0%|          | 4/3886 [00:32<8:07:35,  7.54s/it]

  0%|          | 5/3886 [00:38<7:25:08,  6.88s/it]

  0%|          | 6/3886 [00:47<8:06:02,  7.52s/it]
{'loss': 2.3103, 'grad_norm': 1.2588572390102306, 'learning_rate': 1.0256410256410256e-05, 'epoch': 0.0}

  0%|          | 7/3886 [00:58<9:18:44,  8.64s/it]

  0%|          | 8/3886 [01:08<9:57:51,  9.25s/it]

  0%|          | 9/3886 [01:14<8:44:06,  8.11s/it]

  0%|          | 10/3886 [01:20<8:02:03,  7.46s/it]

  0%|          | 11/3886 [01:25<7:21:11,  6.83s/it]

  0%|          | 12/3886 [01:31<7:11:21,  6.68s/it]

  0%|          | 13/3886 [01:37<6:55:45,  6.44s/it]

  0%|          | 14/3886 [01:44<7:00:35,  6.52s/it]

  0%|          | 15/3886 [01:49<6:29:32,  6.04s/it]

  0%|          | 16/3886 [01:54<6:14:22,  5.80s/it]

  0%|          | 17/3886 [02:03<7:19:47,  6.82s/it]

  0%|          | 18/3886 [02:17<9:38:31,  8.97s/it]


  1%|          | 20/3886 [02:29<7:45:10,  7.22s/it]
{'loss': 1.8511, 'grad_norm': 1.2076206214160543, 'learning_rate': 3.418803418803419e-05, 'epoch': 0.01}

  1%|          | 21/3886 [02:35<7:24:14,  6.90s/it]

  1%|          | 22/3886 [02:44<8:00:33,  7.46s/it]

  1%|          | 23/3886 [02:51<7:57:43,  7.42s/it]

  1%|          | 24/3886 [02:59<8:19:16,  7.76s/it]

  1%|          | 25/3886 [03:06<7:51:24,  7.33s/it]

  1%|          | 26/3886 [03:13<7:51:32,  7.33s/it]

  1%|          | 27/3886 [03:21<8:04:52,  7.54s/it]

  1%|          | 28/3886 [03:30<8:25:45,  7.87s/it]


  1%|          | 30/3886 [03:41<7:04:48,  6.61s/it]
{'loss': 1.6438, 'grad_norm': 0.3689349945010203, 'learning_rate': 5.128205128205128e-05, 'epoch': 0.01}

  1%|          | 31/3886 [03:48<7:12:47,  6.74s/it]

  1%|          | 32/3886 [03:54<6:54:52,  6.46s/it]


  1%|          | 34/3886 [04:07<7:01:32,  6.57s/it]
{'loss': 1.6247, 'grad_norm': 0.405268665903204, 'learning_rate': 5.8119658119658126e-05, 'epoch': 0.01}

  1%|          | 35/3886 [04:14<7:12:45,  6.74s/it]

  1%|          | 36/3886 [04:23<8:01:55,  7.51s/it]

  1%|          | 37/3886 [04:32<8:26:25,  7.89s/it]

  1%|          | 38/3886 [04:40<8:29:13,  7.94s/it]

  1%|          | 39/3886 [04:46<7:53:14,  7.38s/it]

  1%|          | 40/3886 [04:52<7:34:20,  7.09s/it]

  1%|          | 41/3886 [04:58<7:08:57,  6.69s/it]

  1%|          | 42/3886 [05:03<6:42:16,  6.28s/it]

  1%|          | 43/3886 [05:08<6:15:05,  5.86s/it]

  1%|          | 44/3886 [05:14<6:03:40,  5.68s/it]

  1%|          | 45/3886 [05:20<6:15:38,  5.87s/it]


  1%|          | 47/3886 [05:31<6:00:55,  5.64s/it]
{'loss': 1.4034, 'grad_norm': 0.2594172184219144, 'learning_rate': 8.034188034188035e-05, 'epoch': 0.01}

  1%|          | 48/3886 [05:37<6:02:54,  5.67s/it]

  1%|▏         | 49/3886 [05:42<6:04:34,  5.70s/it]

  1%|▏         | 50/3886 [05:50<6:43:14,  6.31s/it]

  1%|▏         | 51/3886 [05:58<7:10:07,  6.73s/it]


  1%|▏         | 53/3886 [06:13<7:20:38,  6.90s/it]
{'loss': 1.4564, 'grad_norm': 0.2511807356754305, 'learning_rate': 9.05982905982906e-05, 'epoch': 0.01}

  1%|▏         | 54/3886 [06:20<7:28:15,  7.02s/it]


  1%|▏         | 56/3886 [06:35<7:27:42,  7.01s/it]
{'loss': 1.4873, 'grad_norm': 0.22315424689687963, 'learning_rate': 9.572649572649574e-05, 'epoch': 0.01}

  1%|▏         | 57/3886 [06:40<6:57:48,  6.55s/it]

  1%|▏         | 58/3886 [06:47<7:07:09,  6.70s/it]

  2%|▏         | 59/3886 [06:54<7:02:52,  6.63s/it]


  2%|▏         | 61/3886 [07:09<7:44:43,  7.29s/it]
{'loss': 1.4132, 'grad_norm': 0.2629220162457602, 'learning_rate': 0.00010427350427350428, 'epoch': 0.02}

  2%|▏         | 62/3886 [07:18<8:18:33,  7.82s/it]

  2%|▏         | 63/3886 [07:23<7:31:18,  7.08s/it]

  2%|▏         | 64/3886 [07:31<7:44:50,  7.30s/it]

  2%|▏         | 65/3886 [07:42<8:52:25,  8.36s/it]

  2%|▏         | 66/3886 [07:50<8:50:18,  8.33s/it]

  2%|▏         | 67/3886 [07:59<8:46:45,  8.28s/it]

  2%|▏         | 68/3886 [08:08<9:08:31,  8.62s/it]


  2%|▏         | 70/3886 [08:19<7:22:03,  6.95s/it]
{'loss': 1.4755, 'grad_norm': 0.24174619636532443, 'learning_rate': 0.00011965811965811966, 'epoch': 0.02}

  2%|▏         | 71/3886 [08:24<6:51:04,  6.47s/it]

  2%|▏         | 72/3886 [08:30<6:41:07,  6.31s/it]


  2%|▏         | 74/3886 [08:41<6:14:47,  5.90s/it]
{'loss': 1.5808, 'grad_norm': 0.22432776133680682, 'learning_rate': 0.0001264957264957265, 'epoch': 0.02}

  2%|▏         | 75/3886 [08:46<5:54:28,  5.58s/it]

  2%|▏         | 76/3886 [08:51<5:36:40,  5.30s/it]

  2%|▏         | 77/3886 [08:57<6:04:57,  5.75s/it]

  2%|▏         | 78/3886 [09:05<6:45:46,  6.39s/it]

  2%|▏         | 79/3886 [09:14<7:28:19,  7.07s/it]

  2%|▏         | 80/3886 [09:20<7:12:44,  6.82s/it]

  2%|▏         | 81/3886 [09:27<7:07:30,  6.74s/it]

  2%|▏         | 82/3886 [09:32<6:36:27,  6.25s/it]

  2%|▏         | 83/3886 [09:39<6:48:23,  6.44s/it]

  2%|▏         | 84/3886 [09:44<6:16:27,  5.94s/it]


  2%|▏         | 86/3886 [09:55<6:11:16,  5.86s/it]
{'loss': 1.4913, 'grad_norm': 0.2532258531329231, 'learning_rate': 0.00014700854700854703, 'epoch': 0.02}

  2%|▏         | 87/3886 [10:00<5:56:56,  5.64s/it]

  2%|▏         | 88/3886 [10:06<5:54:55,  5.61s/it]

  2%|▏         | 89/3886 [10:16<7:24:09,  7.02s/it]

  2%|▏         | 90/3886 [10:24<7:41:05,  7.29s/it]


  2%|▏         | 92/3886 [10:37<7:11:28,  6.82s/it]
{'loss': 1.4279, 'grad_norm': 0.24876079885026722, 'learning_rate': 0.00015726495726495727, 'epoch': 0.02}

  2%|▏         | 93/3886 [10:43<6:46:53,  6.44s/it]

  2%|▏         | 94/3886 [10:49<6:53:26,  6.54s/it]

  2%|▏         | 95/3886 [10:57<7:13:53,  6.87s/it]

  2%|▏         | 96/3886 [11:04<7:09:51,  6.81s/it]

  2%|▏         | 97/3886 [11:10<7:10:38,  6.82s/it]

  3%|▎         | 98/3886 [11:17<7:13:26,  6.87s/it]

  3%|▎         | 99/3886 [11:23<6:44:05,  6.40s/it]

  3%|▎         | 100/3886 [11:29<6:34:07,  6.25s/it]

  3%|▎         | 101/3886 [11:35<6:44:37,  6.41s/it]

  3%|▎         | 102/3886 [11:42<6:49:50,  6.50s/it]

  3%|▎         | 103/3886 [11:48<6:35:58,  6.28s/it]

  3%|▎         | 104/3886 [11:58<7:42:36,  7.34s/it]

  3%|▎         | 105/3886 [12:06<8:02:15,  7.65s/it]

  3%|▎         | 106/3886 [12:12<7:37:54,  7.27s/it]


  3%|▎         | 108/3886 [12:23<6:32:35,  6.23s/it]
{'loss': 1.4052, 'grad_norm': 0.23090579252280338, 'learning_rate': 0.00018461538461538463, 'epoch': 0.03}

  3%|▎         | 109/3886 [12:29<6:21:27,  6.06s/it]

  3%|▎         | 110/3886 [12:36<6:52:59,  6.56s/it]

  3%|▎         | 111/3886 [12:42<6:30:19,  6.20s/it]

  3%|▎         | 112/3886 [12:49<6:45:53,  6.45s/it]


  3%|▎         | 114/3886 [13:07<8:30:55,  8.13s/it]
{'loss': 1.3827, 'grad_norm': 0.24314332152670226, 'learning_rate': 0.00019487179487179487, 'epoch': 0.03}

  3%|▎         | 115/3886 [13:17<9:00:44,  8.60s/it]


  3%|▎         | 117/3886 [13:29<7:39:11,  7.31s/it]
{'loss': 1.3609, 'grad_norm': 0.2372435208110376, 'learning_rate': 0.0002, 'epoch': 0.03}

  3%|▎         | 118/3886 [13:40<8:37:38,  8.24s/it]

  3%|▎         | 119/3886 [13:55<10:49:48, 10.35s/it]

  3%|▎         | 120/3886 [14:03<10:07:01,  9.67s/it]

  3%|▎         | 121/3886 [14:09<8:50:29,  8.45s/it]


  3%|▎         | 123/3886 [14:20<7:14:19,  6.93s/it]
{'loss': 1.3624, 'grad_norm': 0.2379423255356518, 'learning_rate': 0.00019999874939813969, 'epoch': 0.03}

  3%|▎         | 124/3886 [14:26<7:14:06,  6.92s/it]

  3%|▎         | 125/3886 [14:32<6:50:56,  6.56s/it]


  3%|▎         | 127/3886 [14:44<6:23:17,  6.12s/it]

  3%|▎         | 128/3886 [14:49<6:15:58,  6.00s/it]
{'loss': 1.5698, 'grad_norm': 0.24856988255915358, 'learning_rate': 0.00019999579660887819, 'epoch': 0.03}

  3%|▎         | 129/3886 [14:56<6:24:14,  6.14s/it]


  3%|▎         | 131/3886 [15:07<6:08:51,  5.89s/it]
{'loss': 1.2565, 'grad_norm': 0.2612743667500581, 'learning_rate': 0.0001999931912307244, 'epoch': 0.03}

  3%|▎         | 132/3886 [15:12<5:49:46,  5.59s/it]


  3%|▎         | 134/3886 [15:25<6:20:00,  6.08s/it]
{'loss': 1.2849, 'grad_norm': 0.23628127514031322, 'learning_rate': 0.00019998996059323836, 'epoch': 0.03}

  3%|▎         | 135/3886 [15:32<6:29:43,  6.23s/it]

  3%|▎         | 136/3886 [15:39<6:37:17,  6.36s/it]

  4%|▎         | 137/3886 [15:46<6:57:45,  6.69s/it]

  4%|▎         | 138/3886 [15:54<7:22:24,  7.08s/it]


  4%|▎         | 140/3886 [16:10<7:44:22,  7.44s/it]
{'loss': 1.3583, 'grad_norm': 0.25161449937533814, 'learning_rate': 0.00019998162362498388, 'epoch': 0.04}

  4%|▎         | 141/3886 [16:18<8:01:33,  7.72s/it]

  4%|▎         | 142/3886 [16:25<7:41:46,  7.40s/it]

  4%|▎         | 143/3886 [16:30<7:01:51,  6.76s/it]

  4%|▎         | 144/3886 [16:37<7:04:18,  6.80s/it]

  4%|▎         | 145/3886 [16:45<7:28:26,  7.19s/it]

  4%|▍         | 146/3886 [16:51<6:59:44,  6.73s/it]

  4%|▍         | 147/3886 [16:58<7:08:42,  6.88s/it]

  4%|▍         | 148/3886 [17:08<8:06:49,  7.81s/it]

  4%|▍         | 149/3886 [17:21<9:42:50,  9.36s/it]

  4%|▍         | 150/3886 [17:28<9:09:33,  8.83s/it]

  4%|▍         | 151/3886 [17:36<8:55:50,  8.61s/it]

  4%|▍         | 152/3886 [17:41<7:42:15,  7.43s/it]


  4%|▍         | 154/3886 [17:56<7:27:04,  7.19s/it]

  4%|▍         | 155/3886 [18:04<7:43:54,  7.46s/it]

  4%|▍         | 156/3886 [18:11<7:48:37,  7.54s/it]
{'loss': 1.4545, 'grad_norm': 0.22454052140015626, 'learning_rate': 0.00019994716661420254, 'epoch': 0.04}

  4%|▍         | 157/3886 [18:17<7:14:35,  6.99s/it]

  4%|▍         | 158/3886 [18:22<6:38:15,  6.41s/it]

  4%|▍         | 159/3886 [18:35<8:33:09,  8.26s/it]

  4%|▍         | 160/3886 [18:42<8:21:23,  8.07s/it]

  4%|▍         | 161/3886 [18:49<7:52:51,  7.62s/it]

  4%|▍         | 162/3886 [18:55<7:17:42,  7.05s/it]

  4%|▍         | 163/3886 [19:00<6:52:25,  6.65s/it]

  4%|▍         | 164/3886 [19:06<6:38:32,  6.42s/it]


  4%|▍         | 166/3886 [19:18<6:16:57,  6.08s/it]
{'loss': 1.4296, 'grad_norm': 0.2287763172186448, 'learning_rate': 0.0001999166032241359, 'epoch': 0.04}


  4%|▍         | 168/3886 [19:32<6:38:52,  6.44s/it]
{'loss': 1.4791, 'grad_norm': 0.2371364168735449, 'learning_rate': 0.00019990965743350283, 'epoch': 0.04}

  4%|▍         | 169/3886 [19:37<6:13:34,  6.03s/it]

  4%|▍         | 170/3886 [19:44<6:39:06,  6.44s/it]

  4%|▍         | 171/3886 [19:50<6:30:59,  6.31s/it]

  4%|▍         | 172/3886 [19:55<5:56:47,  5.76s/it]

  4%|▍         | 173/3886 [20:02<6:34:16,  6.37s/it]

  4%|▍         | 174/3886 [20:07<6:09:11,  5.97s/it]

  5%|▍         | 175/3886 [20:15<6:46:41,  6.58s/it]

  5%|▍         | 176/3886 [20:24<7:16:15,  7.06s/it]

  5%|▍         | 177/3886 [20:36<8:59:14,  8.72s/it]


  5%|▍         | 179/3886 [20:54<9:03:14,  8.79s/it]
{'loss': 1.392, 'grad_norm': 0.24353634971499818, 'learning_rate': 0.00019986649295160205, 'epoch': 0.05}

  5%|▍         | 180/3886 [20:59<8:00:32,  7.78s/it]

  5%|▍         | 181/3886 [21:07<8:07:57,  7.90s/it]

  5%|▍         | 182/3886 [21:15<8:12:16,  7.97s/it]

  5%|▍         | 183/3886 [21:21<7:21:43,  7.16s/it]


  5%|▍         | 185/3886 [21:36<7:45:37,  7.55s/it]
{'loss': 1.2946, 'grad_norm': 0.2776174147720875, 'learning_rate': 0.00019983940980445065, 'epoch': 0.05}

  5%|▍         | 186/3886 [21:47<8:56:05,  8.69s/it]

  5%|▍         | 187/3886 [21:53<7:52:17,  7.66s/it]

  5%|▍         | 188/3886 [21:59<7:35:36,  7.39s/it]

  5%|▍         | 189/3886 [22:05<7:00:51,  6.83s/it]

  5%|▍         | 190/3886 [22:13<7:31:12,  7.32s/it]

  5%|▍         | 191/3886 [22:19<6:58:18,  6.79s/it]

  5%|▍         | 192/3886 [22:26<7:09:27,  6.98s/it]


  5%|▍         | 194/3886 [22:38<6:29:48,  6.33s/it]
{'loss': 1.2795, 'grad_norm': 0.2416103504039011, 'learning_rate': 0.00019979410308680558, 'epoch': 0.05}

  5%|▌         | 195/3886 [22:43<6:16:54,  6.13s/it]

  5%|▌         | 196/3886 [22:52<7:05:51,  6.92s/it]

  5%|▌         | 197/3886 [22:59<6:57:29,  6.79s/it]

  5%|▌         | 198/3886 [23:05<6:50:59,  6.69s/it]

  5%|▌         | 199/3886 [23:13<7:02:35,  6.88s/it]

  5%|▌         | 200/3886 [23:21<7:33:01,  7.37s/it]

  5%|▌         | 201/3886 [23:26<6:56:47,  6.79s/it]

  5%|▌         | 202/3886 [23:32<6:42:38,  6.56s/it]

  5%|▌         | 203/3886 [23:40<7:05:27,  6.93s/it]

  5%|▌         | 204/3886 [23:46<6:38:55,  6.50s/it]

  5%|▌         | 205/3886 [23:55<7:20:50,  7.19s/it]

  5%|▌         | 206/3886 [24:02<7:31:58,  7.37s/it]

  5%|▌         | 207/3886 [24:08<7:08:50,  6.99s/it]

  5%|▌         | 208/3886 [24:17<7:39:05,  7.49s/it]


  5%|▌         | 210/3886 [24:30<7:02:45,  6.90s/it]
{'loss': 1.4639, 'grad_norm': 0.23892614010264215, 'learning_rate': 0.0001996996927047411, 'epoch': 0.05}

  5%|▌         | 211/3886 [24:35<6:33:34,  6.43s/it]


  5%|▌         | 213/3886 [24:48<6:29:24,  6.36s/it]

  6%|▌         | 214/3886 [24:54<6:22:25,  6.25s/it]
{'loss': 1.3095, 'grad_norm': 0.23987795842166665, 'learning_rate': 0.0001996733186498975, 'epoch': 0.06}


  6%|▌         | 216/3886 [25:22<10:56:14, 10.73s/it]

  6%|▌         | 217/3886 [25:30<10:08:19,  9.95s/it]

  6%|▌         | 218/3886 [25:36<8:59:29,  8.82s/it]
{'loss': 1.3843, 'grad_norm': 0.22520661250577914, 'learning_rate': 0.00019964583657921322, 'epoch': 0.06}

  6%|▌         | 219/3886 [25:48<9:45:17,  9.58s/it]

  6%|▌         | 220/3886 [25:53<8:35:14,  8.43s/it]

  6%|▌         | 221/3886 [26:01<8:14:12,  8.09s/it]

  6%|▌         | 222/3886 [26:11<8:52:58,  8.73s/it]

  6%|▌         | 223/3886 [26:18<8:18:15,  8.16s/it]

  6%|▌         | 224/3886 [26:24<7:36:12,  7.47s/it]


  6%|▌         | 226/3886 [26:44<9:25:29,  9.27s/it]
{'loss': 1.5406, 'grad_norm': 0.23022796614385083, 'learning_rate': 0.00019958754962465152, 'epoch': 0.06}

  6%|▌         | 227/3886 [26:49<8:04:19,  7.94s/it]


  6%|▌         | 229/3886 [27:02<7:31:35,  7.41s/it]
{'loss': 1.3109, 'grad_norm': 0.2054665751326753, 'learning_rate': 0.0001995645502162851, 'epoch': 0.06}

  6%|▌         | 230/3886 [27:10<7:44:12,  7.62s/it]

  6%|▌         | 231/3886 [27:19<7:53:08,  7.77s/it]

  6%|▌         | 232/3886 [27:26<7:39:11,  7.54s/it]

  6%|▌         | 233/3886 [27:31<7:05:57,  7.00s/it]

  6%|▌         | 234/3886 [27:36<6:21:21,  6.27s/it]

  6%|▌         | 235/3886 [27:44<6:50:23,  6.74s/it]

  6%|▌         | 236/3886 [27:50<6:33:47,  6.47s/it]

  6%|▌         | 237/3886 [28:03<8:32:18,  8.42s/it]

  6%|▌         | 238/3886 [28:10<8:10:00,  8.06s/it]


  6%|▌         | 240/3886 [28:24<7:43:31,  7.63s/it]
{'loss': 1.57, 'grad_norm': 0.21751736277184908, 'learning_rate': 0.00019947489367842845, 'epoch': 0.06}

  6%|▌         | 241/3886 [28:32<7:34:34,  7.48s/it]

  6%|▌         | 242/3886 [28:38<7:14:29,  7.15s/it]

  6%|▋         | 243/3886 [28:45<7:10:43,  7.09s/it]

  6%|▋         | 244/3886 [28:54<7:38:08,  7.55s/it]

  6%|▋         | 245/3886 [28:59<7:04:22,  6.99s/it]

  6%|▋         | 246/3886 [29:06<6:55:26,  6.85s/it]

  6%|▋         | 247/3886 [29:14<7:23:24,  7.31s/it]

  6%|▋         | 248/3886 [29:24<8:06:02,  8.02s/it]

  6%|▋         | 249/3886 [29:31<7:58:36,  7.90s/it]


  6%|▋         | 251/3886 [29:44<7:15:50,  7.19s/it]
{'loss': 1.3299, 'grad_norm': 0.20830086971969236, 'learning_rate': 0.0001993768745028732, 'epoch': 0.06}


  7%|▋         | 253/3886 [29:55<6:10:03,  6.11s/it]
{'loss': 1.1447, 'grad_norm': 0.23863607584553856, 'learning_rate': 0.00019935815500202075, 'epoch': 0.07}

  7%|▋         | 254/3886 [30:01<6:23:02,  6.33s/it]

  7%|▋         | 255/3886 [30:06<5:54:40,  5.86s/it]

  7%|▋         | 256/3886 [30:13<6:17:47,  6.24s/it]

  7%|▋         | 257/3886 [30:19<6:16:04,  6.22s/it]

  7%|▋         | 258/3886 [30:26<6:19:54,  6.28s/it]

  7%|▋         | 259/3886 [30:34<6:49:59,  6.78s/it]

  7%|▋         | 260/3886 [30:41<6:52:40,  6.83s/it]


  7%|▋         | 262/3886 [30:55<6:53:55,  6.85s/it]
{'loss': 1.4715, 'grad_norm': 0.20871811903101234, 'learning_rate': 0.0001992705009298779, 'epoch': 0.07}

  7%|▋         | 263/3886 [31:01<6:39:03,  6.61s/it]

  7%|▋         | 264/3886 [31:08<6:56:23,  6.90s/it]

  7%|▋         | 265/3886 [31:14<6:38:24,  6.60s/it]

  7%|▋         | 266/3886 [31:21<6:38:57,  6.61s/it]

  7%|▋         | 267/3886 [31:29<7:08:59,  7.11s/it]

  7%|▋         | 268/3886 [31:44<9:26:16,  9.39s/it]

  7%|▋         | 269/3886 [31:50<8:26:58,  8.41s/it]

  7%|▋         | 270/3886 [31:56<7:47:43,  7.76s/it]

  7%|▋         | 271/3886 [32:03<7:28:55,  7.45s/it]

  7%|▋         | 272/3886 [32:09<7:01:44,  7.00s/it]

  7%|▋         | 273/3886 [32:15<6:51:38,  6.84s/it]


  7%|▋         | 275/3886 [32:27<6:11:22,  6.17s/it]
{'loss': 1.2658, 'grad_norm': 0.24658093273147977, 'learning_rate': 0.00019913402802021943, 'epoch': 0.07}

  7%|▋         | 276/3886 [32:31<5:46:19,  5.76s/it]

  7%|▋         | 277/3886 [32:38<6:06:44,  6.10s/it]


  7%|▋         | 279/3886 [32:51<6:06:47,  6.10s/it]
{'loss': 1.2058, 'grad_norm': 0.22494476471915525, 'learning_rate': 0.00019908969380160897, 'epoch': 0.07}

  7%|▋         | 280/3886 [32:56<5:54:09,  5.89s/it]

  7%|▋         | 281/3886 [33:02<5:53:39,  5.89s/it]

  7%|▋         | 282/3886 [33:18<8:49:07,  8.81s/it]


  7%|▋         | 284/3886 [33:29<7:11:19,  7.18s/it]
{'loss': 1.333, 'grad_norm': 0.2549500468647273, 'learning_rate': 0.00019903272706353023, 'epoch': 0.07}

  7%|▋         | 285/3886 [33:36<7:11:07,  7.18s/it]

  7%|▋         | 286/3886 [33:41<6:35:23,  6.59s/it]

  7%|▋         | 287/3886 [33:46<6:05:52,  6.10s/it]

  7%|▋         | 288/3886 [33:52<6:01:13,  6.02s/it]


  7%|▋         | 290/3886 [34:07<6:49:09,  6.83s/it]

  7%|▋         | 291/3886 [34:13<6:34:36,  6.59s/it]
{'loss': 1.4103, 'grad_norm': 0.2237076527021689, 'learning_rate': 0.00019895008401041655, 'epoch': 0.07}

  8%|▊         | 292/3886 [34:18<6:18:26,  6.32s/it]

  8%|▊         | 293/3886 [34:25<6:32:06,  6.55s/it]

  8%|▊         | 294/3886 [34:38<8:17:11,  8.31s/it]

  8%|▊         | 295/3886 [34:47<8:36:31,  8.63s/it]

  8%|▊         | 296/3886 [34:53<7:53:34,  7.92s/it]


  8%|▊         | 298/3886 [35:05<6:40:19,  6.69s/it]

  8%|▊         | 299/3886 [35:13<7:06:25,  7.13s/it]
{'loss': 1.4135, 'grad_norm': 0.2199833394987697, 'learning_rate': 0.0001988515100739661, 'epoch': 0.08}

  8%|▊         | 300/3886 [35:19<6:50:36,  6.87s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2217, 'grad_norm': 0.25896551824897374, 'learning_rate': 0.0001988261796161068, 'epoch': 0.08}
  8%|▊         | 301/3886 [36:01<17:21:45, 17.44s/it]

  8%|▊         | 302/3886 [36:06<13:41:36, 13.75s/it]


  8%|▊         | 304/3886 [36:19<9:58:13, 10.02s/it]
{'loss': 1.4131, 'grad_norm': 0.30342258755265517, 'learning_rate': 0.00019878766898305425, 'epoch': 0.08}

  8%|▊         | 305/3886 [36:27<9:29:52,  9.55s/it]


  8%|▊         | 307/3886 [36:45<9:12:43,  9.27s/it]
{'loss': 1.3935, 'grad_norm': 0.2094810185968469, 'learning_rate': 0.00019874854062882304, 'epoch': 0.08}

  8%|▊         | 308/3886 [36:52<8:44:46,  8.80s/it]

  8%|▊         | 309/3886 [36:58<7:42:11,  7.75s/it]


  8%|▊         | 311/3886 [37:11<7:07:49,  7.18s/it]
{'loss': 1.4354, 'grad_norm': 0.2273330174174399, 'learning_rate': 0.00019869540901367997, 'epoch': 0.08}

  8%|▊         | 312/3886 [37:16<6:34:05,  6.62s/it]

  8%|▊         | 313/3886 [37:22<6:19:24,  6.37s/it]

  8%|▊         | 314/3886 [37:27<5:59:34,  6.04s/it]

  8%|▊         | 315/3886 [37:33<5:47:08,  5.83s/it]

  8%|▊         | 316/3886 [37:38<5:49:02,  5.87s/it]

  8%|▊         | 317/3886 [37:49<7:16:02,  7.33s/it]


  8%|▊         | 319/3886 [38:05<7:28:05,  7.54s/it]
{'loss': 1.3917, 'grad_norm': 0.23687353153503382, 'learning_rate': 0.00019858585495142529, 'epoch': 0.08}

  8%|▊         | 320/3886 [38:12<7:25:46,  7.50s/it]


  8%|▊         | 322/3886 [38:33<9:06:22,  9.20s/it]

  8%|▊         | 323/3886 [38:39<8:07:52,  8.22s/it]
{'loss': 1.4335, 'grad_norm': 0.18949868761154107, 'learning_rate': 0.0001985294337221685, 'epoch': 0.08}

  8%|▊         | 324/3886 [38:46<7:42:24,  7.79s/it]

  8%|▊         | 325/3886 [38:52<7:22:02,  7.45s/it]

  8%|▊         | 326/3886 [38:58<6:50:44,  6.92s/it]

  8%|▊         | 327/3886 [39:03<6:11:32,  6.26s/it]

  8%|▊         | 328/3886 [39:09<6:03:48,  6.13s/it]

  8%|▊         | 329/3886 [39:16<6:18:38,  6.39s/it]


  9%|▊         | 331/3886 [39:29<6:37:29,  6.71s/it]
{'loss': 1.3255, 'grad_norm': 0.21424453828141132, 'learning_rate': 0.00019841330600341476, 'epoch': 0.09}

  9%|▊         | 332/3886 [39:36<6:40:22,  6.76s/it]

  9%|▊         | 333/3886 [39:43<6:48:09,  6.89s/it]

  9%|▊         | 334/3886 [39:50<6:35:34,  6.68s/it]

  9%|▊         | 335/3886 [39:55<6:07:36,  6.21s/it]

  9%|▊         | 336/3886 [40:00<5:58:22,  6.06s/it]

  9%|▊         | 337/3886 [40:06<5:53:25,  5.98s/it]

  9%|▊         | 338/3886 [40:18<7:43:10,  7.83s/it]


  9%|▊         | 340/3886 [40:33<7:16:28,  7.39s/it]
{'loss': 1.4366, 'grad_norm': 0.2146905796591443, 'learning_rate': 0.00019827743186884468, 'epoch': 0.09}

  9%|▉         | 341/3886 [40:40<7:01:11,  7.13s/it]

  9%|▉         | 342/3886 [40:56<9:50:27, 10.00s/it]

  9%|▉         | 343/3886 [41:02<8:38:18,  8.78s/it]

  9%|▉         | 344/3886 [41:08<7:44:33,  7.87s/it]


  9%|▉         | 346/3886 [41:23<7:39:21,  7.79s/it]
{'loss': 1.5305, 'grad_norm': 0.20337409451246144, 'learning_rate': 0.00019818377598716737, 'epoch': 0.09}

  9%|▉         | 347/3886 [41:30<7:17:07,  7.41s/it]


  9%|▉         | 349/3886 [41:43<6:55:46,  7.05s/it]
{'loss': 1.195, 'grad_norm': 0.23454436044013793, 'learning_rate': 0.0001981360269814241, 'epoch': 0.09}


  9%|▉         | 351/3886 [41:55<6:25:22,  6.54s/it]

  9%|▉         | 352/3886 [42:01<6:11:03,  6.30s/it]
{'loss': 1.3361, 'grad_norm': 0.2013505170088802, 'learning_rate': 0.00019808766432923204, 'epoch': 0.09}

  9%|▉         | 353/3886 [42:08<6:26:05,  6.56s/it]

  9%|▉         | 354/3886 [42:16<6:39:08,  6.78s/it]


  9%|▉         | 356/3886 [42:31<7:14:02,  7.38s/it]
{'loss': 1.5071, 'grad_norm': 0.17596996202251175, 'learning_rate': 0.00019802222675507415, 'epoch': 0.09}


  9%|▉         | 358/3886 [42:43<6:33:42,  6.70s/it]
{'loss': 1.1835, 'grad_norm': 0.24152397072813542, 'learning_rate': 0.00019798909929898698, 'epoch': 0.09}

  9%|▉         | 359/3886 [42:50<6:29:26,  6.62s/it]


  9%|▉         | 361/3886 [43:05<6:57:43,  7.11s/it]
{'loss': 1.3032, 'grad_norm': 0.22480069225076563, 'learning_rate': 0.000197938897537263, 'epoch': 0.09}

  9%|▉         | 362/3886 [43:12<6:45:52,  6.91s/it]

  9%|▉         | 363/3886 [43:18<6:37:14,  6.77s/it]

  9%|▉         | 364/3886 [43:26<7:00:01,  7.16s/it]

  9%|▉         | 365/3886 [43:33<6:56:49,  7.10s/it]


  9%|▉         | 367/3886 [43:47<6:58:26,  7.13s/it]
{'loss': 1.5265, 'grad_norm': 0.22891699536502486, 'learning_rate': 0.00019783665709017321, 'epoch': 0.09}

  9%|▉         | 368/3886 [43:56<7:16:41,  7.45s/it]

  9%|▉         | 369/3886 [44:02<6:52:45,  7.04s/it]


 10%|▉         | 371/3886 [44:11<5:43:07,  5.86s/it]
{'loss': 1.4569, 'grad_norm': 0.23775269060696355, 'learning_rate': 0.00019776713713501312, 'epoch': 0.1}

 10%|▉         | 372/3886 [44:18<6:01:00,  6.16s/it]

 10%|▉         | 373/3886 [44:28<7:07:28,  7.30s/it]

 10%|▉         | 374/3886 [44:35<6:54:52,  7.09s/it]

 10%|▉         | 375/3886 [44:40<6:32:48,  6.71s/it]


 10%|▉         | 377/3886 [44:54<6:27:09,  6.62s/it]
{'loss': 1.4016, 'grad_norm': 0.2288513865758708, 'learning_rate': 0.00019766081964866266, 'epoch': 0.1}


 10%|▉         | 379/3886 [45:04<5:41:03,  5.83s/it]
{'loss': 1.4059, 'grad_norm': 0.22364424909623368, 'learning_rate': 0.0001976248375321204, 'epoch': 0.1}

 10%|▉         | 380/3886 [45:10<5:59:53,  6.16s/it]

 10%|▉         | 381/3886 [45:20<7:01:02,  7.21s/it]

 10%|▉         | 382/3886 [45:27<6:49:04,  7.00s/it]

 10%|▉         | 383/3886 [45:34<6:51:15,  7.04s/it]


 10%|▉         | 385/3886 [45:46<6:17:39,  6.47s/it]

 10%|▉         | 386/3886 [45:54<6:40:49,  6.87s/it]
{'loss': 1.4222, 'grad_norm': 0.20648308422832165, 'learning_rate': 0.00019749676421174783, 'epoch': 0.1}

 10%|▉         | 387/3886 [46:00<6:42:22,  6.90s/it]


 10%|█         | 389/3886 [46:14<6:29:24,  6.68s/it]
{'loss': 1.4206, 'grad_norm': 0.22467444185216273, 'learning_rate': 0.00019744085930811157, 'epoch': 0.1}

 10%|█         | 390/3886 [46:21<6:36:40,  6.81s/it]


 10%|█         | 392/3886 [46:33<6:21:07,  6.54s/it]
{'loss': 1.363, 'grad_norm': 0.2374583999326196, 'learning_rate': 0.00019738434510492317, 'epoch': 0.1}

 10%|█         | 393/3886 [46:45<7:43:07,  7.96s/it]

 10%|█         | 394/3886 [46:50<6:56:29,  7.16s/it]

 10%|█         | 395/3886 [47:00<7:48:25,  8.05s/it]

 10%|█         | 396/3886 [47:06<7:09:43,  7.39s/it]

 10%|█         | 397/3886 [47:11<6:27:31,  6.66s/it]

 10%|█         | 398/3886 [47:16<6:07:41,  6.33s/it]

 10%|█         | 399/3886 [47:24<6:36:55,  6.83s/it]

 10%|█         | 400/3886 [47:30<6:17:54,  6.50s/it]

 10%|█         | 401/3886 [47:44<8:26:35,  8.72s/it]

 10%|█         | 402/3886 [47:52<8:20:20,  8.62s/it]

 10%|█         | 403/3886 [47:59<7:46:53,  8.04s/it]

 10%|█         | 404/3886 [48:05<7:01:12,  7.26s/it]

 10%|█         | 405/3886 [48:11<6:46:43,  7.01s/it]


 10%|█         | 407/3886 [48:26<6:55:59,  7.17s/it]
{'loss': 1.4205, 'grad_norm': 0.20595808639483182, 'learning_rate': 0.00019709264709737777, 'epoch': 0.1}

 10%|█         | 408/3886 [48:32<6:43:53,  6.97s/it]


 11%|█         | 410/3886 [48:44<6:11:26,  6.41s/it]
{'loss': 1.4844, 'grad_norm': 0.20896339046842735, 'learning_rate': 0.00019703248465114168, 'epoch': 0.11}


 11%|█         | 412/3886 [48:56<5:55:24,  6.14s/it]
{'loss': 1.3764, 'grad_norm': 0.21076115501494486, 'learning_rate': 0.00019699203924917412, 'epoch': 0.11}

 11%|█         | 413/3886 [49:01<5:48:41,  6.02s/it]

 11%|█         | 414/3886 [49:11<6:42:37,  6.96s/it]

 11%|█         | 415/3886 [49:18<6:55:34,  7.18s/it]

 11%|█         | 416/3886 [49:23<6:17:48,  6.53s/it]

 11%|█         | 417/3886 [49:30<6:27:28,  6.70s/it]

 11%|█         | 418/3886 [49:39<6:55:10,  7.18s/it]


 11%|█         | 420/3886 [49:54<7:00:33,  7.28s/it]

 11%|█         | 421/3886 [50:00<6:39:27,  6.92s/it]
{'loss': 1.4403, 'grad_norm': 0.2218658177834777, 'learning_rate': 0.0001968067008678444, 'epoch': 0.11}

 11%|█         | 422/3886 [50:05<6:14:56,  6.49s/it]


 11%|█         | 424/3886 [50:18<6:12:01,  6.45s/it]

 11%|█         | 425/3886 [50:26<6:32:07,  6.80s/it]

 11%|█         | 426/3886 [50:32<6:18:35,  6.57s/it]

 11%|█         | 427/3886 [50:38<6:15:09,  6.51s/it]

 11%|█         | 428/3886 [50:44<6:00:35,  6.26s/it]

 11%|█         | 429/3886 [50:50<5:59:13,  6.23s/it]

 11%|█         | 430/3886 [50:56<5:52:33,  6.12s/it]

 11%|█         | 431/3886 [51:04<6:30:12,  6.78s/it]

 11%|█         | 432/3886 [51:10<6:12:32,  6.47s/it]
{'loss': 1.3134, 'grad_norm': 0.2287251311864537, 'learning_rate': 0.00019657277875600787, 'epoch': 0.11}

 11%|█         | 433/3886 [51:15<5:50:47,  6.10s/it]

 11%|█         | 434/3886 [51:26<7:21:24,  7.67s/it]

 11%|█         | 435/3886 [51:33<7:04:47,  7.39s/it]

 11%|█         | 436/3886 [51:39<6:35:57,  6.89s/it]

 11%|█         | 437/3886 [51:45<6:19:11,  6.60s/it]

 11%|█▏        | 438/3886 [51:51<6:15:05,  6.53s/it]

 11%|█▏        | 439/3886 [52:02<7:34:13,  7.91s/it]


 11%|█▏        | 441/3886 [52:14<6:26:16,  6.73s/it]
{'loss': 1.1974, 'grad_norm': 0.2745215056903041, 'learning_rate': 0.00019637534835393233, 'epoch': 0.11}

 11%|█▏        | 442/3886 [52:22<6:57:52,  7.28s/it]


 11%|█▏        | 444/3886 [52:36<6:48:24,  7.12s/it]
{'loss': 1.3458, 'grad_norm': 0.20108880466552173, 'learning_rate': 0.00019630833239366884, 'epoch': 0.11}

 11%|█▏        | 445/3886 [52:43<6:37:43,  6.94s/it]

 11%|█▏        | 446/3886 [52:49<6:29:39,  6.80s/it]


 12%|█▏        | 448/3886 [53:08<7:39:22,  8.02s/it]
{'loss': 1.4924, 'grad_norm': 0.23217285196263274, 'learning_rate': 0.0001962180410696492, 'epoch': 0.12}

 12%|█▏        | 449/3886 [53:15<7:29:37,  7.85s/it]

 12%|█▏        | 450/3886 [53:21<6:49:56,  7.16s/it]

 12%|█▏        | 451/3886 [53:27<6:27:14,  6.76s/it]

 12%|█▏        | 452/3886 [53:35<6:47:21,  7.12s/it]

 12%|█▏        | 453/3886 [53:41<6:29:41,  6.81s/it]

 12%|█▏        | 454/3886 [53:51<7:18:57,  7.67s/it]

 12%|█▏        | 455/3886 [53:57<6:50:45,  7.18s/it]

 12%|█▏        | 456/3886 [54:02<6:26:18,  6.76s/it]

 12%|█▏        | 457/3886 [54:07<5:56:49,  6.24s/it]


 12%|█▏        | 459/3886 [54:22<6:24:53,  6.74s/it]
{'loss': 1.1708, 'grad_norm': 0.24458529119887815, 'learning_rate': 0.00019596422783754066, 'epoch': 0.12}

 12%|█▏        | 460/3886 [54:27<5:54:11,  6.20s/it]


 12%|█▏        | 462/3886 [54:38<5:44:51,  6.04s/it]
{'loss': 1.498, 'grad_norm': 0.2138736046900351, 'learning_rate': 0.0001958936049877259, 'epoch': 0.12}

 12%|█▏        | 463/3886 [54:47<6:39:17,  7.00s/it]

 12%|█▏        | 464/3886 [54:53<6:07:12,  6.44s/it]

 12%|█▏        | 465/3886 [54:59<6:04:01,  6.38s/it]


 12%|█▏        | 467/3886 [55:12<6:12:39,  6.54s/it]
{'loss': 1.4268, 'grad_norm': 0.20920909246178357, 'learning_rate': 0.0001957745679588284, 'epoch': 0.12}

 12%|█▏        | 468/3886 [55:19<6:16:47,  6.61s/it]


 12%|█▏        | 470/3886 [55:30<5:53:52,  6.22s/it]

 12%|█▏        | 471/3886 [55:38<6:28:10,  6.82s/it]
{'loss': 1.2942, 'grad_norm': 0.241896853535425, 'learning_rate': 0.00019567814047620244, 'epoch': 0.12}

 12%|█▏        | 472/3886 [55:47<7:02:32,  7.43s/it]

 12%|█▏        | 473/3886 [55:55<7:07:24,  7.51s/it]


 12%|█▏        | 475/3886 [56:14<8:23:25,  8.86s/it]
{'loss': 1.2624, 'grad_norm': 0.2123360193714655, 'learning_rate': 0.00019558064939002954, 'epoch': 0.12}

 12%|█▏        | 476/3886 [56:21<7:48:47,  8.25s/it]

 12%|█▏        | 477/3886 [56:27<7:18:20,  7.72s/it]


 12%|█▏        | 479/3886 [56:42<7:15:36,  7.67s/it]

 12%|█▏        | 480/3886 [56:50<7:15:56,  7.68s/it]
{'loss': 1.6337, 'grad_norm': 0.21441802366863852, 'learning_rate': 0.00019545729149198664, 'epoch': 0.12}

 12%|█▏        | 481/3886 [56:55<6:31:09,  6.89s/it]

 12%|█▏        | 482/3886 [57:01<6:15:19,  6.62s/it]

 12%|█▏        | 483/3886 [57:16<8:33:38,  9.06s/it]

 12%|█▏        | 484/3886 [57:22<7:36:04,  8.04s/it]


 13%|█▎        | 486/3886 [57:38<7:56:28,  8.41s/it]

 13%|█▎        | 487/3886 [57:44<7:15:31,  7.69s/it]
{'loss': 1.3955, 'grad_norm': 0.20781025560699182, 'learning_rate': 0.0001952818054068457, 'epoch': 0.13}

 13%|█▎        | 488/3886 [57:54<7:46:54,  8.24s/it]

 13%|█▎        | 489/3886 [58:03<7:53:51,  8.37s/it]


 13%|█▎        | 491/3886 [58:16<7:09:45,  7.60s/it]

 13%|█▎        | 492/3886 [58:24<7:14:54,  7.69s/it]

 13%|█▎        | 493/3886 [58:32<7:19:32,  7.77s/it]
{'loss': 1.4883, 'grad_norm': 0.21861478992831745, 'learning_rate': 0.00019512880674356099, 'epoch': 0.13}

 13%|█▎        | 494/3886 [58:41<7:37:55,  8.10s/it]

 13%|█▎        | 495/3886 [58:47<7:02:40,  7.48s/it]


 13%|█▎        | 497/3886 [58:58<6:05:29,  6.47s/it]
{'loss': 1.4153, 'grad_norm': 0.23563337855024444, 'learning_rate': 0.00019502548552644632, 'epoch': 0.13}

 13%|█▎        | 498/3886 [59:05<6:14:11,  6.63s/it]

 13%|█▎        | 499/3886 [59:11<6:04:16,  6.45s/it]

 13%|█▎        | 500/3886 [59:19<6:25:36,  6.83s/it]

 13%|█▎        | 501/3886 [59:25<6:17:29,  6.69s/it]

 13%|█▎        | 502/3886 [59:31<6:06:31,  6.50s/it]

 13%|█▎        | 503/3886 [59:37<5:52:12,  6.25s/it]

 13%|█▎        | 504/3886 [59:43<5:43:28,  6.09s/it]

 13%|█▎        | 505/3886 [59:49<5:44:39,  6.12s/it]

 13%|█▎        | 506/3886 [59:55<5:41:22,  6.06s/it]

 13%|█▎        | 507/3886 [1:00:02<5:58:27,  6.37s/it]

 13%|█▎        | 508/3886 [1:00:07<5:38:55,  6.02s/it]


 13%|█▎        | 510/3886 [1:00:21<6:00:02,  6.40s/it]
{'loss': 1.5424, 'grad_norm': 0.19929398377812116, 'learning_rate': 0.0001946824021942337, 'epoch': 0.13}

 13%|█▎        | 511/3886 [1:00:26<5:37:27,  6.00s/it]


 13%|█▎        | 513/3886 [1:00:36<5:20:53,  5.71s/it]
{'loss': 1.4119, 'grad_norm': 0.2149677629847103, 'learning_rate': 0.0001946016488364767, 'epoch': 0.13}


 13%|█▎        | 515/3886 [1:00:51<5:56:27,  6.34s/it]
{'loss': 1.3553, 'grad_norm': 0.22868480875027372, 'learning_rate': 0.0001945474845966972, 'epoch': 0.13}

 13%|█▎        | 516/3886 [1:00:58<6:16:05,  6.70s/it]


 13%|█▎        | 518/3886 [1:01:12<6:41:58,  7.16s/it]
{'loss': 1.4168, 'grad_norm': 0.22817190267576123, 'learning_rate': 0.00019446574561143794, 'epoch': 0.13}

 13%|█▎        | 519/3886 [1:01:18<6:18:58,  6.75s/it]

 13%|█▎        | 520/3886 [1:01:24<6:02:40,  6.46s/it]

 13%|█▎        | 521/3886 [1:01:32<6:30:44,  6.97s/it]

 13%|█▎        | 522/3886 [1:01:40<6:36:30,  7.07s/it]

 13%|█▎        | 523/3886 [1:01:48<6:57:15,  7.44s/it]

 13%|█▎        | 524/3886 [1:01:53<6:21:42,  6.81s/it]

 14%|█▎        | 525/3886 [1:01:58<5:44:34,  6.15s/it]

 14%|█▎        | 526/3886 [1:02:06<6:16:09,  6.72s/it]

 14%|█▎        | 527/3886 [1:02:11<5:57:12,  6.38s/it]

 14%|█▎        | 528/3886 [1:02:23<7:30:51,  8.06s/it]

 14%|█▎        | 529/3886 [1:02:30<7:06:40,  7.63s/it]

 14%|█▎        | 530/3886 [1:02:35<6:30:18,  6.98s/it]

 14%|█▎        | 531/3886 [1:02:42<6:23:16,  6.85s/it]

 14%|█▎        | 532/3886 [1:02:52<7:14:24,  7.77s/it]

 14%|█▎        | 533/3886 [1:02:57<6:35:54,  7.08s/it]

 14%|█▎        | 534/3886 [1:03:03<6:11:51,  6.66s/it]


 14%|█▍        | 536/3886 [1:03:17<6:21:27,  6.83s/it]
{'loss': 1.4662, 'grad_norm': 0.18737021067512744, 'learning_rate': 0.00019396292522899858, 'epoch': 0.14}

 14%|█▍        | 537/3886 [1:03:23<6:19:57,  6.81s/it]

 14%|█▍        | 538/3886 [1:03:29<5:59:41,  6.45s/it]

 14%|█▍        | 539/3886 [1:03:38<6:38:39,  7.15s/it]


 14%|█▍        | 541/3886 [1:03:50<6:16:35,  6.75s/it]
{'loss': 1.411, 'grad_norm': 0.2394397029969411, 'learning_rate': 0.00019381949421292001, 'epoch': 0.14}

 14%|█▍        | 542/3886 [1:03:56<5:58:22,  6.43s/it]


 14%|█▍        | 544/3886 [1:04:11<6:27:59,  6.97s/it]
{'loss': 1.3212, 'grad_norm': 0.21187512018003216, 'learning_rate': 0.00019373265323659205, 'epoch': 0.14}

 14%|█▍        | 545/3886 [1:04:18<6:22:39,  6.87s/it]


 14%|█▍        | 547/3886 [1:04:33<6:43:06,  7.24s/it]
{'loss': 1.4342, 'grad_norm': 0.23679539006479555, 'learning_rate': 0.00019364522614819532, 'epoch': 0.14}

 14%|█▍        | 548/3886 [1:04:38<6:08:55,  6.63s/it]

 14%|█▍        | 549/3886 [1:04:44<5:58:28,  6.45s/it]

 14%|█▍        | 550/3886 [1:04:50<5:55:32,  6.39s/it]

 14%|█▍        | 551/3886 [1:04:56<5:46:35,  6.24s/it]

 14%|█▍        | 552/3886 [1:05:02<5:43:48,  6.19s/it]

 14%|█▍        | 553/3886 [1:05:09<5:53:41,  6.37s/it]

 14%|█▍        | 554/3886 [1:05:15<5:49:40,  6.30s/it]

 14%|█▍        | 555/3886 [1:05:23<6:15:43,  6.77s/it]

 14%|█▍        | 556/3886 [1:05:32<6:59:14,  7.55s/it]

 14%|█▍        | 557/3886 [1:05:40<7:02:00,  7.61s/it]


 14%|█▍        | 559/3886 [1:05:53<6:27:49,  6.99s/it]
{'loss': 1.3823, 'grad_norm': 0.21811915699597628, 'learning_rate': 0.00019328966766248807, 'epoch': 0.14}

 14%|█▍        | 560/3886 [1:06:00<6:28:58,  7.02s/it]

 14%|█▍        | 561/3886 [1:06:09<7:11:51,  7.79s/it]

 14%|█▍        | 562/3886 [1:06:19<7:45:45,  8.41s/it]

 14%|█▍        | 563/3886 [1:06:26<7:13:36,  7.83s/it]


 15%|█▍        | 565/3886 [1:06:43<7:24:57,  8.04s/it]
{'loss': 1.3176, 'grad_norm': 0.20654352318985772, 'learning_rate': 0.00019310838613474757, 'epoch': 0.15}

 15%|█▍        | 566/3886 [1:06:48<6:38:14,  7.20s/it]

 15%|█▍        | 567/3886 [1:06:54<6:14:11,  6.76s/it]

 15%|█▍        | 568/3886 [1:07:03<6:48:21,  7.38s/it]

 15%|█▍        | 569/3886 [1:07:14<7:45:59,  8.43s/it]

 15%|█▍        | 570/3886 [1:07:22<7:45:31,  8.42s/it]

 15%|█▍        | 571/3886 [1:07:30<7:38:41,  8.30s/it]

 15%|█▍        | 572/3886 [1:07:38<7:26:46,  8.09s/it]

 15%|█▍        | 573/3886 [1:07:44<6:56:11,  7.54s/it]

 15%|█▍        | 574/3886 [1:07:50<6:41:05,  7.27s/it]

 15%|█▍        | 575/3886 [1:07:55<6:01:39,  6.55s/it]

 15%|█▍        | 576/3886 [1:08:02<6:05:38,  6.63s/it]


 15%|█▍        | 578/3886 [1:08:15<5:59:24,  6.52s/it]
{'loss': 1.4865, 'grad_norm': 0.199798741240275, 'learning_rate': 0.00019270762643924897, 'epoch': 0.15}

 15%|█▍        | 579/3886 [1:08:29<8:12:41,  8.94s/it]

 15%|█▍        | 580/3886 [1:08:36<7:39:20,  8.34s/it]

 15%|█▍        | 581/3886 [1:08:41<6:45:44,  7.37s/it]

 15%|█▍        | 582/3886 [1:08:48<6:35:42,  7.19s/it]

 15%|█▌        | 583/3886 [1:08:56<6:45:11,  7.36s/it]


 15%|█▌        | 585/3886 [1:09:13<7:31:47,  8.21s/it]
{'loss': 1.354, 'grad_norm': 0.27072896809913344, 'learning_rate': 0.00019248732095085438, 'epoch': 0.15}


 15%|█▌        | 587/3886 [1:09:29<7:21:11,  8.02s/it]
{'loss': 1.4032, 'grad_norm': 0.1956238523556901, 'learning_rate': 0.00019242379787128184, 'epoch': 0.15}

 15%|█▌        | 588/3886 [1:09:36<6:56:48,  7.58s/it]

 15%|█▌        | 589/3886 [1:09:42<6:42:04,  7.32s/it]

 15%|█▌        | 590/3886 [1:09:49<6:23:01,  6.97s/it]

 15%|█▌        | 591/3886 [1:09:54<6:02:20,  6.60s/it]

 15%|█▌        | 592/3886 [1:10:02<6:24:08,  7.00s/it]


 15%|█▌        | 594/3886 [1:10:15<6:00:58,  6.58s/it]
{'loss': 1.3371, 'grad_norm': 0.22368145727618904, 'learning_rate': 0.0001921994455099655, 'epoch': 0.15}

 15%|█▌        | 595/3886 [1:10:21<5:46:27,  6.32s/it]

 15%|█▌        | 596/3886 [1:10:28<5:53:37,  6.45s/it]
[2024-05-28 01:45:14,040] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 597/3886 [1:10:44<8:36:42,  9.43s/it]

 15%|█▌        | 598/3886 [1:10:49<7:33:11,  8.27s/it]

 15%|█▌        | 599/3886 [1:10:55<6:45:19,  7.40s/it]

 15%|█▌        | 600/3886 [1:11:02<6:43:07,  7.36s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 15%|█▌        | 601/3886 [1:11:39<14:50:15, 16.26s/it]
{'loss': 1.493, 'grad_norm': 0.20701715529941317, 'learning_rate': 0.0001919719542981752, 'epoch': 0.15}


 16%|█▌        | 603/3886 [1:11:59<11:41:50, 12.83s/it]
{'loss': 1.135, 'grad_norm': 0.22492698581683232, 'learning_rate': 0.000191906381366682, 'epoch': 0.16}

 16%|█▌        | 604/3886 [1:12:06<10:06:52, 11.09s/it]


 16%|█▌        | 606/3886 [1:12:23<8:44:21,  9.59s/it]
{'loss': 1.2751, 'grad_norm': 0.22121703447497892, 'learning_rate': 0.00019180754311614958, 'epoch': 0.16}

 16%|█▌        | 607/3886 [1:12:34<9:06:17, 10.00s/it]

 16%|█▌        | 608/3886 [1:12:40<8:01:32,  8.81s/it]

 16%|█▌        | 609/3886 [1:12:48<7:51:44,  8.64s/it]


 16%|█▌        | 611/3886 [1:13:01<6:53:31,  7.58s/it]

 16%|█▌        | 612/3886 [1:13:07<6:29:01,  7.13s/it]
{'loss': 1.4992, 'grad_norm': 0.21226318009375492, 'learning_rate': 0.00019160814501375633, 'epoch': 0.16}

 16%|█▌        | 613/3886 [1:13:14<6:14:30,  6.87s/it]

 16%|█▌        | 614/3886 [1:13:20<6:12:38,  6.83s/it]


 16%|█▌        | 616/3886 [1:13:33<6:08:42,  6.77s/it]
{'loss': 1.3009, 'grad_norm': 0.22195392950986848, 'learning_rate': 0.00019147393968731117, 'epoch': 0.16}

 16%|█▌        | 617/3886 [1:13:40<6:10:56,  6.81s/it]


 16%|█▌        | 619/3886 [1:13:51<5:34:33,  6.14s/it]

 16%|█▌        | 620/3886 [1:13:59<6:04:13,  6.69s/it]

 16%|█▌        | 621/3886 [1:14:05<5:52:36,  6.48s/it]
{'loss': 1.3892, 'grad_norm': 0.20679766581939676, 'learning_rate': 0.00019130475323501694, 'epoch': 0.16}

 16%|█▌        | 622/3886 [1:14:12<5:55:38,  6.54s/it]

 16%|█▌        | 623/3886 [1:14:18<5:49:39,  6.43s/it]

 16%|█▌        | 624/3886 [1:14:23<5:21:48,  5.92s/it]

 16%|█▌        | 625/3886 [1:14:28<5:10:16,  5.71s/it]

 16%|█▌        | 626/3886 [1:14:36<5:50:24,  6.45s/it]

 16%|█▌        | 627/3886 [1:14:42<5:47:56,  6.41s/it]

 16%|█▌        | 628/3886 [1:14:48<5:41:30,  6.29s/it]

 16%|█▌        | 629/3886 [1:14:54<5:30:27,  6.09s/it]

 16%|█▌        | 630/3886 [1:15:06<7:04:01,  7.81s/it]

 16%|█▌        | 631/3886 [1:15:12<6:32:43,  7.24s/it]

 16%|█▋        | 632/3886 [1:15:18<6:15:04,  6.92s/it]


 16%|█▋        | 634/3886 [1:15:31<6:05:11,  6.74s/it]
{'loss': 1.4207, 'grad_norm': 0.22231060070410358, 'learning_rate': 0.00019085745378519804, 'epoch': 0.16}


 16%|█▋        | 636/3886 [1:15:44<5:48:31,  6.43s/it]
{'loss': 1.5048, 'grad_norm': 0.2186592848874878, 'learning_rate': 0.00019078769027329126, 'epoch': 0.16}

 16%|█▋        | 637/3886 [1:15:51<5:57:41,  6.61s/it]

 16%|█▋        | 638/3886 [1:15:58<6:15:44,  6.94s/it]


 16%|█▋        | 640/3886 [1:16:09<5:33:00,  6.16s/it]
{'loss': 1.3543, 'grad_norm': 0.22481030209340389, 'learning_rate': 0.00019064740651429535, 'epoch': 0.16}

 16%|█▋        | 641/3886 [1:16:16<5:43:30,  6.35s/it]

 17%|█▋        | 642/3886 [1:16:23<5:40:53,  6.31s/it]

 17%|█▋        | 643/3886 [1:16:28<5:27:16,  6.05s/it]

 17%|█▋        | 644/3886 [1:16:35<5:39:46,  6.29s/it]

 17%|█▋        | 645/3886 [1:16:40<5:24:38,  6.01s/it]

 17%|█▋        | 646/3886 [1:16:47<5:37:14,  6.25s/it]

 17%|█▋        | 647/3886 [1:16:54<5:55:53,  6.59s/it]


 17%|█▋        | 649/3886 [1:17:08<6:02:47,  6.72s/it]
{'loss': 1.4392, 'grad_norm': 0.20184596909264504, 'learning_rate': 0.0001903280861221651, 'epoch': 0.17}


 17%|█▋        | 651/3886 [1:17:26<7:04:10,  7.87s/it]
{'loss': 1.5288, 'grad_norm': 0.24265597857149596, 'learning_rate': 0.00019025643506007578, 'epoch': 0.17}


 17%|█▋        | 653/3886 [1:17:46<7:38:42,  8.51s/it]

 17%|█▋        | 654/3886 [1:17:54<7:29:39,  8.35s/it]

 17%|█▋        | 655/3886 [1:18:03<7:54:37,  8.81s/it]
{'loss': 1.5358, 'grad_norm': 0.19847087319894066, 'learning_rate': 0.00019011238063522306, 'epoch': 0.17}

 17%|█▋        | 656/3886 [1:18:11<7:30:04,  8.36s/it]

 17%|█▋        | 657/3886 [1:18:16<6:44:51,  7.52s/it]

 17%|█▋        | 658/3886 [1:18:23<6:24:47,  7.15s/it]

 17%|█▋        | 659/3886 [1:18:28<5:59:27,  6.68s/it]


 17%|█▋        | 661/3886 [1:18:44<6:35:11,  7.35s/it]
{'loss': 1.375, 'grad_norm': 0.2168262347330473, 'learning_rate': 0.00018989442125406383, 'epoch': 0.17}

 17%|█▋        | 662/3886 [1:18:51<6:34:34,  7.34s/it]

 17%|█▋        | 663/3886 [1:18:56<6:01:51,  6.74s/it]

 17%|█▋        | 664/3886 [1:19:03<5:58:32,  6.68s/it]

 17%|█▋        | 665/3886 [1:19:08<5:36:11,  6.26s/it]


 17%|█▋        | 667/3886 [1:19:20<5:21:03,  5.98s/it]
{'loss': 1.4439, 'grad_norm': 0.2621411079367361, 'learning_rate': 0.00018967421343029555, 'epoch': 0.17}

 17%|█▋        | 668/3886 [1:19:26<5:30:47,  6.17s/it]

 17%|█▋        | 669/3886 [1:19:34<5:58:15,  6.68s/it]

 17%|█▋        | 670/3886 [1:19:41<5:58:56,  6.70s/it]


 17%|█▋        | 672/3886 [1:19:56<6:16:41,  7.03s/it]
{'loss': 1.3045, 'grad_norm': 0.23235990331647435, 'learning_rate': 0.00018948899332211401, 'epoch': 0.17}

 17%|█▋        | 673/3886 [1:20:03<6:12:27,  6.96s/it]


 17%|█▋        | 675/3886 [1:20:16<5:56:04,  6.65s/it]
{'loss': 1.2591, 'grad_norm': 0.24069024593394314, 'learning_rate': 0.00018937711494902994, 'epoch': 0.17}


 17%|█▋        | 677/3886 [1:20:32<6:29:24,  7.28s/it]
{'loss': 1.3993, 'grad_norm': 0.21782142091677512, 'learning_rate': 0.00018930221883653244, 'epoch': 0.17}


 17%|█▋        | 679/3886 [1:20:50<7:03:08,  7.92s/it]

 17%|█▋        | 680/3886 [1:20:56<6:38:56,  7.47s/it]

 18%|█▊        | 681/3886 [1:21:02<6:12:11,  6.97s/it]
{'loss': 1.5042, 'grad_norm': 0.20099096415431572, 'learning_rate': 0.00018915168227551967, 'epoch': 0.18}

 18%|█▊        | 682/3886 [1:21:07<5:46:34,  6.49s/it]


 18%|█▊        | 684/3886 [1:21:20<5:41:01,  6.39s/it]

 18%|█▊        | 685/3886 [1:21:26<5:35:19,  6.29s/it]
{'loss': 1.5754, 'grad_norm': 0.2015470332269736, 'learning_rate': 0.0001890001546621619, 'epoch': 0.18}

 18%|█▊        | 686/3886 [1:21:32<5:41:19,  6.40s/it]


 18%|█▊        | 688/3886 [1:21:52<7:34:07,  8.52s/it]

 18%|█▊        | 689/3886 [1:22:00<7:28:21,  8.41s/it]

 18%|█▊        | 690/3886 [1:22:06<6:47:22,  7.65s/it]
{'loss': 1.2913, 'grad_norm': 0.23295008766270314, 'learning_rate': 0.00018880935404531055, 'epoch': 0.18}

 18%|█▊        | 691/3886 [1:22:12<6:27:04,  7.27s/it]

 18%|█▊        | 692/3886 [1:22:19<6:19:07,  7.12s/it]

 18%|█▊        | 693/3886 [1:22:23<5:35:06,  6.30s/it]

 18%|█▊        | 694/3886 [1:22:29<5:20:04,  6.02s/it]


 18%|█▊        | 696/3886 [1:22:44<6:01:18,  6.80s/it]
{'loss': 1.3122, 'grad_norm': 0.20557628168367065, 'learning_rate': 0.00018857835740302515, 'epoch': 0.18}


 18%|█▊        | 698/3886 [1:22:58<6:05:58,  6.89s/it]

 18%|█▊        | 699/3886 [1:23:04<5:51:34,  6.62s/it]
{'loss': 1.4489, 'grad_norm': 0.2606609866145196, 'learning_rate': 0.00018846202789667092, 'epoch': 0.18}

 18%|█▊        | 700/3886 [1:23:11<6:01:57,  6.82s/it]

 18%|█▊        | 701/3886 [1:23:17<5:49:31,  6.58s/it]

 18%|█▊        | 702/3886 [1:23:23<5:39:02,  6.39s/it]

 18%|█▊        | 703/3886 [1:23:31<5:56:41,  6.72s/it]


 18%|█▊        | 705/3886 [1:23:42<5:26:20,  6.16s/it]
{'loss': 1.3913, 'grad_norm': 0.2410979201464525, 'learning_rate': 0.00018822771015058803, 'epoch': 0.18}

 18%|█▊        | 706/3886 [1:23:51<6:12:31,  7.03s/it]

 18%|█▊        | 707/3886 [1:23:57<5:54:58,  6.70s/it]

 18%|█▊        | 708/3886 [1:24:03<5:34:40,  6.32s/it]


 18%|█▊        | 710/3886 [1:24:14<5:18:31,  6.02s/it]
{'loss': 1.6185, 'grad_norm': 0.22609959400520574, 'learning_rate': 0.00018803075939635618, 'epoch': 0.18}


 18%|█▊        | 712/3886 [1:24:38<8:19:18,  9.44s/it]
{'loss': 1.2341, 'grad_norm': 0.24434210475531506, 'learning_rate': 0.00018795155076858095, 'epoch': 0.18}

 18%|█▊        | 713/3886 [1:24:47<8:20:11,  9.46s/it]

 18%|█▊        | 714/3886 [1:24:58<8:30:57,  9.66s/it]

 18%|█▊        | 715/3886 [1:25:04<7:35:50,  8.63s/it]


 18%|█▊        | 717/3886 [1:25:16<6:29:25,  7.37s/it]
{'loss': 1.3358, 'grad_norm': 0.2112832520462174, 'learning_rate': 0.00018775246031007506, 'epoch': 0.18}


 19%|█▊        | 719/3886 [1:25:28<5:55:20,  6.73s/it]
{'loss': 1.3833, 'grad_norm': 0.2426959731229524, 'learning_rate': 0.00018767239715200679, 'epoch': 0.18}


 19%|█▊        | 721/3886 [1:25:42<5:57:22,  6.77s/it]
{'loss': 1.4176, 'grad_norm': 0.21000622026989507, 'learning_rate': 0.00018759209034179155, 'epoch': 0.19}

 19%|█▊        | 722/3886 [1:25:49<6:04:41,  6.92s/it]


 19%|█▊        | 724/3886 [1:26:02<6:00:42,  6.84s/it]
{'loss': 1.3375, 'grad_norm': 0.216916981255303, 'learning_rate': 0.0001874711737670895, 'epoch': 0.19}

 19%|█▊        | 725/3886 [1:26:09<5:51:49,  6.68s/it]

 19%|█▊        | 726/3886 [1:26:17<6:10:49,  7.04s/it]

 19%|█▊        | 727/3886 [1:26:23<6:02:50,  6.89s/it]

 19%|█▊        | 728/3886 [1:26:29<5:51:26,  6.68s/it]

 19%|█▉        | 729/3886 [1:26:35<5:34:24,  6.36s/it]

 19%|█▉        | 730/3886 [1:26:45<6:28:28,  7.39s/it]

 19%|█▉        | 731/3886 [1:26:53<6:45:13,  7.71s/it]


 19%|█▉        | 733/3886 [1:27:12<7:49:02,  8.93s/it]
{'loss': 1.4427, 'grad_norm': 0.2397214271102483, 'learning_rate': 0.00018710514533094462, 'epoch': 0.19}

 19%|█▉        | 734/3886 [1:27:19<7:11:36,  8.22s/it]

 19%|█▉        | 735/3886 [1:27:25<6:47:33,  7.76s/it]


 19%|█▉        | 737/3886 [1:27:40<6:48:46,  7.79s/it]

 19%|█▉        | 738/3886 [1:27:48<6:50:24,  7.82s/it]

 19%|█▉        | 739/3886 [1:27:54<6:20:46,  7.26s/it]
{'loss': 1.2455, 'grad_norm': 0.2273236846237001, 'learning_rate': 0.00018685840175128407, 'epoch': 0.19}

 19%|█▉        | 740/3886 [1:28:01<6:19:24,  7.24s/it]

 19%|█▉        | 741/3886 [1:28:07<5:57:15,  6.82s/it]

 19%|█▉        | 742/3886 [1:28:13<5:41:59,  6.53s/it]

 19%|█▉        | 743/3886 [1:28:20<5:43:27,  6.56s/it]


 19%|█▉        | 745/3886 [1:28:32<5:40:40,  6.51s/it]

 19%|█▉        | 746/3886 [1:28:38<5:31:04,  6.33s/it]
{'loss': 1.3834, 'grad_norm': 0.2399218494694574, 'learning_rate': 0.00018656778887238574, 'epoch': 0.19}

 19%|█▉        | 747/3886 [1:28:45<5:34:05,  6.39s/it]

 19%|█▉        | 748/3886 [1:28:52<5:40:41,  6.51s/it]

 19%|█▉        | 749/3886 [1:29:00<6:05:27,  6.99s/it]


 19%|█▉        | 751/3886 [1:29:14<6:20:05,  7.27s/it]
{'loss': 1.2599, 'grad_norm': 0.2430104538926742, 'learning_rate': 0.00018635840330113256, 'epoch': 0.19}

 19%|█▉        | 752/3886 [1:29:21<6:07:46,  7.04s/it]

 19%|█▉        | 753/3886 [1:29:27<5:57:58,  6.86s/it]

 19%|█▉        | 754/3886 [1:29:38<6:57:06,  7.99s/it]


 19%|█▉        | 756/3886 [1:29:55<7:15:41,  8.35s/it]

 19%|█▉        | 757/3886 [1:30:01<6:38:26,  7.64s/it]
{'loss': 1.3595, 'grad_norm': 0.2211233840664454, 'learning_rate': 0.0001861051609366214, 'epoch': 0.19}


 20%|█▉        | 759/3886 [1:30:14<6:29:03,  7.47s/it]
{'loss': 1.4469, 'grad_norm': 0.2240486256409726, 'learning_rate': 0.0001860202679084381, 'epoch': 0.2}

 20%|█▉        | 760/3886 [1:30:30<8:29:25,  9.78s/it]

 20%|█▉        | 761/3886 [1:30:46<10:08:45, 11.69s/it]


 20%|█▉        | 763/3886 [1:30:57<7:24:30,  8.54s/it]
{'loss': 1.3598, 'grad_norm': 0.2412794662534396, 'learning_rate': 0.00018584976490662123, 'epoch': 0.2}


 20%|█▉        | 765/3886 [1:31:06<5:44:53,  6.63s/it]
{'loss': 1.1816, 'grad_norm': 0.24241456719766788, 'learning_rate': 0.00018576415540683607, 'epoch': 0.2}

 20%|█▉        | 766/3886 [1:31:13<5:52:26,  6.78s/it]

 20%|█▉        | 767/3886 [1:31:25<7:07:58,  8.23s/it]

 20%|█▉        | 768/3886 [1:31:32<6:46:41,  7.83s/it]


 20%|█▉        | 770/3886 [1:31:47<6:28:13,  7.48s/it]
{'loss': 1.2771, 'grad_norm': 0.2280221658574475, 'learning_rate': 0.0001855490894029551, 'epoch': 0.2}

 20%|█▉        | 771/3886 [1:31:54<6:17:11,  7.27s/it]

 20%|█▉        | 772/3886 [1:32:04<7:09:52,  8.28s/it]


 20%|█▉        | 774/3886 [1:32:16<6:13:38,  7.20s/it]
{'loss': 1.4114, 'grad_norm': 0.23785548007519536, 'learning_rate': 0.00018537596654048954, 'epoch': 0.2}

 20%|█▉        | 775/3886 [1:32:21<5:33:17,  6.43s/it]

 20%|█▉        | 776/3886 [1:32:28<5:46:05,  6.68s/it]


 20%|██        | 778/3886 [1:32:41<5:41:00,  6.58s/it]
{'loss': 1.3818, 'grad_norm': 0.21477714798690986, 'learning_rate': 0.00018520189459832428, 'epoch': 0.2}

 20%|██        | 779/3886 [1:32:54<7:27:30,  8.64s/it]


 20%|██        | 781/3886 [1:33:10<7:16:58,  8.44s/it]

 20%|██        | 782/3886 [1:33:22<8:11:42,  9.50s/it]

 20%|██        | 783/3886 [1:33:28<7:17:45,  8.46s/it]
{'loss': 1.4336, 'grad_norm': 0.2289065101130327, 'learning_rate': 0.00018498297297624516, 'epoch': 0.2}


 20%|██        | 785/3886 [1:33:42<6:38:08,  7.70s/it]
{'loss': 1.3429, 'grad_norm': 0.2173709863004036, 'learning_rate': 0.0001848949908027624, 'epoch': 0.2}

 20%|██        | 786/3886 [1:33:52<7:09:13,  8.31s/it]

 20%|██        | 787/3886 [1:34:00<6:56:52,  8.07s/it]

 20%|██        | 788/3886 [1:34:06<6:29:25,  7.54s/it]


 20%|██        | 790/3886 [1:34:29<8:25:56,  9.81s/it]
{'loss': 1.3364, 'grad_norm': 0.2502970664884214, 'learning_rate': 0.0001846740036969149, 'epoch': 0.2}

 20%|██        | 791/3886 [1:34:36<7:50:31,  9.12s/it]


 20%|██        | 793/3886 [1:34:49<6:39:59,  7.76s/it]
{'loss': 1.2466, 'grad_norm': 0.2282822126369369, 'learning_rate': 0.0001845407052297901, 'epoch': 0.2}

 20%|██        | 794/3886 [1:34:56<6:30:34,  7.58s/it]


 20%|██        | 796/3886 [1:35:13<6:45:05,  7.87s/it]
{'loss': 1.3755, 'grad_norm': 0.23435694776042076, 'learning_rate': 0.00018440687812802274, 'epoch': 0.2}

 21%|██        | 797/3886 [1:35:18<6:07:27,  7.14s/it]

 21%|██        | 798/3886 [1:35:26<6:22:16,  7.43s/it]


 21%|██        | 800/3886 [1:35:39<5:50:23,  6.81s/it]
{'loss': 1.4528, 'grad_norm': 0.2277547827028367, 'learning_rate': 0.0001842276211186147, 'epoch': 0.21}

 21%|██        | 801/3886 [1:35:46<5:47:13,  6.75s/it]

 21%|██        | 802/3886 [1:35:54<6:08:53,  7.18s/it]

 21%|██        | 803/3886 [1:36:00<5:58:40,  6.98s/it]

 21%|██        | 804/3886 [1:36:08<6:04:37,  7.10s/it]

 21%|██        | 805/3886 [1:36:14<5:51:25,  6.84s/it]

 21%|██        | 806/3886 [1:36:20<5:37:05,  6.57s/it]

 21%|██        | 807/3886 [1:36:30<6:35:04,  7.70s/it]

 21%|██        | 808/3886 [1:36:37<6:28:03,  7.56s/it]

 21%|██        | 809/3886 [1:36:43<6:04:26,  7.11s/it]

 21%|██        | 810/3886 [1:36:50<5:52:26,  6.87s/it]


 21%|██        | 812/3886 [1:37:03<5:45:12,  6.74s/it]
{'loss': 1.1359, 'grad_norm': 0.21662597017763824, 'learning_rate': 0.00018368424022834718, 'epoch': 0.21}

 21%|██        | 813/3886 [1:37:11<6:14:40,  7.32s/it]

 21%|██        | 814/3886 [1:37:18<6:05:47,  7.14s/it]


 21%|██        | 816/3886 [1:37:33<6:15:54,  7.35s/it]
{'loss': 1.3153, 'grad_norm': 0.23527846643541572, 'learning_rate': 0.00018350125002255648, 'epoch': 0.21}

 21%|██        | 817/3886 [1:37:40<6:08:07,  7.20s/it]

 21%|██        | 818/3886 [1:37:47<6:01:22,  7.07s/it]

 21%|██        | 819/3886 [1:37:52<5:32:00,  6.50s/it]


 21%|██        | 821/3886 [1:38:09<6:36:03,  7.75s/it]

 21%|██        | 822/3886 [1:38:17<6:41:33,  7.86s/it]

 21%|██        | 823/3886 [1:38:23<6:19:48,  7.44s/it]
{'loss': 1.5473, 'grad_norm': 0.23181466910279683, 'learning_rate': 0.00018317878481573796, 'epoch': 0.21}

 21%|██        | 824/3886 [1:38:32<6:38:47,  7.81s/it]


 21%|██▏       | 826/3886 [1:38:47<6:42:01,  7.88s/it]
{'loss': 1.4233, 'grad_norm': 0.2436461720190166, 'learning_rate': 0.00018303971793564447, 'epoch': 0.21}

 21%|██▏       | 827/3886 [1:38:54<6:22:42,  7.51s/it]

 21%|██▏       | 828/3886 [1:38:59<5:51:36,  6.90s/it]

 21%|██▏       | 829/3886 [1:39:06<5:41:22,  6.70s/it]

 21%|██▏       | 830/3886 [1:39:15<6:15:44,  7.38s/it]


 21%|██▏       | 832/3886 [1:39:29<6:16:34,  7.40s/it]

 21%|██▏       | 833/3886 [1:39:37<6:21:39,  7.50s/it]
{'loss': 1.3605, 'grad_norm': 0.20705031184970538, 'learning_rate': 0.0001827132107563951, 'epoch': 0.21}

 21%|██▏       | 834/3886 [1:39:46<6:49:52,  8.06s/it]

 21%|██▏       | 835/3886 [1:39:57<7:22:47,  8.71s/it]

 22%|██▏       | 836/3886 [1:40:02<6:32:31,  7.72s/it]

 22%|██▏       | 837/3886 [1:40:09<6:13:50,  7.36s/it]


 22%|██▏       | 839/3886 [1:40:33<8:33:46, 10.12s/it]
[2024-05-28 02:15:03,122] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 840/3886 [1:40:39<7:30:20,  8.87s/it]

 22%|██▏       | 841/3886 [1:40:45<6:48:35,  8.05s/it]
{'loss': 1.272, 'grad_norm': 0.23999201745413443, 'learning_rate': 0.00018233661230635434, 'epoch': 0.22}

 22%|██▏       | 842/3886 [1:40:52<6:26:31,  7.62s/it]

 22%|██▏       | 843/3886 [1:40:58<6:11:43,  7.33s/it]

 22%|██▏       | 844/3886 [1:41:04<5:43:03,  6.77s/it]

 22%|██▏       | 845/3886 [1:41:10<5:29:31,  6.50s/it]

 22%|██▏       | 846/3886 [1:41:16<5:31:48,  6.55s/it]


 22%|██▏       | 848/3886 [1:41:27<4:57:38,  5.88s/it]
{'loss': 1.4115, 'grad_norm': 0.23797796158892454, 'learning_rate': 0.0001820040847900546, 'epoch': 0.22}

 22%|██▏       | 849/3886 [1:41:35<5:25:39,  6.43s/it]


 22%|██▏       | 851/3886 [1:41:49<5:45:44,  6.84s/it]
{'loss': 1.3734, 'grad_norm': 0.24009936767226817, 'learning_rate': 0.00018186071771402666, 'epoch': 0.22}

 22%|██▏       | 852/3886 [1:41:55<5:23:34,  6.40s/it]


 22%|██▏       | 854/3886 [1:42:09<5:39:24,  6.72s/it]

 22%|██▏       | 855/3886 [1:42:15<5:26:17,  6.46s/it]
{'loss': 1.4744, 'grad_norm': 0.241980326928906, 'learning_rate': 0.00018166876551552912, 'epoch': 0.22}

 22%|██▏       | 856/3886 [1:42:20<5:06:56,  6.08s/it]


 22%|██▏       | 858/3886 [1:42:36<5:37:23,  6.69s/it]
{'loss': 1.164, 'grad_norm': 0.25341261557883293, 'learning_rate': 0.00018152420546075032, 'epoch': 0.22}

 22%|██▏       | 859/3886 [1:42:42<5:29:53,  6.54s/it]

 22%|██▏       | 860/3886 [1:42:48<5:25:57,  6.46s/it]


 22%|██▏       | 862/3886 [1:43:01<5:35:45,  6.66s/it]
{'loss': 1.2548, 'grad_norm': 0.2605128095868984, 'learning_rate': 0.00018133066589843297, 'epoch': 0.22}

 22%|██▏       | 863/3886 [1:43:11<6:15:14,  7.45s/it]


 22%|██▏       | 865/3886 [1:43:24<5:44:21,  6.84s/it]
{'loss': 1.4609, 'grad_norm': 0.2443998839619166, 'learning_rate': 0.00018118491778632546, 'epoch': 0.22}

 22%|██▏       | 866/3886 [1:43:30<5:40:14,  6.76s/it]


 22%|██▏       | 868/3886 [1:43:44<5:41:44,  6.79s/it]
{'loss': 1.3491, 'grad_norm': 0.20561506654107226, 'learning_rate': 0.0001810386620233783, 'epoch': 0.22}


 22%|██▏       | 870/3886 [1:44:05<7:08:25,  8.52s/it]
{'loss': 1.2782, 'grad_norm': 0.22386221430476724, 'learning_rate': 0.00018094087660466354, 'epoch': 0.22}

 22%|██▏       | 871/3886 [1:44:12<6:40:28,  7.97s/it]

 22%|██▏       | 872/3886 [1:44:21<6:56:26,  8.29s/it]

 22%|██▏       | 873/3886 [1:44:27<6:18:34,  7.54s/it]

 22%|██▏       | 874/3886 [1:44:34<6:17:34,  7.52s/it]

 23%|██▎       | 875/3886 [1:44:41<6:07:14,  7.32s/it]

 23%|██▎       | 876/3886 [1:44:48<6:05:55,  7.29s/it]


 23%|██▎       | 878/3886 [1:45:01<5:36:20,  6.71s/it]

 23%|██▎       | 879/3886 [1:45:07<5:24:48,  6.48s/it]

 23%|██▎       | 880/3886 [1:45:17<6:20:08,  7.59s/it]

 23%|██▎       | 881/3886 [1:45:24<6:01:42,  7.22s/it]
{'loss': 1.385, 'grad_norm': 0.24927010400194105, 'learning_rate': 0.00018039904323774973, 'epoch': 0.23}


 23%|██▎       | 883/3886 [1:45:36<5:27:28,  6.54s/it]
{'loss': 1.2572, 'grad_norm': 0.23198267429280423, 'learning_rate': 0.0001802998005657318, 'epoch': 0.23}

 23%|██▎       | 884/3886 [1:45:43<5:36:28,  6.72s/it]

 23%|██▎       | 885/3886 [1:45:53<6:23:49,  7.67s/it]


 23%|██▎       | 887/3886 [1:46:06<5:50:23,  7.01s/it]
{'loss': 1.5232, 'grad_norm': 0.23566525394214682, 'learning_rate': 0.00018010064600968323, 'epoch': 0.23}

 23%|██▎       | 888/3886 [1:46:13<5:47:38,  6.96s/it]

 23%|██▎       | 889/3886 [1:46:18<5:18:19,  6.37s/it]

 23%|██▎       | 890/3886 [1:46:24<5:12:28,  6.26s/it]

 23%|██▎       | 891/3886 [1:46:31<5:21:54,  6.45s/it]

 23%|██▎       | 892/3886 [1:46:39<5:46:14,  6.94s/it]

 23%|██▎       | 893/3886 [1:46:44<5:25:07,  6.52s/it]

 23%|██▎       | 894/3886 [1:46:58<7:14:19,  8.71s/it]

 23%|██▎       | 895/3886 [1:47:03<6:24:09,  7.71s/it]

 23%|██▎       | 896/3886 [1:47:10<6:02:26,  7.27s/it]

 23%|██▎       | 897/3886 [1:47:19<6:39:30,  8.02s/it]

 23%|██▎       | 898/3886 [1:47:27<6:40:05,  8.03s/it]

 23%|██▎       | 899/3886 [1:47:33<5:58:58,  7.21s/it]

 23%|██▎       | 900/3886 [1:47:47<7:49:59,  9.44s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2487, 'grad_norm': 0.2591519637381854, 'learning_rate': 0.00017939660749215642, 'epoch': 0.23}
 23%|██▎       | 901/3886 [1:48:24<14:40:29, 17.70s/it]


 23%|██▎       | 903/3886 [1:48:42<11:02:21, 13.32s/it]
{'loss': 1.3019, 'grad_norm': 0.23052596495103528, 'learning_rate': 0.0001792951457054126, 'epoch': 0.23}

 23%|██▎       | 904/3886 [1:48:49<9:36:21, 11.60s/it]

 23%|██▎       | 905/3886 [1:48:56<8:24:12, 10.15s/it]

 23%|██▎       | 906/3886 [1:49:03<7:36:58,  9.20s/it]

 23%|██▎       | 907/3886 [1:49:10<6:53:22,  8.33s/it]


 23%|██▎       | 909/3886 [1:49:22<5:56:33,  7.19s/it]
{'loss': 1.1495, 'grad_norm': 0.2567508063497928, 'learning_rate': 0.00017898943925160108, 'epoch': 0.23}


 23%|██▎       | 911/3886 [1:49:38<6:11:48,  7.50s/it]
{'loss': 1.2598, 'grad_norm': 0.22310319219411526, 'learning_rate': 0.00017888709767979846, 'epoch': 0.23}

 23%|██▎       | 912/3886 [1:49:46<6:15:48,  7.58s/it]

 23%|██▎       | 913/3886 [1:49:56<7:02:20,  8.52s/it]

 24%|██▎       | 914/3886 [1:50:03<6:36:32,  8.01s/it]

 24%|██▎       | 915/3886 [1:50:12<6:52:45,  8.34s/it]


 24%|██▎       | 917/3886 [1:50:28<6:35:58,  8.00s/it]
{'loss': 1.4228, 'grad_norm': 0.24825992351513398, 'learning_rate': 0.00017857875868466247, 'epoch': 0.24}


 24%|██▎       | 919/3886 [1:50:40<5:45:45,  6.99s/it]
{'loss': 1.4502, 'grad_norm': 0.23541798988109067, 'learning_rate': 0.00017847554187849806, 'epoch': 0.24}

 24%|██▎       | 920/3886 [1:50:47<5:41:35,  6.91s/it]

 24%|██▎       | 921/3886 [1:50:55<5:56:01,  7.20s/it]

 24%|██▎       | 922/3886 [1:51:00<5:33:15,  6.75s/it]

 24%|██▍       | 923/3886 [1:51:06<5:23:19,  6.55s/it]

 24%|██▍       | 924/3886 [1:51:12<5:03:51,  6.16s/it]

 24%|██▍       | 925/3886 [1:51:16<4:44:27,  5.76s/it]

 24%|██▍       | 926/3886 [1:51:23<4:49:32,  5.87s/it]

 24%|██▍       | 927/3886 [1:51:29<4:53:34,  5.95s/it]

 24%|██▍       | 928/3886 [1:51:37<5:23:19,  6.56s/it]

 24%|██▍       | 929/3886 [1:51:52<7:29:53,  9.13s/it]

 24%|██▍       | 930/3886 [1:51:58<6:46:00,  8.24s/it]

 24%|██▍       | 931/3886 [1:52:07<7:02:48,  8.58s/it]

 24%|██▍       | 932/3886 [1:52:15<6:43:27,  8.19s/it]

 24%|██▍       | 933/3886 [1:52:21<6:10:19,  7.52s/it]

 24%|██▍       | 934/3886 [1:52:27<5:47:09,  7.06s/it]

 24%|██▍       | 935/3886 [1:52:34<5:47:16,  7.06s/it]

 24%|██▍       | 936/3886 [1:52:40<5:36:39,  6.85s/it]

 24%|██▍       | 937/3886 [1:52:47<5:33:54,  6.79s/it]

 24%|██▍       | 938/3886 [1:52:55<5:59:09,  7.31s/it]

 24%|██▍       | 939/3886 [1:53:04<6:26:12,  7.86s/it]

 24%|██▍       | 940/3886 [1:53:11<6:08:47,  7.51s/it]

 24%|██▍       | 941/3886 [1:53:26<7:53:20,  9.64s/it]

 24%|██▍       | 942/3886 [1:53:32<7:02:45,  8.62s/it]

 24%|██▍       | 943/3886 [1:53:41<7:06:03,  8.69s/it]

 24%|██▍       | 944/3886 [1:53:48<6:51:50,  8.40s/it]


 24%|██▍       | 946/3886 [1:54:00<5:42:34,  6.99s/it]

 24%|██▍       | 947/3886 [1:54:06<5:29:33,  6.73s/it]
{'loss': 1.1128, 'grad_norm': 0.25359057059942697, 'learning_rate': 0.00017700773844665392, 'epoch': 0.24}

 24%|██▍       | 948/3886 [1:54:13<5:34:09,  6.82s/it]

 24%|██▍       | 949/3886 [1:54:19<5:17:18,  6.48s/it]

 24%|██▍       | 950/3886 [1:54:27<5:32:05,  6.79s/it]

 24%|██▍       | 951/3886 [1:54:36<6:05:42,  7.48s/it]

 24%|██▍       | 952/3886 [1:54:42<5:42:33,  7.01s/it]

 25%|██▍       | 953/3886 [1:54:48<5:36:16,  6.88s/it]

 25%|██▍       | 954/3886 [1:54:55<5:28:17,  6.72s/it]

 25%|██▍       | 955/3886 [1:55:00<5:11:08,  6.37s/it]


 25%|██▍       | 957/3886 [1:55:12<5:02:41,  6.20s/it]
{'loss': 1.3892, 'grad_norm': 0.22601016993651282, 'learning_rate': 0.00017647331551778973, 'epoch': 0.25}

 25%|██▍       | 958/3886 [1:55:18<4:51:40,  5.98s/it]

 25%|██▍       | 959/3886 [1:55:34<7:13:54,  8.89s/it]

 25%|██▍       | 960/3886 [1:55:41<6:45:12,  8.31s/it]

 25%|██▍       | 961/3886 [1:55:50<6:58:56,  8.59s/it]

 25%|██▍       | 962/3886 [1:55:57<6:37:25,  8.16s/it]

 25%|██▍       | 963/3886 [1:56:04<6:14:12,  7.68s/it]

 25%|██▍       | 964/3886 [1:56:12<6:23:03,  7.87s/it]

 25%|██▍       | 965/3886 [1:56:19<6:12:30,  7.65s/it]

 25%|██▍       | 966/3886 [1:56:25<5:43:37,  7.06s/it]

 25%|██▍       | 967/3886 [1:56:32<5:46:04,  7.11s/it]

 25%|██▍       | 968/3886 [1:56:39<5:42:24,  7.04s/it]

 25%|██▍       | 969/3886 [1:56:44<5:16:34,  6.51s/it]

 25%|██▍       | 970/3886 [1:56:54<6:09:16,  7.60s/it]

 25%|██▍       | 971/3886 [1:57:00<5:48:34,  7.17s/it]

 25%|██▌       | 972/3886 [1:57:07<5:39:45,  7.00s/it]

 25%|██▌       | 973/3886 [1:57:13<5:22:32,  6.64s/it]

 25%|██▌       | 974/3886 [1:57:20<5:26:17,  6.72s/it]

 25%|██▌       | 975/3886 [1:57:28<5:49:16,  7.20s/it]

 25%|██▌       | 976/3886 [1:57:34<5:30:36,  6.82s/it]

 25%|██▌       | 977/3886 [1:57:40<5:26:06,  6.73s/it]

 25%|██▌       | 978/3886 [1:57:46<5:12:57,  6.46s/it]

 25%|██▌       | 979/3886 [1:57:53<5:19:53,  6.60s/it]

 25%|██▌       | 980/3886 [1:58:02<5:46:09,  7.15s/it]

 25%|██▌       | 981/3886 [1:58:07<5:17:35,  6.56s/it]


 25%|██▌       | 983/3886 [1:58:19<5:03:36,  6.28s/it]

 25%|██▌       | 984/3886 [1:58:29<5:57:37,  7.39s/it]
{'loss': 1.3393, 'grad_norm': 0.21085603334849584, 'learning_rate': 0.00017500394057880095, 'epoch': 0.25}

 25%|██▌       | 985/3886 [1:58:37<6:11:43,  7.69s/it]

 25%|██▌       | 986/3886 [1:58:45<6:18:32,  7.83s/it]

 25%|██▌       | 987/3886 [1:58:51<5:49:29,  7.23s/it]

 25%|██▌       | 988/3886 [1:58:58<5:47:21,  7.19s/it]

 25%|██▌       | 989/3886 [1:59:06<5:57:06,  7.40s/it]

 25%|██▌       | 990/3886 [1:59:11<5:24:49,  6.73s/it]

 26%|██▌       | 991/3886 [1:59:18<5:33:59,  6.92s/it]

 26%|██▌       | 992/3886 [1:59:27<5:50:16,  7.26s/it]

 26%|██▌       | 993/3886 [1:59:36<6:23:39,  7.96s/it]


 26%|██▌       | 995/3886 [1:59:51<5:59:36,  7.46s/it]
{'loss': 1.2303, 'grad_norm': 0.2195580236150571, 'learning_rate': 0.00017439437274670158, 'epoch': 0.26}

 26%|██▌       | 996/3886 [1:59:56<5:25:10,  6.75s/it]

 26%|██▌       | 997/3886 [2:00:10<7:11:26,  8.96s/it]


 26%|██▌       | 999/3886 [2:00:23<6:04:59,  7.59s/it]
{'loss': 1.4216, 'grad_norm': 0.22195214251741807, 'learning_rate': 0.0001741711583839979, 'epoch': 0.26}

 26%|██▌       | 1000/3886 [2:00:28<5:31:31,  6.89s/it]

 26%|██▌       | 1001/3886 [2:00:35<5:26:35,  6.79s/it]

 26%|██▌       | 1002/3886 [2:00:43<5:49:20,  7.27s/it]

 26%|██▌       | 1003/3886 [2:00:50<5:50:22,  7.29s/it]

 26%|██▌       | 1004/3886 [2:00:58<5:54:23,  7.38s/it]

 26%|██▌       | 1005/3886 [2:01:06<6:02:24,  7.55s/it]

 26%|██▌       | 1006/3886 [2:01:12<5:36:41,  7.01s/it]

 26%|██▌       | 1007/3886 [2:01:17<5:19:53,  6.67s/it]

 26%|██▌       | 1008/3886 [2:01:23<5:09:13,  6.45s/it]

 26%|██▌       | 1009/3886 [2:01:30<5:17:01,  6.61s/it]

 26%|██▌       | 1010/3886 [2:01:35<4:52:24,  6.10s/it]

 26%|██▌       | 1011/3886 [2:01:42<5:07:43,  6.42s/it]

 26%|██▌       | 1012/3886 [2:01:51<5:34:27,  6.98s/it]


 26%|██▌       | 1014/3886 [2:02:03<5:08:42,  6.45s/it]
{'loss': 1.4132, 'grad_norm': 0.2210615829169584, 'learning_rate': 0.000173326781492492, 'epoch': 0.26}

 26%|██▌       | 1015/3886 [2:02:09<5:07:08,  6.42s/it]

 26%|██▌       | 1016/3886 [2:02:15<4:58:31,  6.24s/it]

 26%|██▌       | 1017/3886 [2:02:25<5:54:59,  7.42s/it]

 26%|██▌       | 1018/3886 [2:02:30<5:18:16,  6.66s/it]

 26%|██▌       | 1019/3886 [2:02:38<5:28:58,  6.88s/it]

 26%|██▌       | 1020/3886 [2:02:45<5:40:14,  7.12s/it]

 26%|██▋       | 1021/3886 [2:02:55<6:20:53,  7.98s/it]

 26%|██▋       | 1022/3886 [2:03:01<5:45:06,  7.23s/it]

 26%|██▋       | 1023/3886 [2:03:06<5:11:48,  6.53s/it]

 26%|██▋       | 1024/3886 [2:03:12<5:03:28,  6.36s/it]

 26%|██▋       | 1025/3886 [2:03:20<5:27:28,  6.87s/it]

 26%|██▋       | 1026/3886 [2:03:26<5:15:49,  6.63s/it]

 26%|██▋       | 1027/3886 [2:03:31<4:53:57,  6.17s/it]

 26%|██▋       | 1028/3886 [2:03:38<5:09:40,  6.50s/it]

 26%|██▋       | 1029/3886 [2:03:44<5:03:53,  6.38s/it]

 27%|██▋       | 1030/3886 [2:03:52<5:18:43,  6.70s/it]

 27%|██▋       | 1031/3886 [2:03:58<5:15:16,  6.63s/it]

 27%|██▋       | 1032/3886 [2:04:17<8:16:41, 10.44s/it]

 27%|██▋       | 1033/3886 [2:04:24<7:25:59,  9.38s/it]

 27%|██▋       | 1034/3886 [2:04:31<6:41:55,  8.46s/it]

 27%|██▋       | 1035/3886 [2:04:37<6:08:15,  7.75s/it]

 27%|██▋       | 1036/3886 [2:04:42<5:33:10,  7.01s/it]

 27%|██▋       | 1037/3886 [2:04:55<7:04:56,  8.95s/it]


 27%|██▋       | 1039/3886 [2:05:07<5:42:48,  7.22s/it]
{'loss': 1.1514, 'grad_norm': 0.2839525802365286, 'learning_rate': 0.00017189407999998658, 'epoch': 0.27}

 27%|██▋       | 1040/3886 [2:05:22<7:36:51,  9.63s/it]

 27%|██▋       | 1041/3886 [2:05:29<6:51:41,  8.68s/it]


 27%|██▋       | 1043/3886 [2:05:47<7:02:36,  8.92s/it]
{'loss': 1.386, 'grad_norm': 0.2197390360763366, 'learning_rate': 0.000171661934375412, 'epoch': 0.27}


 27%|██▋       | 1045/3886 [2:05:59<5:52:07,  7.44s/it]
{'loss': 1.2007, 'grad_norm': 0.2871021704656148, 'learning_rate': 0.00017154556266598097, 'epoch': 0.27}


 27%|██▋       | 1047/3886 [2:06:11<5:18:11,  6.72s/it]
{'loss': 1.3813, 'grad_norm': 0.21688705767965555, 'learning_rate': 0.00017142899212281766, 'epoch': 0.27}

 27%|██▋       | 1048/3886 [2:06:21<5:57:26,  7.56s/it]

 27%|██▋       | 1049/3886 [2:06:29<6:05:26,  7.73s/it]

 27%|██▋       | 1050/3886 [2:06:34<5:32:51,  7.04s/it]

 27%|██▋       | 1051/3886 [2:06:40<5:07:20,  6.50s/it]

 27%|██▋       | 1052/3886 [2:06:44<4:40:36,  5.94s/it]

 27%|██▋       | 1053/3886 [2:06:52<5:12:37,  6.62s/it]

 27%|██▋       | 1054/3886 [2:07:00<5:24:29,  6.87s/it]

 27%|██▋       | 1055/3886 [2:07:06<5:17:21,  6.73s/it]

 27%|██▋       | 1056/3886 [2:07:12<5:05:55,  6.49s/it]


 27%|██▋       | 1058/3886 [2:07:31<6:07:12,  7.79s/it]

 27%|██▋       | 1059/3886 [2:07:39<6:09:08,  7.83s/it]
{'loss': 1.2871, 'grad_norm': 0.2134370441078981, 'learning_rate': 0.00017072541153598632, 'epoch': 0.27}

 27%|██▋       | 1060/3886 [2:07:46<5:50:56,  7.45s/it]

 27%|██▋       | 1061/3886 [2:07:52<5:30:26,  7.02s/it]

 27%|██▋       | 1062/3886 [2:08:00<5:45:21,  7.34s/it]

 27%|██▋       | 1063/3886 [2:08:06<5:33:53,  7.10s/it]

 27%|██▋       | 1064/3886 [2:08:12<5:10:15,  6.60s/it]

 27%|██▋       | 1065/3886 [2:08:21<5:42:48,  7.29s/it]

 27%|██▋       | 1066/3886 [2:08:28<5:47:43,  7.40s/it]

 27%|██▋       | 1067/3886 [2:08:34<5:23:31,  6.89s/it]

 27%|██▋       | 1068/3886 [2:08:40<5:05:02,  6.49s/it]

 28%|██▊       | 1069/3886 [2:08:44<4:41:47,  6.00s/it]

 28%|██▊       | 1070/3886 [2:08:50<4:41:02,  5.99s/it]

 28%|██▊       | 1071/3886 [2:08:55<4:29:07,  5.74s/it]

 28%|██▊       | 1072/3886 [2:09:04<5:04:21,  6.49s/it]

 28%|██▊       | 1073/3886 [2:09:10<5:00:36,  6.41s/it]

 28%|██▊       | 1074/3886 [2:09:16<4:51:40,  6.22s/it]

 28%|██▊       | 1075/3886 [2:09:23<5:03:42,  6.48s/it]

 28%|██▊       | 1076/3886 [2:09:32<5:40:59,  7.28s/it]

 28%|██▊       | 1077/3886 [2:09:38<5:23:37,  6.91s/it]

 28%|██▊       | 1078/3886 [2:09:45<5:19:14,  6.82s/it]

 28%|██▊       | 1079/3886 [2:09:52<5:26:08,  6.97s/it]

 28%|██▊       | 1080/3886 [2:09:57<4:55:35,  6.32s/it]

 28%|██▊       | 1081/3886 [2:10:02<4:45:50,  6.11s/it]

 28%|██▊       | 1082/3886 [2:10:08<4:34:45,  5.88s/it]

 28%|██▊       | 1083/3886 [2:10:17<5:15:52,  6.76s/it]

 28%|██▊       | 1084/3886 [2:10:26<5:49:32,  7.48s/it]

 28%|██▊       | 1085/3886 [2:10:33<5:45:58,  7.41s/it]


 28%|██▊       | 1087/3886 [2:10:45<5:14:54,  6.75s/it]
{'loss': 1.5477, 'grad_norm': 0.2105414575785656, 'learning_rate': 0.00016905632909136248, 'epoch': 0.28}

 28%|██▊       | 1088/3886 [2:10:52<5:08:27,  6.61s/it]


 28%|██▊       | 1090/3886 [2:11:03<4:47:43,  6.17s/it]
{'loss': 1.5099, 'grad_norm': 0.24215965961001598, 'learning_rate': 0.00016887525166819236, 'epoch': 0.28}

 28%|██▊       | 1091/3886 [2:11:11<5:05:05,  6.55s/it]

 28%|██▊       | 1092/3886 [2:11:16<4:48:27,  6.19s/it]

 28%|██▊       | 1093/3886 [2:11:23<4:59:14,  6.43s/it]

 28%|██▊       | 1094/3886 [2:11:31<5:13:20,  6.73s/it]

 28%|██▊       | 1095/3886 [2:11:43<6:25:19,  8.28s/it]

 28%|██▊       | 1096/3886 [2:11:49<6:04:09,  7.83s/it]

 28%|██▊       | 1097/3886 [2:12:05<7:50:12, 10.12s/it]

 28%|██▊       | 1098/3886 [2:12:15<7:48:56, 10.09s/it]

 28%|██▊       | 1099/3886 [2:12:22<7:13:48,  9.34s/it]

 28%|██▊       | 1100/3886 [2:12:28<6:21:26,  8.21s/it]

 28%|██▊       | 1101/3886 [2:12:35<6:01:24,  7.79s/it]

 28%|██▊       | 1102/3886 [2:12:43<6:01:19,  7.79s/it]

 28%|██▊       | 1103/3886 [2:12:49<5:39:09,  7.31s/it]

 28%|██▊       | 1104/3886 [2:12:55<5:20:40,  6.92s/it]

 28%|██▊       | 1105/3886 [2:13:00<5:02:32,  6.53s/it]

 28%|██▊       | 1106/3886 [2:13:09<5:29:33,  7.11s/it]

 28%|██▊       | 1107/3886 [2:13:15<5:18:02,  6.87s/it]


 29%|██▊       | 1109/3886 [2:13:26<4:41:14,  6.08s/it]
{'loss': 1.5225, 'grad_norm': 0.2506399911766499, 'learning_rate': 0.00016771847365790405, 'epoch': 0.29}

 29%|██▊       | 1110/3886 [2:13:32<4:43:59,  6.14s/it]

 29%|██▊       | 1111/3886 [2:13:39<4:59:02,  6.47s/it]

 29%|██▊       | 1112/3886 [2:13:49<5:38:06,  7.31s/it]

 29%|██▊       | 1113/3886 [2:14:01<6:48:50,  8.85s/it]

 29%|██▊       | 1114/3886 [2:14:07<6:16:53,  8.16s/it]


 29%|██▊       | 1116/3886 [2:14:22<5:51:51,  7.62s/it]
{'loss': 1.2903, 'grad_norm': 0.2977301903276772, 'learning_rate': 0.0001672879953428278, 'epoch': 0.29}

 29%|██▊       | 1117/3886 [2:14:30<6:03:39,  7.88s/it]

 29%|██▉       | 1118/3886 [2:14:38<6:07:50,  7.97s/it]


 29%|██▉       | 1120/3886 [2:14:58<6:37:12,  8.62s/it]
{'loss': 1.487, 'grad_norm': 0.2279931526268765, 'learning_rate': 0.00016704097828576254, 'epoch': 0.29}

 29%|██▉       | 1121/3886 [2:15:05<6:20:21,  8.25s/it]

 29%|██▉       | 1122/3886 [2:15:13<6:20:18,  8.26s/it]

 29%|██▉       | 1123/3886 [2:15:23<6:34:58,  8.58s/it]

 29%|██▉       | 1124/3886 [2:15:30<6:18:37,  8.23s/it]


 29%|██▉       | 1126/3886 [2:15:44<5:44:10,  7.48s/it]
{'loss': 1.5023, 'grad_norm': 0.22817965145813018, 'learning_rate': 0.0001666690561993888, 'epoch': 0.29}

 29%|██▉       | 1127/3886 [2:15:50<5:30:18,  7.18s/it]

 29%|██▉       | 1128/3886 [2:15:57<5:31:08,  7.20s/it]

 29%|██▉       | 1129/3886 [2:16:03<5:13:35,  6.82s/it]

 29%|██▉       | 1130/3886 [2:16:10<5:03:49,  6.61s/it]

 29%|██▉       | 1131/3886 [2:16:21<6:05:29,  7.96s/it]

 29%|██▉       | 1132/3886 [2:16:25<5:19:47,  6.97s/it]

 29%|██▉       | 1133/3886 [2:16:33<5:32:45,  7.25s/it]

 29%|██▉       | 1134/3886 [2:16:40<5:26:32,  7.12s/it]

 29%|██▉       | 1135/3886 [2:16:47<5:28:33,  7.17s/it]

 29%|██▉       | 1136/3886 [2:16:55<5:42:45,  7.48s/it]

 29%|██▉       | 1137/3886 [2:17:04<5:57:33,  7.80s/it]


 29%|██▉       | 1139/3886 [2:17:18<5:39:46,  7.42s/it]

 29%|██▉       | 1140/3886 [2:17:24<5:20:45,  7.01s/it]
{'loss': 1.4173, 'grad_norm': 0.24994527283563567, 'learning_rate': 0.0001657947693196957, 'epoch': 0.29}

 29%|██▉       | 1141/3886 [2:17:31<5:16:09,  6.91s/it]

 29%|██▉       | 1142/3886 [2:17:36<5:01:37,  6.60s/it]

 29%|██▉       | 1143/3886 [2:17:44<5:21:45,  7.04s/it]

 29%|██▉       | 1144/3886 [2:17:52<5:22:02,  7.05s/it]


 29%|██▉       | 1146/3886 [2:18:04<5:01:14,  6.60s/it]
{'loss': 1.0811, 'grad_norm': 0.2491104660213179, 'learning_rate': 0.00016541732521395953, 'epoch': 0.29}

 30%|██▉       | 1147/3886 [2:18:12<5:26:18,  7.15s/it]

 30%|██▉       | 1148/3886 [2:18:24<6:36:00,  8.68s/it]

 30%|██▉       | 1149/3886 [2:18:33<6:34:56,  8.66s/it]

 30%|██▉       | 1150/3886 [2:18:38<5:39:47,  7.45s/it]

 30%|██▉       | 1151/3886 [2:18:50<6:43:21,  8.85s/it]

 30%|██▉       | 1152/3886 [2:18:58<6:30:14,  8.56s/it]

 30%|██▉       | 1153/3886 [2:19:06<6:24:30,  8.44s/it]

 30%|██▉       | 1154/3886 [2:19:12<5:56:59,  7.84s/it]

 30%|██▉       | 1155/3886 [2:19:18<5:33:29,  7.33s/it]

 30%|██▉       | 1156/3886 [2:19:23<4:55:38,  6.50s/it]

 30%|██▉       | 1157/3886 [2:19:30<4:56:58,  6.53s/it]

 30%|██▉       | 1158/3886 [2:19:35<4:44:34,  6.26s/it]

 30%|██▉       | 1159/3886 [2:19:41<4:42:14,  6.21s/it]

 30%|██▉       | 1160/3886 [2:19:49<5:00:42,  6.62s/it]

 30%|██▉       | 1161/3886 [2:19:54<4:45:35,  6.29s/it]

 30%|██▉       | 1162/3886 [2:20:00<4:35:01,  6.06s/it]

 30%|██▉       | 1163/3886 [2:20:05<4:24:54,  5.84s/it]

 30%|██▉       | 1164/3886 [2:20:11<4:30:07,  5.95s/it]

 30%|██▉       | 1165/3886 [2:20:18<4:32:06,  6.00s/it]

 30%|███       | 1166/3886 [2:20:23<4:31:32,  5.99s/it]

 30%|███       | 1167/3886 [2:20:40<6:52:20,  9.10s/it]

 30%|███       | 1168/3886 [2:20:47<6:20:26,  8.40s/it]


 30%|███       | 1170/3886 [2:21:02<6:04:50,  8.06s/it]
{'loss': 1.4955, 'grad_norm': 0.2278805270976907, 'learning_rate': 0.0001638912816042073, 'epoch': 0.3}

 30%|███       | 1171/3886 [2:21:07<5:27:23,  7.24s/it]

 30%|███       | 1172/3886 [2:21:14<5:14:52,  6.96s/it]

 30%|███       | 1173/3886 [2:21:19<4:56:26,  6.56s/it]

 30%|███       | 1174/3886 [2:21:26<4:58:09,  6.60s/it]

 30%|███       | 1175/3886 [2:21:32<4:48:05,  6.38s/it]

 30%|███       | 1176/3886 [2:21:38<4:50:23,  6.43s/it]

 30%|███       | 1177/3886 [2:21:48<5:33:18,  7.38s/it]

 30%|███       | 1178/3886 [2:21:58<6:02:44,  8.04s/it]

 30%|███       | 1179/3886 [2:22:03<5:31:17,  7.34s/it]
[2024-05-28 02:56:48,561] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 1180/3886 [2:22:18<7:16:01,  9.67s/it]

 30%|███       | 1181/3886 [2:22:27<6:58:51,  9.29s/it]

 30%|███       | 1182/3886 [2:22:33<6:19:59,  8.43s/it]

 30%|███       | 1183/3886 [2:22:39<5:46:46,  7.70s/it]

 30%|███       | 1184/3886 [2:22:49<6:10:08,  8.22s/it]


 31%|███       | 1186/3886 [2:23:00<5:13:56,  6.98s/it]

 31%|███       | 1187/3886 [2:23:06<4:58:40,  6.64s/it]
{'loss': 1.2339, 'grad_norm': 0.24984703613315817, 'learning_rate': 0.00016279482817569527, 'epoch': 0.31}


 31%|███       | 1189/3886 [2:23:20<5:11:37,  6.93s/it]
{'loss': 1.3309, 'grad_norm': 0.24447017909765606, 'learning_rate': 0.0001626650004419294, 'epoch': 0.31}

 31%|███       | 1190/3886 [2:23:29<5:34:44,  7.45s/it]

 31%|███       | 1191/3886 [2:23:35<5:21:17,  7.15s/it]

 31%|███       | 1192/3886 [2:23:43<5:19:53,  7.12s/it]

 31%|███       | 1193/3886 [2:23:50<5:21:08,  7.16s/it]

 31%|███       | 1194/3886 [2:23:55<4:58:30,  6.65s/it]

 31%|███       | 1195/3886 [2:24:02<4:57:27,  6.63s/it]


 31%|███       | 1197/3886 [2:24:12<4:28:11,  5.98s/it]
{'loss': 1.5043, 'grad_norm': 0.2203791910063046, 'learning_rate': 0.00016214395158638164, 'epoch': 0.31}

 31%|███       | 1198/3886 [2:24:18<4:23:59,  5.89s/it]

 31%|███       | 1199/3886 [2:24:23<4:16:26,  5.73s/it]

 31%|███       | 1200/3886 [2:24:30<4:24:37,  5.91s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2256, 'grad_norm': 0.20491161990149706, 'learning_rate': 0.0001618823894762039, 'epoch': 0.31}
 31%|███       | 1201/3886 [2:25:04<10:43:45, 14.39s/it]

 31%|███       | 1202/3886 [2:25:11<8:58:57, 12.05s/it]

 31%|███       | 1203/3886 [2:25:17<7:39:34, 10.28s/it]

 31%|███       | 1204/3886 [2:25:24<6:53:23,  9.25s/it]

 31%|███       | 1205/3886 [2:25:32<6:39:53,  8.95s/it]

 31%|███       | 1206/3886 [2:25:38<6:03:46,  8.14s/it]

 31%|███       | 1207/3886 [2:25:43<5:25:39,  7.29s/it]

 31%|███       | 1208/3886 [2:25:51<5:27:14,  7.33s/it]

 31%|███       | 1209/3886 [2:25:57<5:14:49,  7.06s/it]

 31%|███       | 1210/3886 [2:26:05<5:19:46,  7.17s/it]

 31%|███       | 1211/3886 [2:26:10<4:49:01,  6.48s/it]

 31%|███       | 1212/3886 [2:26:15<4:33:56,  6.15s/it]

 31%|███       | 1213/3886 [2:26:21<4:36:07,  6.20s/it]

 31%|███       | 1214/3886 [2:26:29<4:58:43,  6.71s/it]

 31%|███▏      | 1215/3886 [2:26:35<4:53:09,  6.59s/it]

 31%|███▏      | 1216/3886 [2:26:43<5:03:13,  6.81s/it]

 31%|███▏      | 1217/3886 [2:26:47<4:35:06,  6.18s/it]

 31%|███▏      | 1218/3886 [2:26:53<4:25:54,  5.98s/it]


 31%|███▏      | 1220/3886 [2:27:07<4:43:05,  6.37s/it]
{'loss': 1.4999, 'grad_norm': 0.24724435295588917, 'learning_rate': 0.00016063062498201898, 'epoch': 0.31}

 31%|███▏      | 1221/3886 [2:27:11<4:22:43,  5.92s/it]

 31%|███▏      | 1222/3886 [2:27:16<4:08:11,  5.59s/it]

 31%|███▏      | 1223/3886 [2:27:22<4:06:54,  5.56s/it]


 32%|███▏      | 1225/3886 [2:27:39<5:07:02,  6.92s/it]

 32%|███▏      | 1226/3886 [2:27:45<4:53:31,  6.62s/it]
{'loss': 1.5729, 'grad_norm': 0.2716200311017885, 'learning_rate': 0.00016023215664363874, 'epoch': 0.32}

 32%|███▏      | 1227/3886 [2:27:58<6:24:56,  8.69s/it]


 32%|███▏      | 1229/3886 [2:28:11<5:26:19,  7.37s/it]
{'loss': 1.4916, 'grad_norm': 0.28468159538844096, 'learning_rate': 0.00016003235690171362, 'epoch': 0.32}

 32%|███▏      | 1230/3886 [2:28:18<5:20:40,  7.24s/it]


 32%|███▏      | 1232/3886 [2:28:31<4:59:46,  6.78s/it]
{'loss': 1.4463, 'grad_norm': 0.250054152591649, 'learning_rate': 0.00015983218177631551, 'epoch': 0.32}

 32%|███▏      | 1233/3886 [2:28:37<4:53:23,  6.64s/it]

 32%|███▏      | 1234/3886 [2:28:45<5:16:04,  7.15s/it]

 32%|███▏      | 1235/3886 [2:28:54<5:30:18,  7.48s/it]

 32%|███▏      | 1236/3886 [2:28:59<5:07:25,  6.96s/it]

 32%|███▏      | 1237/3886 [2:29:09<5:49:31,  7.92s/it]


 32%|███▏      | 1239/3886 [2:29:23<5:12:45,  7.09s/it]
{'loss': 1.2978, 'grad_norm': 0.22490358118127265, 'learning_rate': 0.00015936365369463058, 'epoch': 0.32}

 32%|███▏      | 1240/3886 [2:29:35<6:23:41,  8.70s/it]


 32%|███▏      | 1242/3886 [2:29:53<6:27:28,  8.79s/it]
{'loss': 1.3107, 'grad_norm': 0.2602661260332541, 'learning_rate': 0.0001591622363441636, 'epoch': 0.32}

 32%|███▏      | 1243/3886 [2:29:59<5:59:51,  8.17s/it]

 32%|███▏      | 1244/3886 [2:30:08<6:07:06,  8.34s/it]

 32%|███▏      | 1245/3886 [2:30:18<6:28:40,  8.83s/it]

 32%|███▏      | 1246/3886 [2:30:24<5:53:21,  8.03s/it]

 32%|███▏      | 1247/3886 [2:30:29<5:13:53,  7.14s/it]

 32%|███▏      | 1248/3886 [2:30:44<6:57:24,  9.49s/it]

 32%|███▏      | 1249/3886 [2:30:51<6:20:35,  8.66s/it]


 32%|███▏      | 1251/3886 [2:31:03<5:12:51,  7.12s/it]
{'loss': 1.5275, 'grad_norm': 0.22043459110254873, 'learning_rate': 0.00015855576968663225, 'epoch': 0.32}

 32%|███▏      | 1252/3886 [2:31:07<4:35:42,  6.28s/it]

 32%|███▏      | 1253/3886 [2:31:15<4:57:13,  6.77s/it]

 32%|███▏      | 1254/3886 [2:31:21<4:51:03,  6.64s/it]

 32%|███▏      | 1255/3886 [2:31:27<4:43:49,  6.47s/it]


 32%|███▏      | 1257/3886 [2:31:43<5:15:17,  7.20s/it]

 32%|███▏      | 1258/3886 [2:31:51<5:28:13,  7.49s/it]
{'loss': 1.3988, 'grad_norm': 0.2382962264400645, 'learning_rate': 0.00015808179338365385, 'epoch': 0.32}


 32%|███▏      | 1260/3886 [2:32:05<5:16:24,  7.23s/it]
{'loss': 1.2646, 'grad_norm': 0.22848464752505607, 'learning_rate': 0.00015794600768973406, 'epoch': 0.32}

 32%|███▏      | 1261/3886 [2:32:14<5:39:16,  7.75s/it]

 32%|███▏      | 1262/3886 [2:32:22<5:36:00,  7.68s/it]


 33%|███▎      | 1264/3886 [2:32:35<5:16:10,  7.24s/it]
{'loss': 1.3758, 'grad_norm': 0.20872719434311507, 'learning_rate': 0.00015767395356291165, 'epoch': 0.33}


 33%|███▎      | 1266/3886 [2:32:51<5:40:02,  7.79s/it]
{'loss': 1.5342, 'grad_norm': 0.24806133402034944, 'learning_rate': 0.00015753768588608016, 'epoch': 0.33}

 33%|███▎      | 1267/3886 [2:32:57<5:20:52,  7.35s/it]


 33%|███▎      | 1269/3886 [2:33:17<6:08:54,  8.46s/it]
{'loss': 1.2444, 'grad_norm': 0.23743884099205356, 'learning_rate': 0.0001573329846690066, 'epoch': 0.33}

 33%|███▎      | 1270/3886 [2:33:34<7:59:35, 11.00s/it]

 33%|███▎      | 1271/3886 [2:33:40<6:58:19,  9.60s/it]

 33%|███▎      | 1272/3886 [2:33:48<6:36:22,  9.10s/it]

 33%|███▎      | 1273/3886 [2:33:54<5:55:33,  8.16s/it]

 33%|███▎      | 1274/3886 [2:34:01<5:36:45,  7.74s/it]

 33%|███▎      | 1275/3886 [2:34:08<5:27:08,  7.52s/it]

 33%|███▎      | 1276/3886 [2:34:14<5:15:46,  7.26s/it]

 33%|███▎      | 1277/3886 [2:34:22<5:14:02,  7.22s/it]

 33%|███▎      | 1278/3886 [2:34:29<5:22:35,  7.42s/it]

 33%|███▎      | 1279/3886 [2:34:36<5:11:36,  7.17s/it]

 33%|███▎      | 1280/3886 [2:34:45<5:39:39,  7.82s/it]

 33%|███▎      | 1281/3886 [2:34:53<5:32:33,  7.66s/it]

 33%|███▎      | 1282/3886 [2:35:00<5:24:11,  7.47s/it]

 33%|███▎      | 1283/3886 [2:35:06<5:15:22,  7.27s/it]

 33%|███▎      | 1284/3886 [2:35:15<5:29:04,  7.59s/it]

 33%|███▎      | 1285/3886 [2:35:21<5:09:22,  7.14s/it]

 33%|███▎      | 1286/3886 [2:35:27<4:52:14,  6.74s/it]


 33%|███▎      | 1288/3886 [2:35:39<4:43:44,  6.55s/it]
{'loss': 1.2614, 'grad_norm': 0.20090395418424217, 'learning_rate': 0.00015602827139794397, 'epoch': 0.33}

 33%|███▎      | 1289/3886 [2:35:46<4:52:55,  6.77s/it]

 33%|███▎      | 1290/3886 [2:35:54<5:02:21,  6.99s/it]

 33%|███▎      | 1291/3886 [2:36:02<5:15:28,  7.29s/it]

 33%|███▎      | 1292/3886 [2:36:10<5:29:46,  7.63s/it]

 33%|███▎      | 1293/3886 [2:36:20<6:00:18,  8.34s/it]


 33%|███▎      | 1295/3886 [2:36:33<5:21:20,  7.44s/it]
{'loss': 1.3497, 'grad_norm': 0.20683007738026538, 'learning_rate': 0.00015554402776996165, 'epoch': 0.33}


 33%|███▎      | 1297/3886 [2:36:47<5:17:50,  7.37s/it]
{'loss': 1.4536, 'grad_norm': 0.22581577853858284, 'learning_rate': 0.00015540532440805953, 'epoch': 0.33}

 33%|███▎      | 1298/3886 [2:36:53<4:55:08,  6.84s/it]

 33%|███▎      | 1299/3886 [2:37:03<5:35:33,  7.78s/it]

 33%|███▎      | 1300/3886 [2:37:11<5:34:48,  7.77s/it]


 34%|███▎      | 1302/3886 [2:37:23<5:02:16,  7.02s/it]
{'loss': 1.411, 'grad_norm': 0.2359658169032852, 'learning_rate': 0.0001550578931935368, 'epoch': 0.34}


 34%|███▎      | 1304/3886 [2:37:37<5:02:46,  7.04s/it]
{'loss': 1.4387, 'grad_norm': 0.25276349333900466, 'learning_rate': 0.00015491865259765163, 'epoch': 0.34}

 34%|███▎      | 1305/3886 [2:37:42<4:37:24,  6.45s/it]

 34%|███▎      | 1306/3886 [2:37:50<4:55:52,  6.88s/it]

 34%|███▎      | 1307/3886 [2:38:05<6:36:36,  9.23s/it]

 34%|███▎      | 1308/3886 [2:38:11<5:49:24,  8.13s/it]

 34%|███▎      | 1309/3886 [2:38:16<5:19:47,  7.45s/it]

 34%|███▎      | 1310/3886 [2:38:22<4:54:14,  6.85s/it]

 34%|███▎      | 1311/3886 [2:38:29<4:55:37,  6.89s/it]

 34%|███▍      | 1312/3886 [2:38:44<6:37:49,  9.27s/it]

 34%|███▍      | 1313/3886 [2:38:50<5:56:33,  8.31s/it]

 34%|███▍      | 1314/3886 [2:38:56<5:22:41,  7.53s/it]

 34%|███▍      | 1315/3886 [2:39:05<5:49:27,  8.16s/it]

 34%|███▍      | 1316/3886 [2:39:11<5:16:26,  7.39s/it]

 34%|███▍      | 1317/3886 [2:39:18<5:16:33,  7.39s/it]


 34%|███▍      | 1319/3886 [2:39:39<6:41:56,  9.39s/it]
{'loss': 1.3789, 'grad_norm': 0.22130543421008628, 'learning_rate': 0.00015386950998343192, 'epoch': 0.34}


 34%|███▍      | 1321/3886 [2:39:53<5:43:10,  8.03s/it]
{'loss': 1.4716, 'grad_norm': 0.20754482912765726, 'learning_rate': 0.00015372898444758408, 'epoch': 0.34}

 34%|███▍      | 1322/3886 [2:40:01<5:34:42,  7.83s/it]

 34%|███▍      | 1323/3886 [2:40:11<6:00:39,  8.44s/it]

 34%|███▍      | 1324/3886 [2:40:17<5:32:19,  7.78s/it]

 34%|███▍      | 1325/3886 [2:40:24<5:29:26,  7.72s/it]

 34%|███▍      | 1326/3886 [2:40:32<5:34:15,  7.83s/it]

 34%|███▍      | 1327/3886 [2:40:38<5:05:16,  7.16s/it]

 34%|███▍      | 1328/3886 [2:40:46<5:17:59,  7.46s/it]

 34%|███▍      | 1329/3886 [2:40:54<5:24:54,  7.62s/it]

 34%|███▍      | 1330/3886 [2:41:00<4:58:07,  7.00s/it]

 34%|███▍      | 1331/3886 [2:41:07<5:04:51,  7.16s/it]

 34%|███▍      | 1332/3886 [2:41:12<4:37:23,  6.52s/it]

 34%|███▍      | 1333/3886 [2:41:18<4:30:07,  6.35s/it]

 34%|███▍      | 1334/3886 [2:41:24<4:20:47,  6.13s/it]

 34%|███▍      | 1335/3886 [2:41:33<5:02:02,  7.10s/it]

 34%|███▍      | 1336/3886 [2:41:46<6:14:49,  8.82s/it]

 34%|███▍      | 1337/3886 [2:41:53<5:53:41,  8.33s/it]


 34%|███▍      | 1339/3886 [2:42:08<5:34:20,  7.88s/it]
{'loss': 1.2995, 'grad_norm': 0.22345409178274428, 'learning_rate': 0.00015245758225663952, 'epoch': 0.34}

 34%|███▍      | 1340/3886 [2:42:15<5:22:22,  7.60s/it]

 35%|███▍      | 1341/3886 [2:42:22<5:25:42,  7.68s/it]

 35%|███▍      | 1342/3886 [2:42:28<5:03:07,  7.15s/it]

 35%|███▍      | 1343/3886 [2:42:35<4:53:36,  6.93s/it]

 35%|███▍      | 1344/3886 [2:42:41<4:47:43,  6.79s/it]

 35%|███▍      | 1345/3886 [2:42:47<4:29:52,  6.37s/it]


 35%|███▍      | 1347/3886 [2:43:00<4:30:19,  6.39s/it]
{'loss': 1.3602, 'grad_norm': 0.24315090498184444, 'learning_rate': 0.00015188870714254797, 'epoch': 0.35}

 35%|███▍      | 1348/3886 [2:43:06<4:32:55,  6.45s/it]

 35%|███▍      | 1349/3886 [2:43:14<4:54:50,  6.97s/it]


 35%|███▍      | 1351/3886 [2:43:28<4:46:01,  6.77s/it]
{'loss': 1.255, 'grad_norm': 0.24633193825937627, 'learning_rate': 0.00015160340277373468, 'epoch': 0.35}

 35%|███▍      | 1352/3886 [2:43:37<5:22:53,  7.65s/it]

 35%|███▍      | 1353/3886 [2:43:43<4:55:56,  7.01s/it]

 35%|███▍      | 1354/3886 [2:43:51<5:06:31,  7.26s/it]

 35%|███▍      | 1355/3886 [2:43:56<4:44:27,  6.74s/it]

 35%|███▍      | 1356/3886 [2:44:02<4:38:59,  6.62s/it]

 35%|███▍      | 1357/3886 [2:44:14<5:41:52,  8.11s/it]

 35%|███▍      | 1358/3886 [2:44:20<5:15:11,  7.48s/it]

 35%|███▍      | 1359/3886 [2:44:27<5:02:54,  7.19s/it]


 35%|███▌      | 1361/3886 [2:44:48<6:34:22,  9.37s/it]
{'loss': 1.2691, 'grad_norm': 0.23703713700412704, 'learning_rate': 0.00015088763909574938, 'epoch': 0.35}

 35%|███▌      | 1362/3886 [2:44:54<5:58:33,  8.52s/it]


 35%|███▌      | 1364/3886 [2:45:08<5:18:11,  7.57s/it]

 35%|███▌      | 1365/3886 [2:45:14<5:00:49,  7.16s/it]
{'loss': 1.28, 'grad_norm': 0.2889245002580237, 'learning_rate': 0.00015060034087678215, 'epoch': 0.35}


 35%|███▌      | 1367/3886 [2:45:34<6:07:59,  8.77s/it]
{'loss': 1.3496, 'grad_norm': 0.21881165431701108, 'learning_rate': 0.00015045648063093258, 'epoch': 0.35}


 35%|███▌      | 1369/3886 [2:45:46<5:07:49,  7.34s/it]
{'loss': 1.3063, 'grad_norm': 0.22362811746871133, 'learning_rate': 0.0001503124801604486, 'epoch': 0.35}

 35%|███▌      | 1370/3886 [2:45:55<5:35:27,  8.00s/it]

 35%|███▌      | 1371/3886 [2:46:01<5:04:30,  7.26s/it]


 35%|███▌      | 1373/3886 [2:46:12<4:26:40,  6.37s/it]
{'loss': 1.3584, 'grad_norm': 0.2496075345645291, 'learning_rate': 0.0001500240601467449, 'epoch': 0.35}

 35%|███▌      | 1374/3886 [2:46:17<4:15:31,  6.10s/it]

 35%|███▌      | 1375/3886 [2:46:23<4:07:50,  5.92s/it]


 35%|███▌      | 1377/3886 [2:46:36<4:23:08,  6.29s/it]
{'loss': 1.4611, 'grad_norm': 0.24910030823049376, 'learning_rate': 0.00014973508404188458, 'epoch': 0.35}

 35%|███▌      | 1378/3886 [2:46:40<4:02:19,  5.80s/it]

 35%|███▌      | 1379/3886 [2:46:46<4:00:55,  5.77s/it]


 36%|███▌      | 1381/3886 [2:46:58<4:04:20,  5.85s/it]
{'loss': 1.3816, 'grad_norm': 0.22371925242590138, 'learning_rate': 0.0001494455550582629, 'epoch': 0.36}

 36%|███▌      | 1382/3886 [2:47:03<4:01:24,  5.78s/it]

 36%|███▌      | 1383/3886 [2:47:11<4:28:32,  6.44s/it]

 36%|███▌      | 1384/3886 [2:47:18<4:32:01,  6.52s/it]

 36%|███▌      | 1385/3886 [2:47:25<4:36:17,  6.63s/it]

 36%|███▌      | 1386/3886 [2:47:31<4:29:16,  6.46s/it]

 36%|███▌      | 1387/3886 [2:47:37<4:18:40,  6.21s/it]

 36%|███▌      | 1388/3886 [2:47:43<4:20:25,  6.26s/it]

 36%|███▌      | 1389/3886 [2:47:51<4:44:28,  6.84s/it]


 36%|███▌      | 1391/3886 [2:48:06<4:52:31,  7.03s/it]
{'loss': 1.4574, 'grad_norm': 0.2418255807588462, 'learning_rate': 0.00014871933489133383, 'epoch': 0.36}


 36%|███▌      | 1393/3886 [2:48:18<4:27:50,  6.45s/it]

 36%|███▌      | 1394/3886 [2:48:26<4:45:52,  6.88s/it]
{'loss': 1.4564, 'grad_norm': 0.222827013139809, 'learning_rate': 0.0001485008064831157, 'epoch': 0.36}

 36%|███▌      | 1395/3886 [2:48:32<4:39:20,  6.73s/it]

 36%|███▌      | 1396/3886 [2:48:40<4:46:34,  6.91s/it]

 36%|███▌      | 1397/3886 [2:48:48<5:00:23,  7.24s/it]

 36%|███▌      | 1398/3886 [2:48:55<5:01:33,  7.27s/it]

 36%|███▌      | 1399/3886 [2:49:01<4:44:02,  6.85s/it]

 36%|███▌      | 1400/3886 [2:49:07<4:29:28,  6.50s/it]

 36%|███▌      | 1401/3886 [2:49:13<4:30:30,  6.53s/it]

 36%|███▌      | 1402/3886 [2:49:21<4:51:26,  7.04s/it]

 36%|███▌      | 1403/3886 [2:49:31<5:16:46,  7.65s/it]

 36%|███▌      | 1404/3886 [2:49:38<5:11:41,  7.53s/it]

 36%|███▌      | 1405/3886 [2:49:48<5:50:17,  8.47s/it]

 36%|███▌      | 1406/3886 [2:50:03<7:03:33, 10.25s/it]

 36%|███▌      | 1407/3886 [2:50:10<6:20:04,  9.20s/it]


 36%|███▋      | 1409/3886 [2:50:22<5:17:50,  7.70s/it]
{'loss': 1.2928, 'grad_norm': 0.2484512246756443, 'learning_rate': 0.0001474036426904427, 'epoch': 0.36}

 36%|███▋      | 1410/3886 [2:50:29<5:01:20,  7.30s/it]

 36%|███▋      | 1411/3886 [2:50:35<4:47:46,  6.98s/it]

 36%|███▋      | 1412/3886 [2:50:41<4:40:02,  6.79s/it]

 36%|███▋      | 1413/3886 [2:50:48<4:37:21,  6.73s/it]

 36%|███▋      | 1414/3886 [2:50:53<4:20:56,  6.33s/it]

 36%|███▋      | 1415/3886 [2:51:01<4:41:24,  6.83s/it]


 36%|███▋      | 1417/3886 [2:51:16<5:02:01,  7.34s/it]
{'loss': 1.4693, 'grad_norm': 0.2073379809712707, 'learning_rate': 0.00014681544772053603, 'epoch': 0.36}


 37%|███▋      | 1419/3886 [2:51:32<5:21:30,  7.82s/it]
{'loss': 1.3837, 'grad_norm': 0.24297914580596167, 'learning_rate': 0.0001466680726910708, 'epoch': 0.37}

 37%|███▋      | 1420/3886 [2:51:40<5:16:51,  7.71s/it]

 37%|███▋      | 1421/3886 [2:51:47<5:12:09,  7.60s/it]

 37%|███▋      | 1422/3886 [2:51:53<4:51:37,  7.10s/it]

 37%|███▋      | 1423/3886 [2:52:03<5:32:02,  8.09s/it]

 37%|███▋      | 1424/3886 [2:52:09<4:55:40,  7.21s/it]

 37%|███▋      | 1425/3886 [2:52:15<4:41:00,  6.85s/it]

 37%|███▋      | 1426/3886 [2:52:20<4:23:58,  6.44s/it]

 37%|███▋      | 1427/3886 [2:52:27<4:33:31,  6.67s/it]

 37%|███▋      | 1428/3886 [2:52:33<4:24:14,  6.45s/it]

 37%|███▋      | 1429/3886 [2:52:39<4:18:34,  6.31s/it]


 37%|███▋      | 1431/3886 [2:52:56<5:11:30,  7.61s/it]

 37%|███▋      | 1432/3886 [2:53:04<5:15:35,  7.72s/it]
{'loss': 1.1192, 'grad_norm': 0.24840909908422343, 'learning_rate': 0.0001457069919917746, 'epoch': 0.37}

 37%|███▋      | 1433/3886 [2:53:12<5:13:15,  7.66s/it]


 37%|███▋      | 1435/3886 [2:53:24<4:44:01,  6.95s/it]
{'loss': 1.2704, 'grad_norm': 0.2302969418806964, 'learning_rate': 0.00014548443787194967, 'epoch': 0.37}

 37%|███▋      | 1436/3886 [2:53:30<4:28:22,  6.57s/it]

 37%|███▋      | 1437/3886 [2:53:37<4:40:32,  6.87s/it]

 37%|███▋      | 1438/3886 [2:53:45<4:47:46,  7.05s/it]

 37%|███▋      | 1439/3886 [2:53:52<4:49:12,  7.09s/it]

 37%|███▋      | 1440/3886 [2:53:59<4:49:26,  7.10s/it]

 37%|███▋      | 1441/3886 [2:54:05<4:40:02,  6.87s/it]


 37%|███▋      | 1443/3886 [2:54:21<4:46:35,  7.04s/it]
{'loss': 1.2647, 'grad_norm': 0.22811389131534968, 'learning_rate': 0.00014488957353084932, 'epoch': 0.37}

 37%|███▋      | 1444/3886 [2:54:26<4:25:02,  6.51s/it]

 37%|███▋      | 1445/3886 [2:54:33<4:30:40,  6.65s/it]

 37%|███▋      | 1446/3886 [2:54:47<6:06:25,  9.01s/it]

 37%|███▋      | 1447/3886 [2:54:53<5:21:16,  7.90s/it]


 37%|███▋      | 1449/3886 [2:55:04<4:42:05,  6.95s/it]
{'loss': 1.4701, 'grad_norm': 0.22845074372576016, 'learning_rate': 0.00014444211391762433, 'epoch': 0.37}


 37%|███▋      | 1451/3886 [2:55:17<4:20:05,  6.41s/it]
{'loss': 1.373, 'grad_norm': 0.21975239288100085, 'learning_rate': 0.0001442927131402145, 'epoch': 0.37}

 37%|███▋      | 1452/3886 [2:55:28<5:15:52,  7.79s/it]

 37%|███▋      | 1453/3886 [2:55:34<4:59:02,  7.37s/it]

 37%|███▋      | 1454/3886 [2:55:40<4:37:47,  6.85s/it]

 37%|███▋      | 1455/3886 [2:55:47<4:42:24,  6.97s/it]


 37%|███▋      | 1457/3886 [2:56:00<4:36:12,  6.82s/it]
{'loss': 1.2258, 'grad_norm': 0.25584288598256727, 'learning_rate': 0.0001438437739018109, 'epoch': 0.37}

 38%|███▊      | 1458/3886 [2:56:05<4:13:04,  6.25s/it]

 38%|███▊      | 1459/3886 [2:56:12<4:20:53,  6.45s/it]

 38%|███▊      | 1460/3886 [2:56:19<4:21:41,  6.47s/it]

 38%|███▊      | 1461/3886 [2:56:27<4:43:07,  7.01s/it]

 38%|███▊      | 1462/3886 [2:56:35<4:55:34,  7.32s/it]


 38%|███▊      | 1464/3886 [2:56:51<5:11:46,  7.72s/it]
{'loss': 1.225, 'grad_norm': 0.221300966193449, 'learning_rate': 0.00014331862623848934, 'epoch': 0.38}

 38%|███▊      | 1465/3886 [2:56:56<4:40:34,  6.95s/it]

 38%|███▊      | 1466/3886 [2:57:03<4:42:34,  7.01s/it]

 38%|███▊      | 1467/3886 [2:57:10<4:42:20,  7.00s/it]

 38%|███▊      | 1468/3886 [2:57:17<4:42:12,  7.00s/it]

 38%|███▊      | 1469/3886 [2:57:24<4:38:58,  6.93s/it]

 38%|███▊      | 1470/3886 [2:57:31<4:43:43,  7.05s/it]

 38%|███▊      | 1471/3886 [2:57:39<4:53:04,  7.28s/it]

 38%|███▊      | 1472/3886 [2:57:47<5:02:10,  7.51s/it]

 38%|███▊      | 1473/3886 [2:57:53<4:48:47,  7.18s/it]

 38%|███▊      | 1474/3886 [2:58:01<4:56:14,  7.37s/it]

 38%|███▊      | 1475/3886 [2:58:08<4:47:50,  7.16s/it]

 38%|███▊      | 1476/3886 [2:58:15<4:48:06,  7.17s/it]

 38%|███▊      | 1477/3886 [2:58:20<4:22:14,  6.53s/it]

 38%|███▊      | 1478/3886 [2:58:26<4:12:35,  6.29s/it]

 38%|███▊      | 1479/3886 [2:58:33<4:23:06,  6.56s/it]

 38%|███▊      | 1480/3886 [2:58:42<4:53:35,  7.32s/it]

 38%|███▊      | 1481/3886 [2:58:49<4:53:11,  7.31s/it]


 38%|███▊      | 1483/3886 [2:59:05<5:00:09,  7.49s/it]
{'loss': 1.259, 'grad_norm': 0.2037273779837108, 'learning_rate': 0.0001418858431860182, 'epoch': 0.38}

 38%|███▊      | 1484/3886 [2:59:13<5:13:26,  7.83s/it]

 38%|███▊      | 1485/3886 [2:59:20<5:06:11,  7.65s/it]

 38%|███▊      | 1486/3886 [2:59:27<4:55:27,  7.39s/it]


 38%|███▊      | 1488/3886 [2:59:37<3:59:53,  6.00s/it]

 38%|███▊      | 1489/3886 [2:59:51<5:35:53,  8.41s/it]

 38%|███▊      | 1490/3886 [2:59:57<5:06:31,  7.68s/it]

 38%|███▊      | 1491/3886 [3:00:03<4:49:34,  7.25s/it]
{'loss': 1.2803, 'grad_norm': 0.21991604119265099, 'learning_rate': 0.00014127940237931255, 'epoch': 0.38}


 38%|███▊      | 1493/3886 [3:00:19<4:53:25,  7.36s/it]
{'loss': 1.3624, 'grad_norm': 0.2023173150135469, 'learning_rate': 0.00014112750432240348, 'epoch': 0.38}


 38%|███▊      | 1495/3886 [3:00:35<5:00:42,  7.55s/it]
{'loss': 1.3034, 'grad_norm': 0.2068076165353204, 'learning_rate': 0.0001409754919672084, 'epoch': 0.38}

 38%|███▊      | 1496/3886 [3:00:41<4:46:50,  7.20s/it]

 39%|███▊      | 1497/3886 [3:00:50<5:03:42,  7.63s/it]

 39%|███▊      | 1498/3886 [3:00:55<4:39:27,  7.02s/it]


 39%|███▊      | 1500/3886 [3:01:15<5:49:59,  8.80s/it]
 39%|███▊      | 1500/3886 [3:01:15<5:49:59,  8.80s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 39%|███▊      | 1501/3886 [3:01:55<12:02:42, 18.18s/it]

 39%|███▊      | 1502/3886 [3:02:01<9:39:27, 14.58s/it]

 39%|███▊      | 1503/3886 [3:02:13<9:09:49, 13.84s/it]
{'loss': 1.3572, 'grad_norm': 0.25891892871512284, 'learning_rate': 0.00014036630801752056, 'epoch': 0.39}

 39%|███▊      | 1504/3886 [3:02:28<9:27:55, 14.31s/it]

 39%|███▊      | 1505/3886 [3:02:34<7:44:18, 11.70s/it]

 39%|███▉      | 1506/3886 [3:02:39<6:28:04,  9.78s/it]

 39%|███▉      | 1507/3886 [3:02:53<7:11:13, 10.88s/it]

 39%|███▉      | 1508/3886 [3:02:58<6:08:38,  9.30s/it]

 39%|███▉      | 1509/3886 [3:03:12<6:58:45, 10.57s/it]

 39%|███▉      | 1510/3886 [3:03:22<6:49:28, 10.34s/it]


 39%|███▉      | 1512/3886 [3:03:39<6:08:34,  9.32s/it]

 39%|███▉      | 1513/3886 [3:03:45<5:28:49,  8.31s/it]
{'loss': 1.3216, 'grad_norm': 0.2771999411126752, 'learning_rate': 0.00013960230715747996, 'epoch': 0.39}

 39%|███▉      | 1514/3886 [3:03:52<5:06:12,  7.75s/it]


 39%|███▉      | 1516/3886 [3:04:03<4:23:15,  6.66s/it]
{'loss': 1.4668, 'grad_norm': 0.211338625790145, 'learning_rate': 0.0001393725679417819, 'epoch': 0.39}

 39%|███▉      | 1517/3886 [3:04:09<4:11:21,  6.37s/it]

 39%|███▉      | 1518/3886 [3:04:14<4:02:17,  6.14s/it]

 39%|███▉      | 1519/3886 [3:04:20<3:56:10,  5.99s/it]

 39%|███▉      | 1520/3886 [3:04:27<4:13:37,  6.43s/it]


 39%|███▉      | 1522/3886 [3:04:45<5:09:37,  7.86s/it]
{'loss': 1.1553, 'grad_norm': 0.2357963302938742, 'learning_rate': 0.00013891235235623364, 'epoch': 0.39}

 39%|███▉      | 1523/3886 [3:04:53<5:04:35,  7.73s/it]

 39%|███▉      | 1524/3886 [3:05:01<5:16:03,  8.03s/it]

 39%|███▉      | 1525/3886 [3:05:08<5:04:49,  7.75s/it]


 39%|███▉      | 1527/3886 [3:05:23<4:52:27,  7.44s/it]
{'loss': 1.3037, 'grad_norm': 0.26335926980251484, 'learning_rate': 0.00013852809540339638, 'epoch': 0.39}

 39%|███▉      | 1528/3886 [3:05:28<4:28:18,  6.83s/it]

 39%|███▉      | 1529/3886 [3:05:36<4:36:29,  7.04s/it]

 39%|███▉      | 1530/3886 [3:05:42<4:31:36,  6.92s/it]

 39%|███▉      | 1531/3886 [3:05:53<5:10:09,  7.90s/it]

 39%|███▉      | 1532/3886 [3:06:01<5:13:45,  8.00s/it]

 39%|███▉      | 1533/3886 [3:06:07<4:56:17,  7.56s/it]

 39%|███▉      | 1534/3886 [3:06:13<4:28:18,  6.84s/it]

 40%|███▉      | 1535/3886 [3:06:19<4:17:41,  6.58s/it]


 40%|███▉      | 1537/3886 [3:06:31<4:12:21,  6.45s/it]
{'loss': 1.228, 'grad_norm': 0.2187685663037419, 'learning_rate': 0.00013775758054458665, 'epoch': 0.4}

 40%|███▉      | 1538/3886 [3:06:37<4:11:44,  6.43s/it]

 40%|███▉      | 1539/3886 [3:06:44<4:13:32,  6.48s/it]


 40%|███▉      | 1541/3886 [3:07:01<5:02:47,  7.75s/it]
{'loss': 1.3458, 'grad_norm': 0.23351771829046117, 'learning_rate': 0.0001374486370710682, 'epoch': 0.4}


 40%|███▉      | 1543/3886 [3:07:19<5:28:31,  8.41s/it]

 40%|███▉      | 1544/3886 [3:07:27<5:22:38,  8.27s/it]
{'loss': 1.3684, 'grad_norm': 0.25103146255756303, 'learning_rate': 0.00013721665608316673, 'epoch': 0.4}

 40%|███▉      | 1545/3886 [3:07:36<5:29:58,  8.46s/it]

 40%|███▉      | 1546/3886 [3:07:42<4:57:57,  7.64s/it]

 40%|███▉      | 1547/3886 [3:07:48<4:42:07,  7.24s/it]

 40%|███▉      | 1548/3886 [3:07:54<4:29:19,  6.91s/it]

 40%|███▉      | 1549/3886 [3:08:02<4:39:54,  7.19s/it]

 40%|███▉      | 1550/3886 [3:08:11<4:53:55,  7.55s/it]

 40%|███▉      | 1551/3886 [3:08:18<4:55:28,  7.59s/it]


 40%|███▉      | 1553/3886 [3:08:33<4:52:55,  7.53s/it]
{'loss': 1.3664, 'grad_norm': 0.23888417573808185, 'learning_rate': 0.00013651932263029264, 'epoch': 0.4}

 40%|███▉      | 1554/3886 [3:08:40<4:44:04,  7.31s/it]

 40%|████      | 1555/3886 [3:08:48<4:50:00,  7.46s/it]

 40%|████      | 1556/3886 [3:08:54<4:37:50,  7.15s/it]

 40%|████      | 1557/3886 [3:09:01<4:31:43,  7.00s/it]


 40%|████      | 1559/3886 [3:09:25<6:37:29, 10.25s/it]

 40%|████      | 1560/3886 [3:09:31<5:44:03,  8.88s/it]

 40%|████      | 1561/3886 [3:09:37<5:09:36,  7.99s/it]

 40%|████      | 1562/3886 [3:09:42<4:40:52,  7.25s/it]

 40%|████      | 1563/3886 [3:09:50<4:51:08,  7.52s/it]

 40%|████      | 1564/3886 [3:10:00<5:12:35,  8.08s/it]

 40%|████      | 1565/3886 [3:10:09<5:24:36,  8.39s/it]

 40%|████      | 1566/3886 [3:10:24<6:43:18, 10.43s/it]

 40%|████      | 1567/3886 [3:10:29<5:41:05,  8.83s/it]

 40%|████      | 1568/3886 [3:10:38<5:42:16,  8.86s/it]

 40%|████      | 1569/3886 [3:10:43<4:53:09,  7.59s/it]

 40%|████      | 1570/3886 [3:10:49<4:38:02,  7.20s/it]

 40%|████      | 1571/3886 [3:10:56<4:34:49,  7.12s/it]

 40%|████      | 1572/3886 [3:11:03<4:33:27,  7.09s/it]

 40%|████      | 1573/3886 [3:11:13<5:10:05,  8.04s/it]
{'loss': 1.3039, 'grad_norm': 0.19568250753853839, 'learning_rate': 0.000134962392046902, 'epoch': 0.4}

 41%|████      | 1574/3886 [3:11:20<4:51:39,  7.57s/it]


 41%|████      | 1576/3886 [3:11:39<5:24:57,  8.44s/it]

 41%|████      | 1577/3886 [3:11:53<6:30:42, 10.15s/it]

 41%|████      | 1578/3886 [3:11:59<5:47:27,  9.03s/it]

 41%|████      | 1579/3886 [3:12:06<5:22:21,  8.38s/it]

 41%|████      | 1580/3886 [3:12:12<4:51:20,  7.58s/it]

 41%|████      | 1581/3886 [3:12:19<4:48:38,  7.51s/it]

 41%|████      | 1582/3886 [3:12:25<4:24:19,  6.88s/it]

 41%|████      | 1583/3886 [3:12:31<4:13:24,  6.60s/it]

 41%|████      | 1584/3886 [3:12:36<4:05:33,  6.40s/it]

 41%|████      | 1585/3886 [3:12:43<4:05:36,  6.40s/it]

 41%|████      | 1586/3886 [3:12:50<4:16:33,  6.69s/it]

 41%|████      | 1587/3886 [3:12:58<4:28:41,  7.01s/it]

 41%|████      | 1588/3886 [3:13:05<4:32:30,  7.11s/it]

 41%|████      | 1589/3886 [3:13:12<4:23:43,  6.89s/it]

 41%|████      | 1590/3886 [3:13:20<4:45:04,  7.45s/it]

 41%|████      | 1591/3886 [3:13:28<4:48:19,  7.54s/it]

 41%|████      | 1592/3886 [3:13:35<4:34:34,  7.18s/it]

 41%|████      | 1593/3886 [3:13:44<4:59:44,  7.84s/it]

 41%|████      | 1594/3886 [3:13:49<4:31:06,  7.10s/it]

 41%|████      | 1595/3886 [3:13:56<4:24:03,  6.92s/it]

 41%|████      | 1596/3886 [3:14:03<4:29:02,  7.05s/it]

 41%|████      | 1597/3886 [3:14:10<4:28:55,  7.05s/it]

 41%|████      | 1598/3886 [3:14:15<4:08:01,  6.50s/it]

 41%|████      | 1599/3886 [3:14:22<4:06:43,  6.47s/it]

 41%|████      | 1600/3886 [3:14:28<4:03:54,  6.40s/it]

 41%|████      | 1601/3886 [3:14:37<4:34:42,  7.21s/it]

 41%|████      | 1602/3886 [3:14:46<4:53:55,  7.72s/it]

 41%|████▏     | 1603/3886 [3:14:54<4:50:16,  7.63s/it]

 41%|████▏     | 1604/3886 [3:14:58<4:11:05,  6.60s/it]

 41%|████▏     | 1605/3886 [3:15:07<4:39:47,  7.36s/it]

 41%|████▏     | 1606/3886 [3:15:19<5:37:21,  8.88s/it]

 41%|████▏     | 1607/3886 [3:15:27<5:20:29,  8.44s/it]

 41%|████▏     | 1608/3886 [3:15:42<6:38:15, 10.49s/it]

 41%|████▏     | 1609/3886 [3:15:50<6:10:03,  9.75s/it]

 41%|████▏     | 1610/3886 [3:15:58<5:48:01,  9.17s/it]

 41%|████▏     | 1611/3886 [3:16:04<5:08:37,  8.14s/it]

 41%|████▏     | 1612/3886 [3:16:11<4:59:01,  7.89s/it]

 42%|████▏     | 1613/3886 [3:16:18<4:48:11,  7.61s/it]

 42%|████▏     | 1614/3886 [3:16:24<4:33:40,  7.23s/it]

 42%|████▏     | 1615/3886 [3:16:29<4:09:33,  6.59s/it]

 42%|████▏     | 1616/3886 [3:16:34<3:48:41,  6.04s/it]

 42%|████▏     | 1617/3886 [3:16:40<3:48:44,  6.05s/it]

 42%|████▏     | 1618/3886 [3:16:45<3:38:14,  5.77s/it]

 42%|████▏     | 1619/3886 [3:16:52<3:47:49,  6.03s/it]

 42%|████▏     | 1620/3886 [3:16:57<3:40:53,  5.85s/it]

 42%|████▏     | 1621/3886 [3:17:04<3:51:22,  6.13s/it]

 42%|████▏     | 1622/3886 [3:17:10<3:46:18,  6.00s/it]

 42%|████▏     | 1623/3886 [3:17:15<3:41:42,  5.88s/it]

 42%|████▏     | 1624/3886 [3:17:22<3:48:31,  6.06s/it]

 42%|████▏     | 1625/3886 [3:17:30<4:11:11,  6.67s/it]

 42%|████▏     | 1626/3886 [3:17:37<4:14:53,  6.77s/it]

 42%|████▏     | 1627/3886 [3:17:44<4:17:41,  6.84s/it]

 42%|████▏     | 1628/3886 [3:17:50<4:12:38,  6.71s/it]

 42%|████▏     | 1629/3886 [3:17:57<4:06:31,  6.55s/it]

 42%|████▏     | 1630/3886 [3:18:08<4:56:32,  7.89s/it]

 42%|████▏     | 1631/3886 [3:18:14<4:46:13,  7.62s/it]

 42%|████▏     | 1632/3886 [3:18:20<4:24:20,  7.04s/it]

 42%|████▏     | 1633/3886 [3:18:29<4:43:26,  7.55s/it]

 42%|████▏     | 1634/3886 [3:18:39<5:08:39,  8.22s/it]

 42%|████▏     | 1635/3886 [3:18:46<5:03:15,  8.08s/it]

 42%|████▏     | 1636/3886 [3:18:51<4:27:50,  7.14s/it]

 42%|████▏     | 1637/3886 [3:18:56<4:00:18,  6.41s/it]

 42%|████▏     | 1638/3886 [3:19:05<4:33:06,  7.29s/it]

 42%|████▏     | 1639/3886 [3:19:11<4:12:51,  6.75s/it]

 42%|████▏     | 1640/3886 [3:19:19<4:25:22,  7.09s/it]

 42%|████▏     | 1641/3886 [3:19:30<5:13:21,  8.37s/it]

 42%|████▏     | 1642/3886 [3:19:37<5:00:53,  8.05s/it]

 42%|████▏     | 1643/3886 [3:19:46<5:03:34,  8.12s/it]

 42%|████▏     | 1644/3886 [3:19:53<4:49:50,  7.76s/it]

 42%|████▏     | 1645/3886 [3:20:03<5:19:38,  8.56s/it]

 42%|████▏     | 1646/3886 [3:20:10<4:58:58,  8.01s/it]

 42%|████▏     | 1647/3886 [3:20:17<4:45:54,  7.66s/it]

 42%|████▏     | 1648/3886 [3:20:24<4:42:33,  7.58s/it]

 42%|████▏     | 1649/3886 [3:20:29<4:17:32,  6.91s/it]

 42%|████▏     | 1650/3886 [3:20:36<4:08:25,  6.67s/it]

 42%|████▏     | 1651/3886 [3:20:43<4:18:46,  6.95s/it]

 43%|████▎     | 1652/3886 [3:20:50<4:19:37,  6.97s/it]

 43%|████▎     | 1653/3886 [3:20:56<4:01:13,  6.48s/it]

 43%|████▎     | 1654/3886 [3:21:02<4:03:23,  6.54s/it]

 43%|████▎     | 1655/3886 [3:21:11<4:31:49,  7.31s/it]

 43%|████▎     | 1656/3886 [3:21:21<4:59:03,  8.05s/it]

 43%|████▎     | 1657/3886 [3:21:29<4:55:15,  7.95s/it]

 43%|████▎     | 1658/3886 [3:21:35<4:35:51,  7.43s/it]

 43%|████▎     | 1659/3886 [3:21:41<4:19:11,  6.98s/it]

 43%|████▎     | 1660/3886 [3:21:50<4:42:05,  7.60s/it]

 43%|████▎     | 1661/3886 [3:21:58<4:46:19,  7.72s/it]

 43%|████▎     | 1662/3886 [3:22:05<4:42:41,  7.63s/it]

 43%|████▎     | 1663/3886 [3:22:19<5:51:06,  9.48s/it]

 43%|████▎     | 1664/3886 [3:22:27<5:28:18,  8.87s/it]

 43%|████▎     | 1665/3886 [3:22:38<5:58:34,  9.69s/it]

 43%|████▎     | 1666/3886 [3:22:43<5:07:26,  8.31s/it]

 43%|████▎     | 1667/3886 [3:22:49<4:42:59,  7.65s/it]

 43%|████▎     | 1668/3886 [3:22:55<4:19:36,  7.02s/it]

 43%|████▎     | 1669/3886 [3:23:02<4:22:27,  7.10s/it]

 43%|████▎     | 1670/3886 [3:23:08<4:10:11,  6.77s/it]

 43%|████▎     | 1671/3886 [3:23:14<3:58:52,  6.47s/it]

 43%|████▎     | 1672/3886 [3:23:20<3:50:52,  6.26s/it]

 43%|████▎     | 1673/3886 [3:23:27<3:57:14,  6.43s/it]

 43%|████▎     | 1674/3886 [3:23:35<4:17:13,  6.98s/it]

 43%|████▎     | 1675/3886 [3:23:45<4:46:32,  7.78s/it]

 43%|████▎     | 1676/3886 [3:23:52<4:42:41,  7.67s/it]

 43%|████▎     | 1677/3886 [3:23:57<4:14:22,  6.91s/it]

 43%|████▎     | 1678/3886 [3:24:02<3:54:01,  6.36s/it]

 43%|████▎     | 1679/3886 [3:24:12<4:31:58,  7.39s/it]

 43%|████▎     | 1680/3886 [3:24:18<4:21:07,  7.10s/it]

 43%|████▎     | 1681/3886 [3:24:23<3:54:27,  6.38s/it]

 43%|████▎     | 1682/3886 [3:24:30<3:59:36,  6.52s/it]

 43%|████▎     | 1683/3886 [3:24:36<3:54:24,  6.38s/it]

 43%|████▎     | 1684/3886 [3:24:42<3:51:06,  6.30s/it]

 43%|████▎     | 1685/3886 [3:24:57<5:21:42,  8.77s/it]

 43%|████▎     | 1686/3886 [3:25:03<4:51:30,  7.95s/it]

 43%|████▎     | 1687/3886 [3:25:09<4:28:41,  7.33s/it]

 43%|████▎     | 1688/3886 [3:25:14<4:12:13,  6.89s/it]

 43%|████▎     | 1689/3886 [3:25:23<4:35:36,  7.53s/it]

 43%|████▎     | 1690/3886 [3:25:31<4:36:23,  7.55s/it]

 44%|████▎     | 1691/3886 [3:25:36<4:11:05,  6.86s/it]

 44%|████▎     | 1692/3886 [3:25:43<4:04:43,  6.69s/it]

 44%|████▎     | 1693/3886 [3:25:51<4:18:32,  7.07s/it]

 44%|████▎     | 1694/3886 [3:25:58<4:26:39,  7.30s/it]

 44%|████▎     | 1695/3886 [3:26:04<4:07:44,  6.78s/it]

 44%|████▎     | 1696/3886 [3:26:10<4:03:07,  6.66s/it]

 44%|████▎     | 1697/3886 [3:26:18<4:08:54,  6.82s/it]

 44%|████▎     | 1698/3886 [3:26:27<4:36:37,  7.59s/it]

 44%|████▎     | 1699/3886 [3:26:36<4:48:58,  7.93s/it]

 44%|████▎     | 1700/3886 [3:26:41<4:23:01,  7.22s/it]

 44%|████▍     | 1701/3886 [3:26:49<4:26:43,  7.32s/it]

 44%|████▍     | 1702/3886 [3:27:07<6:24:10, 10.55s/it]

 44%|████▍     | 1703/3886 [3:27:14<5:44:35,  9.47s/it]

 44%|████▍     | 1704/3886 [3:27:21<5:17:57,  8.74s/it]

 44%|████▍     | 1705/3886 [3:27:28<5:00:13,  8.26s/it]

 44%|████▍     | 1706/3886 [3:27:34<4:31:56,  7.48s/it]

 44%|████▍     | 1707/3886 [3:27:40<4:16:12,  7.05s/it]

 44%|████▍     | 1708/3886 [3:27:45<3:59:44,  6.60s/it]

 44%|████▍     | 1709/3886 [3:27:51<3:52:14,  6.40s/it]

 44%|████▍     | 1710/3886 [3:27:57<3:46:52,  6.26s/it]

 44%|████▍     | 1711/3886 [3:28:05<4:03:34,  6.72s/it]

 44%|████▍     | 1712/3886 [3:28:12<4:10:03,  6.90s/it]

 44%|████▍     | 1713/3886 [3:28:18<3:52:59,  6.43s/it]

 44%|████▍     | 1714/3886 [3:28:24<3:56:38,  6.54s/it]

 44%|████▍     | 1715/3886 [3:28:32<4:11:09,  6.94s/it]

 44%|████▍     | 1716/3886 [3:28:38<4:01:25,  6.68s/it]

 44%|████▍     | 1717/3886 [3:28:45<4:00:27,  6.65s/it]

 44%|████▍     | 1718/3886 [3:28:55<4:34:23,  7.59s/it]

 44%|████▍     | 1719/3886 [3:29:02<4:26:32,  7.38s/it]

 44%|████▍     | 1720/3886 [3:29:07<4:02:59,  6.73s/it]

 44%|████▍     | 1721/3886 [3:29:13<3:59:23,  6.63s/it]

 44%|████▍     | 1722/3886 [3:29:20<3:59:18,  6.64s/it]

 44%|████▍     | 1723/3886 [3:29:26<3:57:26,  6.59s/it]

 44%|████▍     | 1724/3886 [3:29:34<4:09:46,  6.93s/it]

 44%|████▍     | 1725/3886 [3:29:42<4:25:58,  7.38s/it]

 44%|████▍     | 1726/3886 [3:29:50<4:27:43,  7.44s/it]

 44%|████▍     | 1727/3886 [3:29:56<4:16:16,  7.12s/it]

 44%|████▍     | 1728/3886 [3:30:06<4:44:49,  7.92s/it]

 44%|████▍     | 1729/3886 [3:30:13<4:30:01,  7.51s/it]

 45%|████▍     | 1730/3886 [3:30:27<5:46:53,  9.65s/it]

 45%|████▍     | 1731/3886 [3:30:33<5:03:24,  8.45s/it]

 45%|████▍     | 1732/3886 [3:30:40<4:46:34,  7.98s/it]

 45%|████▍     | 1733/3886 [3:30:46<4:24:04,  7.36s/it]

 45%|████▍     | 1734/3886 [3:30:52<4:10:00,  6.97s/it]

 45%|████▍     | 1735/3886 [3:30:58<3:55:54,  6.58s/it]

 45%|████▍     | 1736/3886 [3:31:03<3:45:59,  6.31s/it]

 45%|████▍     | 1737/3886 [3:31:12<4:09:23,  6.96s/it]

 45%|████▍     | 1738/3886 [3:31:19<4:09:30,  6.97s/it]

 45%|████▍     | 1739/3886 [3:31:24<3:52:56,  6.51s/it]

 45%|████▍     | 1740/3886 [3:31:31<3:50:49,  6.45s/it]

 45%|████▍     | 1741/3886 [3:31:37<3:50:18,  6.44s/it]

 45%|████▍     | 1742/3886 [3:31:48<4:36:42,  7.74s/it]

 45%|████▍     | 1743/3886 [3:32:02<5:46:59,  9.72s/it]

 45%|████▍     | 1744/3886 [3:32:08<5:10:46,  8.71s/it]

 45%|████▍     | 1745/3886 [3:32:13<4:25:51,  7.45s/it]

 45%|████▍     | 1746/3886 [3:32:19<4:15:33,  7.16s/it]

 45%|████▍     | 1747/3886 [3:32:25<4:01:10,  6.76s/it]

 45%|████▍     | 1748/3886 [3:32:31<3:52:30,  6.52s/it]

 45%|████▌     | 1749/3886 [3:32:37<3:45:30,  6.33s/it]

 45%|████▌     | 1750/3886 [3:32:43<3:37:52,  6.12s/it]

 45%|████▌     | 1751/3886 [3:32:50<3:46:16,  6.36s/it]

 45%|████▌     | 1752/3886 [3:32:58<4:08:06,  6.98s/it]

 45%|████▌     | 1753/3886 [3:33:03<3:51:30,  6.51s/it]

 45%|████▌     | 1754/3886 [3:33:12<4:16:49,  7.23s/it]

 45%|████▌     | 1755/3886 [3:33:18<4:01:24,  6.80s/it]

 45%|████▌     | 1756/3886 [3:33:26<4:08:00,  6.99s/it]

 45%|████▌     | 1757/3886 [3:33:30<3:44:30,  6.33s/it]

 45%|████▌     | 1758/3886 [3:33:36<3:41:08,  6.24s/it]

 45%|████▌     | 1759/3886 [3:33:43<3:47:39,  6.42s/it]

 45%|████▌     | 1760/3886 [3:33:51<3:59:33,  6.76s/it]

 45%|████▌     | 1761/3886 [3:34:01<4:37:07,  7.82s/it]

 45%|████▌     | 1762/3886 [3:34:08<4:30:12,  7.63s/it]

 45%|████▌     | 1763/3886 [3:34:15<4:24:52,  7.49s/it]

 45%|████▌     | 1764/3886 [3:34:26<4:57:01,  8.40s/it]

 45%|████▌     | 1765/3886 [3:34:33<4:41:01,  7.95s/it]

 45%|████▌     | 1766/3886 [3:34:40<4:34:07,  7.76s/it]

 45%|████▌     | 1767/3886 [3:34:48<4:33:45,  7.75s/it]

 45%|████▌     | 1768/3886 [3:34:56<4:39:39,  7.92s/it]

 46%|████▌     | 1769/3886 [3:35:10<5:45:29,  9.79s/it]

 46%|████▌     | 1770/3886 [3:35:16<5:06:10,  8.68s/it]

 46%|████▌     | 1771/3886 [3:35:23<4:40:40,  7.96s/it]

 46%|████▌     | 1772/3886 [3:35:29<4:26:02,  7.55s/it]

 46%|████▌     | 1773/3886 [3:35:35<4:08:47,  7.06s/it]
{'loss': 1.4385, 'grad_norm': 0.23683156735854116, 'learning_rate': 0.00011893132494177706, 'epoch': 0.46}


 46%|████▌     | 1775/3886 [3:35:50<4:23:29,  7.49s/it]

 46%|████▌     | 1776/3886 [3:35:57<4:08:50,  7.08s/it]

 46%|████▌     | 1777/3886 [3:36:02<3:50:55,  6.57s/it]

 46%|████▌     | 1778/3886 [3:36:12<4:23:27,  7.50s/it]

 46%|████▌     | 1779/3886 [3:36:21<4:42:08,  8.03s/it]

 46%|████▌     | 1780/3886 [3:36:26<4:14:53,  7.26s/it]

 46%|████▌     | 1781/3886 [3:36:34<4:14:09,  7.24s/it]

 46%|████▌     | 1782/3886 [3:36:43<4:34:20,  7.82s/it]

 46%|████▌     | 1783/3886 [3:36:48<4:03:30,  6.95s/it]

 46%|████▌     | 1784/3886 [3:36:53<3:41:47,  6.33s/it]

 46%|████▌     | 1785/3886 [3:36:59<3:37:51,  6.22s/it]

 46%|████▌     | 1786/3886 [3:37:08<4:11:37,  7.19s/it]

 46%|████▌     | 1787/3886 [3:37:14<3:55:25,  6.73s/it]

 46%|████▌     | 1788/3886 [3:37:20<3:46:39,  6.48s/it]

 46%|████▌     | 1789/3886 [3:37:26<3:50:04,  6.58s/it]

 46%|████▌     | 1790/3886 [3:37:32<3:38:37,  6.26s/it]
{'loss': 1.6045, 'grad_norm': 0.22580371170598593, 'learning_rate': 0.00011753808589336044, 'epoch': 0.46}


 46%|████▌     | 1792/3886 [3:37:44<3:33:13,  6.11s/it]

 46%|████▌     | 1793/3886 [3:37:50<3:33:48,  6.13s/it]

 46%|████▌     | 1794/3886 [3:37:58<3:52:49,  6.68s/it]

 46%|████▌     | 1795/3886 [3:38:05<3:52:48,  6.68s/it]

 46%|████▌     | 1796/3886 [3:38:14<4:19:44,  7.46s/it]

 46%|████▌     | 1797/3886 [3:38:21<4:11:01,  7.21s/it]

 46%|████▋     | 1798/3886 [3:38:27<3:59:48,  6.89s/it]

 46%|████▋     | 1799/3886 [3:38:32<3:42:06,  6.39s/it]

 46%|████▋     | 1800/3886 [3:38:38<3:37:37,  6.26s/it]
 46%|████▋     | 1800/3886 [3:38:38<3:37:37,  6.26s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 46%|████▋     | 1801/3886 [3:39:19<9:33:09, 16.49s/it]

 46%|████▋     | 1802/3886 [3:39:26<7:56:37, 13.72s/it]

 46%|████▋     | 1803/3886 [3:39:34<7:00:14, 12.10s/it]
{'loss': 1.4132, 'grad_norm': 0.2322707415662864, 'learning_rate': 0.00011647027690892378, 'epoch': 0.46}

 46%|████▋     | 1804/3886 [3:39:39<5:48:56, 10.06s/it]


 46%|████▋     | 1806/3886 [3:39:54<5:01:18,  8.69s/it]

 47%|████▋     | 1807/3886 [3:40:03<4:59:05,  8.63s/it]

 47%|████▋     | 1808/3886 [3:40:10<4:40:28,  8.10s/it]

 47%|████▋     | 1809/3886 [3:40:25<5:49:53, 10.11s/it]

 47%|████▋     | 1810/3886 [3:40:31<5:14:18,  9.08s/it]

 47%|████▋     | 1811/3886 [3:40:38<4:50:26,  8.40s/it]

 47%|████▋     | 1812/3886 [3:40:44<4:21:17,  7.56s/it]

 47%|████▋     | 1813/3886 [3:40:50<4:10:18,  7.24s/it]

 47%|████▋     | 1814/3886 [3:40:57<4:01:03,  6.98s/it]

 47%|████▋     | 1815/3886 [3:41:02<3:40:25,  6.39s/it]

 47%|████▋     | 1816/3886 [3:41:10<4:00:41,  6.98s/it]

 47%|████▋     | 1817/3886 [3:41:15<3:45:25,  6.54s/it]

 47%|████▋     | 1818/3886 [3:41:22<3:43:16,  6.48s/it]

 47%|████▋     | 1819/3886 [3:41:28<3:39:44,  6.38s/it]

 47%|████▋     | 1820/3886 [3:41:35<3:49:27,  6.66s/it]

 47%|████▋     | 1821/3886 [3:41:40<3:33:12,  6.20s/it]

 47%|████▋     | 1822/3886 [3:41:48<3:50:44,  6.71s/it]

 47%|████▋     | 1823/3886 [3:41:54<3:42:01,  6.46s/it]

 47%|████▋     | 1824/3886 [3:41:59<3:26:36,  6.01s/it]
{'loss': 1.1898, 'grad_norm': 0.2711205781475718, 'learning_rate': 0.00011474132393077464, 'epoch': 0.47}


 47%|████▋     | 1826/3886 [3:42:11<3:22:16,  5.89s/it]
{'loss': 1.4042, 'grad_norm': 0.23113763079831184, 'learning_rate': 0.00011457641783870791, 'epoch': 0.47}


 47%|████▋     | 1828/3886 [3:42:25<3:41:27,  6.46s/it]

 47%|████▋     | 1829/3886 [3:42:30<3:27:47,  6.06s/it]

 47%|████▋     | 1830/3886 [3:42:36<3:26:20,  6.02s/it]
{'loss': 1.2753, 'grad_norm': 0.2573870818979713, 'learning_rate': 0.00011424648458412068, 'epoch': 0.47}


 47%|████▋     | 1832/3886 [3:42:53<4:21:06,  7.63s/it]

 47%|████▋     | 1833/3886 [3:43:02<4:33:34,  8.00s/it]

 47%|████▋     | 1834/3886 [3:43:09<4:19:39,  7.59s/it]

 47%|████▋     | 1835/3886 [3:43:15<4:02:02,  7.08s/it]
{'loss': 1.3164, 'grad_norm': 0.2728841846378706, 'learning_rate': 0.0001138338457371034, 'epoch': 0.47}

 47%|████▋     | 1836/3886 [3:43:20<3:40:32,  6.45s/it]


 47%|████▋     | 1838/3886 [3:43:36<4:01:57,  7.09s/it]

 47%|████▋     | 1839/3886 [3:43:43<4:00:19,  7.04s/it]

 47%|████▋     | 1840/3886 [3:43:51<4:11:13,  7.37s/it]
{'loss': 1.0474, 'grad_norm': 0.24021521739595036, 'learning_rate': 0.00011342096660336061, 'epoch': 0.47}


 47%|████▋     | 1842/3886 [3:44:03<3:42:01,  6.52s/it]

 47%|████▋     | 1843/3886 [3:44:09<3:37:00,  6.37s/it]

 47%|████▋     | 1844/3886 [3:44:17<3:54:51,  6.90s/it]

 47%|████▋     | 1845/3886 [3:44:22<3:34:44,  6.31s/it]

 48%|████▊     | 1846/3886 [3:44:29<3:39:35,  6.46s/it]

 48%|████▊     | 1847/3886 [3:44:35<3:35:36,  6.34s/it]

 48%|████▊     | 1848/3886 [3:44:41<3:35:23,  6.34s/it]

 48%|████▊     | 1849/3886 [3:44:48<3:44:22,  6.61s/it]

 48%|████▊     | 1850/3886 [3:44:55<3:44:46,  6.62s/it]

 48%|████▊     | 1851/3886 [3:45:03<3:57:34,  7.00s/it]

 48%|████▊     | 1852/3886 [3:45:11<4:07:40,  7.31s/it]

 48%|████▊     | 1853/3886 [3:45:21<4:35:04,  8.12s/it]

 48%|████▊     | 1854/3886 [3:45:28<4:22:50,  7.76s/it]

 48%|████▊     | 1855/3886 [3:45:34<4:07:22,  7.31s/it]

 48%|████▊     | 1856/3886 [3:45:39<3:39:52,  6.50s/it]

 48%|████▊     | 1857/3886 [3:45:45<3:40:12,  6.51s/it]

 48%|████▊     | 1858/3886 [3:45:55<4:08:42,  7.36s/it]

 48%|████▊     | 1859/3886 [3:46:01<3:59:08,  7.08s/it]

 48%|████▊     | 1860/3886 [3:46:07<3:51:49,  6.87s/it]

 48%|████▊     | 1861/3886 [3:46:13<3:41:25,  6.56s/it]
{'loss': 1.3502, 'grad_norm': 0.2409134280450076, 'learning_rate': 0.00011168441222904373, 'epoch': 0.48}

 48%|████▊     | 1862/3886 [3:46:20<3:40:41,  6.54s/it]


 48%|████▊     | 1864/3886 [3:46:33<3:41:02,  6.56s/it]

 48%|████▊     | 1865/3886 [3:46:39<3:37:32,  6.46s/it]

 48%|████▊     | 1866/3886 [3:46:45<3:29:12,  6.21s/it]

 48%|████▊     | 1867/3886 [3:46:53<3:54:31,  6.97s/it]

 48%|████▊     | 1868/3886 [3:46:59<3:42:50,  6.63s/it]

 48%|████▊     | 1869/3886 [3:47:05<3:34:19,  6.38s/it]

 48%|████▊     | 1870/3886 [3:47:11<3:35:22,  6.41s/it]

 48%|████▊     | 1871/3886 [3:47:17<3:27:22,  6.17s/it]

 48%|████▊     | 1872/3886 [3:47:23<3:27:06,  6.17s/it]
{'loss': 1.3134, 'grad_norm': 0.23125907454569145, 'learning_rate': 0.00011077332600926148, 'epoch': 0.48}


 48%|████▊     | 1874/3886 [3:47:38<3:52:34,  6.94s/it]

 48%|████▊     | 1875/3886 [3:47:47<4:14:15,  7.59s/it]

 48%|████▊     | 1876/3886 [3:47:55<4:17:52,  7.70s/it]

 48%|████▊     | 1877/3886 [3:48:03<4:12:22,  7.54s/it]

 48%|████▊     | 1878/3886 [3:48:09<3:59:15,  7.15s/it]

 48%|████▊     | 1879/3886 [3:48:15<3:48:26,  6.83s/it]

 48%|████▊     | 1880/3886 [3:48:26<4:26:44,  7.98s/it]
{'loss': 1.27, 'grad_norm': 0.2422415512501606, 'learning_rate': 0.0001101101446097583, 'epoch': 0.48}


 48%|████▊     | 1882/3886 [3:48:38<4:01:05,  7.22s/it]

 48%|████▊     | 1883/3886 [3:48:45<3:58:50,  7.15s/it]

 48%|████▊     | 1884/3886 [3:48:55<4:24:33,  7.93s/it]

 49%|████▊     | 1885/3886 [3:49:01<4:04:08,  7.32s/it]

 49%|████▊     | 1886/3886 [3:49:07<3:49:38,  6.89s/it]

 49%|████▊     | 1887/3886 [3:49:12<3:35:34,  6.47s/it]

 49%|████▊     | 1888/3886 [3:49:19<3:35:03,  6.46s/it]

 49%|████▊     | 1889/3886 [3:49:27<3:47:06,  6.82s/it]

 49%|████▊     | 1890/3886 [3:49:33<3:37:56,  6.55s/it]

 49%|████▊     | 1891/3886 [3:49:39<3:32:52,  6.40s/it]

 49%|████▊     | 1892/3886 [3:49:45<3:31:14,  6.36s/it]

 49%|████▊     | 1893/3886 [3:49:55<4:11:40,  7.58s/it]

 49%|████▊     | 1894/3886 [3:50:00<3:47:30,  6.85s/it]
{'loss': 1.4149, 'grad_norm': 0.24753080191770077, 'learning_rate': 0.00010894851316869692, 'epoch': 0.49}

 49%|████▉     | 1895/3886 [3:50:08<3:56:25,  7.12s/it]


 49%|████▉     | 1897/3886 [3:50:27<4:25:25,  8.01s/it]

 49%|████▉     | 1898/3886 [3:50:34<4:09:14,  7.52s/it]
{'loss': 1.48, 'grad_norm': 0.25645542125956305, 'learning_rate': 0.00010861638773071046, 'epoch': 0.49}


 49%|████▉     | 1900/3886 [3:50:47<3:54:06,  7.07s/it]

 49%|████▉     | 1901/3886 [3:50:55<4:01:11,  7.29s/it]

 49%|████▉     | 1902/3886 [3:51:03<4:10:58,  7.59s/it]

 49%|████▉     | 1903/3886 [3:51:10<4:02:40,  7.34s/it]

 49%|████▉     | 1904/3886 [3:51:15<3:45:01,  6.81s/it]

 49%|████▉     | 1905/3886 [3:51:23<3:48:40,  6.93s/it]

 49%|████▉     | 1906/3886 [3:51:29<3:48:49,  6.93s/it]

 49%|████▉     | 1907/3886 [3:51:38<4:01:54,  7.33s/it]
{'loss': 1.1178, 'grad_norm': 0.2660867187870753, 'learning_rate': 0.0001078687609118919, 'epoch': 0.49}


 49%|████▉     | 1909/3886 [3:51:53<4:11:45,  7.64s/it]

 49%|████▉     | 1910/3886 [3:51:59<3:56:02,  7.17s/it]

 49%|████▉     | 1911/3886 [3:52:05<3:37:26,  6.61s/it]

 49%|████▉     | 1912/3886 [3:52:19<4:51:29,  8.86s/it]
{'loss': 1.4517, 'grad_norm': 0.22913081854515444, 'learning_rate': 0.00010745321863861285, 'epoch': 0.49}


 49%|████▉     | 1914/3886 [3:52:33<4:20:40,  7.93s/it]

 49%|████▉     | 1915/3886 [3:52:38<3:50:15,  7.01s/it]

 49%|████▉     | 1916/3886 [3:52:49<4:32:12,  8.29s/it]
{'loss': 1.4095, 'grad_norm': 0.24056608332858523, 'learning_rate': 0.00010712069126331053, 'epoch': 0.49}


 49%|████▉     | 1918/3886 [3:53:09<5:04:53,  9.30s/it]

 49%|████▉     | 1919/3886 [3:53:16<4:36:45,  8.44s/it]
{'loss': 1.2855, 'grad_norm': 0.1997180775488623, 'learning_rate': 0.00010687124358287925, 'epoch': 0.49}


 49%|████▉     | 1921/3886 [3:53:28<3:52:58,  7.11s/it]

 49%|████▉     | 1922/3886 [3:53:37<4:12:23,  7.71s/it]

 49%|████▉     | 1923/3886 [3:53:43<3:59:11,  7.31s/it]

 50%|████▉     | 1924/3886 [3:53:49<3:44:17,  6.86s/it]

 50%|████▉     | 1925/3886 [3:53:56<3:43:19,  6.83s/it]

 50%|████▉     | 1926/3886 [3:54:02<3:35:01,  6.58s/it]

 50%|████▉     | 1927/3886 [3:54:08<3:28:41,  6.39s/it]

 50%|████▉     | 1928/3886 [3:54:17<3:56:51,  7.26s/it]

 50%|████▉     | 1929/3886 [3:54:24<3:53:31,  7.16s/it]

 50%|████▉     | 1930/3886 [3:54:31<3:49:07,  7.03s/it]

 50%|████▉     | 1931/3886 [3:54:37<3:42:18,  6.82s/it]

 50%|████▉     | 1932/3886 [3:54:45<3:53:45,  7.18s/it]
{'loss': 1.4842, 'grad_norm': 0.20007624877431776, 'learning_rate': 0.00010578982717059013, 'epoch': 0.5}


 50%|████▉     | 1934/3886 [3:54:58<3:40:40,  6.78s/it]

 50%|████▉     | 1935/3886 [3:55:04<3:33:26,  6.56s/it]

 50%|████▉     | 1936/3886 [3:55:09<3:22:24,  6.23s/it]

 50%|████▉     | 1937/3886 [3:55:22<4:23:35,  8.11s/it]

 50%|████▉     | 1938/3886 [3:55:27<4:02:24,  7.47s/it]

 50%|████▉     | 1939/3886 [3:55:35<3:59:26,  7.38s/it]

 50%|████▉     | 1940/3886 [3:55:42<3:56:13,  7.28s/it]

 50%|████▉     | 1941/3886 [3:55:53<4:35:16,  8.49s/it]

 50%|████▉     | 1942/3886 [3:55:59<4:13:50,  7.83s/it]

 50%|█████     | 1943/3886 [3:56:05<3:50:52,  7.13s/it]

 50%|█████     | 1944/3886 [3:56:11<3:42:57,  6.89s/it]
{'loss': 1.3016, 'grad_norm': 0.22658132830945812, 'learning_rate': 0.000104790990352282, 'epoch': 0.5}


 50%|█████     | 1946/3886 [3:56:25<3:41:33,  6.85s/it]

 50%|█████     | 1947/3886 [3:56:31<3:34:19,  6.63s/it]
{'loss': 1.1958, 'grad_norm': 0.2187165075005467, 'learning_rate': 0.00010454120234821405, 'epoch': 0.5}


 50%|█████     | 1949/3886 [3:56:43<3:22:41,  6.28s/it]

 50%|█████     | 1950/3886 [3:56:51<3:42:22,  6.89s/it]

 50%|█████     | 1951/3886 [3:56:57<3:31:14,  6.55s/it]
{'loss': 1.4132, 'grad_norm': 0.24819851262228063, 'learning_rate': 0.00010420810777420684, 'epoch': 0.5}

 50%|█████     | 1952/3886 [3:57:03<3:25:07,  6.36s/it]

 50%|█████     | 1953/3886 [3:57:11<3:40:17,  6.84s/it]


 50%|█████     | 1955/3886 [3:57:22<3:15:58,  6.09s/it]

 50%|█████     | 1956/3886 [3:57:28<3:19:40,  6.21s/it]

 50%|█████     | 1957/3886 [3:57:36<3:33:55,  6.65s/it]

 50%|█████     | 1958/3886 [3:57:41<3:23:38,  6.34s/it]

 50%|█████     | 1959/3886 [3:57:47<3:17:33,  6.15s/it]
{'loss': 1.1454, 'grad_norm': 0.27350220151324717, 'learning_rate': 0.00010354178199158943, 'epoch': 0.5}


 50%|█████     | 1961/3886 [3:58:02<3:32:08,  6.61s/it]
{'loss': 1.2836, 'grad_norm': 0.2738718868327355, 'learning_rate': 0.00010337517478088945, 'epoch': 0.5}


 51%|█████     | 1963/3886 [3:58:18<3:56:53,  7.39s/it]

 51%|█████     | 1964/3886 [3:58:24<3:47:32,  7.10s/it]

 51%|█████     | 1965/3886 [3:58:29<3:28:59,  6.53s/it]

 51%|█████     | 1966/3886 [3:58:36<3:28:15,  6.51s/it]

 51%|█████     | 1967/3886 [3:58:41<3:17:19,  6.17s/it]
{'loss': 1.1936, 'grad_norm': 0.26547131976592314, 'learning_rate': 0.00010287529872090212, 'epoch': 0.51}


 51%|█████     | 1969/3886 [3:58:54<3:20:44,  6.28s/it]

 51%|█████     | 1970/3886 [3:58:59<3:13:37,  6.06s/it]

 51%|█████     | 1971/3886 [3:59:05<3:08:57,  5.92s/it]
{'loss': 1.3913, 'grad_norm': 0.24217335198899237, 'learning_rate': 0.00010254200728844876, 'epoch': 0.51}

 51%|█████     | 1972/3886 [3:59:15<3:46:54,  7.11s/it]


 51%|█████     | 1974/3886 [3:59:31<4:10:11,  7.85s/it]

 51%|█████     | 1975/3886 [3:59:43<4:50:38,  9.13s/it]

 51%|█████     | 1976/3886 [3:59:50<4:29:12,  8.46s/it]

 51%|█████     | 1977/3886 [3:59:58<4:20:08,  8.18s/it]
{'loss': 1.3872, 'grad_norm': 0.25521451453327765, 'learning_rate': 0.00010204201831363083, 'epoch': 0.51}


 51%|█████     | 1979/3886 [4:00:10<3:45:14,  7.09s/it]
{'loss': 1.4372, 'grad_norm': 0.23061574497850937, 'learning_rate': 0.00010187534335440904, 'epoch': 0.51}


 51%|█████     | 1981/3886 [4:00:24<3:39:57,  6.93s/it]

 51%|█████     | 1982/3886 [4:00:31<3:48:07,  7.19s/it]

 51%|█████     | 1983/3886 [4:00:40<3:59:03,  7.54s/it]

 51%|█████     | 1984/3886 [4:00:48<4:09:54,  7.88s/it]

 51%|█████     | 1985/3886 [4:00:57<4:17:57,  8.14s/it]

 51%|█████     | 1986/3886 [4:01:03<3:58:21,  7.53s/it]

 51%|█████     | 1987/3886 [4:01:12<4:12:07,  7.97s/it]

 51%|█████     | 1988/3886 [4:01:17<3:43:29,  7.07s/it]
{'loss': 1.2621, 'grad_norm': 0.25123678892990203, 'learning_rate': 0.00010112524822974055, 'epoch': 0.51}


 51%|█████     | 1990/3886 [4:01:30<3:38:10,  6.90s/it]

 51%|█████     | 1991/3886 [4:01:45<4:54:58,  9.34s/it]
{'loss': 1.2666, 'grad_norm': 0.2218422534309125, 'learning_rate': 0.0001008752003644582, 'epoch': 0.51}


 51%|█████▏    | 1993/3886 [4:02:08<5:35:56, 10.65s/it]

 51%|█████▏    | 1994/3886 [4:02:14<4:57:41,  9.44s/it]

 51%|█████▏    | 1995/3886 [4:02:20<4:23:38,  8.37s/it]

 51%|█████▏    | 1996/3886 [4:02:29<4:27:23,  8.49s/it]

 51%|█████▏    | 1997/3886 [4:02:36<4:13:26,  8.05s/it]

 51%|█████▏    | 1998/3886 [4:02:45<4:17:52,  8.20s/it]
{'loss': 1.1974, 'grad_norm': 0.23410960059000305, 'learning_rate': 0.00010029173676546146, 'epoch': 0.51}


 51%|█████▏    | 2000/3886 [4:02:59<3:54:53,  7.47s/it]

 51%|█████▏    | 2001/3886 [4:03:04<3:34:37,  6.83s/it]

 52%|█████▏    | 2002/3886 [4:03:18<4:44:37,  9.06s/it]

 52%|█████▏    | 2003/3886 [4:03:24<4:12:11,  8.04s/it]

 52%|█████▏    | 2004/3886 [4:03:31<3:59:53,  7.65s/it]

 52%|█████▏    | 2005/3886 [4:03:44<4:50:50,  9.28s/it]

 52%|█████▏    | 2006/3886 [4:03:50<4:18:46,  8.26s/it]

 52%|█████▏    | 2007/3886 [4:03:54<3:44:11,  7.16s/it]

 52%|█████▏    | 2008/3886 [4:04:04<4:09:20,  7.97s/it]

 52%|█████▏    | 2009/3886 [4:04:12<4:09:14,  7.97s/it]
{'loss': 1.3156, 'grad_norm': 0.20471882022194024, 'learning_rate': 9.93748529734687e-05, 'epoch': 0.52}


 52%|█████▏    | 2011/3886 [4:04:26<3:52:25,  7.44s/it]

 52%|█████▏    | 2012/3886 [4:04:31<3:31:54,  6.78s/it]

 52%|█████▏    | 2013/3886 [4:04:39<3:38:42,  7.01s/it]

 52%|█████▏    | 2014/3886 [4:04:46<3:37:46,  6.98s/it]

 52%|█████▏    | 2015/3886 [4:04:52<3:33:18,  6.84s/it]

 52%|█████▏    | 2016/3886 [4:04:58<3:28:06,  6.68s/it]

 52%|█████▏    | 2017/3886 [4:05:05<3:24:05,  6.55s/it]

 52%|█████▏    | 2018/3886 [4:05:11<3:17:41,  6.35s/it]

 52%|█████▏    | 2019/3886 [4:05:16<3:11:08,  6.14s/it]

 52%|█████▏    | 2020/3886 [4:05:30<4:26:50,  8.58s/it]

 52%|█████▏    | 2021/3886 [4:05:38<4:12:50,  8.13s/it]

 52%|█████▏    | 2022/3886 [4:05:45<4:03:52,  7.85s/it]

 52%|█████▏    | 2023/3886 [4:05:51<3:52:26,  7.49s/it]

 52%|█████▏    | 2024/3886 [4:05:57<3:37:16,  7.00s/it]

 52%|█████▏    | 2025/3886 [4:06:03<3:30:15,  6.78s/it]

 52%|█████▏    | 2026/3886 [4:06:09<3:21:56,  6.51s/it]

 52%|█████▏    | 2027/3886 [4:06:15<3:17:14,  6.37s/it]

 52%|█████▏    | 2028/3886 [4:06:24<3:38:18,  7.05s/it]

 52%|█████▏    | 2029/3886 [4:06:29<3:20:48,  6.49s/it]

 52%|█████▏    | 2030/3886 [4:06:39<3:51:24,  7.48s/it]

 52%|█████▏    | 2031/3886 [4:06:44<3:28:44,  6.75s/it]

 52%|█████▏    | 2032/3886 [4:06:52<3:41:03,  7.15s/it]

 52%|█████▏    | 2033/3886 [4:07:04<4:22:01,  8.48s/it]

 52%|█████▏    | 2034/3886 [4:07:10<3:59:08,  7.75s/it]

 52%|█████▏    | 2035/3886 [4:07:16<3:43:28,  7.24s/it]

 52%|█████▏    | 2036/3886 [4:07:22<3:29:13,  6.79s/it]
{'loss': 1.4342, 'grad_norm': 0.23497091736558626, 'learning_rate': 9.712470127909794e-05, 'epoch': 0.52}


 52%|█████▏    | 2038/3886 [4:07:36<3:31:09,  6.86s/it]

 52%|█████▏    | 2039/3886 [4:07:44<3:46:18,  7.35s/it]
{'loss': 1.2526, 'grad_norm': 0.2268616967833469, 'learning_rate': 9.687475347799118e-05, 'epoch': 0.52}


 53%|█████▎    | 2041/3886 [4:07:58<3:36:11,  7.03s/it]

 53%|█████▎    | 2042/3886 [4:08:05<3:38:52,  7.12s/it]

 53%|█████▎    | 2043/3886 [4:08:11<3:26:57,  6.74s/it]

 53%|█████▎    | 2044/3886 [4:08:16<3:08:18,  6.13s/it]

 53%|█████▎    | 2045/3886 [4:08:22<3:13:23,  6.30s/it]

 53%|█████▎    | 2046/3886 [4:08:30<3:22:24,  6.60s/it]

 53%|█████▎    | 2047/3886 [4:08:36<3:21:08,  6.56s/it]

 53%|█████▎    | 2048/3886 [4:08:42<3:16:03,  6.40s/it]
{'loss': 1.2114, 'grad_norm': 0.24282982956253424, 'learning_rate': 9.612503357912042e-05, 'epoch': 0.53}


 53%|█████▎    | 2050/3886 [4:08:56<3:20:47,  6.56s/it]

 53%|█████▎    | 2051/3886 [4:09:03<3:23:42,  6.66s/it]

 53%|█████▎    | 2052/3886 [4:09:08<3:13:53,  6.34s/it]

 53%|█████▎    | 2053/3886 [4:09:14<3:12:52,  6.31s/it]

 53%|█████▎    | 2054/3886 [4:09:20<3:06:25,  6.11s/it]
{'loss': 1.1472, 'grad_norm': 0.2420888125735209, 'learning_rate': 9.562533885993459e-05, 'epoch': 0.53}


 53%|█████▎    | 2056/3886 [4:09:33<3:09:35,  6.22s/it]

 53%|█████▎    | 2057/3886 [4:09:42<3:37:38,  7.14s/it]

 53%|█████▎    | 2058/3886 [4:09:49<3:31:25,  6.94s/it]

 53%|█████▎    | 2059/3886 [4:09:59<4:03:45,  8.01s/it]

 53%|█████▎    | 2060/3886 [4:10:07<3:58:39,  7.84s/it]

 53%|█████▎    | 2061/3886 [4:10:16<4:09:59,  8.22s/it]

 53%|█████▎    | 2062/3886 [4:10:22<3:53:10,  7.67s/it]

 53%|█████▎    | 2063/3886 [4:10:31<4:05:38,  8.08s/it]

 53%|█████▎    | 2064/3886 [4:10:46<5:03:28,  9.99s/it]

 53%|█████▎    | 2065/3886 [4:10:53<4:37:37,  9.15s/it]
{'loss': 1.4344, 'grad_norm': 0.22336298549915232, 'learning_rate': 9.47095250757861e-05, 'epoch': 0.53}


 53%|█████▎    | 2067/3886 [4:11:06<3:56:42,  7.81s/it]

 53%|█████▎    | 2068/3886 [4:11:13<3:42:32,  7.34s/it]
{'loss': 1.4333, 'grad_norm': 0.22163767501205142, 'learning_rate': 9.445983163120875e-05, 'epoch': 0.53}


 53%|█████▎    | 2070/3886 [4:11:26<3:30:01,  6.94s/it]

 53%|█████▎    | 2071/3886 [4:11:32<3:15:20,  6.46s/it]

 53%|█████▎    | 2072/3886 [4:11:38<3:12:56,  6.38s/it]

 53%|█████▎    | 2073/3886 [4:11:44<3:07:40,  6.21s/it]
{'loss': 1.2479, 'grad_norm': 0.2421787791680259, 'learning_rate': 9.404375364513196e-05, 'epoch': 0.53}


 53%|█████▎    | 2075/3886 [4:11:56<3:05:57,  6.16s/it]
{'loss': 1.2244, 'grad_norm': 0.25223381075215207, 'learning_rate': 9.387735101397975e-05, 'epoch': 0.53}

 53%|█████▎    | 2076/3886 [4:12:01<2:59:43,  5.96s/it]


 53%|█████▎    | 2078/3886 [4:12:16<3:21:45,  6.70s/it]
{'loss': 1.1793, 'grad_norm': 0.22982952027684425, 'learning_rate': 9.362777911596512e-05, 'epoch': 0.53}

 53%|█████▎    | 2079/3886 [4:12:24<3:26:21,  6.85s/it]


 54%|█████▎    | 2081/3886 [4:12:37<3:20:59,  6.68s/it]

 54%|█████▎    | 2082/3886 [4:12:42<3:10:54,  6.35s/it]

 54%|█████▎    | 2083/3886 [4:12:50<3:25:39,  6.84s/it]
{'loss': 1.4419, 'grad_norm': 0.2159829717938145, 'learning_rate': 9.321191526897004e-05, 'epoch': 0.54}


 54%|█████▎    | 2085/3886 [4:13:03<3:21:33,  6.71s/it]

 54%|█████▎    | 2086/3886 [4:13:09<3:15:34,  6.52s/it]

 54%|█████▎    | 2087/3886 [4:13:15<3:10:11,  6.34s/it]

 54%|█████▎    | 2088/3886 [4:13:21<3:06:33,  6.23s/it]

 54%|█████▍    | 2089/3886 [4:13:28<3:17:14,  6.59s/it]
{'loss': 1.2728, 'grad_norm': 0.2255514699512207, 'learning_rate': 9.271303492336172e-05, 'epoch': 0.54}


 54%|█████▍    | 2091/3886 [4:13:40<3:05:02,  6.19s/it]

 54%|█████▍    | 2092/3886 [4:13:45<2:55:46,  5.88s/it]
{'loss': 1.3829, 'grad_norm': 0.2659443822084459, 'learning_rate': 9.246366231904672e-05, 'epoch': 0.54}


 54%|█████▍    | 2094/3886 [4:13:58<3:07:16,  6.27s/it]

 54%|█████▍    | 2095/3886 [4:14:07<3:29:36,  7.02s/it]

 54%|█████▍    | 2096/3886 [4:14:13<3:23:33,  6.82s/it]

 54%|█████▍    | 2097/3886 [4:14:20<3:20:24,  6.72s/it]

 54%|█████▍    | 2098/3886 [4:14:25<3:09:18,  6.35s/it]

 54%|█████▍    | 2099/3886 [4:14:31<3:00:48,  6.07s/it]

 54%|█████▍    | 2100/3886 [4:14:39<3:15:56,  6.58s/it]
 54%|█████▍    | 2100/3886 [4:14:39<3:15:56,  6.58s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.3351, 'grad_norm': 0.24019404202662453, 'learning_rate': 9.171583349112491e-05, 'epoch': 0.54}

 54%|█████▍    | 2102/3886 [4:15:27<7:01:55, 14.19s/it]

 54%|█████▍    | 2103/3886 [4:15:35<6:05:16, 12.29s/it]

 54%|█████▍    | 2104/3886 [4:15:41<5:01:47, 10.16s/it]

 54%|█████▍    | 2105/3886 [4:15:48<4:33:44,  9.22s/it]
{'loss': 1.1795, 'grad_norm': 0.246462200963748, 'learning_rate': 9.138361226928956e-05, 'epoch': 0.54}


 54%|█████▍    | 2107/3886 [4:16:01<4:00:28,  8.11s/it]

 54%|█████▍    | 2108/3886 [4:16:07<3:39:07,  7.39s/it]

 54%|█████▍    | 2109/3886 [4:16:15<3:44:07,  7.57s/it]
{'loss': 1.2298, 'grad_norm': 0.2639975196742801, 'learning_rate': 9.105148683130309e-05, 'epoch': 0.54}


 54%|█████▍    | 2111/3886 [4:16:35<4:15:53,  8.65s/it]

 54%|█████▍    | 2112/3886 [4:16:44<4:19:22,  8.77s/it]

 54%|█████▍    | 2113/3886 [4:16:51<4:02:35,  8.21s/it]
{'loss': 1.4134, 'grad_norm': 0.2417114875277293, 'learning_rate': 9.071946086922932e-05, 'epoch': 0.54}


 54%|█████▍    | 2115/3886 [4:17:20<5:41:36, 11.57s/it]

 54%|█████▍    | 2116/3886 [4:17:27<5:05:45, 10.36s/it]

 54%|█████▍    | 2117/3886 [4:17:33<4:29:08,  9.13s/it]

 55%|█████▍    | 2118/3886 [4:17:42<4:24:52,  8.99s/it]

 55%|█████▍    | 2119/3886 [4:17:48<3:53:59,  7.95s/it]

 55%|█████▍    | 2120/3886 [4:17:53<3:31:31,  7.19s/it]

 55%|█████▍    | 2121/3886 [4:17:59<3:20:44,  6.82s/it]

 55%|█████▍    | 2122/3886 [4:18:05<3:12:19,  6.54s/it]

 55%|█████▍    | 2123/3886 [4:18:22<4:48:30,  9.82s/it]
[2024-05-28 04:52:52,534] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.4209, 'grad_norm': 0.23636446801343416, 'learning_rate': 8.98898553902417e-05, 'epoch': 0.55}


 55%|█████▍    | 2125/3886 [4:18:35<3:57:30,  8.09s/it]

 55%|█████▍    | 2126/3886 [4:18:41<3:40:52,  7.53s/it]

 55%|█████▍    | 2127/3886 [4:18:48<3:31:26,  7.21s/it]

 55%|█████▍    | 2128/3886 [4:19:03<4:44:56,  9.73s/it]
{'loss': 1.3292, 'grad_norm': 0.25250913253652624, 'learning_rate': 8.947531246097725e-05, 'epoch': 0.55}


 55%|█████▍    | 2130/3886 [4:19:16<3:49:29,  7.84s/it]

 55%|█████▍    | 2131/3886 [4:19:24<3:51:53,  7.93s/it]

 55%|█████▍    | 2132/3886 [4:19:29<3:25:16,  7.02s/it]

 55%|█████▍    | 2133/3886 [4:19:34<3:13:51,  6.64s/it]

 55%|█████▍    | 2134/3886 [4:19:40<3:04:17,  6.31s/it]

 55%|█████▍    | 2135/3886 [4:19:47<3:09:48,  6.50s/it]
{'loss': 1.424, 'grad_norm': 0.21827876909445385, 'learning_rate': 8.889526109034521e-05, 'epoch': 0.55}


 55%|█████▍    | 2137/3886 [4:19:58<2:53:17,  5.94s/it]

 55%|█████▌    | 2138/3886 [4:20:05<3:07:33,  6.44s/it]

 55%|█████▌    | 2139/3886 [4:20:12<3:08:34,  6.48s/it]

 55%|█████▌    | 2140/3886 [4:20:19<3:12:34,  6.62s/it]

 55%|█████▌    | 2141/3886 [4:20:24<2:59:13,  6.16s/it]
{'loss': 1.5187, 'grad_norm': 0.2395122653507362, 'learning_rate': 8.839837435106274e-05, 'epoch': 0.55}


 55%|█████▌    | 2143/3886 [4:20:38<3:07:28,  6.45s/it]

 55%|█████▌    | 2144/3886 [4:20:44<3:04:35,  6.36s/it]

 55%|█████▌    | 2145/3886 [4:20:51<3:14:54,  6.72s/it]

 55%|█████▌    | 2146/3886 [4:20:59<3:18:24,  6.84s/it]
{'loss': 1.1194, 'grad_norm': 0.25492420366576135, 'learning_rate': 8.798452320661527e-05, 'epoch': 0.55}


 55%|█████▌    | 2148/3886 [4:21:12<3:17:15,  6.81s/it]

 55%|█████▌    | 2149/3886 [4:21:17<3:07:36,  6.48s/it]

 55%|█████▌    | 2150/3886 [4:21:23<2:56:42,  6.11s/it]

 55%|█████▌    | 2151/3886 [4:21:29<2:58:45,  6.18s/it]

 55%|█████▌    | 2152/3886 [4:21:34<2:51:36,  5.94s/it]

 55%|█████▌    | 2153/3886 [4:21:40<2:52:17,  5.97s/it]

 55%|█████▌    | 2154/3886 [4:21:48<3:06:06,  6.45s/it]

 55%|█████▌    | 2155/3886 [4:21:55<3:10:53,  6.62s/it]

 55%|█████▌    | 2156/3886 [4:22:02<3:11:31,  6.64s/it]

 56%|█████▌    | 2157/3886 [4:22:08<3:04:19,  6.40s/it]

 56%|█████▌    | 2158/3886 [4:22:15<3:14:30,  6.75s/it]

 56%|█████▌    | 2159/3886 [4:22:21<3:09:07,  6.57s/it]

 56%|█████▌    | 2160/3886 [4:22:28<3:08:34,  6.56s/it]

 56%|█████▌    | 2161/3886 [4:22:34<3:03:07,  6.37s/it]

 56%|█████▌    | 2162/3886 [4:22:40<3:02:46,  6.36s/it]

 56%|█████▌    | 2163/3886 [4:22:47<3:04:30,  6.42s/it]

 56%|█████▌    | 2164/3886 [4:22:52<2:54:06,  6.07s/it]

 56%|█████▌    | 2165/3886 [4:22:58<2:53:25,  6.05s/it]

 56%|█████▌    | 2166/3886 [4:23:04<2:54:03,  6.07s/it]

 56%|█████▌    | 2167/3886 [4:23:12<3:10:31,  6.65s/it]

 56%|█████▌    | 2168/3886 [4:23:17<2:54:02,  6.08s/it]

 56%|█████▌    | 2169/3886 [4:23:22<2:45:28,  5.78s/it]

 56%|█████▌    | 2170/3886 [4:23:27<2:43:54,  5.73s/it]

 56%|█████▌    | 2171/3886 [4:23:33<2:44:39,  5.76s/it]

 56%|█████▌    | 2172/3886 [4:23:40<2:48:57,  5.91s/it]

 56%|█████▌    | 2173/3886 [4:23:46<2:50:43,  5.98s/it]

 56%|█████▌    | 2174/3886 [4:23:55<3:18:59,  6.97s/it]

 56%|█████▌    | 2175/3886 [4:24:02<3:15:52,  6.87s/it]

 56%|█████▌    | 2176/3886 [4:24:08<3:12:14,  6.75s/it]

 56%|█████▌    | 2177/3886 [4:24:24<4:31:42,  9.54s/it]
{'loss': 1.3882, 'grad_norm': 0.21061703697178558, 'learning_rate': 8.54235821612921e-05, 'epoch': 0.56}


 56%|█████▌    | 2179/3886 [4:24:37<3:45:21,  7.92s/it]

 56%|█████▌    | 2180/3886 [4:24:43<3:29:24,  7.36s/it]

 56%|█████▌    | 2181/3886 [4:24:50<3:24:06,  7.18s/it]
{'loss': 1.2588, 'grad_norm': 0.2202824972092386, 'learning_rate': 8.509381094507266e-05, 'epoch': 0.56}


 56%|█████▌    | 2183/3886 [4:25:02<3:08:46,  6.65s/it]

 56%|█████▌    | 2184/3886 [4:25:11<3:30:44,  7.43s/it]

 56%|█████▌    | 2185/3886 [4:25:21<3:46:48,  8.00s/it]

 56%|█████▋    | 2186/3886 [4:25:26<3:24:47,  7.23s/it]

 56%|█████▋    | 2187/3886 [4:25:33<3:23:28,  7.19s/it]

 56%|█████▋    | 2188/3886 [4:25:40<3:17:58,  7.00s/it]

 56%|█████▋    | 2189/3886 [4:25:46<3:10:44,  6.74s/it]

 56%|█████▋    | 2190/3886 [4:25:54<3:24:25,  7.23s/it]
{'loss': 1.3175, 'grad_norm': 0.25360049216563396, 'learning_rate': 8.435243714655682e-05, 'epoch': 0.56}


 56%|█████▋    | 2192/3886 [4:26:18<4:39:57,  9.92s/it]

 56%|█████▋    | 2193/3886 [4:26:24<4:06:22,  8.73s/it]
{'loss': 1.1079, 'grad_norm': 0.23487623231217342, 'learning_rate': 8.410550617630102e-05, 'epoch': 0.56}


 56%|█████▋    | 2195/3886 [4:26:38<3:46:18,  8.03s/it]

 57%|█████▋    | 2196/3886 [4:26:45<3:34:09,  7.60s/it]

 57%|█████▋    | 2197/3886 [4:26:52<3:24:59,  7.28s/it]

 57%|█████▋    | 2198/3886 [4:26:57<3:10:52,  6.78s/it]

 57%|█████▋    | 2199/3886 [4:27:03<3:05:38,  6.60s/it]

 57%|█████▋    | 2200/3886 [4:27:10<3:02:51,  6.51s/it]

 57%|█████▋    | 2201/3886 [4:27:15<2:57:17,  6.31s/it]

 57%|█████▋    | 2202/3886 [4:27:22<2:55:43,  6.26s/it]

 57%|█████▋    | 2203/3886 [4:27:27<2:51:55,  6.13s/it]

 57%|█████▋    | 2204/3886 [4:27:32<2:41:16,  5.75s/it]
{'loss': 1.1701, 'grad_norm': 0.2501585494522503, 'learning_rate': 8.320095467893676e-05, 'epoch': 0.57}


 57%|█████▋    | 2206/3886 [4:27:45<2:49:26,  6.05s/it]

 57%|█████▋    | 2207/3886 [4:27:49<2:36:33,  5.59s/it]

 57%|█████▋    | 2208/3886 [4:27:56<2:42:39,  5.82s/it]

 57%|█████▋    | 2209/3886 [4:28:04<3:02:20,  6.52s/it]

 57%|█████▋    | 2210/3886 [4:28:10<2:59:57,  6.44s/it]

 57%|█████▋    | 2211/3886 [4:28:16<2:57:04,  6.34s/it]

 57%|█████▋    | 2212/3886 [4:28:23<2:58:04,  6.38s/it]

 57%|█████▋    | 2213/3886 [4:28:29<2:56:18,  6.32s/it]

 57%|█████▋    | 2214/3886 [4:28:34<2:48:35,  6.05s/it]

 57%|█████▋    | 2215/3886 [4:28:40<2:48:01,  6.03s/it]

 57%|█████▋    | 2216/3886 [4:28:55<4:00:04,  8.63s/it]

 57%|█████▋    | 2217/3886 [4:29:02<3:46:53,  8.16s/it]

 57%|█████▋    | 2218/3886 [4:29:09<3:37:08,  7.81s/it]

 57%|█████▋    | 2219/3886 [4:29:14<3:16:13,  7.06s/it]

 57%|█████▋    | 2220/3886 [4:29:20<3:04:33,  6.65s/it]

 57%|█████▋    | 2221/3886 [4:29:30<3:32:27,  7.66s/it]

 57%|█████▋    | 2222/3886 [4:29:38<3:32:29,  7.66s/it]

 57%|█████▋    | 2223/3886 [4:29:45<3:31:05,  7.62s/it]

 57%|█████▋    | 2224/3886 [4:29:52<3:26:20,  7.45s/it]

 57%|█████▋    | 2225/3886 [4:29:59<3:20:27,  7.24s/it]

 57%|█████▋    | 2226/3886 [4:30:05<3:05:22,  6.70s/it]
{'loss': 1.248, 'grad_norm': 0.23395374888433987, 'learning_rate': 8.139616438663484e-05, 'epoch': 0.57}

 57%|█████▋    | 2227/3886 [4:30:11<3:00:38,  6.53s/it]


 57%|█████▋    | 2229/3886 [4:30:26<3:18:17,  7.18s/it]
{'loss': 1.3059, 'grad_norm': 0.24010829460631264, 'learning_rate': 8.115052780284634e-05, 'epoch': 0.57}

 57%|█████▋    | 2230/3886 [4:30:33<3:11:36,  6.94s/it]


 57%|█████▋    | 2232/3886 [4:30:46<3:04:34,  6.70s/it]

 57%|█████▋    | 2233/3886 [4:30:52<3:05:34,  6.74s/it]

 57%|█████▋    | 2234/3886 [4:30:59<3:06:05,  6.76s/it]

 58%|█████▊    | 2235/3886 [4:31:06<3:03:21,  6.66s/it]

 58%|█████▊    | 2236/3886 [4:31:12<2:57:56,  6.47s/it]

 58%|█████▊    | 2237/3886 [4:31:17<2:49:17,  6.16s/it]

 58%|█████▊    | 2238/3886 [4:31:27<3:20:41,  7.31s/it]

 58%|█████▊    | 2239/3886 [4:31:33<3:10:06,  6.93s/it]

 58%|█████▊    | 2240/3886 [4:31:41<3:15:05,  7.11s/it]

 58%|█████▊    | 2241/3886 [4:31:48<3:20:26,  7.31s/it]
{'loss': 1.4093, 'grad_norm': 0.2039196189500346, 'learning_rate': 8.01691754773742e-05, 'epoch': 0.58}


 58%|█████▊    | 2243/3886 [4:32:01<3:06:31,  6.81s/it]

 58%|█████▊    | 2244/3886 [4:32:07<3:01:59,  6.65s/it]

 58%|█████▊    | 2245/3886 [4:32:14<3:05:07,  6.77s/it]

 58%|█████▊    | 2246/3886 [4:32:22<3:13:05,  7.06s/it]

 58%|█████▊    | 2247/3886 [4:32:31<3:29:53,  7.68s/it]

 58%|█████▊    | 2248/3886 [4:32:39<3:32:22,  7.78s/it]

 58%|█████▊    | 2249/3886 [4:32:47<3:29:09,  7.67s/it]
{'loss': 1.3537, 'grad_norm': 0.2794431319423229, 'learning_rate': 7.95160367772205e-05, 'epoch': 0.58}


 58%|█████▊    | 2251/3886 [4:33:00<3:16:27,  7.21s/it]

 58%|█████▊    | 2252/3886 [4:33:06<3:09:06,  6.94s/it]
{'loss': 1.2908, 'grad_norm': 0.23301912910850797, 'learning_rate': 7.927134303076011e-05, 'epoch': 0.58}


 58%|█████▊    | 2254/3886 [4:33:18<2:51:23,  6.30s/it]

 58%|█████▊    | 2255/3886 [4:33:26<3:04:27,  6.79s/it]

 58%|█████▊    | 2256/3886 [4:33:33<3:07:16,  6.89s/it]

 58%|█████▊    | 2257/3886 [4:33:40<3:05:00,  6.81s/it]

 58%|█████▊    | 2258/3886 [4:33:53<4:00:29,  8.86s/it]

 58%|█████▊    | 2259/3886 [4:34:06<4:28:42,  9.91s/it]

 58%|█████▊    | 2260/3886 [4:34:16<4:32:55, 10.07s/it]

 58%|█████▊    | 2261/3886 [4:34:23<4:08:15,  9.17s/it]

 58%|█████▊    | 2262/3886 [4:34:38<4:55:04, 10.90s/it]

 58%|█████▊    | 2263/3886 [4:34:45<4:23:34,  9.74s/it]

 58%|█████▊    | 2264/3886 [4:35:00<5:06:53, 11.35s/it]
{'loss': 1.3476, 'grad_norm': 0.23660291222399238, 'learning_rate': 7.829387950032492e-05, 'epoch': 0.58}

 58%|█████▊    | 2265/3886 [4:35:09<4:44:36, 10.53s/it]


 58%|█████▊    | 2267/3886 [4:35:24<3:58:51,  8.85s/it]

 58%|█████▊    | 2268/3886 [4:35:30<3:41:47,  8.22s/it]

 58%|█████▊    | 2269/3886 [4:35:37<3:31:03,  7.83s/it]

 58%|█████▊    | 2270/3886 [4:35:52<4:28:26,  9.97s/it]

 58%|█████▊    | 2271/3886 [4:35:58<3:53:39,  8.68s/it]

 58%|█████▊    | 2272/3886 [4:36:06<3:44:00,  8.33s/it]

 58%|█████▊    | 2273/3886 [4:36:13<3:33:56,  7.96s/it]

 59%|█████▊    | 2274/3886 [4:36:21<3:33:45,  7.96s/it]

 59%|█████▊    | 2275/3886 [4:36:35<4:25:40,  9.89s/it]

 59%|█████▊    | 2276/3886 [4:36:42<4:01:51,  9.01s/it]

 59%|█████▊    | 2277/3886 [4:36:57<4:47:15, 10.71s/it]
{'loss': 1.3579, 'grad_norm': 0.25741291603008476, 'learning_rate': 7.72374143754109e-05, 'epoch': 0.59}


 59%|█████▊    | 2279/3886 [4:37:10<3:54:47,  8.77s/it]

 59%|█████▊    | 2280/3886 [4:37:18<3:43:08,  8.34s/it]

 59%|█████▊    | 2281/3886 [4:37:23<3:19:46,  7.47s/it]

 59%|█████▊    | 2282/3886 [4:37:34<3:48:42,  8.56s/it]

 59%|█████▊    | 2283/3886 [4:37:40<3:24:46,  7.66s/it]

 59%|█████▉    | 2284/3886 [4:37:47<3:21:44,  7.56s/it]

 59%|█████▉    | 2285/3886 [4:37:53<3:09:25,  7.10s/it]

 59%|█████▉    | 2286/3886 [4:37:59<2:58:29,  6.69s/it]
{'loss': 1.3131, 'grad_norm': 0.23630930441284623, 'learning_rate': 7.65075736715517e-05, 'epoch': 0.59}


 59%|█████▉    | 2288/3886 [4:38:14<3:10:56,  7.17s/it]

 59%|█████▉    | 2289/3886 [4:38:20<2:58:58,  6.72s/it]

 59%|█████▉    | 2290/3886 [4:38:27<2:58:42,  6.72s/it]

 59%|█████▉    | 2291/3886 [4:38:40<3:47:17,  8.55s/it]

 59%|█████▉    | 2292/3886 [4:38:46<3:30:22,  7.92s/it]

 59%|█████▉    | 2293/3886 [4:38:52<3:16:30,  7.40s/it]
{'loss': 1.2517, 'grad_norm': 0.25492078822631126, 'learning_rate': 7.594083172183039e-05, 'epoch': 0.59}


 59%|█████▉    | 2295/3886 [4:39:05<3:03:15,  6.91s/it]
{'loss': 1.4437, 'grad_norm': 0.24789889052057004, 'learning_rate': 7.577905504949103e-05, 'epoch': 0.59}


 59%|█████▉    | 2297/3886 [4:39:18<2:56:16,  6.66s/it]

 59%|█████▉    | 2298/3886 [4:39:25<3:00:06,  6.81s/it]

 59%|█████▉    | 2299/3886 [4:39:31<2:53:24,  6.56s/it]

 59%|█████▉    | 2300/3886 [4:39:37<2:50:09,  6.44s/it]

 59%|█████▉    | 2301/3886 [4:39:44<2:54:35,  6.61s/it]

 59%|█████▉    | 2302/3886 [4:39:55<3:29:47,  7.95s/it]

 59%|█████▉    | 2303/3886 [4:40:02<3:25:15,  7.78s/it]

 59%|█████▉    | 2304/3886 [4:40:13<3:46:02,  8.57s/it]
{'loss': 1.2177, 'grad_norm': 0.24157061648876535, 'learning_rate': 7.505189950802556e-05, 'epoch': 0.59}

 59%|█████▉    | 2305/3886 [4:40:21<3:46:42,  8.60s/it]


 59%|█████▉    | 2307/3886 [4:40:38<3:49:25,  8.72s/it]

 59%|█████▉    | 2308/3886 [4:40:46<3:44:48,  8.55s/it]

 59%|█████▉    | 2309/3886 [4:40:56<3:52:01,  8.83s/it]

 59%|█████▉    | 2310/3886 [4:41:02<3:30:02,  8.00s/it]

 59%|█████▉    | 2311/3886 [4:41:08<3:17:55,  7.54s/it]

 59%|█████▉    | 2312/3886 [4:41:15<3:10:12,  7.25s/it]

 60%|█████▉    | 2313/3886 [4:41:23<3:16:32,  7.50s/it]

 60%|█████▉    | 2314/3886 [4:41:37<4:02:55,  9.27s/it]

 60%|█████▉    | 2315/3886 [4:41:43<3:42:43,  8.51s/it]

 60%|█████▉    | 2316/3886 [4:41:50<3:32:32,  8.12s/it]

 60%|█████▉    | 2317/3886 [4:41:57<3:21:24,  7.70s/it]

 60%|█████▉    | 2318/3886 [4:42:10<3:59:37,  9.17s/it]

 60%|█████▉    | 2319/3886 [4:42:22<4:21:48, 10.02s/it]

 60%|█████▉    | 2320/3886 [4:42:29<3:56:28,  9.06s/it]

 60%|█████▉    | 2321/3886 [4:42:35<3:33:38,  8.19s/it]

 60%|█████▉    | 2322/3886 [4:42:44<3:41:57,  8.52s/it]

 60%|█████▉    | 2323/3886 [4:42:50<3:22:21,  7.77s/it]
{'loss': 0.9822, 'grad_norm': 0.2405745181771268, 'learning_rate': 7.352145371420446e-05, 'epoch': 0.6}

 60%|█████▉    | 2324/3886 [4:42:58<3:21:22,  7.74s/it]


 60%|█████▉    | 2326/3886 [4:43:12<3:11:28,  7.36s/it]

 60%|█████▉    | 2327/3886 [4:43:18<3:01:02,  6.97s/it]
{'loss': 1.2824, 'grad_norm': 0.25798696883243183, 'learning_rate': 7.32000879921935e-05, 'epoch': 0.6}

 60%|█████▉    | 2328/3886 [4:43:24<2:50:02,  6.55s/it]

 60%|█████▉    | 2329/3886 [4:43:30<2:45:01,  6.36s/it]


 60%|█████▉    | 2331/3886 [4:43:43<2:50:15,  6.57s/it]
{'loss': 1.3682, 'grad_norm': 0.2741447281535111, 'learning_rate': 7.287902019070365e-05, 'epoch': 0.6}


 60%|██████    | 2333/3886 [4:43:57<2:55:24,  6.78s/it]

 60%|██████    | 2334/3886 [4:44:02<2:42:10,  6.27s/it]

 60%|██████    | 2335/3886 [4:44:08<2:43:18,  6.32s/it]

 60%|██████    | 2336/3886 [4:44:15<2:44:42,  6.38s/it]

 60%|██████    | 2337/3886 [4:44:21<2:41:16,  6.25s/it]

 60%|██████    | 2338/3886 [4:44:27<2:41:32,  6.26s/it]

 60%|██████    | 2339/3886 [4:44:33<2:39:07,  6.17s/it]
{'loss': 1.4436, 'grad_norm': 0.2397950963902073, 'learning_rate': 7.223779262250297e-05, 'epoch': 0.6}


 60%|██████    | 2341/3886 [4:44:47<2:50:21,  6.62s/it]

 60%|██████    | 2342/3886 [4:44:55<2:59:18,  6.97s/it]

 60%|██████    | 2343/3886 [4:45:01<2:55:29,  6.82s/it]

 60%|██████    | 2344/3886 [4:45:17<4:02:12,  9.42s/it]
{'loss': 1.4334, 'grad_norm': 0.2001911048108305, 'learning_rate': 7.18376504630334e-05, 'epoch': 0.6}


 60%|██████    | 2346/3886 [4:45:33<3:41:40,  8.64s/it]

 60%|██████    | 2347/3886 [4:45:39<3:26:34,  8.05s/it]

 60%|██████    | 2348/3886 [4:45:45<3:08:50,  7.37s/it]

 60%|██████    | 2349/3886 [4:45:55<3:31:35,  8.26s/it]

 60%|██████    | 2350/3886 [4:46:11<4:27:41, 10.46s/it]
[2024-05-28 05:20:41,049] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 60%|██████    | 2351/3886 [4:46:20<4:14:15,  9.94s/it]

 61%|██████    | 2352/3886 [4:46:27<3:50:57,  9.03s/it]

 61%|██████    | 2353/3886 [4:46:34<3:40:54,  8.65s/it]

 61%|██████    | 2354/3886 [4:46:41<3:26:43,  8.10s/it]
{'loss': 1.4386, 'grad_norm': 0.19939901854146466, 'learning_rate': 7.103884058207409e-05, 'epoch': 0.61}

 61%|██████    | 2355/3886 [4:46:46<3:02:07,  7.14s/it]

 61%|██████    | 2356/3886 [4:46:52<2:54:17,  6.84s/it]

 61%|██████    | 2357/3886 [4:47:00<3:02:35,  7.17s/it]

 61%|██████    | 2358/3886 [4:47:06<2:51:46,  6.74s/it]

 61%|██████    | 2359/3886 [4:47:12<2:50:06,  6.68s/it]


 61%|██████    | 2361/3886 [4:47:29<3:14:42,  7.66s/it]
{'loss': 1.2426, 'grad_norm': 0.28547135775934174, 'learning_rate': 7.048086759952055e-05, 'epoch': 0.61}


 61%|██████    | 2363/3886 [4:47:43<3:05:54,  7.32s/it]

 61%|██████    | 2364/3886 [4:47:51<3:11:16,  7.54s/it]
{'loss': 1.3697, 'grad_norm': 0.20583250562885105, 'learning_rate': 7.024204285365012e-05, 'epoch': 0.61}


 61%|██████    | 2366/3886 [4:48:06<3:05:42,  7.33s/it]

 61%|██████    | 2367/3886 [4:48:13<3:03:26,  7.25s/it]

 61%|██████    | 2368/3886 [4:48:19<2:55:55,  6.95s/it]

 61%|██████    | 2369/3886 [4:48:25<2:46:54,  6.60s/it]

 61%|██████    | 2370/3886 [4:48:32<2:48:58,  6.69s/it]
{'loss': 1.2497, 'grad_norm': 0.24212377219862877, 'learning_rate': 6.976495308534253e-05, 'epoch': 0.61}

 61%|██████    | 2371/3886 [4:48:38<2:49:00,  6.69s/it]


 61%|██████    | 2373/3886 [4:48:52<2:50:12,  6.75s/it]

 61%|██████    | 2374/3886 [4:48:59<2:56:34,  7.01s/it]

 61%|██████    | 2375/3886 [4:49:06<2:50:52,  6.78s/it]

 61%|██████    | 2376/3886 [4:49:11<2:38:27,  6.30s/it]

 61%|██████    | 2377/3886 [4:49:17<2:38:09,  6.29s/it]

 61%|██████    | 2378/3886 [4:49:26<2:54:45,  6.95s/it]
{'loss': 1.3648, 'grad_norm': 0.21175404157764133, 'learning_rate': 6.913001182755616e-05, 'epoch': 0.61}

 61%|██████    | 2379/3886 [4:49:32<2:50:33,  6.79s/it]


 61%|██████▏   | 2381/3886 [4:49:43<2:35:29,  6.20s/it]
{'loss': 1.1654, 'grad_norm': 0.2774593081087692, 'learning_rate': 6.889226122974278e-05, 'epoch': 0.61}


 61%|██████▏   | 2383/3886 [4:49:56<2:34:03,  6.15s/it]
{'loss': 1.1218, 'grad_norm': 0.2610766295714196, 'learning_rate': 6.873386880463912e-05, 'epoch': 0.61}


 61%|██████▏   | 2385/3886 [4:50:15<3:27:20,  8.29s/it]

 61%|██████▏   | 2386/3886 [4:50:21<3:05:20,  7.41s/it]
{'loss': 1.1409, 'grad_norm': 0.2698803318426183, 'learning_rate': 6.849644322762043e-05, 'epoch': 0.61}


 61%|██████▏   | 2388/3886 [4:50:33<2:53:47,  6.96s/it]

 61%|██████▏   | 2389/3886 [4:50:40<2:47:38,  6.72s/it]
{'loss': 1.1157, 'grad_norm': 0.31231887513088824, 'learning_rate': 6.82592146429432e-05, 'epoch': 0.61}

 62%|██████▏   | 2390/3886 [4:50:47<2:49:36,  6.80s/it]

 62%|██████▏   | 2391/3886 [4:50:53<2:44:26,  6.60s/it]

 62%|██████▏   | 2392/3886 [4:50:59<2:38:21,  6.36s/it]


 62%|██████▏   | 2394/3886 [4:51:15<2:58:13,  7.17s/it]

 62%|██████▏   | 2395/3886 [4:51:25<3:19:40,  8.04s/it]

 62%|██████▏   | 2396/3886 [4:51:32<3:10:06,  7.66s/it]

 62%|██████▏   | 2397/3886 [4:51:47<4:08:28, 10.01s/it]

 62%|██████▏   | 2398/3886 [4:51:55<3:52:18,  9.37s/it]
{'loss': 1.2857, 'grad_norm': 0.2387101403485414, 'learning_rate': 6.754872567069442e-05, 'epoch': 0.62}


 62%|██████▏   | 2400/3886 [4:52:09<3:23:03,  8.20s/it]
 62%|██████▏   | 2400/3886 [4:52:09<3:23:03,  8.20s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.3858, 'grad_norm': 0.239682580436697, 'learning_rate': 6.73122998768747e-05, 'epoch': 0.62}
 62%|██████▏   | 2401/3886 [4:52:41<6:19:12, 15.32s/it]

 62%|██████▏   | 2402/3886 [4:52:46<5:05:44, 12.36s/it]

 62%|██████▏   | 2403/3886 [4:52:54<4:33:57, 11.08s/it]


 62%|██████▏   | 2405/3886 [4:53:08<3:35:28,  8.73s/it]
{'loss': 1.3447, 'grad_norm': 0.22156684403792953, 'learning_rate': 6.69973836910499e-05, 'epoch': 0.62}


 62%|██████▏   | 2407/3886 [4:53:19<2:55:24,  7.12s/it]

 62%|██████▏   | 2408/3886 [4:53:28<3:07:42,  7.62s/it]

 62%|██████▏   | 2409/3886 [4:53:33<2:51:01,  6.95s/it]

 62%|██████▏   | 2410/3886 [4:53:39<2:45:01,  6.71s/it]
{'loss': 1.159, 'grad_norm': 0.25379917461495866, 'learning_rate': 6.660425478339047e-05, 'epoch': 0.62}


 62%|██████▏   | 2412/3886 [4:53:51<2:35:25,  6.33s/it]
{'loss': 1.3238, 'grad_norm': 0.2354628429864425, 'learning_rate': 6.644716525696764e-05, 'epoch': 0.62}

 62%|██████▏   | 2413/3886 [4:53:57<2:28:03,  6.03s/it]


 62%|██████▏   | 2415/3886 [4:54:11<2:44:14,  6.70s/it]
{'loss': 1.4134, 'grad_norm': 0.2361089414033503, 'learning_rate': 6.621170594250456e-05, 'epoch': 0.62}


 62%|██████▏   | 2417/3886 [4:54:24<2:34:51,  6.33s/it]

 62%|██████▏   | 2418/3886 [4:54:34<3:01:41,  7.43s/it]
{'loss': 1.4031, 'grad_norm': 0.25505462837665527, 'learning_rate': 6.597645790688885e-05, 'epoch': 0.62}

 62%|██████▏   | 2419/3886 [4:54:39<2:46:31,  6.81s/it]


 62%|██████▏   | 2421/3886 [4:54:50<2:31:23,  6.20s/it]

 62%|██████▏   | 2422/3886 [4:54:56<2:26:33,  6.01s/it]

 62%|██████▏   | 2423/3886 [4:55:02<2:30:30,  6.17s/it]
{'loss': 1.1531, 'grad_norm': 0.2555575410871561, 'learning_rate': 6.558485135081398e-05, 'epoch': 0.62}

 62%|██████▏   | 2424/3886 [4:55:09<2:35:13,  6.37s/it]


 62%|██████▏   | 2426/3886 [4:55:24<2:40:56,  6.61s/it]
{'loss': 1.432, 'grad_norm': 0.23813529131198918, 'learning_rate': 6.535017391344701e-05, 'epoch': 0.62}

 62%|██████▏   | 2427/3886 [4:55:30<2:42:06,  6.67s/it]


 63%|██████▎   | 2429/3886 [4:55:44<2:46:30,  6.86s/it]

 63%|██████▎   | 2430/3886 [4:55:54<3:07:20,  7.72s/it]

 63%|██████▎   | 2431/3886 [4:56:01<3:04:19,  7.60s/it]

 63%|██████▎   | 2432/3886 [4:56:07<2:52:55,  7.14s/it]

 63%|██████▎   | 2433/3886 [4:56:16<3:01:03,  7.48s/it]

 63%|██████▎   | 2434/3886 [4:56:22<2:49:44,  7.01s/it]
{'loss': 1.3782, 'grad_norm': 0.26458989783234477, 'learning_rate': 6.472543065126609e-05, 'epoch': 0.63}


 63%|██████▎   | 2436/3886 [4:56:34<2:39:37,  6.61s/it]

 63%|██████▎   | 2437/3886 [4:56:42<2:50:35,  7.06s/it]
{'loss': 1.2618, 'grad_norm': 0.22247591575072445, 'learning_rate': 6.449155481981629e-05, 'epoch': 0.63}

 63%|██████▎   | 2438/3886 [4:56:47<2:32:37,  6.32s/it]


 63%|██████▎   | 2440/3886 [4:57:00<2:32:58,  6.35s/it]

 63%|██████▎   | 2441/3886 [4:57:05<2:28:45,  6.18s/it]
{'loss': 1.2475, 'grad_norm': 0.24282966640387477, 'learning_rate': 6.418006601820373e-05, 'epoch': 0.63}


 63%|██████▎   | 2443/3886 [4:57:24<3:12:20,  8.00s/it]

 63%|██████▎   | 2444/3886 [4:57:30<3:02:21,  7.59s/it]
{'loss': 1.4007, 'grad_norm': 0.2600419480428128, 'learning_rate': 6.394671054087602e-05, 'epoch': 0.63}


 63%|██████▎   | 2446/3886 [4:57:45<2:59:30,  7.48s/it]

 63%|██████▎   | 2447/3886 [4:57:52<2:54:22,  7.27s/it]

 63%|██████▎   | 2448/3886 [4:57:58<2:45:45,  6.92s/it]
{'loss': 1.1305, 'grad_norm': 0.2400884358390222, 'learning_rate': 6.363592084385002e-05, 'epoch': 0.63}


 63%|██████▎   | 2450/3886 [4:58:14<2:52:12,  7.20s/it]

 63%|██████▎   | 2451/3886 [4:58:20<2:45:14,  6.91s/it]

 63%|██████▎   | 2452/3886 [4:58:26<2:39:42,  6.68s/it]

 63%|██████▎   | 2453/3886 [4:58:32<2:32:34,  6.39s/it]

 63%|██████▎   | 2454/3886 [4:58:40<2:43:22,  6.85s/it]

 63%|██████▎   | 2455/3886 [4:58:46<2:38:26,  6.64s/it]
{'loss': 1.2542, 'grad_norm': 0.22398029935050678, 'learning_rate': 6.309301365315725e-05, 'epoch': 0.63}

 63%|██████▎   | 2456/3886 [4:58:53<2:36:36,  6.57s/it]


 63%|██████▎   | 2458/3886 [4:59:10<2:58:00,  7.48s/it]

 63%|██████▎   | 2459/3886 [4:59:15<2:44:16,  6.91s/it]

 63%|██████▎   | 2460/3886 [4:59:22<2:41:49,  6.81s/it]

 63%|██████▎   | 2461/3886 [4:59:28<2:33:01,  6.44s/it]

 63%|██████▎   | 2462/3886 [4:59:34<2:35:26,  6.55s/it]

 63%|██████▎   | 2463/3886 [4:59:42<2:43:18,  6.89s/it]
{'loss': 1.5799, 'grad_norm': 0.2194474272748782, 'learning_rate': 6.247408789849352e-05, 'epoch': 0.63}


 63%|██████▎   | 2465/3886 [5:00:03<3:26:45,  8.73s/it]
{'loss': 1.179, 'grad_norm': 0.25913686947614223, 'learning_rate': 6.231961610807237e-05, 'epoch': 0.63}


 63%|██████▎   | 2467/3886 [5:00:26<4:07:34, 10.47s/it]

 64%|██████▎   | 2468/3886 [5:00:31<3:34:05,  9.06s/it]

 64%|██████▎   | 2469/3886 [5:00:38<3:19:03,  8.43s/it]
{'loss': 1.3804, 'grad_norm': 0.22060660476498858, 'learning_rate': 6.201098711120464e-05, 'epoch': 0.64}


 64%|██████▎   | 2471/3886 [5:00:50<2:48:25,  7.14s/it]
{'loss': 1.3248, 'grad_norm': 0.2381198835822979, 'learning_rate': 6.185683076247528e-05, 'epoch': 0.64}


 64%|██████▎   | 2473/3886 [5:01:02<2:34:50,  6.58s/it]

 64%|██████▎   | 2474/3886 [5:01:12<2:54:38,  7.42s/it]
{'loss': 1.2127, 'grad_norm': 0.2280399078017015, 'learning_rate': 6.162579513154327e-05, 'epoch': 0.64}


 64%|██████▎   | 2476/3886 [5:01:36<3:37:22,  9.25s/it]

 64%|██████▎   | 2477/3886 [5:01:41<3:07:42,  7.99s/it]
{'loss': 1.3833, 'grad_norm': 0.23272416174701596, 'learning_rate': 6.139499945524632e-05, 'epoch': 0.64}

 64%|██████▍   | 2478/3886 [5:01:51<3:26:46,  8.81s/it]


 64%|██████▍   | 2480/3886 [5:02:04<2:57:08,  7.56s/it]

 64%|██████▍   | 2481/3886 [5:02:10<2:49:36,  7.24s/it]

 64%|██████▍   | 2482/3886 [5:02:16<2:37:41,  6.74s/it]

 64%|██████▍   | 2483/3886 [5:02:22<2:34:17,  6.60s/it]

 64%|██████▍   | 2484/3886 [5:02:31<2:46:02,  7.11s/it]

 64%|██████▍   | 2485/3886 [5:02:37<2:38:48,  6.80s/it]

 64%|██████▍   | 2486/3886 [5:02:46<2:55:03,  7.50s/it]

 64%|██████▍   | 2487/3886 [5:02:52<2:44:16,  7.05s/it]

 64%|██████▍   | 2488/3886 [5:02:58<2:41:06,  6.91s/it]

 64%|██████▍   | 2489/3886 [5:03:05<2:37:35,  6.77s/it]

 64%|██████▍   | 2490/3886 [5:03:12<2:43:22,  7.02s/it]

 64%|██████▍   | 2491/3886 [5:03:18<2:34:09,  6.63s/it]

 64%|██████▍   | 2492/3886 [5:03:27<2:47:25,  7.21s/it]

 64%|██████▍   | 2493/3886 [5:03:34<2:47:35,  7.22s/it]
{'loss': 1.2494, 'grad_norm': 0.22162067247969586, 'learning_rate': 6.016820126080418e-05, 'epoch': 0.64}

 64%|██████▍   | 2494/3886 [5:03:39<2:34:26,  6.66s/it]


 64%|██████▍   | 2496/3886 [5:03:53<2:35:08,  6.70s/it]
{'loss': 1.31, 'grad_norm': 0.24902070152572797, 'learning_rate': 5.99389587480856e-05, 'epoch': 0.64}


 64%|██████▍   | 2498/3886 [5:04:04<2:27:15,  6.37s/it]
{'loss': 1.3375, 'grad_norm': 0.21217170723290973, 'learning_rate': 5.978626948588324e-05, 'epoch': 0.64}


 64%|██████▍   | 2500/3886 [5:04:20<2:47:18,  7.24s/it]

 64%|██████▍   | 2501/3886 [5:04:26<2:36:23,  6.78s/it]
{'loss': 1.2777, 'grad_norm': 0.2597499527483718, 'learning_rate': 5.9557445272848946e-05, 'epoch': 0.64}


 64%|██████▍   | 2503/3886 [5:04:36<2:19:07,  6.04s/it]

 64%|██████▍   | 2504/3886 [5:04:43<2:19:31,  6.06s/it]
{'loss': 1.4617, 'grad_norm': 0.27028818196960674, 'learning_rate': 5.9328873947880846e-05, 'epoch': 0.64}


 64%|██████▍   | 2506/3886 [5:04:57<2:33:14,  6.66s/it]
{'loss': 1.2571, 'grad_norm': 0.25425650977712155, 'learning_rate': 5.9176634263812116e-05, 'epoch': 0.64}

 65%|██████▍   | 2507/3886 [5:05:01<2:19:17,  6.06s/it]

 65%|██████▍   | 2508/3886 [5:05:10<2:34:08,  6.71s/it]


 65%|██████▍   | 2510/3886 [5:05:30<3:12:54,  8.41s/it]

 65%|██████▍   | 2511/3886 [5:05:39<3:12:04,  8.38s/it]

 65%|██████▍   | 2512/3886 [5:05:44<2:55:17,  7.65s/it]

 65%|██████▍   | 2513/3886 [5:05:57<3:27:16,  9.06s/it]
{'loss': 1.343, 'grad_norm': 0.2671596904009237, 'learning_rate': 5.864469158602229e-05, 'epoch': 0.65}

 65%|██████▍   | 2514/3886 [5:06:05<3:21:48,  8.83s/it]

 65%|██████▍   | 2515/3886 [5:06:11<3:02:42,  8.00s/it]


 65%|██████▍   | 2517/3886 [5:06:23<2:37:46,  6.91s/it]
{'loss': 1.2541, 'grad_norm': 0.2514331004338746, 'learning_rate': 5.834135530294458e-05, 'epoch': 0.65}

 65%|██████▍   | 2518/3886 [5:06:29<2:34:55,  6.79s/it]

 65%|██████▍   | 2519/3886 [5:06:36<2:31:30,  6.65s/it]


 65%|██████▍   | 2521/3886 [5:06:51<2:46:04,  7.30s/it]

 65%|██████▍   | 2522/3886 [5:06:57<2:36:53,  6.90s/it]

 65%|██████▍   | 2523/3886 [5:07:05<2:41:42,  7.12s/it]

 65%|██████▍   | 2524/3886 [5:07:12<2:46:09,  7.32s/it]
{'loss': 1.2056, 'grad_norm': 0.24403474959152782, 'learning_rate': 5.781163316086369e-05, 'epoch': 0.65}


 65%|██████▌   | 2526/3886 [5:07:26<2:38:48,  7.01s/it]

 65%|██████▌   | 2527/3886 [5:07:32<2:34:02,  6.80s/it]

 65%|██████▌   | 2528/3886 [5:07:38<2:29:02,  6.59s/it]

 65%|██████▌   | 2529/3886 [5:07:44<2:24:54,  6.41s/it]

 65%|██████▌   | 2530/3886 [5:07:52<2:34:45,  6.85s/it]
{'loss': 1.2035, 'grad_norm': 0.22199854811930264, 'learning_rate': 5.735872807921743e-05, 'epoch': 0.65}

 65%|██████▌   | 2531/3886 [5:07:59<2:36:35,  6.93s/it]

 65%|██████▌   | 2532/3886 [5:08:05<2:29:12,  6.61s/it]


 65%|██████▌   | 2534/3886 [5:08:19<2:28:58,  6.61s/it]
{'loss': 1.1779, 'grad_norm': 0.24248137947742968, 'learning_rate': 5.705738318499483e-05, 'epoch': 0.65}

 65%|██████▌   | 2535/3886 [5:08:25<2:26:13,  6.49s/it]


 65%|██████▌   | 2537/3886 [5:08:38<2:25:19,  6.46s/it]
{'loss': 1.3945, 'grad_norm': 0.2668382185714361, 'learning_rate': 5.6831687605705344e-05, 'epoch': 0.65}

 65%|██████▌   | 2538/3886 [5:08:44<2:19:38,  6.22s/it]


 65%|██████▌   | 2540/3886 [5:08:59<2:33:17,  6.83s/it]

 65%|██████▌   | 2541/3886 [5:09:05<2:27:45,  6.59s/it]

 65%|██████▌   | 2542/3886 [5:09:10<2:20:52,  6.29s/it]
{'loss': 1.0244, 'grad_norm': 0.272066161829959, 'learning_rate': 5.645612885256665e-05, 'epoch': 0.65}


 65%|██████▌   | 2544/3886 [5:09:24<2:27:44,  6.61s/it]

 65%|██████▌   | 2545/3886 [5:09:30<2:23:43,  6.43s/it]

 66%|██████▌   | 2546/3886 [5:09:37<2:25:01,  6.49s/it]

 66%|██████▌   | 2547/3886 [5:09:46<2:43:21,  7.32s/it]
{'loss': 1.2852, 'grad_norm': 0.22939385995414047, 'learning_rate': 5.6081326433885504e-05, 'epoch': 0.66}

 66%|██████▌   | 2548/3886 [5:09:52<2:31:40,  6.80s/it]

 66%|██████▌   | 2549/3886 [5:10:02<2:52:41,  7.75s/it]

 66%|██████▌   | 2550/3886 [5:10:09<2:52:13,  7.73s/it]

 66%|██████▌   | 2551/3886 [5:10:16<2:43:06,  7.33s/it]


 66%|██████▌   | 2553/3886 [5:10:33<2:56:15,  7.93s/it]
{'loss': 1.3559, 'grad_norm': 0.2520471591013576, 'learning_rate': 5.563257105827462e-05, 'epoch': 0.66}

 66%|██████▌   | 2554/3886 [5:10:40<2:53:06,  7.80s/it]


 66%|██████▌   | 2556/3886 [5:10:55<2:43:21,  7.37s/it]
{'loss': 1.2123, 'grad_norm': 0.2746990646332569, 'learning_rate': 5.5408608814951044e-05, 'epoch': 0.66}


 66%|██████▌   | 2558/3886 [5:11:11<2:51:46,  7.76s/it]

 66%|██████▌   | 2559/3886 [5:11:17<2:44:36,  7.44s/it]

 66%|██████▌   | 2560/3886 [5:11:23<2:34:57,  7.01s/it]

 66%|██████▌   | 2561/3886 [5:11:29<2:26:12,  6.62s/it]

 66%|██████▌   | 2562/3886 [5:11:34<2:17:41,  6.24s/it]
{'loss': 1.3213, 'grad_norm': 0.22325986436463077, 'learning_rate': 5.496152221945972e-05, 'epoch': 0.66}


 66%|██████▌   | 2564/3886 [5:11:47<2:17:58,  6.26s/it]
{'loss': 1.3286, 'grad_norm': 0.20963567196724986, 'learning_rate': 5.481274313712086e-05, 'epoch': 0.66}


 66%|██████▌   | 2566/3886 [5:11:59<2:13:42,  6.08s/it]

 66%|██████▌   | 2567/3886 [5:12:04<2:07:28,  5.80s/it]

 66%|██████▌   | 2568/3886 [5:12:10<2:09:20,  5.89s/it]

 66%|██████▌   | 2569/3886 [5:12:19<2:28:47,  6.78s/it]

 66%|██████▌   | 2570/3886 [5:12:25<2:22:02,  6.48s/it]

 66%|██████▌   | 2571/3886 [5:12:31<2:15:09,  6.17s/it]

 66%|██████▌   | 2572/3886 [5:12:37<2:16:14,  6.22s/it]

 66%|██████▌   | 2573/3886 [5:12:45<2:31:13,  6.91s/it]

 66%|██████▌   | 2574/3886 [5:12:51<2:23:12,  6.55s/it]
{'loss': 1.2847, 'grad_norm': 0.22225774556362682, 'learning_rate': 5.407073969509337e-05, 'epoch': 0.66}


 66%|██████▋   | 2576/3886 [5:13:08<2:42:41,  7.45s/it]

 66%|██████▋   | 2577/3886 [5:13:17<2:47:18,  7.67s/it]
{'loss': 1.2986, 'grad_norm': 0.24403383066915893, 'learning_rate': 5.384875857850218e-05, 'epoch': 0.66}


 66%|██████▋   | 2579/3886 [5:13:27<2:22:40,  6.55s/it]
{'loss': 1.4774, 'grad_norm': 0.233469116266131, 'learning_rate': 5.370093140660719e-05, 'epoch': 0.66}

 66%|██████▋   | 2580/3886 [5:13:36<2:33:38,  7.06s/it]

 66%|██████▋   | 2581/3886 [5:13:46<2:53:34,  7.98s/it]


 66%|██████▋   | 2583/3886 [5:14:01<2:49:31,  7.81s/it]

 66%|██████▋   | 2584/3886 [5:14:07<2:40:36,  7.40s/it]

 67%|██████▋   | 2585/3886 [5:14:13<2:31:21,  6.98s/it]
{'loss': 1.4312, 'grad_norm': 0.22186517427100114, 'learning_rate': 5.325822355656575e-05, 'epoch': 0.67}


 67%|██████▋   | 2587/3886 [5:14:29<2:35:47,  7.20s/it]

 67%|██████▋   | 2588/3886 [5:14:35<2:24:44,  6.69s/it]
{'loss': 1.3855, 'grad_norm': 0.2569996606825887, 'learning_rate': 5.303730735576431e-05, 'epoch': 0.67}

 67%|██████▋   | 2589/3886 [5:14:40<2:18:33,  6.41s/it]


 67%|██████▋   | 2591/3886 [5:14:55<2:24:54,  6.71s/it]
{'loss': 1.1704, 'grad_norm': 0.2502315570043027, 'learning_rate': 5.2816684813575845e-05, 'epoch': 0.67}


 67%|██████▋   | 2593/3886 [5:15:16<3:00:06,  8.36s/it]
{'loss': 1.3587, 'grad_norm': 0.21002348303007337, 'learning_rate': 5.2669766943795954e-05, 'epoch': 0.67}


 67%|██████▋   | 2595/3886 [5:15:35<3:18:19,  9.22s/it]

 67%|██████▋   | 2596/3886 [5:15:40<2:50:40,  7.94s/it]

 67%|██████▋   | 2597/3886 [5:15:45<2:34:12,  7.18s/it]
{'loss': 1.1739, 'grad_norm': 0.2508772758844586, 'learning_rate': 5.237632622142087e-05, 'epoch': 0.67}

 67%|██████▋   | 2598/3886 [5:15:56<3:00:04,  8.39s/it]

 67%|██████▋   | 2599/3886 [5:16:02<2:43:46,  7.64s/it]


 67%|██████▋   | 2601/3886 [5:16:13<2:20:08,  6.54s/it]
{'loss': 1.2423, 'grad_norm': 0.24018035733299858, 'learning_rate': 5.208341490637014e-05, 'epoch': 0.67}

 67%|██████▋   | 2602/3886 [5:16:20<2:21:32,  6.61s/it]

 67%|██████▋   | 2603/3886 [5:16:26<2:17:02,  6.41s/it]


 67%|██████▋   | 2605/3886 [5:16:37<2:09:25,  6.06s/it]
{'loss': 1.3333, 'grad_norm': 0.22805878564388618, 'learning_rate': 5.1791036254784764e-05, 'epoch': 0.67}


 67%|██████▋   | 2607/3886 [5:16:59<3:06:10,  8.73s/it]

 67%|██████▋   | 2608/3886 [5:17:07<3:01:28,  8.52s/it]

 67%|██████▋   | 2609/3886 [5:17:17<3:10:18,  8.94s/it]

 67%|██████▋   | 2610/3886 [5:17:24<2:57:44,  8.36s/it]

 67%|██████▋   | 2611/3886 [5:17:31<2:52:04,  8.10s/it]
{'loss': 1.3293, 'grad_norm': 0.26776281066319696, 'learning_rate': 5.135347412963159e-05, 'epoch': 0.67}

 67%|██████▋   | 2612/3886 [5:17:36<2:33:28,  7.23s/it]


 67%|██████▋   | 2614/3886 [5:17:47<2:12:40,  6.26s/it]

 67%|██████▋   | 2615/3886 [5:18:01<3:03:52,  8.68s/it]
{'loss': 1.2909, 'grad_norm': 0.24716668284516194, 'learning_rate': 5.1062441343378564e-05, 'epoch': 0.67}


 67%|██████▋   | 2617/3886 [5:18:15<2:44:26,  7.77s/it]

 67%|██████▋   | 2618/3886 [5:18:21<2:33:46,  7.28s/it]

 67%|██████▋   | 2619/3886 [5:18:27<2:23:15,  6.78s/it]

 67%|██████▋   | 2620/3886 [5:18:34<2:22:37,  6.76s/it]

 67%|██████▋   | 2621/3886 [5:18:49<3:18:56,  9.44s/it]

 67%|██████▋   | 2622/3886 [5:18:57<3:07:19,  8.89s/it]

 67%|██████▋   | 2623/3886 [5:19:03<2:48:31,  8.01s/it]

 68%|██████▊   | 2624/3886 [5:19:09<2:35:03,  7.37s/it]

 68%|██████▊   | 2625/3886 [5:19:17<2:38:39,  7.55s/it]

 68%|██████▊   | 2626/3886 [5:19:23<2:32:55,  7.28s/it]

 68%|██████▊   | 2627/3886 [5:19:29<2:23:39,  6.85s/it]

 68%|██████▊   | 2628/3886 [5:19:43<3:09:57,  9.06s/it]

 68%|██████▊   | 2629/3886 [5:19:51<3:03:03,  8.74s/it]

 68%|██████▊   | 2630/3886 [5:19:57<2:42:35,  7.77s/it]

 68%|██████▊   | 2631/3886 [5:20:03<2:33:36,  7.34s/it]
{'loss': 1.2832, 'grad_norm': 0.2241337273099467, 'learning_rate': 4.990378259093462e-05, 'epoch': 0.68}


 68%|██████▊   | 2633/3886 [5:20:15<2:17:02,  6.56s/it]

 68%|██████▊   | 2634/3886 [5:20:21<2:14:18,  6.44s/it]

 68%|██████▊   | 2635/3886 [5:20:28<2:15:00,  6.47s/it]

 68%|██████▊   | 2636/3886 [5:20:33<2:08:57,  6.19s/it]
{'loss': 1.2203, 'grad_norm': 0.243065699456639, 'learning_rate': 4.954351936906745e-05, 'epoch': 0.68}


 68%|██████▊   | 2638/3886 [5:20:52<2:36:05,  7.50s/it]

 68%|██████▊   | 2639/3886 [5:20:57<2:24:08,  6.94s/it]

 68%|██████▊   | 2640/3886 [5:21:04<2:22:18,  6.85s/it]

 68%|██████▊   | 2641/3886 [5:21:10<2:15:55,  6.55s/it]

 68%|██████▊   | 2642/3886 [5:21:21<2:45:49,  8.00s/it]

 68%|██████▊   | 2643/3886 [5:21:29<2:45:37,  7.99s/it]
{'loss': 1.3912, 'grad_norm': 0.22863981637065767, 'learning_rate': 4.904062461412702e-05, 'epoch': 0.68}

 68%|██████▊   | 2644/3886 [5:21:35<2:30:25,  7.27s/it]


 68%|██████▊   | 2646/3886 [5:21:49<2:28:56,  7.21s/it]

 68%|██████▊   | 2647/3886 [5:21:56<2:26:16,  7.08s/it]

 68%|██████▊   | 2648/3886 [5:22:04<2:32:56,  7.41s/it]
{'loss': 1.2298, 'grad_norm': 0.2573016107453615, 'learning_rate': 4.8682475242956275e-05, 'epoch': 0.68}


 68%|██████▊   | 2650/3886 [5:22:23<2:57:19,  8.61s/it]
{'loss': 1.3959, 'grad_norm': 0.23044493944154493, 'learning_rate': 4.853946472709926e-05, 'epoch': 0.68}


 68%|██████▊   | 2652/3886 [5:22:34<2:21:31,  6.88s/it]

 68%|██████▊   | 2653/3886 [5:22:40<2:18:03,  6.72s/it]

 68%|██████▊   | 2654/3886 [5:22:46<2:12:03,  6.43s/it]
{'loss': 1.3972, 'grad_norm': 0.2451709584372298, 'learning_rate': 4.825387313750047e-05, 'epoch': 0.68}

 68%|██████▊   | 2655/3886 [5:22:52<2:12:58,  6.48s/it]


 68%|██████▊   | 2657/3886 [5:23:04<2:03:09,  6.01s/it]
{'loss': 1.538, 'grad_norm': 0.2262713449334685, 'learning_rate': 4.804005676953959e-05, 'epoch': 0.68}


 68%|██████▊   | 2659/3886 [5:23:18<2:13:42,  6.54s/it]

 68%|██████▊   | 2660/3886 [5:23:25<2:20:56,  6.90s/it]

 68%|██████▊   | 2661/3886 [5:23:31<2:15:17,  6.63s/it]

 69%|██████▊   | 2662/3886 [5:23:38<2:15:27,  6.64s/it]

 69%|██████▊   | 2663/3886 [5:23:46<2:23:19,  7.03s/it]
{'loss': 1.245, 'grad_norm': 0.23725754971723104, 'learning_rate': 4.761340008813282e-05, 'epoch': 0.69}


 69%|██████▊   | 2665/3886 [5:24:04<2:40:33,  7.89s/it]
{'loss': 1.5178, 'grad_norm': 0.20934924375079672, 'learning_rate': 4.747147184507873e-05, 'epoch': 0.69}

 69%|██████▊   | 2666/3886 [5:24:09<2:24:28,  7.11s/it]


 69%|██████▊   | 2668/3886 [5:24:24<2:25:45,  7.18s/it]

 69%|██████▊   | 2669/3886 [5:24:29<2:16:31,  6.73s/it]
{'loss': 1.4318, 'grad_norm': 0.22945910711636633, 'learning_rate': 4.718805370231212e-05, 'epoch': 0.69}


 69%|██████▊   | 2671/3886 [5:24:43<2:18:41,  6.85s/it]

 69%|██████▉   | 2672/3886 [5:24:50<2:15:41,  6.71s/it]

 69%|██████▉   | 2673/3886 [5:24:58<2:25:14,  7.18s/it]

 69%|██████▉   | 2674/3886 [5:25:05<2:26:23,  7.25s/it]
{'loss': 1.4343, 'grad_norm': 0.20445136990658244, 'learning_rate': 4.683460697737973e-05, 'epoch': 0.69}

 69%|██████▉   | 2675/3886 [5:25:13<2:26:25,  7.25s/it]


 69%|██████▉   | 2677/3886 [5:25:23<2:05:30,  6.23s/it]
{'loss': 1.0924, 'grad_norm': 0.26126333286297887, 'learning_rate': 4.6622981808723475e-05, 'epoch': 0.69}

 69%|██████▉   | 2678/3886 [5:25:31<2:14:20,  6.67s/it]

 69%|██████▉   | 2679/3886 [5:25:37<2:08:54,  6.41s/it]


 69%|██████▉   | 2681/3886 [5:25:50<2:09:55,  6.47s/it]

 69%|██████▉   | 2682/3886 [5:25:58<2:17:54,  6.87s/it]
{'loss': 1.4474, 'grad_norm': 0.24231634070663632, 'learning_rate': 4.627101555241592e-05, 'epoch': 0.69}

 69%|██████▉   | 2683/3886 [5:26:05<2:19:09,  6.94s/it]


 69%|██████▉   | 2685/3886 [5:26:21<2:37:42,  7.88s/it]

 69%|██████▉   | 2686/3886 [5:26:27<2:25:12,  7.26s/it]

 69%|██████▉   | 2687/3886 [5:26:34<2:21:00,  7.06s/it]

 69%|██████▉   | 2688/3886 [5:26:42<2:28:40,  7.45s/it]

 69%|██████▉   | 2689/3886 [5:26:48<2:20:09,  7.03s/it]

 69%|██████▉   | 2690/3886 [5:26:54<2:13:55,  6.72s/it]

 69%|██████▉   | 2691/3886 [5:27:00<2:05:48,  6.32s/it]
{'loss': 1.253, 'grad_norm': 0.2480353411210101, 'learning_rate': 4.563983216534381e-05, 'epoch': 0.69}


 69%|██████▉   | 2693/3886 [5:27:21<2:44:00,  8.25s/it]
{'loss': 1.3738, 'grad_norm': 0.24235734323279517, 'learning_rate': 4.5499983393303325e-05, 'epoch': 0.69}


 69%|██████▉   | 2695/3886 [5:27:48<3:22:50, 10.22s/it]

 69%|██████▉   | 2696/3886 [5:27:58<3:22:30, 10.21s/it]
{'loss': 1.3426, 'grad_norm': 0.28412204296400684, 'learning_rate': 4.529049434803458e-05, 'epoch': 0.69}


 69%|██████▉   | 2698/3886 [5:28:12<2:55:40,  8.87s/it]

 69%|██████▉   | 2699/3886 [5:28:18<2:33:29,  7.76s/it]
{'loss': 1.3107, 'grad_norm': 0.23913463638254137, 'learning_rate': 4.5081347402348375e-05, 'epoch': 0.69}

 69%|██████▉   | 2700/3886 [5:28:23<2:18:54,  7.03s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|██████▉   | 2701/3886 [5:29:00<5:19:03, 16.15s/it]

 70%|██████▉   | 2702/3886 [5:29:06<4:17:39, 13.06s/it]

 70%|██████▉   | 2703/3886 [5:29:12<3:35:23, 10.92s/it]
{'loss': 1.3315, 'grad_norm': 0.2644571567722503, 'learning_rate': 4.480301922309188e-05, 'epoch': 0.7}

 70%|██████▉   | 2704/3886 [5:29:19<3:09:28,  9.62s/it]

 70%|██████▉   | 2705/3886 [5:29:25<2:48:01,  8.54s/it]


 70%|██████▉   | 2707/3886 [5:29:40<2:42:59,  8.29s/it]

 70%|██████▉   | 2708/3886 [5:29:50<2:53:29,  8.84s/it]

 70%|██████▉   | 2709/3886 [5:29:56<2:36:05,  7.96s/it]

 70%|██████▉   | 2710/3886 [5:30:02<2:24:19,  7.36s/it]

 70%|██████▉   | 2711/3886 [5:30:12<2:34:53,  7.91s/it]
{'loss': 1.3329, 'grad_norm': 0.2663793172409444, 'learning_rate': 4.424820673916648e-05, 'epoch': 0.7}


 70%|██████▉   | 2713/3886 [5:30:25<2:27:27,  7.54s/it]

 70%|██████▉   | 2714/3886 [5:30:32<2:21:11,  7.23s/it]

 70%|██████▉   | 2715/3886 [5:30:37<2:06:18,  6.47s/it]
{'loss': 1.4266, 'grad_norm': 0.23371422463353927, 'learning_rate': 4.3971728602056065e-05, 'epoch': 0.7}

 70%|██████▉   | 2716/3886 [5:30:43<2:07:09,  6.52s/it]


 70%|██████▉   | 2718/3886 [5:31:00<2:24:45,  7.44s/it]

 70%|██████▉   | 2719/3886 [5:31:08<2:30:50,  7.76s/it]

 70%|██████▉   | 2720/3886 [5:31:14<2:20:39,  7.24s/it]

 70%|███████   | 2721/3886 [5:31:20<2:13:01,  6.85s/it]

 70%|███████   | 2722/3886 [5:31:28<2:18:35,  7.14s/it]
{'loss': 1.2094, 'grad_norm': 0.2220634198258951, 'learning_rate': 4.348939240817229e-05, 'epoch': 0.7}


 70%|███████   | 2724/3886 [5:31:40<2:05:24,  6.48s/it]

 70%|███████   | 2725/3886 [5:31:46<2:03:45,  6.40s/it]
{'loss': 1.3231, 'grad_norm': 0.2126455941344756, 'learning_rate': 4.3283264876492226e-05, 'epoch': 0.7}


 70%|███████   | 2727/3886 [5:32:09<2:58:36,  9.25s/it]

 70%|███████   | 2728/3886 [5:32:16<2:48:40,  8.74s/it]
{'loss': 1.3434, 'grad_norm': 0.25722741031015794, 'learning_rate': 4.30774919956388e-05, 'epoch': 0.7}


 70%|███████   | 2730/3886 [5:32:28<2:24:18,  7.49s/it]
{'loss': 1.4071, 'grad_norm': 0.25906560341554646, 'learning_rate': 4.2940507738806734e-05, 'epoch': 0.7}


 70%|███████   | 2732/3886 [5:32:43<2:17:32,  7.15s/it]

 70%|███████   | 2733/3886 [5:32:48<2:10:09,  6.77s/it]

 70%|███████   | 2734/3886 [5:32:56<2:16:40,  7.12s/it]

 70%|███████   | 2735/3886 [5:33:04<2:17:31,  7.17s/it]
{'loss': 1.2111, 'grad_norm': 0.22518192296174985, 'learning_rate': 4.259874169490759e-05, 'epoch': 0.7}


 70%|███████   | 2737/3886 [5:33:16<2:07:46,  6.67s/it]

 70%|███████   | 2738/3886 [5:33:23<2:09:21,  6.76s/it]

 70%|███████   | 2739/3886 [5:33:31<2:15:45,  7.10s/it]

 71%|███████   | 2740/3886 [5:33:38<2:14:12,  7.03s/it]

 71%|███████   | 2741/3886 [5:33:44<2:09:10,  6.77s/it]
{'loss': 1.3423, 'grad_norm': 0.2645600119862058, 'learning_rate': 4.2189939043117474e-05, 'epoch': 0.71}


 71%|███████   | 2743/3886 [5:33:56<2:04:42,  6.55s/it]
{'loss': 1.4269, 'grad_norm': 0.22882417320100307, 'learning_rate': 4.205399231026595e-05, 'epoch': 0.71}

 71%|███████   | 2744/3886 [5:34:03<2:05:18,  6.58s/it]


 71%|███████   | 2746/3886 [5:34:17<2:09:21,  6.81s/it]

 71%|███████   | 2747/3886 [5:34:30<2:48:12,  8.86s/it]

 71%|███████   | 2748/3886 [5:34:38<2:41:23,  8.51s/it]

 71%|███████   | 2749/3886 [5:34:46<2:36:55,  8.28s/it]

 71%|███████   | 2750/3886 [5:34:55<2:41:44,  8.54s/it]

 71%|███████   | 2751/3886 [5:35:01<2:27:11,  7.78s/it]
{'loss': 1.0279, 'grad_norm': 0.27443050678395303, 'learning_rate': 4.151181953958108e-05, 'epoch': 0.71}


 71%|███████   | 2753/3886 [5:35:14<2:17:59,  7.31s/it]

 71%|███████   | 2754/3886 [5:35:20<2:10:08,  6.90s/it]
{'loss': 1.3404, 'grad_norm': 0.23127293072184213, 'learning_rate': 4.130917395804402e-05, 'epoch': 0.71}

 71%|███████   | 2755/3886 [5:35:25<1:59:12,  6.32s/it]


 71%|███████   | 2757/3886 [5:35:36<1:50:49,  5.89s/it]
{'loss': 1.2803, 'grad_norm': 0.24963479928967963, 'learning_rate': 4.1106895371361833e-05, 'epoch': 0.71}

 71%|███████   | 2758/3886 [5:35:42<1:46:26,  5.66s/it]


 71%|███████   | 2760/3886 [5:35:58<2:14:18,  7.16s/it]

 71%|███████   | 2761/3886 [5:36:07<2:20:03,  7.47s/it]
{'loss': 1.2282, 'grad_norm': 0.24313309117916504, 'learning_rate': 4.0837763655836445e-05, 'epoch': 0.71}


 71%|███████   | 2763/3886 [5:36:22<2:22:34,  7.62s/it]

 71%|███████   | 2764/3886 [5:36:39<3:11:49, 10.26s/it]

 71%|███████   | 2765/3886 [5:36:45<2:46:04,  8.89s/it]

 71%|███████   | 2766/3886 [5:36:52<2:39:42,  8.56s/it]
{'loss': 1.3672, 'grad_norm': 0.2177239491589946, 'learning_rate': 4.050227421744145e-05, 'epoch': 0.71}

 71%|███████   | 2767/3886 [5:37:00<2:31:54,  8.14s/it]


 71%|███████▏  | 2769/3886 [5:37:12<2:15:58,  7.30s/it]

 71%|███████▏  | 2770/3886 [5:37:18<2:05:54,  6.77s/it]
{'loss': 1.2423, 'grad_norm': 0.214595466093866, 'learning_rate': 4.0234626467757865e-05, 'epoch': 0.71}


 71%|███████▏  | 2772/3886 [5:37:33<2:12:28,  7.13s/it]

 71%|███████▏  | 2773/3886 [5:37:39<2:06:27,  6.82s/it]

 71%|███████▏  | 2774/3886 [5:37:47<2:12:40,  7.16s/it]

 71%|███████▏  | 2775/3886 [5:37:54<2:11:50,  7.12s/it]
{'loss': 1.4344, 'grad_norm': 0.23545980726827184, 'learning_rate': 3.9901001413291884e-05, 'epoch': 0.71}


 71%|███████▏  | 2777/3886 [5:38:08<2:10:33,  7.06s/it]

 71%|███████▏  | 2778/3886 [5:38:22<2:48:50,  9.14s/it]
[2024-05-28 06:12:52,187] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.2805, 'grad_norm': 0.2074225243333994, 'learning_rate': 3.970132707694084e-05, 'epoch': 0.71}


 72%|███████▏  | 2780/3886 [5:38:36<2:32:51,  8.29s/it]
{'loss': 1.3498, 'grad_norm': 0.24377010569972715, 'learning_rate': 3.956842024726064e-05, 'epoch': 0.72}


 72%|███████▏  | 2782/3886 [5:38:51<2:20:36,  7.64s/it]

 72%|███████▏  | 2783/3886 [5:38:56<2:09:03,  7.02s/it]

 72%|███████▏  | 2784/3886 [5:39:02<2:03:53,  6.75s/it]

 72%|███████▏  | 2785/3886 [5:39:13<2:23:21,  7.81s/it]

 72%|███████▏  | 2786/3886 [5:39:19<2:15:46,  7.41s/it]

 72%|███████▏  | 2787/3886 [5:39:27<2:19:29,  7.62s/it]
{'loss': 1.3904, 'grad_norm': 0.21894436199369371, 'learning_rate': 3.910457134287377e-05, 'epoch': 0.72}

 72%|███████▏  | 2788/3886 [5:39:34<2:13:14,  7.28s/it]

 72%|███████▏  | 2789/3886 [5:39:40<2:08:10,  7.01s/it]

 72%|███████▏  | 2790/3886 [5:39:45<1:59:38,  6.55s/it]

 72%|███████▏  | 2791/3886 [5:39:52<1:59:26,  6.54s/it]


 72%|███████▏  | 2793/3886 [5:40:05<2:00:37,  6.62s/it]

 72%|███████▏  | 2794/3886 [5:40:11<1:55:43,  6.36s/it]
{'loss': 1.27, 'grad_norm': 0.23787762249880756, 'learning_rate': 3.864279557074975e-05, 'epoch': 0.72}

 72%|███████▏  | 2795/3886 [5:40:18<1:58:01,  6.49s/it]


 72%|███████▏  | 2797/3886 [5:40:32<2:04:53,  6.88s/it]

 72%|███████▏  | 2798/3886 [5:40:38<2:00:11,  6.63s/it]

 72%|███████▏  | 2799/3886 [5:40:44<1:54:53,  6.34s/it]

 72%|███████▏  | 2800/3886 [5:40:59<2:42:04,  8.95s/it]

 72%|███████▏  | 2801/3886 [5:41:04<2:20:42,  7.78s/it]

 72%|███████▏  | 2802/3886 [5:41:13<2:26:04,  8.09s/it]
{'loss': 1.1733, 'grad_norm': 0.26713100360707187, 'learning_rate': 3.811761052379611e-05, 'epoch': 0.72}


 72%|███████▏  | 2804/3886 [5:41:26<2:11:57,  7.32s/it]

 72%|███████▏  | 2805/3886 [5:41:33<2:08:55,  7.16s/it]
{'loss': 1.3985, 'grad_norm': 0.23869576650339114, 'learning_rate': 3.792137429008563e-05, 'epoch': 0.72}


 72%|███████▏  | 2807/3886 [5:41:53<2:28:28,  8.26s/it]

 72%|███████▏  | 2808/3886 [5:41:59<2:20:28,  7.82s/it]

 72%|███████▏  | 2809/3886 [5:42:05<2:07:48,  7.12s/it]

 72%|███████▏  | 2810/3886 [5:42:13<2:10:43,  7.29s/it]
{'loss': 1.2362, 'grad_norm': 0.23323876889350054, 'learning_rate': 3.759517712505361e-05, 'epoch': 0.72}


 72%|███████▏  | 2812/3886 [5:42:27<2:11:53,  7.37s/it]
{'loss': 1.3002, 'grad_norm': 0.22475821274611102, 'learning_rate': 3.746500144541717e-05, 'epoch': 0.72}

 72%|███████▏  | 2813/3886 [5:42:34<2:09:01,  7.21s/it]


 72%|███████▏  | 2815/3886 [5:42:48<2:06:55,  7.11s/it]
{'loss': 1.352, 'grad_norm': 0.22485709885359054, 'learning_rate': 3.72700638994283e-05, 'epoch': 0.72}


 72%|███████▏  | 2817/3886 [5:43:01<1:58:12,  6.63s/it]

 73%|███████▎  | 2818/3886 [5:43:07<1:58:01,  6.63s/it]

 73%|███████▎  | 2819/3886 [5:43:13<1:52:16,  6.31s/it]

 73%|███████▎  | 2820/3886 [5:43:19<1:52:15,  6.32s/it]

 73%|███████▎  | 2821/3886 [5:43:25<1:52:00,  6.31s/it]

 73%|███████▎  | 2822/3886 [5:43:33<1:57:31,  6.63s/it]
{'loss': 1.2542, 'grad_norm': 0.2310831468619558, 'learning_rate': 3.681673715013461e-05, 'epoch': 0.73}

 73%|███████▎  | 2823/3886 [5:43:40<2:00:56,  6.83s/it]

 73%|███████▎  | 2824/3886 [5:43:46<1:57:21,  6.63s/it]

 73%|███████▎  | 2825/3886 [5:43:52<1:52:54,  6.38s/it]


 73%|███████▎  | 2827/3886 [5:44:07<2:01:02,  6.86s/it]

 73%|███████▎  | 2828/3886 [5:44:14<2:01:51,  6.91s/it]

 73%|███████▎  | 2829/3886 [5:44:23<2:08:42,  7.31s/it]

 73%|███████▎  | 2830/3886 [5:44:37<2:45:26,  9.40s/it]
{'loss': 1.3922, 'grad_norm': 0.22476812448880212, 'learning_rate': 3.630128423474839e-05, 'epoch': 0.73}


 73%|███████▎  | 2832/3886 [5:44:51<2:23:29,  8.17s/it]

 73%|███████▎  | 2833/3886 [5:44:56<2:09:06,  7.36s/it]
{'loss': 1.3902, 'grad_norm': 0.25204353038823474, 'learning_rate': 3.610871839579273e-05, 'epoch': 0.73}


 73%|███████▎  | 2835/3886 [5:45:15<2:22:15,  8.12s/it]
{'loss': 1.4387, 'grad_norm': 0.20006892549935554, 'learning_rate': 3.598056304748709e-05, 'epoch': 0.73}


 73%|███████▎  | 2837/3886 [5:45:26<1:57:29,  6.72s/it]

 73%|███████▎  | 2838/3886 [5:45:35<2:10:26,  7.47s/it]

 73%|███████▎  | 2839/3886 [5:45:43<2:14:05,  7.68s/it]

 73%|███████▎  | 2840/3886 [5:45:51<2:13:56,  7.68s/it]

 73%|███████▎  | 2841/3886 [5:45:57<2:07:22,  7.31s/it]

 73%|███████▎  | 2842/3886 [5:46:03<1:58:47,  6.83s/it]
{'loss': 1.4178, 'grad_norm': 0.23692336024277585, 'learning_rate': 3.553342276331506e-05, 'epoch': 0.73}


 73%|███████▎  | 2844/3886 [5:46:17<2:02:38,  7.06s/it]
{'loss': 1.4263, 'grad_norm': 0.2741160214319952, 'learning_rate': 3.5406070842367955e-05, 'epoch': 0.73}

 73%|███████▎  | 2845/3886 [5:46:24<2:01:46,  7.02s/it]


 73%|███████▎  | 2847/3886 [5:46:37<1:55:23,  6.66s/it]

 73%|███████▎  | 2848/3886 [5:46:43<1:54:05,  6.59s/it]
{'loss': 1.2296, 'grad_norm': 0.2248683189000889, 'learning_rate': 3.515190589681876e-05, 'epoch': 0.73}

 73%|███████▎  | 2849/3886 [5:46:50<1:54:58,  6.65s/it]


 73%|███████▎  | 2851/3886 [5:47:07<2:07:22,  7.38s/it]
{'loss': 1.3092, 'grad_norm': 0.23878727255176274, 'learning_rate': 3.496175511234886e-05, 'epoch': 0.73}


 73%|███████▎  | 2853/3886 [5:47:25<2:16:43,  7.94s/it]

 73%|███████▎  | 2854/3886 [5:47:31<2:06:47,  7.37s/it]

 73%|███████▎  | 2855/3886 [5:47:38<2:03:25,  7.18s/it]

 73%|███████▎  | 2856/3886 [5:47:43<1:53:27,  6.61s/it]

 74%|███████▎  | 2857/3886 [5:47:51<2:04:05,  7.24s/it]

 74%|███████▎  | 2858/3886 [5:47:59<2:05:10,  7.31s/it]

 74%|███████▎  | 2859/3886 [5:48:05<2:00:08,  7.02s/it]

 74%|███████▎  | 2860/3886 [5:48:11<1:53:21,  6.63s/it]
{'loss': 1.4065, 'grad_norm': 0.22705871048332305, 'learning_rate': 3.4393747614598546e-05, 'epoch': 0.74}


 74%|███████▎  | 2862/3886 [5:48:25<1:55:47,  6.78s/it]

 74%|███████▎  | 2863/3886 [5:48:31<1:51:01,  6.51s/it]
{'loss': 1.1615, 'grad_norm': 0.26105643254640104, 'learning_rate': 3.42052306803043e-05, 'epoch': 0.74}


 74%|███████▎  | 2865/3886 [5:48:44<1:48:33,  6.38s/it]

 74%|███████▍  | 2866/3886 [5:48:49<1:43:40,  6.10s/it]

 74%|███████▍  | 2867/3886 [5:48:55<1:43:32,  6.10s/it]
{'loss': 1.3801, 'grad_norm': 0.22882279570624306, 'learning_rate': 3.395451495164792e-05, 'epoch': 0.74}


 74%|███████▍  | 2869/3886 [5:49:11<1:59:06,  7.03s/it]

 74%|███████▍  | 2870/3886 [5:49:17<1:52:49,  6.66s/it]
{'loss': 1.3289, 'grad_norm': 0.24527856598128212, 'learning_rate': 3.3766959817139655e-05, 'epoch': 0.74}


 74%|███████▍  | 2872/3886 [5:49:33<2:03:27,  7.30s/it]

 74%|███████▍  | 2873/3886 [5:49:40<2:00:53,  7.16s/it]

 74%|███████▍  | 2874/3886 [5:49:45<1:52:27,  6.67s/it]

 74%|███████▍  | 2875/3886 [5:49:52<1:51:45,  6.63s/it]

 74%|███████▍  | 2876/3886 [5:49:59<1:53:39,  6.75s/it]

 74%|███████▍  | 2877/3886 [5:50:05<1:49:36,  6.52s/it]
{'loss': 1.3215, 'grad_norm': 0.2520662499371905, 'learning_rate': 3.333094380061124e-05, 'epoch': 0.74}

 74%|███████▍  | 2878/3886 [5:50:14<2:03:25,  7.35s/it]


 74%|███████▍  | 2880/3886 [5:50:26<1:51:15,  6.64s/it]
{'loss': 1.3303, 'grad_norm': 0.288870327558791, 'learning_rate': 3.314477373392173e-05, 'epoch': 0.74}


 74%|███████▍  | 2882/3886 [5:50:37<1:44:34,  6.25s/it]

 74%|███████▍  | 2883/3886 [5:50:43<1:42:50,  6.15s/it]

 74%|███████▍  | 2884/3886 [5:50:50<1:44:28,  6.26s/it]

 74%|███████▍  | 2885/3886 [5:50:56<1:41:53,  6.11s/it]

 74%|███████▍  | 2886/3886 [5:51:02<1:43:43,  6.22s/it]

 74%|███████▍  | 2887/3886 [5:51:08<1:41:37,  6.10s/it]
{'loss': 1.2182, 'grad_norm': 0.2399059826321002, 'learning_rate': 3.271200465717219e-05, 'epoch': 0.74}

 74%|███████▍  | 2888/3886 [5:51:16<1:52:40,  6.77s/it]

 74%|███████▍  | 2889/3886 [5:51:22<1:48:29,  6.53s/it]


 74%|███████▍  | 2891/3886 [5:51:39<2:08:14,  7.73s/it]
{'loss': 1.4169, 'grad_norm': 0.21435878529138444, 'learning_rate': 3.2465735605347816e-05, 'epoch': 0.74}


 74%|███████▍  | 2893/3886 [5:51:54<2:04:40,  7.53s/it]
{'loss': 1.3455, 'grad_norm': 0.22439071275945893, 'learning_rate': 3.234288243724013e-05, 'epoch': 0.74}

 74%|███████▍  | 2894/3886 [5:51:58<1:49:48,  6.64s/it]


 75%|███████▍  | 2896/3886 [5:52:15<2:04:03,  7.52s/it]

 75%|███████▍  | 2897/3886 [5:52:22<2:01:24,  7.37s/it]

 75%|███████▍  | 2898/3886 [5:52:28<1:54:10,  6.93s/it]
{'loss': 1.4594, 'grad_norm': 0.2364572866922922, 'learning_rate': 3.203657288187838e-05, 'epoch': 0.75}


 75%|███████▍  | 2900/3886 [5:52:40<1:45:36,  6.43s/it]

 75%|███████▍  | 2901/3886 [5:52:48<1:52:30,  6.85s/it]
{'loss': 1.4074, 'grad_norm': 0.22906156566782654, 'learning_rate': 3.185335344395534e-05, 'epoch': 0.75}


 75%|███████▍  | 2903/3886 [5:52:58<1:35:30,  5.83s/it]

 75%|███████▍  | 2904/3886 [5:53:03<1:35:05,  5.81s/it]
{'loss': 1.2265, 'grad_norm': 0.26269940131496516, 'learning_rate': 3.167056012831322e-05, 'epoch': 0.75}

 75%|███████▍  | 2905/3886 [5:53:09<1:32:56,  5.68s/it]


 75%|███████▍  | 2907/3886 [5:53:20<1:30:28,  5.54s/it]
{'loss': 1.5341, 'grad_norm': 0.255181223618751, 'learning_rate': 3.148819407796208e-05, 'epoch': 0.75}


 75%|███████▍  | 2909/3886 [5:53:32<1:35:34,  5.87s/it]

 75%|███████▍  | 2910/3886 [5:53:38<1:36:56,  5.96s/it]

 75%|███████▍  | 2911/3886 [5:53:44<1:36:06,  5.91s/it]

 75%|███████▍  | 2912/3886 [5:53:51<1:44:25,  6.43s/it]
{'loss': 1.1792, 'grad_norm': 0.2339068356631678, 'learning_rate': 3.118520323513767e-05, 'epoch': 0.75}

 75%|███████▍  | 2913/3886 [5:53:59<1:49:44,  6.77s/it]


 75%|███████▌  | 2915/3886 [5:54:11<1:45:50,  6.54s/it]

 75%|███████▌  | 2916/3886 [5:54:20<1:56:33,  7.21s/it]

 75%|███████▌  | 2917/3886 [5:54:26<1:51:09,  6.88s/it]
{'loss': 1.1961, 'grad_norm': 0.2483768465272796, 'learning_rate': 3.088340766964193e-05, 'epoch': 0.75}


 75%|███████▌  | 2919/3886 [5:54:40<1:50:33,  6.86s/it]

 75%|███████▌  | 2920/3886 [5:54:45<1:43:45,  6.44s/it]
{'loss': 1.5029, 'grad_norm': 0.23626737114569604, 'learning_rate': 3.070290624501408e-05, 'epoch': 0.75}


 75%|███████▌  | 2922/3886 [5:54:58<1:42:01,  6.35s/it]

 75%|███████▌  | 2923/3886 [5:55:04<1:42:34,  6.39s/it]

 75%|███████▌  | 2924/3886 [5:55:12<1:50:52,  6.92s/it]

 75%|███████▌  | 2925/3886 [5:55:21<2:01:10,  7.57s/it]

 75%|███████▌  | 2926/3886 [5:55:29<2:03:56,  7.75s/it]

 75%|███████▌  | 2927/3886 [5:55:35<1:53:53,  7.13s/it]
{'loss': 1.1124, 'grad_norm': 0.2511533055254942, 'learning_rate': 3.028342331791362e-05, 'epoch': 0.75}


 75%|███████▌  | 2929/3886 [5:55:53<2:06:29,  7.93s/it]

 75%|███████▌  | 2930/3886 [5:55:58<1:52:35,  7.07s/it]

 75%|███████▌  | 2931/3886 [5:56:04<1:43:16,  6.49s/it]
{'loss': 1.4899, 'grad_norm': 0.23578697017040437, 'learning_rate': 3.0044783501893616e-05, 'epoch': 0.75}


 75%|███████▌  | 2933/3886 [5:56:16<1:42:55,  6.48s/it]

 76%|███████▌  | 2934/3886 [5:56:22<1:37:29,  6.14s/it]
{'loss': 1.3789, 'grad_norm': 0.2666938997017266, 'learning_rate': 2.9866313831440153e-05, 'epoch': 0.75}


 76%|███████▌  | 2936/3886 [5:56:35<1:43:48,  6.56s/it]
{'loss': 1.3014, 'grad_norm': 0.2648561226293246, 'learning_rate': 2.974757761973973e-05, 'epoch': 0.76}

 76%|███████▌  | 2937/3886 [5:56:41<1:37:03,  6.14s/it]


 76%|███████▌  | 2939/3886 [5:56:52<1:32:52,  5.88s/it]

 76%|███████▌  | 2940/3886 [5:57:02<1:49:49,  6.97s/it]
{'loss': 1.2107, 'grad_norm': 0.24494782427510894, 'learning_rate': 2.9510691245597888e-05, 'epoch': 0.76}

 76%|███████▌  | 2941/3886 [5:57:09<1:49:27,  6.95s/it]


 76%|███████▌  | 2943/3886 [5:57:20<1:39:43,  6.34s/it]
{'loss': 1.4273, 'grad_norm': 0.2717583167961577, 'learning_rate': 2.9333540553923832e-05, 'epoch': 0.76}

 76%|███████▌  | 2944/3886 [5:57:26<1:39:48,  6.36s/it]

 76%|███████▌  | 2945/3886 [5:57:33<1:40:14,  6.39s/it]


 76%|███████▌  | 2947/3886 [5:57:51<2:06:51,  8.11s/it]

 76%|███████▌  | 2948/3886 [5:58:02<2:17:50,  8.82s/it]

 76%|███████▌  | 2949/3886 [5:58:08<2:04:53,  8.00s/it]

 76%|███████▌  | 2950/3886 [5:58:20<2:25:04,  9.30s/it]

 76%|███████▌  | 2951/3886 [5:58:27<2:11:19,  8.43s/it]

 76%|███████▌  | 2952/3886 [5:58:34<2:08:17,  8.24s/it]

 76%|███████▌  | 2953/3886 [5:58:41<2:02:13,  7.86s/it]

 76%|███████▌  | 2954/3886 [5:58:48<1:54:15,  7.36s/it]

 76%|███████▌  | 2955/3886 [5:58:53<1:47:39,  6.94s/it]

 76%|███████▌  | 2956/3886 [5:59:00<1:45:01,  6.78s/it]

 76%|███████▌  | 2957/3886 [5:59:06<1:41:10,  6.53s/it]
{'loss': 1.313, 'grad_norm': 0.24621259694563638, 'learning_rate': 2.851269777161896e-05, 'epoch': 0.76}

 76%|███████▌  | 2958/3886 [5:59:13<1:43:16,  6.68s/it]


 76%|███████▌  | 2960/3886 [5:59:32<2:04:01,  8.04s/it]
{'loss': 1.3025, 'grad_norm': 0.2305217710712844, 'learning_rate': 2.8338065624588038e-05, 'epoch': 0.76}


 76%|███████▌  | 2962/3886 [5:59:46<1:57:29,  7.63s/it]

 76%|███████▌  | 2963/3886 [6:00:03<2:41:43, 10.51s/it]
[2024-05-28 06:34:33,628] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▋  | 2964/3886 [6:00:10<2:21:14,  9.19s/it]
{'loss': 1.3267, 'grad_norm': 0.2364728628127585, 'learning_rate': 2.810592000001342e-05, 'epoch': 0.76}


 76%|███████▋  | 2966/3886 [6:00:22<1:57:16,  7.65s/it]

 76%|███████▋  | 2967/3886 [6:00:28<1:49:55,  7.18s/it]

 76%|███████▋  | 2968/3886 [6:00:35<1:45:04,  6.87s/it]

 76%|███████▋  | 2969/3886 [6:00:42<1:45:37,  6.91s/it]

 76%|███████▋  | 2970/3886 [6:00:48<1:41:25,  6.64s/it]

 76%|███████▋  | 2971/3886 [6:00:54<1:41:08,  6.63s/it]
{'loss': 1.3046, 'grad_norm': 0.2501203779259398, 'learning_rate': 2.7701589800254247e-05, 'epoch': 0.76}


 77%|███████▋  | 2973/3886 [6:01:06<1:34:43,  6.23s/it]

 77%|███████▋  | 2974/3886 [6:01:13<1:35:52,  6.31s/it]

 77%|███████▋  | 2975/3886 [6:01:22<1:47:28,  7.08s/it]

 77%|███████▋  | 2976/3886 [6:01:26<1:36:32,  6.37s/it]
{'loss': 1.0916, 'grad_norm': 0.22959343232942556, 'learning_rate': 2.7414288656365228e-05, 'epoch': 0.77}


 77%|███████▋  | 2978/3886 [6:01:39<1:34:12,  6.22s/it]

 77%|███████▋  | 2979/3886 [6:01:46<1:37:52,  6.47s/it]

 77%|███████▋  | 2980/3886 [6:01:52<1:37:02,  6.43s/it]

 77%|███████▋  | 2981/3886 [6:01:58<1:35:49,  6.35s/it]

 77%|███████▋  | 2982/3886 [6:02:06<1:42:27,  6.80s/it]

 77%|███████▋  | 2983/3886 [6:02:12<1:40:25,  6.67s/it]

 77%|███████▋  | 2984/3886 [6:02:21<1:47:45,  7.17s/it]

 77%|███████▋  | 2985/3886 [6:02:28<1:47:38,  7.17s/it]

 77%|███████▋  | 2986/3886 [6:02:34<1:41:54,  6.79s/it]

 77%|███████▋  | 2987/3886 [6:02:40<1:38:06,  6.55s/it]
{'loss': 1.3626, 'grad_norm': 0.2575321271753135, 'learning_rate': 2.6786671065811407e-05, 'epoch': 0.77}

 77%|███████▋  | 2988/3886 [6:02:45<1:31:54,  6.14s/it]


 77%|███████▋  | 2990/3886 [6:03:00<1:39:48,  6.68s/it]

 77%|███████▋  | 2991/3886 [6:03:08<1:48:38,  7.28s/it]

 77%|███████▋  | 2992/3886 [6:03:15<1:42:57,  6.91s/it]
{'loss': 1.2307, 'grad_norm': 0.2418656911247783, 'learning_rate': 2.650342186334236e-05, 'epoch': 0.77}


 77%|███████▋  | 2994/3886 [6:03:27<1:35:40,  6.44s/it]

 77%|███████▋  | 2995/3886 [6:03:34<1:41:14,  6.82s/it]

 77%|███████▋  | 2996/3886 [6:03:40<1:37:38,  6.58s/it]
{'loss': 1.2312, 'grad_norm': 0.23739889050054927, 'learning_rate': 2.62777414161155e-05, 'epoch': 0.77}

 77%|███████▋  | 2997/3886 [6:03:49<1:46:26,  7.18s/it]


 77%|███████▋  | 2999/3886 [6:04:00<1:34:09,  6.37s/it]
{'loss': 1.225, 'grad_norm': 0.2894071739675878, 'learning_rate': 2.610901876142543e-05, 'epoch': 0.77}

 77%|███████▋  | 3000/3886 [6:04:06<1:29:09,  6.04s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 77%|███████▋  | 3001/3886 [6:04:42<3:44:32, 15.22s/it]
{'loss': 1.298, 'grad_norm': 0.23567031975915054, 'learning_rate': 2.599679361646763e-05, 'epoch': 0.77}

 77%|███████▋  | 3002/3886 [6:04:47<2:59:58, 12.22s/it]


 77%|███████▋  | 3004/3886 [6:05:00<2:15:57,  9.25s/it]
{'loss': 1.1748, 'grad_norm': 0.20621479391776276, 'learning_rate': 2.5828841616002097e-05, 'epoch': 0.77}

 77%|███████▋  | 3005/3886 [6:05:08<2:07:51,  8.71s/it]
[2024-05-28 06:39:55,611] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 3006/3886 [6:05:25<2:48:07, 11.46s/it]

 77%|███████▋  | 3007/3886 [6:05:34<2:33:37, 10.49s/it]


 77%|███████▋  | 3009/3886 [6:05:56<2:38:39, 10.85s/it]

 77%|███████▋  | 3010/3886 [6:06:03<2:18:52,  9.51s/it]

 77%|███████▋  | 3011/3886 [6:06:08<2:02:58,  8.43s/it]
{'loss': 1.1996, 'grad_norm': 0.24361775352274395, 'learning_rate': 2.5438759066700913e-05, 'epoch': 0.77}


 78%|███████▊  | 3013/3886 [6:06:21<1:47:19,  7.38s/it]

 78%|███████▊  | 3014/3886 [6:06:28<1:46:45,  7.35s/it]

 78%|███████▊  | 3015/3886 [6:06:37<1:52:31,  7.75s/it]
{'loss': 1.5074, 'grad_norm': 0.212423915707587, 'learning_rate': 2.5216993582282232e-05, 'epoch': 0.78}


 78%|███████▊  | 3017/3886 [6:06:49<1:41:02,  6.98s/it]

 78%|███████▊  | 3018/3886 [6:06:59<1:54:56,  7.95s/it]

 78%|███████▊  | 3019/3886 [6:07:14<2:24:51, 10.02s/it]

 78%|███████▊  | 3020/3886 [6:07:18<2:00:37,  8.36s/it]

 78%|███████▊  | 3021/3886 [6:07:25<1:53:35,  7.88s/it]
{'loss': 1.4281, 'grad_norm': 0.21686960445742356, 'learning_rate': 2.4885904854772812e-05, 'epoch': 0.78}


 78%|███████▊  | 3023/3886 [6:07:37<1:36:46,  6.73s/it]

 78%|███████▊  | 3024/3886 [6:07:43<1:34:44,  6.59s/it]

 78%|███████▊  | 3025/3886 [6:07:54<1:54:14,  7.96s/it]

 78%|███████▊  | 3026/3886 [6:08:00<1:45:48,  7.38s/it]

 78%|███████▊  | 3027/3886 [6:08:12<2:06:14,  8.82s/it]
{'loss': 1.4282, 'grad_norm': 0.22074285114149605, 'learning_rate': 2.455669488380584e-05, 'epoch': 0.78}


 78%|███████▊  | 3029/3886 [6:08:26<1:53:17,  7.93s/it]

 78%|███████▊  | 3030/3886 [6:08:33<1:47:41,  7.55s/it]

 78%|███████▊  | 3031/3886 [6:08:39<1:39:09,  6.96s/it]

 78%|███████▊  | 3032/3886 [6:08:46<1:42:38,  7.21s/it]

 78%|███████▊  | 3033/3886 [6:08:55<1:48:47,  7.65s/it]

 78%|███████▊  | 3034/3886 [6:09:03<1:50:20,  7.77s/it]
{'loss': 1.3386, 'grad_norm': 0.22212640074182347, 'learning_rate': 2.4175002106544632e-05, 'epoch': 0.78}

 78%|███████▊  | 3035/3886 [6:09:12<1:53:19,  7.99s/it]

 78%|███████▊  | 3036/3886 [6:09:17<1:43:48,  7.33s/it]

 78%|███████▊  | 3037/3886 [6:09:23<1:37:25,  6.89s/it]


 78%|███████▊  | 3039/3886 [6:09:37<1:36:14,  6.82s/it]

 78%|███████▊  | 3040/3886 [6:09:52<2:14:10,  9.52s/it]

 78%|███████▊  | 3041/3886 [6:10:00<2:06:56,  9.01s/it]

 78%|███████▊  | 3042/3886 [6:10:06<1:53:26,  8.06s/it]
{'loss': 1.4465, 'grad_norm': 0.23576491570725874, 'learning_rate': 2.3741943437077906e-05, 'epoch': 0.78}


 78%|███████▊  | 3044/3886 [6:10:18<1:40:10,  7.14s/it]

 78%|███████▊  | 3045/3886 [6:10:27<1:45:47,  7.55s/it]

 78%|███████▊  | 3046/3886 [6:10:33<1:37:40,  6.98s/it]

 78%|███████▊  | 3047/3886 [6:10:39<1:34:23,  6.75s/it]

 78%|███████▊  | 3048/3886 [6:10:46<1:37:28,  6.98s/it]
{'loss': 1.2948, 'grad_norm': 0.24312771689688223, 'learning_rate': 2.341937364819241e-05, 'epoch': 0.78}

 78%|███████▊  | 3049/3886 [6:10:55<1:46:17,  7.62s/it]


 79%|███████▊  | 3051/3886 [6:11:13<1:49:04,  7.84s/it]
{'loss': 1.2814, 'grad_norm': 0.26979137238649775, 'learning_rate': 2.3258806540414103e-05, 'epoch': 0.79}


 79%|███████▊  | 3053/3886 [6:11:25<1:36:19,  6.94s/it]
{'loss': 1.3995, 'grad_norm': 0.2345122305130498, 'learning_rate': 2.3152028331234156e-05, 'epoch': 0.79}


 79%|███████▊  | 3055/3886 [6:11:41<1:42:51,  7.43s/it]

 79%|███████▊  | 3056/3886 [6:11:49<1:46:29,  7.70s/it]

 79%|███████▊  | 3057/3886 [6:11:54<1:36:33,  6.99s/it]
{'loss': 1.3469, 'grad_norm': 0.23331001525893635, 'learning_rate': 2.2939112918326123e-05, 'epoch': 0.79}


 79%|███████▊  | 3059/3886 [6:12:06<1:29:05,  6.46s/it]
{'loss': 1.3508, 'grad_norm': 0.2243913139629363, 'learning_rate': 2.2832976306315612e-05, 'epoch': 0.79}

 79%|███████▊  | 3060/3886 [6:12:12<1:26:07,  6.26s/it]

 79%|███████▉  | 3061/3886 [6:12:26<1:57:51,  8.57s/it]


 79%|███████▉  | 3063/3886 [6:12:38<1:41:22,  7.39s/it]

 79%|███████▉  | 3064/3886 [6:12:43<1:31:34,  6.68s/it]
{'loss': 1.4386, 'grad_norm': 0.2160583794157031, 'learning_rate': 2.2568573667027217e-05, 'epoch': 0.79}


 79%|███████▉  | 3066/3886 [6:12:56<1:25:59,  6.29s/it]
{'loss': 1.3296, 'grad_norm': 0.22431476876667228, 'learning_rate': 2.2463188939154112e-05, 'epoch': 0.79}

 79%|███████▉  | 3067/3886 [6:13:02<1:25:13,  6.24s/it]


 79%|███████▉  | 3069/3886 [6:13:14<1:25:42,  6.29s/it]

 79%|███████▉  | 3070/3886 [6:13:19<1:21:00,  5.96s/it]
{'loss': 1.2043, 'grad_norm': 0.23754457185269628, 'learning_rate': 2.2253066228084674e-05, 'epoch': 0.79}


 79%|███████▉  | 3072/3886 [6:13:32<1:20:59,  5.97s/it]

 79%|███████▉  | 3073/3886 [6:13:39<1:26:40,  6.40s/it]
{'loss': 1.4186, 'grad_norm': 0.25575578568694196, 'learning_rate': 2.209604124574335e-05, 'epoch': 0.79}


 79%|███████▉  | 3075/3886 [6:13:53<1:29:18,  6.61s/it]

 79%|███████▉  | 3076/3886 [6:13:59<1:29:03,  6.60s/it]
{'loss': 1.2324, 'grad_norm': 0.22878509993680407, 'learning_rate': 2.1939503398342286e-05, 'epoch': 0.79}


 79%|███████▉  | 3078/3886 [6:14:10<1:22:47,  6.15s/it]

 79%|███████▉  | 3079/3886 [6:14:21<1:39:24,  7.39s/it]

 79%|███████▉  | 3080/3886 [6:14:27<1:33:48,  6.98s/it]
{'loss': 1.4671, 'grad_norm': 0.22687173208834138, 'learning_rate': 2.1731545725309386e-05, 'epoch': 0.79}

 79%|███████▉  | 3081/3886 [6:14:32<1:26:20,  6.43s/it]


 79%|███████▉  | 3083/3886 [6:14:47<1:32:14,  6.89s/it]
{'loss': 1.4053, 'grad_norm': 0.22804751607371826, 'learning_rate': 2.157614832740319e-05, 'epoch': 0.79}


 79%|███████▉  | 3085/3886 [6:15:01<1:31:28,  6.85s/it]

 79%|███████▉  | 3086/3886 [6:15:07<1:29:08,  6.69s/it]
{'loss': 1.2909, 'grad_norm': 0.23219093893268983, 'learning_rate': 2.1421241315337525e-05, 'epoch': 0.79}

 79%|███████▉  | 3087/3886 [6:15:12<1:23:35,  6.28s/it]


 79%|███████▉  | 3089/3886 [6:15:29<1:38:56,  7.45s/it]
{'loss': 1.0639, 'grad_norm': 0.23470435498624573, 'learning_rate': 2.1266825657748977e-05, 'epoch': 0.79}


 80%|███████▉  | 3091/3886 [6:15:41<1:28:09,  6.65s/it]

 80%|███████▉  | 3092/3886 [6:15:51<1:41:44,  7.69s/it]

 80%|███████▉  | 3093/3886 [6:15:58<1:38:43,  7.47s/it]
{'loss': 1.356, 'grad_norm': 0.25051918866422146, 'learning_rate': 2.1061704111916992e-05, 'epoch': 0.8}


 80%|███████▉  | 3095/3886 [6:16:14<1:44:00,  7.89s/it]

 80%|███████▉  | 3096/3886 [6:16:19<1:34:17,  7.16s/it]

 80%|███████▉  | 3097/3886 [6:16:28<1:39:52,  7.59s/it]

 80%|███████▉  | 3098/3886 [6:16:35<1:37:21,  7.41s/it]

 80%|███████▉  | 3099/3886 [6:16:51<2:10:47,  9.97s/it]

 80%|███████▉  | 3100/3886 [6:17:06<2:29:47, 11.43s/it]

 80%|███████▉  | 3101/3886 [6:17:12<2:09:25,  9.89s/it]

 80%|███████▉  | 3102/3886 [6:17:18<1:52:53,  8.64s/it]
{'loss': 1.2883, 'grad_norm': 0.22323945430816422, 'learning_rate': 2.060339250784359e-05, 'epoch': 0.8}


 80%|███████▉  | 3104/3886 [6:17:31<1:41:08,  7.76s/it]

 80%|███████▉  | 3105/3886 [6:17:37<1:34:27,  7.26s/it]
{'loss': 1.266, 'grad_norm': 0.23106186742490287, 'learning_rate': 2.045161363954806e-05, 'epoch': 0.8}

 80%|███████▉  | 3106/3886 [6:17:44<1:32:34,  7.12s/it]


 80%|███████▉  | 3108/3886 [6:17:59<1:36:50,  7.47s/it]

 80%|████████  | 3109/3886 [6:18:05<1:30:36,  7.00s/it]

 80%|████████  | 3110/3886 [6:18:12<1:28:02,  6.81s/it]

 80%|████████  | 3111/3886 [6:18:17<1:23:33,  6.47s/it]
{'loss': 1.3326, 'grad_norm': 0.2496774667590018, 'learning_rate': 2.014954910165506e-05, 'epoch': 0.8}

 80%|████████  | 3112/3886 [6:18:26<1:32:46,  7.19s/it]


 80%|████████  | 3114/3886 [6:18:43<1:42:28,  7.96s/it]

 80%|████████  | 3115/3886 [6:18:49<1:35:56,  7.47s/it]

 80%|████████  | 3116/3886 [6:18:55<1:29:21,  6.96s/it]
{'loss': 1.3226, 'grad_norm': 0.2231663086478887, 'learning_rate': 1.9899353990316806e-05, 'epoch': 0.8}

 80%|████████  | 3117/3886 [6:19:03<1:31:59,  7.18s/it]


 80%|████████  | 3119/3886 [6:19:21<1:42:38,  8.03s/it]
{'loss': 1.1593, 'grad_norm': 0.2379738224433829, 'learning_rate': 1.9749904473594437e-05, 'epoch': 0.8}

 80%|████████  | 3120/3886 [6:19:28<1:38:04,  7.68s/it]


 80%|████████  | 3122/3886 [6:19:41<1:28:49,  6.98s/it]

 80%|████████  | 3123/3886 [6:19:49<1:35:31,  7.51s/it]

 80%|████████  | 3124/3886 [6:19:55<1:28:07,  6.94s/it]

 80%|████████  | 3125/3886 [6:20:01<1:25:40,  6.75s/it]

 80%|████████  | 3126/3886 [6:20:07<1:22:56,  6.55s/it]

 80%|████████  | 3127/3886 [6:20:14<1:21:18,  6.43s/it]

 80%|████████  | 3128/3886 [6:20:24<1:35:16,  7.54s/it]

 81%|████████  | 3129/3886 [6:20:29<1:27:59,  6.97s/it]
{'loss': 1.2051, 'grad_norm': 0.21929207545589838, 'learning_rate': 1.9255368793706075e-05, 'epoch': 0.81}


 81%|████████  | 3131/3886 [6:20:41<1:20:48,  6.42s/it]

 81%|████████  | 3132/3886 [6:20:48<1:20:29,  6.40s/it]

 81%|████████  | 3133/3886 [6:20:53<1:15:07,  5.99s/it]
{'loss': 1.2874, 'grad_norm': 0.25798487849933127, 'learning_rate': 1.9059123395336475e-05, 'epoch': 0.81}

 81%|████████  | 3134/3886 [6:20:58<1:12:27,  5.78s/it]

 81%|████████  | 3135/3886 [6:21:05<1:15:07,  6.00s/it]


 81%|████████  | 3137/3886 [6:21:18<1:18:47,  6.31s/it]

 81%|████████  | 3138/3886 [6:21:25<1:24:29,  6.78s/it]

 81%|████████  | 3139/3886 [6:21:37<1:43:51,  8.34s/it]

 81%|████████  | 3140/3886 [6:21:47<1:50:11,  8.86s/it]

 81%|████████  | 3141/3886 [6:21:53<1:37:57,  7.89s/it]
{'loss': 1.3037, 'grad_norm': 0.23283706698913673, 'learning_rate': 1.8669334101567047e-05, 'epoch': 0.81}

 81%|████████  | 3142/3886 [6:21:59<1:29:22,  7.21s/it]


 81%|████████  | 3144/3886 [6:22:11<1:23:50,  6.78s/it]
{'loss': 1.4083, 'grad_norm': 0.24415316126283235, 'learning_rate': 1.8524094551664906e-05, 'epoch': 0.81}


 81%|████████  | 3146/3886 [6:22:29<1:41:32,  8.23s/it]
{'loss': 1.2617, 'grad_norm': 0.23617144073910212, 'learning_rate': 1.842755116823984e-05, 'epoch': 0.81}


 81%|████████  | 3148/3886 [6:22:40<1:24:06,  6.84s/it]

 81%|████████  | 3149/3886 [6:22:47<1:25:14,  6.94s/it]

 81%|████████  | 3150/3886 [6:22:56<1:30:00,  7.34s/it]

 81%|████████  | 3151/3886 [6:23:01<1:23:42,  6.83s/it]

 81%|████████  | 3152/3886 [6:23:06<1:15:49,  6.20s/it]
{'loss': 1.4028, 'grad_norm': 0.27040615947571756, 'learning_rate': 1.8139282285973348e-05, 'epoch': 0.81}

 81%|████████  | 3153/3886 [6:23:12<1:16:45,  6.28s/it]


 81%|████████  | 3155/3886 [6:23:23<1:09:59,  5.74s/it]

 81%|████████  | 3156/3886 [6:23:28<1:06:41,  5.48s/it]

 81%|████████  | 3157/3886 [6:23:34<1:08:35,  5.65s/it]

 81%|████████▏ | 3158/3886 [6:23:39<1:06:59,  5.52s/it]
{'loss': 1.4375, 'grad_norm': 0.23197999710644843, 'learning_rate': 1.7853060907024057e-05, 'epoch': 0.81}


 81%|████████▏ | 3160/3886 [6:23:52<1:11:27,  5.91s/it]

 81%|████████▏ | 3161/3886 [6:24:02<1:26:10,  7.13s/it]

 81%|████████▏ | 3162/3886 [6:24:11<1:34:46,  7.85s/it]

 81%|████████▏ | 3163/3886 [6:24:18<1:31:03,  7.56s/it]

 81%|████████▏ | 3164/3886 [6:24:30<1:46:09,  8.82s/it]

 81%|████████▏ | 3165/3886 [6:24:39<1:49:12,  9.09s/it]
{'loss': 1.4404, 'grad_norm': 0.23065765015625012, 'learning_rate': 1.7521733329584354e-05, 'epoch': 0.81}
[2024-05-28 06:59:24,981] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 81%|████████▏ | 3167/3886 [6:25:02<1:57:20,  9.79s/it]

 82%|████████▏ | 3168/3886 [6:25:08<1:44:01,  8.69s/it]

 82%|████████▏ | 3169/3886 [6:25:15<1:38:40,  8.26s/it]

 82%|████████▏ | 3170/3886 [6:25:22<1:32:44,  7.77s/it]

 82%|████████▏ | 3171/3886 [6:25:28<1:26:29,  7.26s/it]

 82%|████████▏ | 3172/3886 [6:25:36<1:27:40,  7.37s/it]

 82%|████████▏ | 3173/3886 [6:25:42<1:22:53,  6.98s/it]

 82%|████████▏ | 3174/3886 [6:25:50<1:28:34,  7.46s/it]

 82%|████████▏ | 3175/3886 [6:26:02<1:43:33,  8.74s/it]

 82%|████████▏ | 3176/3886 [6:26:10<1:39:27,  8.40s/it]

 82%|████████▏ | 3177/3886 [6:26:16<1:32:17,  7.81s/it]

 82%|████████▏ | 3178/3886 [6:26:22<1:25:40,  7.26s/it]

 82%|████████▏ | 3179/3886 [6:26:28<1:21:15,  6.90s/it]

 82%|████████▏ | 3180/3886 [6:26:36<1:23:38,  7.11s/it]
{'loss': 1.3277, 'grad_norm': 0.2607403619478604, 'learning_rate': 1.682121518426205e-05, 'epoch': 0.82}


 82%|████████▏ | 3182/3886 [6:26:48<1:16:34,  6.53s/it]

 82%|████████▏ | 3183/3886 [6:26:54<1:14:42,  6.38s/it]
{'loss': 1.1692, 'grad_norm': 0.22481133783719934, 'learning_rate': 1.66826684226968e-05, 'epoch': 0.82}


 82%|████████▏ | 3185/3886 [6:27:18<1:49:47,  9.40s/it]

 82%|████████▏ | 3186/3886 [6:27:22<1:33:07,  7.98s/it]
{'loss': 0.9871, 'grad_norm': 0.2644653671943538, 'learning_rate': 1.654464264599531e-05, 'epoch': 0.82}

 82%|████████▏ | 3187/3886 [6:27:29<1:27:51,  7.54s/it]


 82%|████████▏ | 3189/3886 [6:27:44<1:28:09,  7.59s/it]

 82%|████████▏ | 3190/3886 [6:27:52<1:30:13,  7.78s/it]

 82%|████████▏ | 3191/3886 [6:28:00<1:29:05,  7.69s/it]
{'loss': 1.2911, 'grad_norm': 0.23281217977610213, 'learning_rate': 1.6315759771652827e-05, 'epoch': 0.82}


 82%|████████▏ | 3193/3886 [6:28:12<1:19:43,  6.90s/it]

 82%|████████▏ | 3194/3886 [6:28:18<1:14:12,  6.43s/it]

 82%|████████▏ | 3195/3886 [6:28:25<1:18:00,  6.77s/it]
{'loss': 1.2628, 'grad_norm': 0.22277048500854413, 'learning_rate': 1.6133699839530568e-05, 'epoch': 0.82}


 82%|████████▏ | 3197/3886 [6:28:36<1:10:16,  6.12s/it]

 82%|████████▏ | 3198/3886 [6:28:43<1:12:58,  6.36s/it]

 82%|████████▏ | 3199/3886 [6:28:50<1:15:09,  6.56s/it]

 82%|████████▏ | 3200/3886 [6:28:56<1:12:27,  6.34s/it]
{'loss': 1.3816, 'grad_norm': 0.24205263372109362, 'learning_rate': 1.5907436203968506e-05, 'epoch': 0.82}

 82%|████████▏ | 3201/3886 [6:29:03<1:14:54,  6.56s/it]


 82%|████████▏ | 3203/3886 [6:29:14<1:07:59,  5.97s/it]

 82%|████████▏ | 3204/3886 [6:29:20<1:07:04,  5.90s/it]

 82%|████████▏ | 3205/3886 [6:29:26<1:06:40,  5.87s/it]
{'loss': 1.31, 'grad_norm': 0.24536105573960731, 'learning_rate': 1.5682633212624475e-05, 'epoch': 0.82}


 83%|████████▎ | 3207/3886 [6:29:38<1:08:17,  6.03s/it]

 83%|████████▎ | 3208/3886 [6:29:48<1:20:18,  7.11s/it]

 83%|████████▎ | 3209/3886 [6:29:53<1:14:13,  6.58s/it]
{'loss': 1.3376, 'grad_norm': 0.22108336386342697, 'learning_rate': 1.5503845108206526e-05, 'epoch': 0.83}


 83%|████████▎ | 3211/3886 [6:30:10<1:25:05,  7.56s/it]
{'loss': 0.9941, 'grad_norm': 0.23646795973537174, 'learning_rate': 1.541480316942303e-05, 'epoch': 0.83}


 83%|████████▎ | 3213/3886 [6:30:21<1:14:45,  6.67s/it]

 83%|████████▎ | 3214/3886 [6:30:28<1:15:01,  6.70s/it]
{'loss': 1.2719, 'grad_norm': 0.21979323957088945, 'learning_rate': 1.5281681099235278e-05, 'epoch': 0.83}


 83%|████████▎ | 3216/3886 [6:30:40<1:10:52,  6.35s/it]

 83%|████████▎ | 3217/3886 [6:30:48<1:14:07,  6.65s/it]
{'loss': 1.3207, 'grad_norm': 0.2142895883049196, 'learning_rate': 1.5149088774311781e-05, 'epoch': 0.83}

 83%|████████▎ | 3218/3886 [6:30:53<1:08:16,  6.13s/it]


 83%|████████▎ | 3220/3886 [6:31:05<1:06:53,  6.03s/it]

 83%|████████▎ | 3221/3886 [6:31:12<1:11:15,  6.43s/it]
{'loss': 1.1829, 'grad_norm': 0.21823437071108231, 'learning_rate': 1.4973124488474532e-05, 'epoch': 0.83}


 83%|████████▎ | 3223/3886 [6:31:29<1:20:46,  7.31s/it]
{'loss': 1.4319, 'grad_norm': 0.29936394448541304, 'learning_rate': 1.488549667334913e-05, 'epoch': 0.83}


 83%|████████▎ | 3225/3886 [6:31:44<1:21:00,  7.35s/it]
{'loss': 1.3647, 'grad_norm': 0.2266245158738986, 'learning_rate': 1.479810540167572e-05, 'epoch': 0.83}


 83%|████████▎ | 3227/3886 [6:31:57<1:16:36,  6.98s/it]
{'loss': 1.2726, 'grad_norm': 0.2576280300286004, 'learning_rate': 1.471095091632525e-05, 'epoch': 0.83}


 83%|████████▎ | 3229/3886 [6:32:12<1:20:26,  7.35s/it]
{'loss': 1.1445, 'grad_norm': 0.24063757327187937, 'learning_rate': 1.4624033459510467e-05, 'epoch': 0.83}

 83%|████████▎ | 3230/3886 [6:32:21<1:26:31,  7.91s/it]

 83%|████████▎ | 3231/3886 [6:32:27<1:20:09,  7.34s/it]

 83%|████████▎ | 3232/3886 [6:32:34<1:18:58,  7.25s/it]


 83%|████████▎ | 3234/3886 [6:32:47<1:14:56,  6.90s/it]

 83%|████████▎ | 3235/3886 [6:32:57<1:23:58,  7.74s/it]
{'loss': 1.3901, 'grad_norm': 0.22770156251855017, 'learning_rate': 1.4364705672523459e-05, 'epoch': 0.83}

 83%|████████▎ | 3236/3886 [6:33:06<1:29:09,  8.23s/it]


 83%|████████▎ | 3238/3886 [6:33:23<1:28:11,  8.17s/it]

 83%|████████▎ | 3239/3886 [6:33:30<1:27:02,  8.07s/it]

 83%|████████▎ | 3240/3886 [6:33:37<1:21:04,  7.53s/it]

 83%|████████▎ | 3241/3886 [6:33:42<1:14:58,  6.98s/it]
{'loss': 1.4118, 'grad_norm': 0.21785385654905162, 'learning_rate': 1.4107519798704327e-05, 'epoch': 0.83}

 83%|████████▎ | 3242/3886 [6:33:47<1:08:27,  6.38s/it]


 83%|████████▎ | 3244/3886 [6:34:03<1:14:42,  6.98s/it]
{'loss': 1.3303, 'grad_norm': 0.22766611972121775, 'learning_rate': 1.3979732091561938e-05, 'epoch': 0.83}

 84%|████████▎ | 3245/3886 [6:34:09<1:12:42,  6.81s/it]


 84%|████████▎ | 3247/3886 [6:34:28<1:28:00,  8.26s/it]
{'loss': 1.2069, 'grad_norm': 0.2436750352634791, 'learning_rate': 1.3852482270795731e-05, 'epoch': 0.84}

 84%|████████▎ | 3248/3886 [6:34:34<1:20:33,  7.58s/it]

 84%|████████▎ | 3249/3886 [6:34:44<1:26:36,  8.16s/it]

 84%|████████▎ | 3250/3886 [6:34:51<1:24:20,  7.96s/it]


 84%|████████▎ | 3252/3886 [6:35:04<1:17:22,  7.32s/it]
{'loss': 1.24, 'grad_norm': 0.25839933847893837, 'learning_rate': 1.3641596698867455e-05, 'epoch': 0.84}

 84%|████████▎ | 3253/3886 [6:35:13<1:22:44,  7.84s/it]


 84%|████████▍ | 3255/3886 [6:35:37<1:46:05, 10.09s/it]
{'loss': 1.1953, 'grad_norm': 0.2643250785108946, 'learning_rate': 1.3515785122687375e-05, 'epoch': 0.84}


 84%|████████▍ | 3257/3886 [6:35:52<1:36:02,  9.16s/it]

 84%|████████▍ | 3258/3886 [6:35:58<1:26:24,  8.26s/it]
{'loss': 1.5125, 'grad_norm': 0.23520837395765945, 'learning_rate': 1.3390514333952763e-05, 'epoch': 0.84}


 84%|████████▍ | 3260/3886 [6:36:17<1:31:37,  8.78s/it]
{'loss': 1.3491, 'grad_norm': 0.2503212916009078, 'learning_rate': 1.3307301299230323e-05, 'epoch': 0.84}

 84%|████████▍ | 3261/3886 [6:36:24<1:25:23,  8.20s/it]

 84%|████████▍ | 3262/3886 [6:36:31<1:22:42,  7.95s/it]

 84%|████████▍ | 3263/3886 [6:36:36<1:13:26,  7.07s/it]


 84%|████████▍ | 3265/3886 [6:36:50<1:15:02,  7.25s/it]
{'loss': 1.358, 'grad_norm': 0.2172799183130934, 'learning_rate': 1.3100323282986083e-05, 'epoch': 0.84}

 84%|████████▍ | 3266/3886 [6:36:57<1:13:51,  7.15s/it]


 84%|████████▍ | 3268/3886 [6:37:11<1:12:04,  7.00s/it]

 84%|████████▍ | 3269/3886 [6:37:19<1:15:45,  7.37s/it]

 84%|████████▍ | 3270/3886 [6:37:25<1:10:34,  6.87s/it]

 84%|████████▍ | 3271/3886 [6:37:37<1:27:18,  8.52s/it]
{'loss': 1.1379, 'grad_norm': 0.25197535895115225, 'learning_rate': 1.2853942389070772e-05, 'epoch': 0.84}


 84%|████████▍ | 3273/3886 [6:37:51<1:18:25,  7.68s/it]
{'loss': 1.4119, 'grad_norm': 0.21301826593341794, 'learning_rate': 1.2772299499555474e-05, 'epoch': 0.84}

 84%|████████▍ | 3274/3886 [6:37:58<1:16:42,  7.52s/it]

 84%|████████▍ | 3275/3886 [6:38:05<1:16:40,  7.53s/it]

 84%|████████▍ | 3276/3886 [6:38:12<1:13:18,  7.21s/it]

 84%|████████▍ | 3277/3886 [6:38:18<1:10:28,  6.94s/it]


 84%|████████▍ | 3279/3886 [6:38:33<1:14:20,  7.35s/it]

 84%|████████▍ | 3280/3886 [6:38:39<1:10:36,  6.99s/it]

 84%|████████▍ | 3281/3886 [6:38:45<1:06:59,  6.64s/it]

 84%|████████▍ | 3282/3886 [6:38:50<1:03:23,  6.30s/it]
{'loss': 1.1901, 'grad_norm': 0.26445635148070146, 'learning_rate': 1.2407909658208461e-05, 'epoch': 0.84}

 84%|████████▍ | 3283/3886 [6:38:58<1:06:03,  6.57s/it]

 85%|████████▍ | 3284/3886 [6:39:04<1:05:57,  6.57s/it]

 85%|████████▍ | 3285/3886 [6:39:11<1:07:22,  6.73s/it]

 85%|████████▍ | 3286/3886 [6:39:16<1:01:43,  6.17s/it]

 85%|████████▍ | 3287/3886 [6:39:28<1:17:42,  7.78s/it]

 85%|████████▍ | 3288/3886 [6:39:36<1:19:54,  8.02s/it]


 85%|████████▍ | 3290/3886 [6:39:47<1:04:57,  6.54s/it]
{'loss': 1.4169, 'grad_norm': 0.2431211665570033, 'learning_rate': 1.2088145219572045e-05, 'epoch': 0.85}


 85%|████████▍ | 3292/3886 [6:40:03<1:14:15,  7.50s/it]
{'loss': 1.2777, 'grad_norm': 0.20778136518889195, 'learning_rate': 1.200881435026383e-05, 'epoch': 0.85}

 85%|████████▍ | 3293/3886 [6:40:10<1:13:39,  7.45s/it]

 85%|████████▍ | 3294/3886 [6:40:16<1:06:46,  6.77s/it]

 85%|████████▍ | 3295/3886 [6:40:24<1:11:13,  7.23s/it]

 85%|████████▍ | 3296/3886 [6:40:30<1:07:10,  6.83s/it]

 85%|████████▍ | 3297/3886 [6:40:37<1:09:42,  7.10s/it]

 85%|████████▍ | 3298/3886 [6:40:44<1:08:27,  6.99s/it]

 85%|████████▍ | 3299/3886 [6:40:54<1:15:42,  7.74s/it]

 85%|████████▍ | 3300/3886 [6:41:00<1:12:28,  7.42s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.3674, 'grad_norm': 0.26461010974714955, 'learning_rate': 1.1654854764431289e-05, 'epoch': 0.85}

 85%|████████▍ | 3302/3886 [6:41:57<2:36:33, 16.08s/it]
{'loss': 1.1059, 'grad_norm': 0.24906722697262096, 'learning_rate': 1.1615832460802844e-05, 'epoch': 0.85}

 85%|████████▍ | 3303/3886 [6:42:08<2:21:37, 14.57s/it]


 85%|████████▌ | 3305/3886 [6:42:23<1:45:18, 10.87s/it]
{'loss': 1.138, 'grad_norm': 0.19502277037475169, 'learning_rate': 1.1499134103579611e-05, 'epoch': 0.85}

 85%|████████▌ | 3306/3886 [6:42:27<1:26:59,  9.00s/it]


 85%|████████▌ | 3308/3886 [6:42:51<1:43:53, 10.79s/it]
{'loss': 1.5062, 'grad_norm': 0.23176145799626233, 'learning_rate': 1.1382989143959144e-05, 'epoch': 0.85}

 85%|████████▌ | 3309/3886 [6:42:56<1:27:51,  9.14s/it]

 85%|████████▌ | 3310/3886 [6:43:06<1:29:14,  9.30s/it]

 85%|████████▌ | 3311/3886 [6:43:13<1:21:59,  8.56s/it]

 85%|████████▌ | 3312/3886 [6:43:19<1:14:55,  7.83s/it]

 85%|████████▌ | 3313/3886 [6:43:24<1:08:50,  7.21s/it]


 85%|████████▌ | 3315/3886 [6:43:39<1:09:20,  7.29s/it]
{'loss': 1.29, 'grad_norm': 0.2474775648940182, 'learning_rate': 1.1114140413067897e-05, 'epoch': 0.85}

 85%|████████▌ | 3316/3886 [6:43:46<1:09:11,  7.28s/it]


 85%|████████▌ | 3318/3886 [6:43:59<1:04:34,  6.82s/it]
{'loss': 1.2212, 'grad_norm': 0.26292881422623104, 'learning_rate': 1.0999845337838121e-05, 'epoch': 0.85}

 85%|████████▌ | 3319/3886 [6:44:06<1:05:04,  6.89s/it]

 85%|████████▌ | 3320/3886 [6:44:12<1:01:20,  6.50s/it]

 85%|████████▌ | 3321/3886 [6:44:18<1:00:14,  6.40s/it]

 85%|████████▌ | 3322/3886 [6:44:25<1:01:01,  6.49s/it]


 86%|████████▌ | 3324/3886 [6:44:37<1:00:09,  6.42s/it]

 86%|████████▌ | 3325/3886 [6:44:44<1:00:12,  6.44s/it]

 86%|████████▌ | 3326/3886 [6:44:49<58:33,  6.27s/it]

 86%|████████▌ | 3327/3886 [6:44:56<58:00,  6.23s/it]

 86%|████████▌ | 3328/3886 [6:45:03<1:01:18,  6.59s/it]

 86%|████████▌ | 3329/3886 [6:45:11<1:04:59,  7.00s/it]
{'loss': 1.2605, 'grad_norm': 0.2259652252511055, 'learning_rate': 1.0585530127900312e-05, 'epoch': 0.86}


 86%|████████▌ | 3331/3886 [6:45:26<1:06:12,  7.16s/it]
{'loss': 1.3439, 'grad_norm': 0.2295753738291856, 'learning_rate': 1.0511006677886003e-05, 'epoch': 0.86}

 86%|████████▌ | 3332/3886 [6:45:34<1:09:55,  7.57s/it]


 86%|████████▌ | 3334/3886 [6:45:48<1:07:20,  7.32s/it]
{'loss': 1.2944, 'grad_norm': 0.23486195193420972, 'learning_rate': 1.0399687881184317e-05, 'epoch': 0.86}


 86%|████████▌ | 3336/3886 [6:46:04<1:10:05,  7.65s/it]
{'loss': 1.423, 'grad_norm': 0.24942218579113146, 'learning_rate': 1.0325786569704476e-05, 'epoch': 0.86}

 86%|████████▌ | 3337/3886 [6:46:10<1:07:25,  7.37s/it]


 86%|████████▌ | 3339/3886 [6:46:25<1:06:49,  7.33s/it]

 86%|████████▌ | 3340/3886 [6:46:32<1:04:21,  7.07s/it]
{'loss': 1.4266, 'grad_norm': 0.20139593598168937, 'learning_rate': 1.017873179773895e-05, 'epoch': 0.86}

 86%|████████▌ | 3341/3886 [6:46:40<1:09:19,  7.63s/it]

 86%|████████▌ | 3342/3886 [6:46:46<1:03:09,  6.97s/it]

 86%|████████▌ | 3343/3886 [6:46:52<1:00:36,  6.70s/it]

 86%|████████▌ | 3344/3886 [6:46:58<58:36,  6.49s/it]

 86%|████████▌ | 3345/3886 [6:47:03<53:15,  5.91s/it]

 86%|████████▌ | 3346/3886 [6:47:09<54:19,  6.04s/it]

 86%|████████▌ | 3347/3886 [6:47:15<54:17,  6.04s/it]

 86%|████████▌ | 3348/3886 [6:47:21<54:18,  6.06s/it]


 86%|████████▌ | 3350/3886 [6:47:36<1:01:32,  6.89s/it]
{'loss': 1.307, 'grad_norm': 0.2112352481555677, 'learning_rate': 9.815466835512931e-06, 'epoch': 0.86}


 86%|████████▋ | 3352/3886 [6:47:50<1:02:07,  6.98s/it]

 86%|████████▋ | 3353/3886 [6:47:58<1:04:38,  7.28s/it]
{'loss': 1.299, 'grad_norm': 0.26557223047039574, 'learning_rate': 9.707708042230402e-06, 'epoch': 0.86}

 86%|████████▋ | 3354/3886 [6:48:05<1:04:02,  7.22s/it]


 86%|████████▋ | 3356/3886 [6:48:18<1:01:05,  6.92s/it]
{'loss': 1.1274, 'grad_norm': 0.24443774491848008, 'learning_rate': 9.60051384837195e-06, 'epoch': 0.86}

 86%|████████▋ | 3357/3886 [6:48:25<1:01:56,  7.03s/it]


 86%|████████▋ | 3359/3886 [6:48:41<1:05:48,  7.49s/it]
{'loss': 1.2252, 'grad_norm': 0.29371615981147997, 'learning_rate': 9.493884924224917e-06, 'epoch': 0.86}

 86%|████████▋ | 3360/3886 [6:48:48<1:03:32,  7.25s/it]

 86%|████████▋ | 3361/3886 [6:48:54<1:00:34,  6.92s/it]

 87%|████████▋ | 3362/3886 [6:49:01<59:18,  6.79s/it]


 87%|████████▋ | 3364/3886 [6:49:25<1:25:01,  9.77s/it]
{'loss': 1.2617, 'grad_norm': 0.24854540752758175, 'learning_rate': 9.317428014892249e-06, 'epoch': 0.87}

 87%|████████▋ | 3365/3886 [6:49:32<1:17:40,  8.95s/it]

 87%|████████▋ | 3366/3886 [6:49:39<1:11:08,  8.21s/it]

 87%|████████▋ | 3367/3886 [6:49:46<1:09:16,  8.01s/it]

 87%|████████▋ | 3368/3886 [6:49:52<1:03:38,  7.37s/it]

 87%|████████▋ | 3369/3886 [6:49:58<1:00:36,  7.03s/it]

 87%|████████▋ | 3370/3886 [6:50:07<1:03:11,  7.35s/it]

 87%|████████▋ | 3371/3886 [6:50:15<1:06:26,  7.74s/it]

 87%|████████▋ | 3372/3886 [6:50:22<1:04:15,  7.50s/it]


 87%|████████▋ | 3374/3886 [6:50:37<1:05:39,  7.69s/it]
{'loss': 1.5635, 'grad_norm': 0.22196169266146962, 'learning_rate': 8.969242561560232e-06, 'epoch': 0.87}


 87%|████████▋ | 3376/3886 [6:50:50<59:45,  7.03s/it]
{'loss': 1.3721, 'grad_norm': 0.24860425777017695, 'learning_rate': 8.900363656586919e-06, 'epoch': 0.87}

 87%|████████▋ | 3377/3886 [6:50:56<58:07,  6.85s/it]

 87%|████████▋ | 3378/3886 [6:51:02<55:27,  6.55s/it]

 87%|████████▋ | 3379/3886 [6:51:07<51:06,  6.05s/it]

 87%|████████▋ | 3380/3886 [6:51:13<49:30,  5.87s/it]


 87%|████████▋ | 3382/3886 [6:51:23<47:49,  5.69s/it]
{'loss': 1.3637, 'grad_norm': 0.24609207936008784, 'learning_rate': 8.695246764983078e-06, 'epoch': 0.87}

 87%|████████▋ | 3383/3886 [6:51:31<51:57,  6.20s/it]

 87%|████████▋ | 3384/3886 [6:51:36<49:58,  5.97s/it]

 87%|████████▋ | 3385/3886 [6:51:43<50:51,  6.09s/it]

 87%|████████▋ | 3386/3886 [6:51:52<59:42,  7.16s/it]


 87%|████████▋ | 3388/3886 [6:52:08<1:01:49,  7.45s/it]

 87%|████████▋ | 3389/3886 [6:52:14<57:35,  6.95s/it]
{'loss': 1.172, 'grad_norm': 0.26053992337866017, 'learning_rate': 8.458830447500653e-06, 'epoch': 0.87}

 87%|████████▋ | 3390/3886 [6:52:20<56:32,  6.84s/it]

 87%|████████▋ | 3391/3886 [6:52:27<55:16,  6.70s/it]

 87%|████████▋ | 3392/3886 [6:52:32<52:22,  6.36s/it]

 87%|████████▋ | 3393/3886 [6:52:39<53:57,  6.57s/it]

 87%|████████▋ | 3394/3886 [6:52:47<55:19,  6.75s/it]

 87%|████████▋ | 3395/3886 [6:52:53<55:18,  6.76s/it]

 87%|████████▋ | 3396/3886 [6:53:01<57:10,  7.00s/it]

 87%|████████▋ | 3397/3886 [6:53:07<53:45,  6.60s/it]

 87%|████████▋ | 3398/3886 [6:53:12<51:47,  6.37s/it]


 87%|████████▋ | 3400/3886 [6:53:24<49:03,  6.06s/it]

 88%|████████▊ | 3401/3886 [6:53:34<57:38,  7.13s/it]
{'loss': 1.3671, 'grad_norm': 0.2419181849418264, 'learning_rate': 8.060800228802712e-06, 'epoch': 0.88}

 88%|████████▊ | 3402/3886 [6:53:39<52:55,  6.56s/it]

 88%|████████▊ | 3403/3886 [6:53:47<56:11,  6.98s/it]


 88%|████████▊ | 3405/3886 [6:54:00<53:25,  6.66s/it]

 88%|████████▊ | 3406/3886 [6:54:06<53:20,  6.67s/it]
{'loss': 1.2378, 'grad_norm': 0.2541332853773153, 'learning_rate': 7.897666823764826e-06, 'epoch': 0.88}


 88%|████████▊ | 3408/3886 [6:54:18<50:20,  6.32s/it]

 88%|████████▊ | 3409/3886 [6:54:24<48:40,  6.12s/it]
{'loss': 1.2878, 'grad_norm': 0.24431750191733495, 'learning_rate': 7.800554490034517e-06, 'epoch': 0.88}

 88%|████████▊ | 3410/3886 [6:54:29<46:16,  5.83s/it]


 88%|████████▊ | 3412/3886 [6:54:46<56:15,  7.12s/it]

 88%|████████▊ | 3413/3886 [6:54:52<54:20,  6.89s/it]
{'loss': 1.4553, 'grad_norm': 0.23454518084211629, 'learning_rate': 7.671968299329002e-06, 'epoch': 0.88}

 88%|████████▊ | 3414/3886 [6:54:57<49:40,  6.31s/it]


 88%|████████▊ | 3416/3886 [6:55:12<55:40,  7.11s/it]

 88%|████████▊ | 3417/3886 [6:55:18<52:37,  6.73s/it]

 88%|████████▊ | 3418/3886 [6:55:24<50:27,  6.47s/it]
{'loss': 1.3155, 'grad_norm': 0.2502722643494488, 'learning_rate': 7.51267904914561e-06, 'epoch': 0.88}

 88%|████████▊ | 3419/3886 [6:55:31<51:17,  6.59s/it]


 88%|████████▊ | 3421/3886 [6:55:52<1:10:15,  9.07s/it]
{'loss': 1.3277, 'grad_norm': 0.2335935725943536, 'learning_rate': 7.417876422384428e-06, 'epoch': 0.88}

 88%|████████▊ | 3422/3886 [6:55:59<1:06:22,  8.58s/it]


 88%|████████▊ | 3424/3886 [6:56:14<1:00:24,  7.85s/it]
{'loss': 1.2147, 'grad_norm': 0.24884740412971323, 'learning_rate': 7.323652713408136e-06, 'epoch': 0.88}

 88%|████████▊ | 3425/3886 [6:56:21<59:24,  7.73s/it]

 88%|████████▊ | 3426/3886 [6:56:29<58:34,  7.64s/it]


 88%|████████▊ | 3428/3886 [6:56:42<53:59,  7.07s/it]

 88%|████████▊ | 3429/3886 [6:56:50<56:23,  7.40s/it]
{'loss': 1.3553, 'grad_norm': 0.2085468682898132, 'learning_rate': 7.167901281022993e-06, 'epoch': 0.88}


 88%|████████▊ | 3431/3886 [6:57:00<46:48,  6.17s/it]
{'loss': 1.1293, 'grad_norm': 0.2717941854756229, 'learning_rate': 7.106052042225386e-06, 'epoch': 0.88}

 88%|████████▊ | 3432/3886 [6:57:09<54:28,  7.20s/it]

 88%|████████▊ | 3433/3886 [6:57:18<56:52,  7.53s/it]


 88%|████████▊ | 3435/3886 [6:57:34<58:03,  7.72s/it]

 88%|████████▊ | 3436/3886 [6:57:40<54:39,  7.29s/it]

 88%|████████▊ | 3437/3886 [6:57:47<52:43,  7.05s/it]

 88%|████████▊ | 3438/3886 [6:57:52<49:16,  6.60s/it]
{'loss': 1.548, 'grad_norm': 0.23397062071249358, 'learning_rate': 6.891613865252456e-06, 'epoch': 0.88}

 88%|████████▊ | 3439/3886 [6:58:01<55:10,  7.41s/it]


 89%|████████▊ | 3441/3886 [6:58:16<53:49,  7.26s/it]
{'loss': 1.17, 'grad_norm': 0.24135282613274017, 'learning_rate': 6.800681712824619e-06, 'epoch': 0.89}

 89%|████████▊ | 3442/3886 [6:58:25<57:54,  7.83s/it]


 89%|████████▊ | 3444/3886 [6:58:37<49:35,  6.73s/it]
{'loss': 1.4293, 'grad_norm': 0.22115106290075728, 'learning_rate': 6.710332337511949e-06, 'epoch': 0.89}


 89%|████████▊ | 3446/3886 [6:58:50<49:12,  6.71s/it]

 89%|████████▊ | 3447/3886 [6:58:56<46:37,  6.37s/it]
{'loss': 1.245, 'grad_norm': 0.24609230328723206, 'learning_rate': 6.620566304270848e-06, 'epoch': 0.89}

 89%|████████▊ | 3448/3886 [6:59:02<45:05,  6.18s/it]


 89%|████████▉ | 3450/3886 [6:59:14<45:28,  6.26s/it]
{'loss': 1.2906, 'grad_norm': 0.2532092119472347, 'learning_rate': 6.531384174410005e-06, 'epoch': 0.89}


 89%|████████▉ | 3452/3886 [6:59:35<1:01:38,  8.52s/it]
{'loss': 1.5235, 'grad_norm': 0.21126337172796297, 'learning_rate': 6.472254087473062e-06, 'epoch': 0.89}

 89%|████████▉ | 3453/3886 [6:59:45<1:04:59,  9.01s/it]

 89%|████████▉ | 3454/3886 [6:59:55<1:07:33,  9.38s/it]

 89%|████████▉ | 3455/3886 [7:00:06<1:10:19,  9.79s/it]

 89%|████████▉ | 3456/3886 [7:00:11<1:00:58,  8.51s/it]


 89%|████████▉ | 3458/3886 [7:00:24<53:20,  7.48s/it]
{'loss': 1.4724, 'grad_norm': 0.2854690172382702, 'learning_rate': 6.296424029564996e-06, 'epoch': 0.89}

 89%|████████▉ | 3459/3886 [7:00:29<47:58,  6.74s/it]

 89%|████████▉ | 3460/3886 [7:00:35<46:03,  6.49s/it]


 89%|████████▉ | 3462/3886 [7:00:48<47:14,  6.68s/it]

 89%|████████▉ | 3463/3886 [7:00:54<45:51,  6.51s/it]
{'loss': 1.2865, 'grad_norm': 0.29419533382481805, 'learning_rate': 6.151689136057759e-06, 'epoch': 0.89}

 89%|████████▉ | 3464/3886 [7:01:01<46:10,  6.57s/it]


 89%|████████▉ | 3466/3886 [7:01:16<49:31,  7.07s/it]

 89%|████████▉ | 3467/3886 [7:01:22<47:18,  6.77s/it]

 89%|████████▉ | 3468/3886 [7:01:28<45:23,  6.52s/it]

 89%|████████▉ | 3469/3886 [7:01:34<43:28,  6.25s/it]

 89%|████████▉ | 3470/3886 [7:01:40<42:57,  6.20s/it]
{'loss': 1.3483, 'grad_norm': 0.21800189855279764, 'learning_rate': 5.951799404981296e-06, 'epoch': 0.89}


 89%|████████▉ | 3472/3886 [7:01:53<43:41,  6.33s/it]
{'loss': 1.2508, 'grad_norm': 0.23379998377997385, 'learning_rate': 5.8952758421996655e-06, 'epoch': 0.89}

 89%|████████▉ | 3473/3886 [7:01:59<43:22,  6.30s/it]

 89%|████████▉ | 3474/3886 [7:02:05<42:21,  6.17s/it]


 89%|████████▉ | 3476/3886 [7:02:17<40:45,  5.97s/it]

 89%|████████▉ | 3477/3886 [7:02:22<39:25,  5.78s/it]
{'loss': 1.4633, 'grad_norm': 0.2123510169471715, 'learning_rate': 5.755111463648577e-06, 'epoch': 0.89}


 90%|████████▉ | 3479/3886 [7:02:34<40:10,  5.92s/it]

 90%|████████▉ | 3480/3886 [7:02:40<40:29,  5.98s/it]

 90%|████████▉ | 3481/3886 [7:02:47<41:00,  6.08s/it]

 90%|████████▉ | 3482/3886 [7:02:56<48:00,  7.13s/it]
{'loss': 1.2879, 'grad_norm': 0.21680492940024973, 'learning_rate': 5.616584069930752e-06, 'epoch': 0.9}


 90%|████████▉ | 3484/3886 [7:03:09<45:37,  6.81s/it]
{'loss': 1.0235, 'grad_norm': 0.24053545446087227, 'learning_rate': 5.561632008130635e-06, 'epoch': 0.9}

 90%|████████▉ | 3485/3886 [7:03:23<1:00:54,  9.11s/it]

 90%|████████▉ | 3486/3886 [7:03:30<55:06,  8.27s/it]


 90%|████████▉ | 3488/3886 [7:03:43<49:23,  7.45s/it]

 90%|████████▉ | 3489/3886 [7:03:51<50:06,  7.57s/it]
{'loss': 1.317, 'grad_norm': 0.21134077144585728, 'learning_rate': 5.425400429132543e-06, 'epoch': 0.9}

 90%|████████▉ | 3490/3886 [7:03:56<44:46,  6.78s/it]

 90%|████████▉ | 3491/3886 [7:04:02<43:46,  6.65s/it]


 90%|████████▉ | 3493/3886 [7:04:16<46:26,  7.09s/it]

 90%|████████▉ | 3494/3886 [7:04:25<48:22,  7.40s/it]
{'loss': 1.3705, 'grad_norm': 0.22776386268053636, 'learning_rate': 5.290811561877363e-06, 'epoch': 0.9}


 90%|████████▉ | 3496/3886 [7:04:39<46:30,  7.15s/it]
{'loss': 1.4406, 'grad_norm': 0.2401732454332184, 'learning_rate': 5.237436498876458e-06, 'epoch': 0.9}

 90%|████████▉ | 3497/3886 [7:04:46<46:06,  7.11s/it]

 90%|█████████ | 3498/3886 [7:04:51<43:20,  6.70s/it]


 90%|█████████ | 3500/3886 [7:05:05<43:26,  6.75s/it]
{'loss': 1.2963, 'grad_norm': 0.21973849110891333, 'learning_rate': 5.131476590196927e-06, 'epoch': 0.9}

 90%|█████████ | 3501/3886 [7:05:11<42:50,  6.68s/it]

 90%|█████████ | 3502/3886 [7:05:17<41:54,  6.55s/it]

 90%|█████████ | 3503/3886 [7:05:24<41:25,  6.49s/it]

 90%|█████████ | 3504/3886 [7:05:32<44:11,  6.94s/it]

 90%|█████████ | 3505/3886 [7:05:38<41:51,  6.59s/it]


 90%|█████████ | 3507/3886 [7:05:50<40:51,  6.47s/it]

 90%|█████████ | 3508/3886 [7:05:57<40:19,  6.40s/it]

 90%|█████████ | 3509/3886 [7:06:03<39:56,  6.36s/it]
{'loss': 1.5759, 'grad_norm': 0.22997769112915925, 'learning_rate': 4.896924465049113e-06, 'epoch': 0.9}

 90%|█████████ | 3510/3886 [7:06:08<37:06,  5.92s/it]

 90%|█████████ | 3511/3886 [7:06:14<37:29,  6.00s/it]

 90%|█████████ | 3512/3886 [7:06:22<40:50,  6.55s/it]

 90%|█████████ | 3513/3886 [7:06:28<40:22,  6.49s/it]


 90%|█████████ | 3515/3886 [7:06:43<44:27,  7.19s/it]
{'loss': 1.2552, 'grad_norm': 0.21917933039685045, 'learning_rate': 4.74352897352982e-06, 'epoch': 0.9}


 91%|█████████ | 3517/3886 [7:06:55<39:43,  6.46s/it]

 91%|█████████ | 3518/3886 [7:07:00<38:03,  6.20s/it]
{'loss': 1.2905, 'grad_norm': 0.22592893875687148, 'learning_rate': 4.667724449700095e-06, 'epoch': 0.91}


 91%|█████████ | 3520/3886 [7:07:15<40:40,  6.67s/it]

 91%|█████████ | 3521/3886 [7:07:23<43:19,  7.12s/it]
{'loss': 1.167, 'grad_norm': 0.25039789195484935, 'learning_rate': 4.592516040407991e-06, 'epoch': 0.91}


 91%|█████████ | 3523/3886 [7:07:39<45:52,  7.58s/it]
{'loss': 1.3463, 'grad_norm': 0.22642327774477994, 'learning_rate': 4.542708508013383e-06, 'epoch': 0.91}


 91%|█████████ | 3525/3886 [7:07:51<41:27,  6.89s/it]
{'loss': 1.35, 'grad_norm': 0.24775551242586824, 'learning_rate': 4.493166262924342e-06, 'epoch': 0.91}

 91%|█████████ | 3526/3886 [7:08:02<49:49,  8.30s/it]

 91%|█████████ | 3527/3886 [7:08:08<45:32,  7.61s/it]


 91%|█████████ | 3529/3886 [7:08:25<48:29,  8.15s/it]
{'loss': 1.1141, 'grad_norm': 0.23089897113516186, 'learning_rate': 4.394878184660745e-06, 'epoch': 0.91}


 91%|█████████ | 3531/3886 [7:08:41<47:14,  7.99s/it]
{'loss': 1.12, 'grad_norm': 0.23135779686608074, 'learning_rate': 4.346132624640575e-06, 'epoch': 0.91}

 91%|█████████ | 3532/3886 [7:08:48<44:11,  7.49s/it]

 91%|█████████ | 3533/3886 [7:08:54<42:03,  7.15s/it]


 91%|█████████ | 3535/3886 [7:09:07<39:14,  6.71s/it]
{'loss': 1.3935, 'grad_norm': 0.21895610948133026, 'learning_rate': 4.2494391401720515e-06, 'epoch': 0.91}

 91%|█████████ | 3536/3886 [7:09:12<37:20,  6.40s/it]

 91%|█████████ | 3537/3886 [7:09:22<42:32,  7.31s/it]

 91%|█████████ | 3538/3886 [7:09:26<38:01,  6.56s/it]


 91%|█████████ | 3540/3886 [7:09:39<36:46,  6.38s/it]
{'loss': 1.3202, 'grad_norm': 0.2412245758017462, 'learning_rate': 4.130069234026168e-06, 'epoch': 0.91}

 91%|█████████ | 3541/3886 [7:09:46<38:02,  6.62s/it]

 91%|█████████ | 3542/3886 [7:10:04<56:47,  9.91s/it]

 91%|█████████ | 3543/3886 [7:10:10<49:45,  8.70s/it]


 91%|█████████ | 3545/3886 [7:10:25<47:07,  8.29s/it]

 91%|█████████▏| 3546/3886 [7:10:31<41:54,  7.40s/it]

 91%|█████████▏| 3547/3886 [7:10:37<40:27,  7.16s/it]

 91%|█████████▏| 3548/3886 [7:10:43<37:27,  6.65s/it]

 91%|█████████▏| 3549/3886 [7:10:49<37:03,  6.60s/it]

 91%|█████████▏| 3550/3886 [7:10:55<36:11,  6.46s/it]

 91%|█████████▏| 3551/3886 [7:11:03<37:55,  6.79s/it]

 91%|█████████▏| 3552/3886 [7:11:13<42:46,  7.68s/it]

 91%|█████████▏| 3553/3886 [7:11:19<40:39,  7.33s/it]

 91%|█████████▏| 3554/3886 [7:11:25<38:29,  6.96s/it]

 91%|█████████▏| 3555/3886 [7:11:35<43:18,  7.85s/it]
{'loss': 1.3941, 'grad_norm': 0.21357295151716965, 'learning_rate': 3.781958930350815e-06, 'epoch': 0.91}

 92%|█████████▏| 3556/3886 [7:11:40<38:09,  6.94s/it]

 92%|█████████▏| 3557/3886 [7:11:54<48:59,  8.94s/it]

 92%|█████████▏| 3558/3886 [7:11:58<41:51,  7.66s/it]


 92%|█████████▏| 3560/3886 [7:12:13<40:43,  7.49s/it]

 92%|█████████▏| 3561/3886 [7:12:19<38:50,  7.17s/it]

 92%|█████████▏| 3562/3886 [7:12:27<39:52,  7.39s/it]

 92%|█████████▏| 3563/3886 [7:12:33<36:39,  6.81s/it]
{'loss': 1.1854, 'grad_norm': 0.23332706538467485, 'learning_rate': 3.6024468913704147e-06, 'epoch': 0.92}

 92%|█████████▏| 3564/3886 [7:12:48<49:25,  9.21s/it]


 92%|█████████▏| 3566/3886 [7:12:59<39:02,  7.32s/it]
{'loss': 1.4798, 'grad_norm': 0.22764280285067012, 'learning_rate': 3.5362345394340914e-06, 'epoch': 0.92}


 92%|█████████▏| 3568/3886 [7:13:11<35:55,  6.78s/it]
{'loss': 1.369, 'grad_norm': 0.21612530029703694, 'learning_rate': 3.492428051499841e-06, 'epoch': 0.92}

 92%|█████████▏| 3569/3886 [7:13:19<36:44,  6.95s/it]


 92%|█████████▏| 3571/3886 [7:13:39<43:56,  8.37s/it]

 92%|█████████▏| 3572/3886 [7:13:46<40:31,  7.74s/it]

 92%|█████████▏| 3573/3886 [7:13:51<36:32,  7.01s/it]
{'loss': 1.3309, 'grad_norm': 0.25926332156590487, 'learning_rate': 3.3840854980998313e-06, 'epoch': 0.92}


 92%|█████████▏| 3575/3886 [7:14:03<33:32,  6.47s/it]

 92%|█████████▏| 3576/3886 [7:14:11<36:18,  7.03s/it]
{'loss': 1.1657, 'grad_norm': 0.24119068452293985, 'learning_rate': 3.319885367629172e-06, 'epoch': 0.92}

 92%|█████████▏| 3577/3886 [7:14:16<32:17,  6.27s/it]

 92%|█████████▏| 3578/3886 [7:14:22<32:01,  6.24s/it]

 92%|█████████▏| 3579/3886 [7:14:28<30:52,  6.03s/it]


 92%|█████████▏| 3581/3886 [7:14:49<45:14,  8.90s/it]
{'loss': 1.4537, 'grad_norm': 0.215909739917274, 'learning_rate': 3.214228774713679e-06, 'epoch': 0.92}


 92%|█████████▏| 3583/3886 [7:15:01<37:17,  7.38s/it]

 92%|█████████▏| 3584/3886 [7:15:10<38:22,  7.62s/it]
{'loss': 1.4084, 'grad_norm': 0.21542871816738313, 'learning_rate': 3.151641639676839e-06, 'epoch': 0.92}


 92%|█████████▏| 3586/3886 [7:15:22<33:56,  6.79s/it]

 92%|█████████▏| 3587/3886 [7:15:31<37:30,  7.53s/it]

 92%|█████████▏| 3588/3886 [7:15:37<35:49,  7.21s/it]

 92%|█████████▏| 3589/3886 [7:15:43<33:36,  6.79s/it]
{'loss': 1.2491, 'grad_norm': 0.22854335085271563, 'learning_rate': 3.0486757053478476e-06, 'epoch': 0.92}

 92%|█████████▏| 3590/3886 [7:15:50<33:27,  6.78s/it]

 92%|█████████▏| 3591/3886 [7:15:58<35:45,  7.27s/it]

 92%|█████████▏| 3592/3886 [7:16:08<39:38,  8.09s/it]

 92%|█████████▏| 3593/3886 [7:16:18<42:11,  8.64s/it]


 93%|█████████▎| 3595/3886 [7:16:31<36:56,  7.62s/it]
{'loss': 1.5037, 'grad_norm': 0.23820653539175357, 'learning_rate': 2.927339611847735e-06, 'epoch': 0.92}

 93%|█████████▎| 3596/3886 [7:16:37<33:35,  6.95s/it]

 93%|█████████▎| 3597/3886 [7:16:44<33:25,  6.94s/it]

 93%|█████████▎| 3598/3886 [7:16:50<32:49,  6.84s/it]


 93%|█████████▎| 3600/3886 [7:17:12<44:11,  9.27s/it]
[2024-05-28 07:51:41,997] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 93%|█████████▎| 3600/3886 [7:17:12<44:11,  9.27s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.3223, 'grad_norm': 0.22931239272801893, 'learning_rate': 2.808431503340958e-06, 'epoch': 0.93}

 93%|█████████▎| 3602/3886 [7:18:02<1:13:18, 15.49s/it]
{'loss': 1.331, 'grad_norm': 0.24417996711221093, 'learning_rate': 2.788849749074085e-06, 'epoch': 0.93}

 93%|█████████▎| 3603/3886 [7:18:12<1:06:09, 14.03s/it]


 93%|█████████▎| 3605/3886 [7:18:27<49:47, 10.63s/it]
{'loss': 1.304, 'grad_norm': 0.2762929041341252, 'learning_rate': 2.730509782764534e-06, 'epoch': 0.93}

 93%|█████████▎| 3606/3886 [7:18:32<41:45,  8.95s/it]


 93%|█████████▎| 3608/3886 [7:18:47<37:49,  8.16s/it]
{'loss': 1.2445, 'grad_norm': 0.22353202692352034, 'learning_rate': 2.6727780444329e-06, 'epoch': 0.93}

 93%|█████████▎| 3609/3886 [7:18:54<35:37,  7.72s/it]


 93%|█████████▎| 3611/3886 [7:19:07<33:41,  7.35s/it]

 93%|█████████▎| 3612/3886 [7:19:17<37:00,  8.10s/it]
{'loss': 1.3665, 'grad_norm': 0.24785906380605496, 'learning_rate': 2.5967491491326046e-06, 'epoch': 0.93}

 93%|█████████▎| 3613/3886 [7:19:22<32:57,  7.24s/it]

 93%|█████████▎| 3614/3886 [7:19:28<30:43,  6.78s/it]


 93%|█████████▎| 3616/3886 [7:19:49<37:43,  8.38s/it]
{'loss': 1.3583, 'grad_norm': 0.237893231924178, 'learning_rate': 2.5218030345236175e-06, 'epoch': 0.93}


 93%|█████████▎| 3618/3886 [7:20:04<35:05,  7.86s/it]
{'loss': 1.4443, 'grad_norm': 0.22935584051808394, 'learning_rate': 2.4847362808039452e-06, 'epoch': 0.93}

 93%|█████████▎| 3619/3886 [7:20:10<32:49,  7.38s/it]

 93%|█████████▎| 3620/3886 [7:20:18<33:36,  7.58s/it]


 93%|█████████▎| 3622/3886 [7:20:33<33:52,  7.70s/it]
{'loss': 1.2916, 'grad_norm': 0.2550789355973604, 'learning_rate': 2.4114158955988676e-06, 'epoch': 0.93}

 93%|█████████▎| 3623/3886 [7:20:39<31:08,  7.11s/it]


 93%|█████████▎| 3625/3886 [7:20:55<33:33,  7.71s/it]

 93%|█████████▎| 3626/3886 [7:21:08<39:17,  9.07s/it]

 93%|█████████▎| 3627/3886 [7:21:14<35:11,  8.15s/it]
{'loss': 1.3646, 'grad_norm': 0.24476296320165045, 'learning_rate': 2.3212910660365907e-06, 'epoch': 0.93}


 93%|█████████▎| 3629/3886 [7:21:24<28:06,  6.56s/it]

 93%|█████████▎| 3630/3886 [7:21:30<27:03,  6.34s/it]
{'loss': 1.5364, 'grad_norm': 0.23030978016865356, 'learning_rate': 2.268030451023917e-06, 'epoch': 0.93}


 93%|█████████▎| 3632/3886 [7:21:44<28:09,  6.65s/it]

 93%|█████████▎| 3633/3886 [7:21:49<27:03,  6.42s/it]
{'loss': 1.3916, 'grad_norm': 0.22375007134413247, 'learning_rate': 2.215380955881219e-06, 'epoch': 0.93}


 94%|█████████▎| 3635/3886 [7:22:08<33:05,  7.91s/it]
{'loss': 1.2623, 'grad_norm': 0.22876761192448997, 'learning_rate': 2.180620966225899e-06, 'epoch': 0.94}


 94%|█████████▎| 3637/3886 [7:22:20<29:37,  7.14s/it]

 94%|█████████▎| 3638/3886 [7:22:27<29:56,  7.24s/it]
{'loss': 1.3883, 'grad_norm': 0.23356560281065822, 'learning_rate': 2.128990733908054e-06, 'epoch': 0.94}

 94%|█████████▎| 3639/3886 [7:22:37<32:35,  7.92s/it]

 94%|█████████▎| 3640/3886 [7:22:46<34:34,  8.43s/it]

 94%|█████████▎| 3641/3886 [7:22:55<33:59,  8.32s/it]


 94%|█████████▎| 3643/3886 [7:23:10<32:33,  8.04s/it]

 94%|█████████▍| 3644/3886 [7:23:20<34:47,  8.62s/it]

 94%|█████████▍| 3645/3886 [7:23:28<34:03,  8.48s/it]

 94%|█████████▍| 3646/3886 [7:23:34<31:06,  7.78s/it]
{'loss': 1.242, 'grad_norm': 0.24007610684406397, 'learning_rate': 1.994302926759617e-06, 'epoch': 0.94}

 94%|█████████▍| 3647/3886 [7:23:39<27:57,  7.02s/it]

 94%|█████████▍| 3648/3886 [7:23:44<25:56,  6.54s/it]


 94%|█████████▍| 3650/3886 [7:24:02<28:52,  7.34s/it]

 94%|█████████▍| 3651/3886 [7:24:08<26:55,  6.88s/it]

 94%|█████████▍| 3652/3886 [7:24:14<25:43,  6.59s/it]
{'loss': 1.2844, 'grad_norm': 0.2519621367649048, 'learning_rate': 1.8961466223537227e-06, 'epoch': 0.94}

 94%|█████████▍| 3653/3886 [7:24:21<26:31,  6.83s/it]


 94%|█████████▍| 3655/3886 [7:24:34<25:56,  6.74s/it]

 94%|█████████▍| 3656/3886 [7:24:46<31:08,  8.12s/it]

 94%|█████████▍| 3657/3886 [7:24:58<36:22,  9.53s/it]
{'loss': 1.2803, 'grad_norm': 0.22084847879049446, 'learning_rate': 1.8162240128326458e-06, 'epoch': 0.94}

 94%|█████████▍| 3658/3886 [7:25:05<32:51,  8.65s/it]

 94%|█████████▍| 3659/3886 [7:25:11<29:57,  7.92s/it]

 94%|█████████▍| 3660/3886 [7:25:17<27:57,  7.42s/it]


 94%|█████████▍| 3662/3886 [7:25:30<25:43,  6.89s/it]
{'loss': 1.3846, 'grad_norm': 0.23319492987892704, 'learning_rate': 1.738006804576886e-06, 'epoch': 0.94}


 94%|█████████▍| 3664/3886 [7:25:44<25:55,  7.01s/it]
{'loss': 1.258, 'grad_norm': 0.23276766726328266, 'learning_rate': 1.7071977389494064e-06, 'epoch': 0.94}

 94%|█████████▍| 3665/3886 [7:25:51<25:28,  6.92s/it]


 94%|█████████▍| 3667/3886 [7:26:04<24:30,  6.71s/it]

 94%|█████████▍| 3668/3886 [7:26:10<24:19,  6.70s/it]
{'loss': 1.2526, 'grad_norm': 0.22779213550776725, 'learning_rate': 1.6463991951514823e-06, 'epoch': 0.94}

 94%|█████████▍| 3669/3886 [7:26:17<23:51,  6.60s/it]

 94%|█████████▍| 3670/3886 [7:26:23<23:04,  6.41s/it]


 94%|█████████▍| 3672/3886 [7:26:36<23:11,  6.50s/it]
{'loss': 1.323, 'grad_norm': 0.23908924849533722, 'learning_rate': 1.5866939965852424e-06, 'epoch': 0.94}

 95%|█████████▍| 3673/3886 [7:26:41<21:53,  6.17s/it]

 95%|█████████▍| 3674/3886 [7:26:47<21:16,  6.02s/it]

 95%|█████████▍| 3675/3886 [7:26:53<20:56,  5.96s/it]


 95%|█████████▍| 3677/3886 [7:27:12<26:10,  7.51s/it]

 95%|█████████▍| 3678/3886 [7:27:26<33:19,  9.61s/it]
[2024-05-28 08:01:56,377] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▍| 3679/3886 [7:27:32<29:25,  8.53s/it]
{'loss': 1.3875, 'grad_norm': 0.2499820699751426, 'learning_rate': 1.4848427505187556e-06, 'epoch': 0.95}


 95%|█████████▍| 3681/3886 [7:27:46<25:36,  7.50s/it]

 95%|█████████▍| 3682/3886 [7:27:55<26:30,  7.80s/it]
{'loss': 1.3582, 'grad_norm': 0.23986816923660365, 'learning_rate': 1.442218711233123e-06, 'epoch': 0.95}

 95%|█████████▍| 3683/3886 [7:28:00<23:36,  6.98s/it]


 95%|█████████▍| 3685/3886 [7:28:12<21:43,  6.49s/it]

 95%|█████████▍| 3686/3886 [7:28:18<21:28,  6.44s/it]
{'loss': 1.3499, 'grad_norm': 0.24513203956749238, 'learning_rate': 1.386345367876407e-06, 'epoch': 0.95}

 95%|█████████▍| 3687/3886 [7:28:29<26:00,  7.84s/it]


 95%|█████████▍| 3689/3886 [7:28:43<23:51,  7.27s/it]
{'loss': 1.1324, 'grad_norm': 0.24554811159776643, 'learning_rate': 1.3451597317991171e-06, 'epoch': 0.95}

 95%|█████████▍| 3690/3886 [7:28:49<22:40,  6.94s/it]


 95%|█████████▌| 3692/3886 [7:29:03<22:33,  6.98s/it]

 95%|█████████▌| 3693/3886 [7:29:10<22:40,  7.05s/it]

 95%|█████████▌| 3694/3886 [7:29:18<23:26,  7.33s/it]
{'loss': 1.4056, 'grad_norm': 0.23123528787743483, 'learning_rate': 1.2778879984310931e-06, 'epoch': 0.95}

 95%|█████████▌| 3695/3886 [7:29:26<23:44,  7.46s/it]


 95%|█████████▌| 3697/3886 [7:29:39<22:11,  7.04s/it]

 95%|█████████▌| 3698/3886 [7:29:44<20:25,  6.52s/it]
{'loss': 1.1397, 'grad_norm': 0.2675762829410108, 'learning_rate': 1.2253051780059977e-06, 'epoch': 0.95}


 95%|█████████▌| 3700/3886 [7:29:56<19:44,  6.37s/it]

 95%|█████████▌| 3701/3886 [7:30:02<19:33,  6.34s/it]
{'loss': 1.3043, 'grad_norm': 0.25188130714056606, 'learning_rate': 1.1865886109467262e-06, 'epoch': 0.95}

 95%|█████████▌| 3702/3886 [7:30:08<18:29,  6.03s/it]


 95%|█████████▌| 3704/3886 [7:30:21<18:42,  6.17s/it]
{'loss': 1.2622, 'grad_norm': 0.2516362594704519, 'learning_rate': 1.148489926033891e-06, 'epoch': 0.95}


 95%|█████████▌| 3706/3886 [7:30:35<19:59,  6.67s/it]
{'loss': 1.1968, 'grad_norm': 0.2524434132734245, 'learning_rate': 1.1234341884221899e-06, 'epoch': 0.95}

 95%|█████████▌| 3707/3886 [7:30:43<21:19,  7.15s/it]


 95%|█████████▌| 3709/3886 [7:30:56<20:21,  6.90s/it]

 95%|█████████▌| 3710/3886 [7:31:05<21:38,  7.38s/it]

 95%|█████████▌| 3711/3886 [7:31:10<20:01,  6.87s/it]

 96%|█████████▌| 3712/3886 [7:31:17<19:31,  6.73s/it]
{'loss': 1.4971, 'grad_norm': 0.21535699886919948, 'learning_rate': 1.0499159895834608e-06, 'epoch': 0.96}


 96%|█████████▌| 3714/3886 [7:31:28<17:46,  6.20s/it]

 96%|█████████▌| 3715/3886 [7:31:37<19:23,  6.81s/it]
{'loss': 1.4855, 'grad_norm': 0.2384633262420136, 'learning_rate': 1.014084881347732e-06, 'epoch': 0.96}

 96%|█████████▌| 3716/3886 [7:31:45<20:39,  7.29s/it]


 96%|█████████▌| 3718/3886 [7:31:58<19:09,  6.84s/it]

 96%|█████████▌| 3719/3886 [7:32:04<18:45,  6.74s/it]

 96%|█████████▌| 3720/3886 [7:32:11<18:16,  6.61s/it]

 96%|█████████▌| 3721/3886 [7:32:16<17:16,  6.28s/it]

 96%|█████████▌| 3722/3886 [7:32:26<20:19,  7.44s/it]

 96%|█████████▌| 3723/3886 [7:32:33<19:08,  7.05s/it]
{'loss': 1.2896, 'grad_norm': 0.20940025012887165, 'learning_rate': 9.215618863128494e-07, 'epoch': 0.96}


 96%|█████████▌| 3725/3886 [7:32:45<17:35,  6.56s/it]

 96%|█████████▌| 3726/3886 [7:32:52<18:18,  6.87s/it]
{'loss': 1.2601, 'grad_norm': 0.25104743225034903, 'learning_rate': 8.880013669974708e-07, 'epoch': 0.96}


 96%|█████████▌| 3728/3886 [7:33:06<18:12,  6.91s/it]

 96%|█████████▌| 3729/3886 [7:33:13<17:52,  6.83s/it]

 96%|█████████▌| 3730/3886 [7:33:18<16:42,  6.43s/it]

 96%|█████████▌| 3731/3886 [7:33:28<19:23,  7.51s/it]
{'loss': 1.3164, 'grad_norm': 0.1923766911940001, 'learning_rate': 8.33444490501023e-07, 'epoch': 0.96}

 96%|█████████▌| 3732/3886 [7:33:34<17:51,  6.96s/it]

 96%|█████████▌| 3733/3886 [7:33:39<16:28,  6.46s/it]

 96%|█████████▌| 3734/3886 [7:33:49<18:51,  7.44s/it]


 96%|█████████▌| 3736/3886 [7:34:06<20:36,  8.25s/it]
{'loss': 1.3183, 'grad_norm': 0.24206096682374068, 'learning_rate': 7.806100856404985e-07, 'epoch': 0.96}


 96%|█████████▌| 3738/3886 [7:34:19<17:47,  7.21s/it]
{'loss': 1.3762, 'grad_norm': 0.22242941516828407, 'learning_rate': 7.599588223261678e-07, 'epoch': 0.96}


 96%|█████████▌| 3740/3886 [7:34:39<20:13,  8.31s/it]

 96%|█████████▋| 3741/3886 [7:34:44<18:03,  7.47s/it]

 96%|█████████▋| 3742/3886 [7:34:53<18:51,  7.86s/it]

 96%|█████████▋| 3743/3886 [7:34:59<17:18,  7.26s/it]

 96%|█████████▋| 3744/3886 [7:35:05<16:25,  6.94s/it]

 96%|█████████▋| 3745/3886 [7:35:11<15:20,  6.53s/it]
{'loss': 1.3868, 'grad_norm': 0.22380340060858903, 'learning_rate': 6.898516956225787e-07, 'epoch': 0.96}

 96%|█████████▋| 3746/3886 [7:35:17<15:23,  6.60s/it]

 96%|█████████▋| 3747/3886 [7:35:23<14:56,  6.45s/it]


 96%|█████████▋| 3749/3886 [7:35:37<15:10,  6.64s/it]
{'loss': 1.4034, 'grad_norm': 0.2531510749319387, 'learning_rate': 6.513082997900321e-07, 'epoch': 0.96}


 97%|█████████▋| 3751/3886 [7:35:55<17:31,  7.79s/it]

 97%|█████████▋| 3752/3886 [7:36:01<16:24,  7.35s/it]
{'loss': 1.1652, 'grad_norm': 0.23202827519554536, 'learning_rate': 6.231254971268419e-07, 'epoch': 0.97}


 97%|█████████▋| 3754/3886 [7:36:13<14:37,  6.65s/it]

 97%|█████████▋| 3755/3886 [7:36:21<15:10,  6.95s/it]
{'loss': 1.342, 'grad_norm': 0.2224615119091154, 'learning_rate': 5.955640999556922e-07, 'epoch': 0.97}


 97%|█████████▋| 3757/3886 [7:36:36<15:43,  7.31s/it]

 97%|█████████▋| 3758/3886 [7:36:42<14:43,  6.90s/it]
{'loss': 1.4324, 'grad_norm': 0.22634985473505811, 'learning_rate': 5.686242806185238e-07, 'epoch': 0.97}


 97%|█████████▋| 3760/3886 [7:36:53<12:48,  6.10s/it]
{'loss': 0.9715, 'grad_norm': 0.2547445544794488, 'learning_rate': 5.510098054270274e-07, 'epoch': 0.97}

 97%|█████████▋| 3761/3886 [7:37:00<13:20,  6.40s/it]

 97%|█████████▋| 3762/3886 [7:37:07<13:38,  6.60s/it]

 97%|█████████▋| 3763/3886 [7:37:14<13:19,  6.50s/it]

 97%|█████████▋| 3764/3886 [7:37:22<14:28,  7.12s/it]

 97%|█████████▋| 3765/3886 [7:37:30<14:41,  7.29s/it]


 97%|█████████▋| 3767/3886 [7:37:44<14:00,  7.07s/it]
{'loss': 1.2114, 'grad_norm': 0.25916394140901644, 'learning_rate': 4.915359547230102e-07, 'epoch': 0.97}

 97%|█████████▋| 3768/3886 [7:37:50<13:09,  6.69s/it]


 97%|█████████▋| 3770/3886 [7:38:04<13:20,  6.90s/it]

 97%|█████████▋| 3771/3886 [7:38:09<12:02,  6.28s/it]

 97%|█████████▋| 3772/3886 [7:38:15<11:25,  6.01s/it]
{'loss': 1.4977, 'grad_norm': 0.24231438464753907, 'learning_rate': 4.5112860890350603e-07, 'epoch': 0.97}


 97%|█████████▋| 3774/3886 [7:38:30<12:43,  6.82s/it]
{'loss': 1.295, 'grad_norm': 0.2731155479815859, 'learning_rate': 4.354497837149141e-07, 'epoch': 0.97}

 97%|█████████▋| 3775/3886 [7:38:42<15:02,  8.13s/it]

 97%|█████████▋| 3776/3886 [7:38:48<13:54,  7.59s/it]


 97%|█████████▋| 3778/3886 [7:39:00<12:29,  6.94s/it]
{'loss': 1.3062, 'grad_norm': 0.24003656733859707, 'learning_rate': 4.0492228175820213e-07, 'epoch': 0.97}


 97%|█████████▋| 3780/3886 [7:39:13<11:31,  6.52s/it]

 97%|█████████▋| 3781/3886 [7:39:21<11:56,  6.82s/it]
{'loss': 1.3147, 'grad_norm': 0.2475521528569853, 'learning_rate': 3.827532018079727e-07, 'epoch': 0.97}

 97%|█████████▋| 3782/3886 [7:39:26<11:13,  6.48s/it]


 97%|█████████▋| 3784/3886 [7:39:39<11:08,  6.55s/it]

 97%|█████████▋| 3785/3886 [7:39:45<10:27,  6.21s/it]
{'loss': 1.4027, 'grad_norm': 0.22502881469113203, 'learning_rate': 3.5416342078680166e-07, 'epoch': 0.97}


 97%|█████████▋| 3787/3886 [7:39:57<10:27,  6.34s/it]

 97%|█████████▋| 3788/3886 [7:40:03<10:01,  6.14s/it]

 98%|█████████▊| 3789/3886 [7:40:11<10:53,  6.74s/it]
{'loss': 1.3015, 'grad_norm': 0.2537030933567721, 'learning_rate': 3.266813501025312e-07, 'epoch': 0.97}

 98%|█████████▊| 3790/3886 [7:40:16<09:52,  6.17s/it]


 98%|█████████▊| 3792/3886 [7:40:29<09:55,  6.34s/it]
{'loss': 1.4003, 'grad_norm': 0.2636811108857878, 'learning_rate': 3.067969162405704e-07, 'epoch': 0.98}


 98%|█████████▊| 3794/3886 [7:40:45<10:47,  7.04s/it]

 98%|█████████▊| 3795/3886 [7:40:51<10:21,  6.83s/it]

 98%|█████████▊| 3796/3886 [7:41:03<12:20,  8.23s/it]
{'loss': 1.288, 'grad_norm': 0.2705407999925878, 'learning_rate': 2.8125406635475247e-07, 'epoch': 0.98}

 98%|█████████▊| 3797/3886 [7:41:08<11:01,  7.43s/it]


 98%|█████████▊| 3799/3886 [7:41:21<10:05,  6.96s/it]

 98%|█████████▊| 3800/3886 [7:41:27<09:45,  6.81s/it]
{'loss': 1.2277, 'grad_norm': 0.23061864570668164, 'learning_rate': 2.5681973730077303e-07, 'epoch': 0.98}

 98%|█████████▊| 3801/3886 [7:41:35<09:51,  6.96s/it]


 98%|█████████▊| 3803/3886 [7:41:49<09:34,  6.93s/it]
{'loss': 1.0957, 'grad_norm': 0.25417745942205067, 'learning_rate': 2.392216211681286e-07, 'epoch': 0.98}

 98%|█████████▊| 3804/3886 [7:41:54<08:59,  6.58s/it]

 98%|█████████▊| 3805/3886 [7:42:00<08:35,  6.36s/it]

 98%|█████████▊| 3806/3886 [7:42:08<08:56,  6.70s/it]


 98%|█████████▊| 3808/3886 [7:42:21<08:32,  6.58s/it]

 98%|█████████▊| 3809/3886 [7:42:27<08:19,  6.49s/it]

 98%|█████████▊| 3810/3886 [7:42:34<08:15,  6.52s/it]

 98%|█████████▊| 3811/3886 [7:42:39<07:36,  6.09s/it]

 98%|█████████▊| 3812/3886 [7:42:45<07:32,  6.12s/it]

 98%|█████████▊| 3813/3886 [7:42:51<07:22,  6.06s/it]

 98%|█████████▊| 3814/3886 [7:43:00<08:16,  6.89s/it]

 98%|█████████▊| 3815/3886 [7:43:05<07:37,  6.44s/it]
{'loss': 1.4202, 'grad_norm': 0.2108405705754021, 'learning_rate': 1.7506825904014534e-07, 'epoch': 0.98}

 98%|█████████▊| 3816/3886 [7:43:10<07:06,  6.09s/it]

 98%|█████████▊| 3817/3886 [7:43:16<07:02,  6.12s/it]

 98%|█████████▊| 3818/3886 [7:43:23<06:57,  6.14s/it]

 98%|█████████▊| 3819/3886 [7:43:28<06:30,  5.83s/it]


 98%|█████████▊| 3821/3886 [7:43:42<06:55,  6.39s/it]
{'loss': 1.1242, 'grad_norm': 0.25611337420577446, 'learning_rate': 1.46736429792671e-07, 'epoch': 0.98}


 98%|█████████▊| 3823/3886 [7:43:57<07:29,  7.13s/it]

 98%|█████████▊| 3824/3886 [7:44:04<07:12,  6.98s/it]
{'loss': 1.1373, 'grad_norm': 0.26441016076531165, 'learning_rate': 1.335070483979517e-07, 'epoch': 0.98}

 98%|█████████▊| 3825/3886 [7:44:10<06:58,  6.87s/it]


 98%|█████████▊| 3827/3886 [7:44:33<09:34,  9.74s/it]
{'loss': 1.4359, 'grad_norm': 0.2398968651580059, 'learning_rate': 1.2090213408878503e-07, 'epoch': 0.98}

 99%|█████████▊| 3828/3886 [7:44:48<10:53, 11.27s/it]


 99%|█████████▊| 3830/3886 [7:45:01<08:14,  8.84s/it]

 99%|█████████▊| 3831/3886 [7:45:07<07:18,  7.96s/it]
{'loss': 1.3651, 'grad_norm': 0.22738560663645696, 'learning_rate': 1.050671105882306e-07, 'epoch': 0.99}


 99%|█████████▊| 3833/3886 [7:45:22<06:54,  7.82s/it]
{'loss': 1.2087, 'grad_norm': 0.22961038603832157, 'learning_rate': 9.756601809686183e-08, 'epoch': 0.99}


 99%|█████████▊| 3835/3886 [7:45:37<06:30,  7.66s/it]

 99%|█████████▊| 3836/3886 [7:45:46<06:35,  7.91s/it]

 99%|█████████▊| 3837/3886 [7:45:54<06:28,  7.93s/it]
{'loss': 1.2939, 'grad_norm': 0.24401747622742948, 'learning_rate': 8.339677586407524e-08, 'epoch': 0.99}


 99%|█████████▉| 3839/3886 [7:46:16<07:45,  9.90s/it]
[2024-05-28 08:20:45,894] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 3840/3886 [7:46:25<07:28,  9.74s/it]
[2024-05-28 08:20:55,253] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 3841/3886 [7:46:31<06:31,  8.70s/it]

 99%|█████████▉| 3842/3886 [7:46:39<06:09,  8.39s/it]

 99%|█████████▉| 3843/3886 [7:46:45<05:28,  7.63s/it]
{'loss': 1.2901, 'grad_norm': 0.2885156996472559, 'learning_rate': 6.422555893729199e-08, 'epoch': 0.99}

 99%|█████████▉| 3844/3886 [7:46:51<04:57,  7.09s/it]


 99%|█████████▉| 3846/3886 [7:47:02<04:10,  6.26s/it]

 99%|█████████▉| 3847/3886 [7:47:07<03:57,  6.09s/it]
{'loss': 1.0719, 'grad_norm': 0.2363635592376968, 'learning_rate': 5.2833385797468326e-08, 'epoch': 0.99}


 99%|█████████▉| 3849/3886 [7:47:19<03:44,  6.07s/it]

 99%|█████████▉| 3850/3886 [7:47:26<03:41,  6.14s/it]
{'loss': 1.04, 'grad_norm': 0.23391690301369691, 'learning_rate': 4.5018382648520654e-08, 'epoch': 0.99}

 99%|█████████▉| 3851/3886 [7:47:35<04:05,  7.01s/it]

 99%|█████████▉| 3852/3886 [7:47:41<03:47,  6.70s/it]


 99%|█████████▉| 3854/3886 [7:47:53<03:29,  6.56s/it]
{'loss': 1.2511, 'grad_norm': 0.23624839840337936, 'learning_rate': 3.557064029944268e-08, 'epoch': 0.99}

 99%|█████████▉| 3855/3886 [7:48:00<03:25,  6.63s/it]


 99%|█████████▉| 3857/3886 [7:48:14<03:14,  6.71s/it]
{'loss': 1.4393, 'grad_norm': 0.22638637828920152, 'learning_rate': 2.9214087360673882e-08, 'epoch': 0.99}


 99%|█████████▉| 3859/3886 [7:48:27<03:05,  6.86s/it]

 99%|█████████▉| 3860/3886 [7:48:35<03:08,  7.24s/it]
{'loss': 1.5542, 'grad_norm': 0.25392956841756115, 'learning_rate': 2.3482653653317255e-08, 'epoch': 0.99}


 99%|█████████▉| 3862/3886 [7:48:49<02:48,  7.03s/it]

 99%|█████████▉| 3863/3886 [7:48:56<02:40,  6.97s/it]

 99%|█████████▉| 3864/3886 [7:49:02<02:24,  6.57s/it]
{'loss': 1.2512, 'grad_norm': 0.24697948748124302, 'learning_rate': 1.681321111732803e-08, 'epoch': 0.99}


 99%|█████████▉| 3866/3886 [7:49:30<03:30, 10.55s/it]

100%|█████████▉| 3867/3886 [7:49:36<02:54,  9.18s/it]
{'loss': 1.3056, 'grad_norm': 0.2176341370641484, 'learning_rate': 1.2540521566417606e-08, 'epoch': 0.99}

100%|█████████▉| 3868/3886 [7:49:42<02:30,  8.34s/it]


100%|█████████▉| 3870/3886 [7:49:55<01:56,  7.26s/it]

100%|█████████▉| 3871/3886 [7:50:01<01:43,  6.92s/it]
{'loss': 1.3867, 'grad_norm': 0.2559432373403353, 'learning_rate': 7.816176095620797e-09, 'epoch': 1.0}

100%|█████████▉| 3872/3886 [7:50:08<01:36,  6.92s/it]

100%|█████████▉| 3873/3886 [7:50:20<01:49,  8.42s/it]


100%|█████████▉| 3875/3886 [7:50:34<01:24,  7.65s/it]

100%|█████████▉| 3876/3886 [7:50:40<01:11,  7.12s/it]
{'loss': 1.4173, 'grad_norm': 0.240550533909661, 'learning_rate': 3.4738811839574172e-09, 'epoch': 1.0}

100%|█████████▉| 3877/3886 [7:50:45<00:58,  6.53s/it]

100%|█████████▉| 3878/3886 [7:50:53<00:55,  6.96s/it]

100%|█████████▉| 3879/3886 [7:51:03<00:54,  7.78s/it]


100%|█████████▉| 3881/3886 [7:51:16<00:35,  7.17s/it]

100%|█████████▉| 3882/3886 [7:51:24<00:29,  7.45s/it]

100%|█████████▉| 3883/3886 [7:51:30<00:21,  7.10s/it]

100%|█████████▉| 3884/3886 [7:51:36<00:13,  6.70s/it]

100%|█████████▉| 3885/3886 [7:51:42<00:06,  6.59s/it]
{'loss': 1.4374, 'grad_norm': 0.23387151785081944, 'learning_rate': 3.473901095674137e-11, 'epoch': 1.0}
{'loss': 1.3179, 'grad_norm': 0.22767425125387125, 'learning_rate': 0.0, 'epoch': 1.0}

100%|██████████| 3886/3886 [7:51:49<00:00,  7.29s/it]
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(