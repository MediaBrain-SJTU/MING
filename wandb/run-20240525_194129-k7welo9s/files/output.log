/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3844 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3844 [00:48<51:38:45, 48.38s/it]

  0%|          | 2/3844 [00:56<26:25:38, 24.76s/it]

  0%|          | 3/3844 [01:05<18:36:56, 17.45s/it]

  0%|          | 4/3844 [01:14<14:56:53, 14.01s/it]
{'loss': 1.7616, 'grad_norm': 0.8649746885820413, 'learning_rate': 6.896551724137931e-07, 'epoch': 0.0}


  0%|          | 6/3844 [01:28<10:55:37, 10.25s/it]

  0%|          | 7/3844 [01:38<10:48:36, 10.14s/it]

  0%|          | 8/3844 [01:45<9:46:49,  9.18s/it]

  0%|          | 9/3844 [01:52<9:09:22,  8.60s/it]

  0%|          | 10/3844 [01:59<8:22:57,  7.87s/it]

  0%|          | 11/3844 [02:05<8:03:05,  7.56s/it]

  0%|          | 12/3844 [02:11<7:23:58,  6.95s/it]
{'loss': 1.9421, 'grad_norm': 0.9782667599079446, 'learning_rate': 2.0689655172413796e-06, 'epoch': 0.0}

  0%|          | 13/3844 [02:17<7:09:10,  6.72s/it]


  0%|          | 15/3844 [02:32<7:19:15,  6.88s/it]

  0%|          | 16/3844 [02:38<7:03:27,  6.64s/it]

  0%|          | 17/3844 [02:46<7:30:26,  7.06s/it]

  0%|          | 18/3844 [02:52<7:18:09,  6.87s/it]
{'loss': 2.0167, 'grad_norm': 0.9564271906304264, 'learning_rate': 3.103448275862069e-06, 'epoch': 0.0}

  0%|          | 19/3844 [03:00<7:33:54,  7.12s/it]


  1%|          | 21/3844 [03:14<7:23:29,  6.96s/it]
{'loss': 1.8812, 'grad_norm': 0.9003677019606086, 'learning_rate': 3.620689655172414e-06, 'epoch': 0.01}

  1%|          | 22/3844 [03:22<7:38:54,  7.20s/it]

  1%|          | 23/3844 [03:32<8:34:55,  8.09s/it]

  1%|          | 24/3844 [03:38<7:55:41,  7.47s/it]

  1%|          | 25/3844 [03:48<8:48:40,  8.31s/it]

  1%|          | 26/3844 [03:58<9:20:51,  8.81s/it]

  1%|          | 27/3844 [04:04<8:18:25,  7.83s/it]

  1%|          | 28/3844 [04:10<7:51:35,  7.41s/it]

  1%|          | 29/3844 [04:15<7:15:17,  6.85s/it]


  1%|          | 31/3844 [04:32<8:08:06,  7.68s/it]

  1%|          | 32/3844 [04:40<8:17:20,  7.83s/it]
{'loss': 1.8578, 'grad_norm': 0.9039406395895024, 'learning_rate': 5.517241379310345e-06, 'epoch': 0.01}


  1%|          | 34/3844 [04:56<8:16:04,  7.81s/it]

  1%|          | 35/3844 [05:04<8:16:38,  7.82s/it]

  1%|          | 36/3844 [05:14<8:59:44,  8.50s/it]
{'loss': 1.7144, 'grad_norm': 0.8354851709403371, 'learning_rate': 6.206896551724138e-06, 'epoch': 0.01}

  1%|          | 37/3844 [05:20<8:08:40,  7.70s/it]

  1%|          | 38/3844 [05:28<8:13:15,  7.78s/it]

  1%|          | 39/3844 [05:34<7:32:12,  7.13s/it]


  1%|          | 41/3844 [05:47<7:10:32,  6.79s/it]

  1%|          | 42/3844 [05:53<7:08:20,  6.76s/it]

  1%|          | 43/3844 [06:01<7:28:08,  7.07s/it]
{'loss': 1.638, 'grad_norm': 0.8069317115266623, 'learning_rate': 7.413793103448277e-06, 'epoch': 0.01}

  1%|          | 44/3844 [06:09<7:52:59,  7.47s/it]


  1%|          | 46/3844 [06:23<7:36:55,  7.22s/it]

  1%|          | 47/3844 [06:29<7:10:17,  6.80s/it]

  1%|          | 48/3844 [06:35<6:45:06,  6.40s/it]

  1%|▏         | 49/3844 [06:41<6:38:40,  6.30s/it]

  1%|▏         | 50/3844 [06:47<6:35:17,  6.25s/it]

  1%|▏         | 51/3844 [06:55<7:03:12,  6.69s/it]

  1%|▏         | 52/3844 [07:05<8:09:55,  7.75s/it]
{'loss': 1.5846, 'grad_norm': 0.9168108774501131, 'learning_rate': 8.965517241379312e-06, 'epoch': 0.01}

  1%|▏         | 53/3844 [07:10<7:22:59,  7.01s/it]


  1%|▏         | 55/3844 [07:27<8:01:53,  7.63s/it]
{'loss': 1.78, 'grad_norm': 1.0095692960766818, 'learning_rate': 9.482758620689655e-06, 'epoch': 0.01}


  1%|▏         | 57/3844 [07:47<9:27:43,  8.99s/it]

  2%|▏         | 58/3844 [07:53<8:25:53,  8.02s/it]

  2%|▏         | 59/3844 [07:58<7:39:07,  7.28s/it]

  2%|▏         | 60/3844 [08:05<7:22:17,  7.01s/it]

  2%|▏         | 61/3844 [08:11<6:57:32,  6.62s/it]

  2%|▏         | 62/3844 [08:17<6:47:32,  6.47s/it]

  2%|▏         | 63/3844 [08:26<7:38:54,  7.28s/it]

  2%|▏         | 64/3844 [08:32<7:13:53,  6.89s/it]

  2%|▏         | 65/3844 [08:38<7:00:36,  6.68s/it]

  2%|▏         | 66/3844 [08:46<7:17:41,  6.95s/it]

  2%|▏         | 67/3844 [08:53<7:32:03,  7.18s/it]

  2%|▏         | 68/3844 [09:00<7:20:31,  7.00s/it]
{'loss': 1.488, 'grad_norm': 0.46070230594867073, 'learning_rate': 1.1724137931034483e-05, 'epoch': 0.02}

  2%|▏         | 69/3844 [09:06<7:09:45,  6.83s/it]

  2%|▏         | 70/3844 [09:14<7:27:43,  7.12s/it]

  2%|▏         | 71/3844 [09:20<7:14:19,  6.91s/it]


  2%|▏         | 73/3844 [09:33<6:51:25,  6.55s/it]

  2%|▏         | 74/3844 [09:39<6:52:45,  6.57s/it]

  2%|▏         | 75/3844 [09:46<6:45:56,  6.46s/it]

  2%|▏         | 76/3844 [09:53<7:05:40,  6.78s/it]

  2%|▏         | 77/3844 [09:59<6:40:59,  6.39s/it]
{'loss': 1.5722, 'grad_norm': 0.25300644200994454, 'learning_rate': 1.327586206896552e-05, 'epoch': 0.02}


  2%|▏         | 79/3844 [10:15<7:32:24,  7.21s/it]

  2%|▏         | 80/3844 [10:21<7:17:48,  6.98s/it]
{'loss': 1.4704, 'grad_norm': 0.2718295538458367, 'learning_rate': 1.3793103448275863e-05, 'epoch': 0.02}


  2%|▏         | 82/3844 [10:37<7:42:42,  7.38s/it]

  2%|▏         | 83/3844 [10:51<9:42:26,  9.29s/it]
{'loss': 1.3462, 'grad_norm': 0.22335794581916515, 'learning_rate': 1.4310344827586209e-05, 'epoch': 0.02}


  2%|▏         | 85/3844 [11:08<9:07:04,  8.73s/it]
{'loss': 1.5078, 'grad_norm': 0.23080478325401052, 'learning_rate': 1.4655172413793105e-05, 'epoch': 0.02}


  2%|▏         | 87/3844 [11:23<8:44:03,  8.37s/it]

  2%|▏         | 88/3844 [11:30<8:07:28,  7.79s/it]

  2%|▏         | 89/3844 [11:35<7:27:05,  7.14s/it]

  2%|▏         | 90/3844 [11:43<7:40:13,  7.36s/it]

  2%|▏         | 91/3844 [11:49<7:12:16,  6.91s/it]

  2%|▏         | 92/3844 [11:57<7:39:54,  7.35s/it]
{'loss': 1.5205, 'grad_norm': 0.19387775813267838, 'learning_rate': 1.586206896551724e-05, 'epoch': 0.02}

  2%|▏         | 93/3844 [12:04<7:33:08,  7.25s/it]


  2%|▏         | 95/3844 [12:18<7:19:11,  7.03s/it]
{'loss': 1.5888, 'grad_norm': 0.2537477470315232, 'learning_rate': 1.637931034482759e-05, 'epoch': 0.02}

  2%|▏         | 96/3844 [12:27<7:51:06,  7.54s/it]


  3%|▎         | 98/3844 [12:39<7:15:35,  6.98s/it]
{'loss': 1.5566, 'grad_norm': 0.1941739287361966, 'learning_rate': 1.6896551724137932e-05, 'epoch': 0.03}


  3%|▎         | 100/3844 [12:53<7:18:12,  7.02s/it]

  3%|▎         | 101/3844 [12:59<6:51:40,  6.60s/it]

  3%|▎         | 102/3844 [13:07<7:15:08,  6.98s/it]

  3%|▎         | 103/3844 [13:13<6:57:43,  6.70s/it]

  3%|▎         | 104/3844 [13:19<6:50:46,  6.59s/it]

  3%|▎         | 105/3844 [13:25<6:38:55,  6.40s/it]

  3%|▎         | 106/3844 [13:32<6:44:27,  6.49s/it]
{'loss': 1.4469, 'grad_norm': 0.19664657936526628, 'learning_rate': 1.827586206896552e-05, 'epoch': 0.03}

  3%|▎         | 107/3844 [13:38<6:42:36,  6.46s/it]

  3%|▎         | 108/3844 [13:44<6:33:24,  6.32s/it]


  3%|▎         | 110/3844 [14:00<7:16:11,  7.01s/it]

  3%|▎         | 111/3844 [14:08<7:35:05,  7.31s/it]

  3%|▎         | 112/3844 [14:16<7:46:09,  7.49s/it]
{'loss': 1.3601, 'grad_norm': 0.1621631891408783, 'learning_rate': 1.931034482758621e-05, 'epoch': 0.03}

  3%|▎         | 113/3844 [14:26<8:41:24,  8.38s/it]


  3%|▎         | 115/3844 [14:39<7:42:20,  7.44s/it]

  3%|▎         | 116/3844 [14:47<7:52:32,  7.61s/it]

  3%|▎         | 117/3844 [14:53<7:23:39,  7.14s/it]
{'loss': 1.4972, 'grad_norm': 0.1700731374278455, 'learning_rate': 1.9999996449267818e-05, 'epoch': 0.03}


  3%|▎         | 119/3844 [15:05<6:55:41,  6.70s/it]
{'loss': 1.4526, 'grad_norm': 0.18350765988493808, 'learning_rate': 1.9999968043425472e-05, 'epoch': 0.03}

  3%|▎         | 120/3844 [15:15<7:43:18,  7.46s/it]

  3%|▎         | 121/3844 [15:20<7:09:34,  6.92s/it]


  3%|▎         | 123/3844 [15:33<6:52:07,  6.65s/it]
{'loss': 1.3796, 'grad_norm': 0.1769176599985527, 'learning_rate': 1.9999826014617196e-05, 'epoch': 0.03}


  3%|▎         | 125/3844 [15:46<6:43:18,  6.51s/it]
{'loss': 1.3799, 'grad_norm': 0.1713624970192378, 'learning_rate': 1.9999712392054706e-05, 'epoch': 0.03}


  3%|▎         | 127/3844 [16:01<7:17:50,  7.07s/it]

  3%|▎         | 128/3844 [16:11<8:06:11,  7.85s/it]
{'loss': 1.271, 'grad_norm': 0.1946225619478232, 'learning_rate': 1.9999488698892415e-05, 'epoch': 0.03}


  3%|▎         | 130/3844 [16:25<7:43:54,  7.49s/it]

  3%|▎         | 131/3844 [16:36<8:39:16,  8.39s/it]

  3%|▎         | 132/3844 [16:42<7:47:42,  7.56s/it]

  3%|▎         | 133/3844 [16:52<8:34:16,  8.31s/it]
{'loss': 1.3853, 'grad_norm': 0.16714866214116214, 'learning_rate': 1.9998973855888032e-05, 'epoch': 0.03}

  3%|▎         | 134/3844 [16:58<8:04:34,  7.84s/it]

  4%|▎         | 135/3844 [17:06<8:06:17,  7.87s/it]


  4%|▎         | 137/3844 [17:26<8:57:20,  8.70s/it]
{'loss': 1.3328, 'grad_norm': 0.18753611000979098, 'learning_rate': 1.999843416787962e-05, 'epoch': 0.04}


  4%|▎         | 139/3844 [17:40<8:02:09,  7.81s/it]

  4%|▎         | 140/3844 [17:46<7:18:18,  7.10s/it]
{'loss': 1.3302, 'grad_norm': 0.17663580583219868, 'learning_rate': 1.9997954847855428e-05, 'epoch': 0.04}


  4%|▎         | 142/3844 [18:00<7:26:26,  7.24s/it]

  4%|▎         | 143/3844 [18:10<8:12:47,  7.99s/it]

  4%|▎         | 144/3844 [18:18<8:16:38,  8.05s/it]
{'loss': 1.3642, 'grad_norm': 0.1802151915650107, 'learning_rate': 1.9997216354957054e-05, 'epoch': 0.04}


  4%|▍         | 146/3844 [18:31<7:26:06,  7.24s/it]
{'loss': 1.4746, 'grad_norm': 0.18152272312729537, 'learning_rate': 1.9996804511045385e-05, 'epoch': 0.04}


  4%|▍         | 148/3844 [18:44<6:52:18,  6.69s/it]

  4%|▍         | 149/3844 [18:50<6:45:45,  6.59s/it]

  4%|▍         | 150/3844 [18:57<6:56:32,  6.77s/it]

  4%|▍         | 151/3844 [19:03<6:38:50,  6.48s/it]
{'loss': 1.3957, 'grad_norm': 0.17873605818415134, 'learning_rate': 1.9995650668131578e-05, 'epoch': 0.04}

  4%|▍         | 152/3844 [19:10<6:52:18,  6.70s/it]


  4%|▍         | 154/3844 [19:22<6:26:24,  6.28s/it]

  4%|▍         | 155/3844 [19:28<6:19:31,  6.17s/it]

  4%|▍         | 156/3844 [19:34<6:15:34,  6.11s/it]

  4%|▍         | 157/3844 [19:41<6:38:48,  6.49s/it]

  4%|▍         | 158/3844 [19:49<7:06:42,  6.95s/it]

  4%|▍         | 159/3844 [19:55<6:48:26,  6.65s/it]

  4%|▍         | 160/3844 [20:02<6:38:15,  6.49s/it]
{'loss': 1.4629, 'grad_norm': 0.17636209679377818, 'learning_rate': 1.9993126569629577e-05, 'epoch': 0.04}


  4%|▍         | 162/3844 [20:14<6:38:15,  6.49s/it]

  4%|▍         | 163/3844 [20:20<6:24:29,  6.27s/it]

  4%|▍         | 164/3844 [20:26<6:15:41,  6.13s/it]
{'loss': 1.4292, 'grad_norm': 0.17807992555805985, 'learning_rate': 1.999182022795116e-05, 'epoch': 0.04}


  4%|▍         | 166/3844 [20:42<7:27:03,  7.29s/it]
{'loss': 1.2602, 'grad_norm': 0.17170028362894632, 'learning_rate': 1.999112448223846e-05, 'epoch': 0.04}

  4%|▍         | 167/3844 [20:49<7:08:18,  6.99s/it]


  4%|▍         | 169/3844 [21:00<6:32:07,  6.40s/it]

  4%|▍         | 170/3844 [21:06<6:12:40,  6.09s/it]

  4%|▍         | 171/3844 [21:12<6:20:13,  6.21s/it]

  4%|▍         | 172/3844 [21:18<6:13:07,  6.10s/it]
{'loss': 1.336, 'grad_norm': 0.17819422069144963, 'learning_rate': 1.9988866969564152e-05, 'epoch': 0.04}


  5%|▍         | 174/3844 [21:33<7:19:14,  7.18s/it]

  5%|▍         | 175/3844 [21:44<8:12:43,  8.06s/it]

  5%|▍         | 176/3844 [21:52<8:20:49,  8.19s/it]

  5%|▍         | 177/3844 [22:00<8:07:17,  7.97s/it]
{'loss': 1.1699, 'grad_norm': 0.16476546298817424, 'learning_rate': 1.998679063390786e-05, 'epoch': 0.05}


  5%|▍         | 179/3844 [22:12<7:07:24,  7.00s/it]

  5%|▍         | 180/3844 [22:18<6:55:44,  6.81s/it]

  5%|▍         | 181/3844 [22:26<7:18:26,  7.18s/it]
{'loss': 1.2612, 'grad_norm': 0.17064190084033284, 'learning_rate': 1.9985001906180847e-05, 'epoch': 0.05}

  5%|▍         | 182/3844 [22:33<7:08:53,  7.03s/it]


  5%|▍         | 184/3844 [22:50<8:05:21,  7.96s/it]
{'loss': 1.3473, 'grad_norm': 0.17688441961557994, 'learning_rate': 1.9983585905749714e-05, 'epoch': 0.05}


  5%|▍         | 186/3844 [23:08<8:41:09,  8.55s/it]

  5%|▍         | 187/3844 [23:14<8:03:11,  7.93s/it]
{'loss': 1.3423, 'grad_norm': 0.17430702644309012, 'learning_rate': 1.998210609707717e-05, 'epoch': 0.05}


  5%|▍         | 189/3844 [23:33<8:58:53,  8.85s/it]
{'loss': 1.3232, 'grad_norm': 0.18678385497983854, 'learning_rate': 1.9981084113592933e-05, 'epoch': 0.05}


  5%|▍         | 191/3844 [23:48<8:10:47,  8.06s/it]
{'loss': 1.3329, 'grad_norm': 0.1727058821510057, 'learning_rate': 1.998003377798845e-05, 'epoch': 0.05}

  5%|▍         | 192/3844 [23:53<7:21:15,  7.25s/it]


  5%|▌         | 194/3844 [24:08<7:26:44,  7.34s/it]
{'loss': 1.3563, 'grad_norm': 0.1911308518532482, 'learning_rate': 1.9978405120904848e-05, 'epoch': 0.05}

  5%|▌         | 195/3844 [24:15<7:25:00,  7.32s/it]


  5%|▌         | 197/3844 [24:34<8:34:04,  8.46s/it]

  5%|▌         | 198/3844 [24:42<8:19:29,  8.22s/it]

  5%|▌         | 199/3844 [24:48<7:41:51,  7.60s/it]

  5%|▌         | 200/3844 [24:54<7:06:20,  7.02s/it]
{'loss': 1.2501, 'grad_norm': 0.17980007381560947, 'learning_rate': 1.997495649216635e-05, 'epoch': 0.05}

  5%|▌         | 201/3844 [24:59<6:38:44,  6.57s/it]


  5%|▌         | 203/3844 [25:10<6:04:26,  6.01s/it]

  5%|▌         | 204/3844 [25:18<6:46:14,  6.70s/it]

  5%|▌         | 205/3844 [25:24<6:23:48,  6.33s/it]
{'loss': 1.3626, 'grad_norm': 0.18952877854040803, 'learning_rate': 1.99718878301554e-05, 'epoch': 0.05}


  5%|▌         | 207/3844 [25:40<7:06:43,  7.04s/it]
{'loss': 1.206, 'grad_norm': 0.20180228396203434, 'learning_rate': 1.9970610791760655e-05, 'epoch': 0.05}

  5%|▌         | 208/3844 [25:49<7:50:19,  7.76s/it]


  5%|▌         | 210/3844 [26:00<6:39:38,  6.60s/it]
{'loss': 1.4187, 'grad_norm': 0.21131481307135508, 'learning_rate': 1.9968642130880613e-05, 'epoch': 0.05}

  5%|▌         | 211/3844 [26:07<6:37:08,  6.56s/it]

  6%|▌         | 212/3844 [26:15<7:07:02,  7.05s/it]

  6%|▌         | 213/3844 [26:24<7:34:01,  7.50s/it]


  6%|▌         | 215/3844 [26:38<7:36:54,  7.55s/it]
{'loss': 1.3992, 'grad_norm': 0.20708288812065734, 'learning_rate': 1.996521945196495e-05, 'epoch': 0.06}


  6%|▌         | 217/3844 [26:52<7:12:16,  7.15s/it]

  6%|▌         | 218/3844 [27:03<8:16:42,  8.22s/it]
{'loss': 1.2845, 'grad_norm': 0.19315387691876607, 'learning_rate': 1.9963080919535492e-05, 'epoch': 0.06}

  6%|▌         | 219/3844 [27:09<7:53:37,  7.84s/it]


  6%|▌         | 221/3844 [27:20<6:40:12,  6.63s/it]

  6%|▌         | 222/3844 [27:29<7:09:59,  7.12s/it]

  6%|▌         | 223/3844 [27:34<6:42:40,  6.67s/it]

  6%|▌         | 224/3844 [27:41<6:35:21,  6.55s/it]
{'loss': 1.105, 'grad_norm': 0.17715858263015216, 'learning_rate': 1.9958612837188834e-05, 'epoch': 0.06}


  6%|▌         | 226/3844 [27:55<6:51:39,  6.83s/it]

  6%|▌         | 227/3844 [28:00<6:27:18,  6.42s/it]

  6%|▌         | 228/3844 [28:07<6:31:49,  6.50s/it]

  6%|▌         | 229/3844 [28:13<6:21:19,  6.33s/it]

  6%|▌         | 230/3844 [28:18<6:07:42,  6.10s/it]

  6%|▌         | 231/3844 [28:25<6:14:42,  6.22s/it]

  6%|▌         | 232/3844 [28:34<7:13:36,  7.20s/it]

  6%|▌         | 233/3844 [28:42<7:19:17,  7.30s/it]

  6%|▌         | 234/3844 [28:55<9:00:19,  8.98s/it]
{'loss': 1.2395, 'grad_norm': 0.19714172762618237, 'learning_rate': 1.9950600327930877e-05, 'epoch': 0.06}


  6%|▌         | 236/3844 [29:10<8:25:35,  8.41s/it]

  6%|▌         | 237/3844 [29:19<8:29:16,  8.47s/it]

  6%|▌         | 238/3844 [29:24<7:33:52,  7.55s/it]
{'loss': 1.2041, 'grad_norm': 0.21188502400068862, 'learning_rate': 1.994719743310194e-05, 'epoch': 0.06}


  6%|▌         | 240/3844 [29:40<7:51:03,  7.84s/it]

  6%|▋         | 241/3844 [29:47<7:31:24,  7.52s/it]

  6%|▋         | 242/3844 [29:56<8:02:43,  8.04s/it]
{'loss': 1.0935, 'grad_norm': 0.19427581347838627, 'learning_rate': 1.9943681514904307e-05, 'epoch': 0.06}


  6%|▋         | 244/3844 [30:12<8:09:41,  8.16s/it]
{'loss': 1.308, 'grad_norm': 0.20039799199607547, 'learning_rate': 1.9941881184476154e-05, 'epoch': 0.06}

  6%|▋         | 245/3844 [30:20<7:56:31,  7.94s/it]


  6%|▋         | 247/3844 [30:34<7:29:11,  7.49s/it]
{'loss': 1.3464, 'grad_norm': 0.20178954363186294, 'learning_rate': 1.9939127739027143e-05, 'epoch': 0.06}


  6%|▋         | 249/3844 [30:48<7:14:56,  7.26s/it]

  7%|▋         | 250/3844 [30:54<6:52:52,  6.89s/it]

  7%|▋         | 251/3844 [31:00<6:36:02,  6.61s/it]
{'loss': 1.2217, 'grad_norm': 0.22158824947801334, 'learning_rate': 1.9935357666269287e-05, 'epoch': 0.07}


  7%|▋         | 253/3844 [31:13<6:34:36,  6.59s/it]
{'loss': 1.3575, 'grad_norm': 0.20700018166962034, 'learning_rate': 1.9933430293847452e-05, 'epoch': 0.07}

  7%|▋         | 254/3844 [31:19<6:26:03,  6.45s/it]

  7%|▋         | 255/3844 [31:26<6:27:53,  6.48s/it]


  7%|▋         | 257/3844 [31:43<7:16:27,  7.30s/it]

  7%|▋         | 258/3844 [31:48<6:45:43,  6.79s/it]

  7%|▋         | 259/3844 [31:56<7:05:56,  7.13s/it]
{'loss': 1.2835, 'grad_norm': 0.22731336573841526, 'learning_rate': 1.992747889834912e-05, 'epoch': 0.07}

  7%|▋         | 260/3844 [32:03<7:02:43,  7.08s/it]


  7%|▋         | 262/3844 [32:18<7:20:51,  7.38s/it]

  7%|▋         | 263/3844 [32:25<7:00:42,  7.05s/it]

  7%|▋         | 264/3844 [32:30<6:39:28,  6.70s/it]
{'loss': 1.3805, 'grad_norm': 0.21747767805202586, 'learning_rate': 1.9922325521846488e-05, 'epoch': 0.07}


  7%|▋         | 266/3844 [32:45<6:45:59,  6.81s/it]
{'loss': 1.3683, 'grad_norm': 0.22531965329146933, 'learning_rate': 1.9920214841958083e-05, 'epoch': 0.07}

  7%|▋         | 267/3844 [32:52<6:46:25,  6.82s/it]


  7%|▋         | 269/3844 [33:04<6:29:24,  6.54s/it]
{'loss': 1.3181, 'grad_norm': 0.21843561257102098, 'learning_rate': 1.991699598799119e-05, 'epoch': 0.07}


  7%|▋         | 271/3844 [33:15<5:56:46,  5.99s/it]

  7%|▋         | 272/3844 [33:24<6:51:40,  6.92s/it]
{'loss': 1.3179, 'grad_norm': 0.2074309351647471, 'learning_rate': 1.991371375138001e-05, 'epoch': 0.07}

  7%|▋         | 273/3844 [33:30<6:31:26,  6.58s/it]

  7%|▋         | 274/3844 [33:36<6:19:14,  6.37s/it]


  7%|▋         | 276/3844 [33:50<6:48:48,  6.87s/it]
{'loss': 1.2317, 'grad_norm': 0.21313950353436345, 'learning_rate': 1.9909238876990283e-05, 'epoch': 0.07}

  7%|▋         | 277/3844 [33:56<6:30:07,  6.56s/it]


  7%|▋         | 279/3844 [34:08<6:16:50,  6.34s/it]

  7%|▋         | 280/3844 [34:17<7:00:07,  7.07s/it]

  7%|▋         | 281/3844 [34:25<7:11:28,  7.27s/it]

  7%|▋         | 282/3844 [34:33<7:28:50,  7.56s/it]
{'loss': 1.2363, 'grad_norm': 0.21671344539968324, 'learning_rate': 1.990231547151157e-05, 'epoch': 0.07}


  7%|▋         | 284/3844 [34:55<9:16:05,  9.37s/it]

  7%|▋         | 285/3844 [35:01<8:16:16,  8.37s/it]
{'loss': 1.2114, 'grad_norm': 0.22602153535177294, 'learning_rate': 1.9898758824383925e-05, 'epoch': 0.07}

  7%|▋         | 286/3844 [35:08<7:52:52,  7.97s/it]

  7%|▋         | 287/3844 [35:17<8:19:09,  8.42s/it]


  8%|▊         | 289/3844 [35:30<7:15:33,  7.35s/it]
{'loss': 1.4259, 'grad_norm': 0.2344208069948388, 'learning_rate': 1.9893918218291174e-05, 'epoch': 0.08}

  8%|▊         | 290/3844 [35:38<7:18:19,  7.40s/it]

  8%|▊         | 291/3844 [35:50<8:43:11,  8.84s/it]


  8%|▊         | 293/3844 [36:05<8:00:01,  8.11s/it]

  8%|▊         | 294/3844 [36:11<7:26:03,  7.54s/it]

  8%|▊         | 295/3844 [36:17<6:56:01,  7.03s/it]
{'loss': 1.2707, 'grad_norm': 0.23793039384102482, 'learning_rate': 1.988644654295305e-05, 'epoch': 0.08}

  8%|▊         | 296/3844 [36:24<6:49:56,  6.93s/it]


  8%|▊         | 298/3844 [36:37<6:50:54,  6.95s/it]

  8%|▊         | 299/3844 [36:43<6:30:12,  6.60s/it]

  8%|▊         | 300/3844 [36:52<7:18:17,  7.42s/it]
  8%|▊         | 300/3844 [36:52<7:18:17,  7.42s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  8%|▊         | 301/3844 [37:33<16:57:34, 17.23s/it]
{'loss': 1.1652, 'grad_norm': 0.21834067663963916, 'learning_rate': 1.9878722118446153e-05, 'epoch': 0.08}

  8%|▊         | 302/3844 [37:38<13:31:29, 13.75s/it]

  8%|▊         | 303/3844 [37:46<11:49:30, 12.02s/it]


  8%|▊         | 305/3844 [38:01<9:36:06,  9.77s/it]
{'loss': 1.2398, 'grad_norm': 0.2078404348410905, 'learning_rate': 1.98734321831569e-05, 'epoch': 0.08}

  8%|▊         | 306/3844 [38:08<8:31:48,  8.68s/it]


  8%|▊         | 308/3844 [38:21<7:27:03,  7.59s/it]

  8%|▊         | 309/3844 [38:27<7:04:43,  7.21s/it]

  8%|▊         | 310/3844 [38:33<6:38:56,  6.77s/it]

  8%|▊         | 311/3844 [38:40<6:55:36,  7.06s/it]
{'loss': 1.1736, 'grad_norm': 0.2180452881955673, 'learning_rate': 1.986528695206091e-05, 'epoch': 0.08}


  8%|▊         | 313/3844 [38:53<6:31:28,  6.65s/it]

  8%|▊         | 314/3844 [39:05<8:01:54,  8.19s/it]
[2024-05-25 20:20:46,691] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.21, 'grad_norm': 0.21478043872045868, 'learning_rate': 1.986111974516412e-05, 'epoch': 0.08}

  8%|▊         | 315/3844 [39:12<7:36:00,  7.75s/it]

  8%|▊         | 316/3844 [39:18<7:03:55,  7.21s/it]


  8%|▊         | 318/3844 [39:35<7:35:09,  7.75s/it]
{'loss': 1.3561, 'grad_norm': 0.24271685929291142, 'learning_rate': 1.9855465434262863e-05, 'epoch': 0.08}


  8%|▊         | 320/3844 [39:49<7:21:36,  7.52s/it]

  8%|▊         | 321/3844 [40:01<8:37:50,  8.82s/it]
{'loss': 1.2506, 'grad_norm': 0.22284043665031908, 'learning_rate': 1.9851151209942398e-05, 'epoch': 0.08}


  8%|▊         | 323/3844 [40:13<7:13:15,  7.38s/it]

  8%|▊         | 324/3844 [40:19<6:44:39,  6.90s/it]

  8%|▊         | 325/3844 [40:27<6:52:39,  7.04s/it]

  8%|▊         | 326/3844 [40:33<6:38:20,  6.79s/it]

  9%|▊         | 327/3844 [40:39<6:24:54,  6.57s/it]

  9%|▊         | 328/3844 [40:45<6:08:27,  6.29s/it]

  9%|▊         | 329/3844 [40:53<6:46:15,  6.93s/it]

  9%|▊         | 330/3844 [41:01<7:00:29,  7.18s/it]
{'loss': 1.2038, 'grad_norm': 0.24393624030630362, 'learning_rate': 1.9837830878429728e-05, 'epoch': 0.09}


  9%|▊         | 332/3844 [41:15<6:54:02,  7.07s/it]

  9%|▊         | 333/3844 [41:25<7:40:38,  7.87s/it]
{'loss': 1.202, 'grad_norm': 0.22452787760532164, 'learning_rate': 1.983326497633558e-05, 'epoch': 0.09}


  9%|▊         | 335/3844 [41:40<7:28:51,  7.67s/it]
{'loss': 1.3026, 'grad_norm': 0.23894527401751858, 'learning_rate': 1.9830186124514355e-05, 'epoch': 0.09}


  9%|▉         | 337/3844 [41:56<7:37:56,  7.83s/it]

  9%|▉         | 338/3844 [42:03<7:33:03,  7.75s/it]
{'loss': 1.241, 'grad_norm': 0.23925963089661334, 'learning_rate': 1.982551549300917e-05, 'epoch': 0.09}

  9%|▉         | 339/3844 [42:10<7:15:32,  7.46s/it]

  9%|▉         | 340/3844 [42:16<6:56:25,  7.13s/it]


  9%|▉         | 342/3844 [42:30<6:45:32,  6.95s/it]

  9%|▉         | 343/3844 [42:37<6:55:03,  7.11s/it]

  9%|▉         | 344/3844 [42:46<7:20:05,  7.54s/it]
{'loss': 1.133, 'grad_norm': 0.24270043947773667, 'learning_rate': 1.9815985866360755e-05, 'epoch': 0.09}

  9%|▉         | 345/3844 [42:52<7:04:21,  7.28s/it]


  9%|▉         | 347/3844 [43:08<7:04:55,  7.29s/it]

  9%|▉         | 348/3844 [43:13<6:38:21,  6.84s/it]

  9%|▉         | 349/3844 [43:21<6:51:25,  7.06s/it]

  9%|▉         | 350/3844 [43:27<6:30:23,  6.70s/it]

  9%|▉         | 351/3844 [43:33<6:27:11,  6.65s/it]
{'loss': 1.2057, 'grad_norm': 0.2318540934805178, 'learning_rate': 1.9804550815898303e-05, 'epoch': 0.09}

  9%|▉         | 352/3844 [43:43<7:09:39,  7.38s/it]


  9%|▉         | 354/3844 [43:54<6:15:56,  6.46s/it]
{'loss': 1.3989, 'grad_norm': 0.2519059848725774, 'learning_rate': 1.979954561665812e-05, 'epoch': 0.09}

  9%|▉         | 355/3844 [44:02<6:51:28,  7.08s/it]


  9%|▉         | 357/3844 [44:15<6:39:26,  6.87s/it]

  9%|▉         | 358/3844 [44:22<6:30:27,  6.72s/it]
{'loss': 1.3157, 'grad_norm': 0.2544834174773078, 'learning_rate': 1.9792774595730462e-05, 'epoch': 0.09}


  9%|▉         | 360/3844 [44:35<6:23:12,  6.60s/it]
{'loss': 1.2893, 'grad_norm': 0.26605667597114313, 'learning_rate': 1.9789347354621947e-05, 'epoch': 0.09}


  9%|▉         | 362/3844 [44:51<6:58:45,  7.22s/it]
{'loss': 1.1445, 'grad_norm': 0.24501287187733903, 'learning_rate': 1.9785892306037796e-05, 'epoch': 0.09}


  9%|▉         | 364/3844 [45:07<7:36:28,  7.87s/it]

  9%|▉         | 365/3844 [45:13<7:10:34,  7.43s/it]
{'loss': 1.3217, 'grad_norm': 0.2470949550200527, 'learning_rate': 1.9780657615635263e-05, 'epoch': 0.09}

 10%|▉         | 366/3844 [45:20<7:00:52,  7.26s/it]

 10%|▉         | 367/3844 [45:27<6:43:55,  6.97s/it]

 10%|▉         | 368/3844 [45:34<6:50:40,  7.09s/it]


 10%|▉         | 370/3844 [45:48<6:43:41,  6.97s/it]
{'loss': 1.1909, 'grad_norm': 0.25552937382061613, 'learning_rate': 1.9771794234416307e-05, 'epoch': 0.1}

 10%|▉         | 371/3844 [45:56<7:14:43,  7.51s/it]


 10%|▉         | 373/3844 [46:08<6:19:31,  6.56s/it]
{'loss': 1.3244, 'grad_norm': 0.24716034054772554, 'learning_rate': 1.9766392922745898e-05, 'epoch': 0.1}

 10%|▉         | 374/3844 [46:17<6:57:38,  7.22s/it]


 10%|▉         | 376/3844 [46:28<6:10:21,  6.41s/it]
{'loss': 1.3178, 'grad_norm': 0.2484247251943376, 'learning_rate': 1.976092919098282e-05, 'epoch': 0.1}

 10%|▉         | 377/3844 [46:36<6:44:11,  7.00s/it]

 10%|▉         | 378/3844 [46:42<6:24:21,  6.65s/it]


 10%|▉         | 380/3844 [46:57<7:01:59,  7.31s/it]

 10%|▉         | 381/3844 [47:04<6:43:08,  6.98s/it]
{'loss': 1.2905, 'grad_norm': 0.25904513923227374, 'learning_rate': 1.9751684355086775e-05, 'epoch': 0.1}

 10%|▉         | 382/3844 [47:11<6:45:26,  7.03s/it]

 10%|▉         | 383/3844 [47:20<7:27:54,  7.77s/it]

 10%|▉         | 384/3844 [47:29<7:43:30,  8.04s/it]


 10%|█         | 386/3844 [47:46<7:57:01,  8.28s/it]

 10%|█         | 387/3844 [47:56<8:27:42,  8.81s/it]

 10%|█         | 388/3844 [48:04<8:17:06,  8.63s/it]

 10%|█         | 389/3844 [48:12<7:58:55,  8.32s/it]

 10%|█         | 390/3844 [48:19<7:47:21,  8.12s/it]

 10%|█         | 391/3844 [48:27<7:45:44,  8.09s/it]

 10%|█         | 392/3844 [48:34<7:23:49,  7.71s/it]

 10%|█         | 393/3844 [48:40<6:49:03,  7.11s/it]

 10%|█         | 394/3844 [48:48<6:59:04,  7.29s/it]
{'loss': 1.2079, 'grad_norm': 0.24828970214010868, 'learning_rate': 1.972683796190045e-05, 'epoch': 0.1}


 10%|█         | 396/3844 [49:02<6:59:44,  7.30s/it]

 10%|█         | 397/3844 [49:08<6:29:03,  6.77s/it]
{'loss': 1.3046, 'grad_norm': 0.2684051088261627, 'learning_rate': 1.9720938289573338e-05, 'epoch': 0.1}


 10%|█         | 399/3844 [49:24<7:07:18,  7.44s/it]

 10%|█         | 400/3844 [49:32<7:15:44,  7.59s/it]

 10%|█         | 401/3844 [49:40<7:21:29,  7.69s/it]

 10%|█         | 402/3844 [49:46<6:46:12,  7.08s/it]

 10%|█         | 403/3844 [49:57<8:07:43,  8.50s/it]

 11%|█         | 404/3844 [50:06<8:05:40,  8.47s/it]
{'loss': 1.3846, 'grad_norm': 0.29253076414120593, 'learning_rate': 1.9706930838367517e-05, 'epoch': 0.11}

 11%|█         | 405/3844 [50:15<8:12:02,  8.58s/it]


 11%|█         | 407/3844 [50:26<6:45:31,  7.08s/it]
{'loss': 1.3681, 'grad_norm': 0.24588751957161767, 'learning_rate': 1.970082421631491e-05, 'epoch': 0.11}


 11%|█         | 409/3844 [50:38<6:10:34,  6.47s/it]
{'loss': 1.3013, 'grad_norm': 0.2731761137828003, 'learning_rate': 1.969671868750853e-05, 'epoch': 0.11}

 11%|█         | 410/3844 [50:44<6:10:28,  6.47s/it]


 11%|█         | 412/3844 [51:02<7:28:04,  7.83s/it]

 11%|█         | 413/3844 [51:08<6:58:04,  7.31s/it]

 11%|█         | 414/3844 [51:13<6:22:12,  6.69s/it]
{'loss': 1.3323, 'grad_norm': 0.26388326011147367, 'learning_rate': 1.9686334384638418e-05, 'epoch': 0.11}

 11%|█         | 415/3844 [51:21<6:39:25,  6.99s/it]


 11%|█         | 417/3844 [51:36<6:54:26,  7.26s/it]
{'loss': 1.1178, 'grad_norm': 0.2487566049844101, 'learning_rate': 1.9680021246517368e-05, 'epoch': 0.11}

 11%|█         | 418/3844 [51:43<6:41:15,  7.03s/it]

 11%|█         | 419/3844 [51:48<6:19:43,  6.65s/it]

 11%|█         | 420/3844 [51:57<6:54:45,  7.27s/it]

 11%|█         | 421/3844 [52:05<7:09:10,  7.52s/it]


 11%|█         | 423/3844 [52:18<6:37:37,  6.97s/it]

 11%|█         | 424/3844 [52:25<6:39:42,  7.01s/it]

 11%|█         | 425/3844 [52:32<6:38:23,  6.99s/it]
{'loss': 1.1815, 'grad_norm': 0.2601288040827586, 'learning_rate': 1.966288385628026e-05, 'epoch': 0.11}


 11%|█         | 427/3844 [52:46<6:32:17,  6.89s/it]

 11%|█         | 428/3844 [52:54<6:38:18,  7.00s/it]
{'loss': 1.2551, 'grad_norm': 0.25492285380692503, 'learning_rate': 1.9656344068860235e-05, 'epoch': 0.11}


 11%|█         | 430/3844 [53:08<6:54:00,  7.28s/it]
{'loss': 1.1821, 'grad_norm': 0.24848176718446896, 'learning_rate': 1.965194992090843e-05, 'epoch': 0.11}


 11%|█         | 432/3844 [53:23<6:58:27,  7.36s/it]

 11%|█▏        | 433/3844 [53:32<7:12:21,  7.61s/it]

 11%|█▏        | 434/3844 [53:40<7:25:48,  7.84s/it]

 11%|█▏        | 435/3844 [53:48<7:34:06,  7.99s/it]

 11%|█▏        | 436/3844 [53:58<8:00:57,  8.47s/it]

 11%|█▏        | 437/3844 [54:04<7:26:44,  7.87s/it]
{'loss': 1.397, 'grad_norm': 0.2774944922391928, 'learning_rate': 1.9636354575348808e-05, 'epoch': 0.11}


 11%|█▏        | 439/3844 [54:18<6:59:37,  7.39s/it]

 11%|█▏        | 440/3844 [54:24<6:33:44,  6.94s/it]
{'loss': 1.4067, 'grad_norm': 0.2673553599673917, 'learning_rate': 1.9629568175715855e-05, 'epoch': 0.11}


 11%|█▏        | 442/3844 [54:40<7:06:03,  7.51s/it]
{'loss': 1.1832, 'grad_norm': 0.2553709045406837, 'learning_rate': 1.962500971459883e-05, 'epoch': 0.11}

 12%|█▏        | 443/3844 [54:47<6:52:54,  7.28s/it]


 12%|█▏        | 445/3844 [55:00<6:32:35,  6.93s/it]

 12%|█▏        | 446/3844 [55:10<7:24:22,  7.85s/it]

 12%|█▏        | 447/3844 [55:16<6:44:16,  7.14s/it]

 12%|█▏        | 448/3844 [55:22<6:25:36,  6.81s/it]

 12%|█▏        | 449/3844 [55:27<6:08:41,  6.52s/it]

 12%|█▏        | 450/3844 [55:36<6:43:53,  7.14s/it]

 12%|█▏        | 451/3844 [55:42<6:19:36,  6.71s/it]

 12%|█▏        | 452/3844 [55:50<6:39:30,  7.07s/it]

 12%|█▏        | 453/3844 [55:56<6:19:25,  6.71s/it]
{'loss': 1.1582, 'grad_norm': 0.2595788328273067, 'learning_rate': 1.9599449815237758e-05, 'epoch': 0.12}


 12%|█▏        | 455/3844 [56:11<6:41:07,  7.10s/it]

 12%|█▏        | 456/3844 [56:16<6:15:05,  6.64s/it]
{'loss': 1.2859, 'grad_norm': 0.271846097704367, 'learning_rate': 1.9592335683191972e-05, 'epoch': 0.12}


 12%|█▏        | 458/3844 [56:32<6:45:30,  7.19s/it]

 12%|█▏        | 459/3844 [56:38<6:18:32,  6.71s/it]
{'loss': 1.2435, 'grad_norm': 0.26482543763710825, 'learning_rate': 1.9585160243508153e-05, 'epoch': 0.12}

 12%|█▏        | 460/3844 [56:45<6:31:12,  6.94s/it]

 12%|█▏        | 461/3844 [56:51<6:19:13,  6.73s/it]

 12%|█▏        | 462/3844 [56:59<6:38:13,  7.06s/it]

 12%|█▏        | 463/3844 [57:07<6:51:00,  7.29s/it]

 12%|█▏        | 464/3844 [57:13<6:30:10,  6.93s/it]


 12%|█▏        | 466/3844 [57:25<5:58:56,  6.38s/it]

 12%|█▏        | 467/3844 [57:31<5:55:09,  6.31s/it]
{'loss': 1.1999, 'grad_norm': 0.2808344825961797, 'learning_rate': 1.9565726361461047e-05, 'epoch': 0.12}

 12%|█▏        | 468/3844 [57:41<7:04:10,  7.54s/it]


 12%|█▏        | 470/3844 [57:56<7:00:40,  7.48s/it]

 12%|█▏        | 471/3844 [58:02<6:35:23,  7.03s/it]

 12%|█▏        | 472/3844 [58:10<6:42:54,  7.17s/it]
{'loss': 1.1722, 'grad_norm': 0.25300421303862153, 'learning_rate': 1.955335935445422e-05, 'epoch': 0.12}


 12%|█▏        | 474/3844 [58:24<7:00:44,  7.49s/it]

 12%|█▏        | 475/3844 [58:30<6:29:19,  6.93s/it]
{'loss': 1.2413, 'grad_norm': 0.2819728969091238, 'learning_rate': 1.9545857724779375e-05, 'epoch': 0.12}


 12%|█▏        | 477/3844 [58:43<6:07:26,  6.55s/it]
{'loss': 1.1643, 'grad_norm': 0.2754677374268546, 'learning_rate': 1.954082274058564e-05, 'epoch': 0.12}


 12%|█▏        | 479/3844 [58:59<6:51:52,  7.34s/it]

 12%|█▏        | 480/3844 [59:04<6:24:20,  6.86s/it]

 13%|█▎        | 481/3844 [59:12<6:39:52,  7.13s/it]
{'loss': 1.2169, 'grad_norm': 0.30287466807755914, 'learning_rate': 1.953067148201661e-05, 'epoch': 0.13}

 13%|█▎        | 482/3844 [59:19<6:30:20,  6.97s/it]


 13%|█▎        | 484/3844 [59:35<6:54:34,  7.40s/it]

 13%|█▎        | 485/3844 [59:42<6:54:58,  7.41s/it]

 13%|█▎        | 486/3844 [59:51<7:15:13,  7.78s/it]

 13%|█▎        | 487/3844 [59:56<6:36:03,  7.08s/it]

 13%|█▎        | 488/3844 [1:00:03<6:30:27,  6.98s/it]

 13%|█▎        | 489/3844 [1:00:09<6:14:42,  6.70s/it]

 13%|█▎        | 490/3844 [1:00:15<6:02:48,  6.49s/it]
{'loss': 1.1907, 'grad_norm': 0.2662274088280995, 'learning_rate': 1.9507435390210553e-05, 'epoch': 0.13}

 13%|█▎        | 491/3844 [1:00:23<6:33:34,  7.04s/it]


 13%|█▎        | 493/3844 [1:00:42<7:40:29,  8.25s/it]

 13%|█▎        | 494/3844 [1:00:48<7:04:59,  7.61s/it]
{'loss': 1.233, 'grad_norm': 0.26789492405548804, 'learning_rate': 1.9496932615079697e-05, 'epoch': 0.13}

 13%|█▎        | 495/3844 [1:00:55<6:56:27,  7.46s/it]

 13%|█▎        | 496/3844 [1:01:06<7:44:19,  8.32s/it]


 13%|█▎        | 498/3844 [1:01:21<7:15:01,  7.80s/it]

 13%|█▎        | 499/3844 [1:01:28<7:12:12,  7.75s/it]

 13%|█▎        | 500/3844 [1:01:35<6:48:57,  7.34s/it]

 13%|█▎        | 501/3844 [1:01:43<7:00:59,  7.56s/it]

 13%|█▎        | 502/3844 [1:01:49<6:33:48,  7.07s/it]

 13%|█▎        | 503/3844 [1:01:54<6:15:00,  6.73s/it]
{'loss': 1.1854, 'grad_norm': 0.2639535209969294, 'learning_rate': 1.9472907018718778e-05, 'epoch': 0.13}


 13%|█▎        | 505/3844 [1:02:08<6:17:31,  6.78s/it]
{'loss': 1.1842, 'grad_norm': 0.27017845576738087, 'learning_rate': 1.9467493949721923e-05, 'epoch': 0.13}


 13%|█▎        | 507/3844 [1:02:20<6:00:57,  6.49s/it]

 13%|█▎        | 508/3844 [1:02:27<5:59:06,  6.46s/it]
{'loss': 1.2605, 'grad_norm': 0.2814432183658842, 'learning_rate': 1.9459323926258366e-05, 'epoch': 0.13}


 13%|█▎        | 510/3844 [1:02:42<6:40:08,  7.20s/it]
{'loss': 1.2352, 'grad_norm': 0.26509233090112366, 'learning_rate': 1.9453843653200134e-05, 'epoch': 0.13}

 13%|█▎        | 511/3844 [1:02:49<6:37:43,  7.16s/it]


 13%|█▎        | 513/3844 [1:03:10<8:25:03,  9.10s/it]

 13%|█▎        | 514/3844 [1:03:16<7:29:38,  8.10s/it]

 13%|█▎        | 515/3844 [1:03:24<7:30:27,  8.12s/it]
{'loss': 1.1307, 'grad_norm': 0.2554954829576137, 'learning_rate': 1.944002551658211e-05, 'epoch': 0.13}

 13%|█▎        | 516/3844 [1:03:31<7:10:49,  7.77s/it]

 13%|█▎        | 517/3844 [1:03:41<7:48:19,  8.45s/it]


 14%|█▎        | 519/3844 [1:03:56<7:24:02,  8.01s/it]

 14%|█▎        | 520/3844 [1:04:02<6:48:13,  7.37s/it]
{'loss': 1.1965, 'grad_norm': 0.2740967814351308, 'learning_rate': 1.9426039785190005e-05, 'epoch': 0.14}

 14%|█▎        | 521/3844 [1:04:08<6:21:58,  6.90s/it]

 14%|█▎        | 522/3844 [1:04:13<5:58:53,  6.48s/it]

 14%|█▎        | 523/3844 [1:04:21<6:24:35,  6.95s/it]

 14%|█▎        | 524/3844 [1:04:27<6:09:06,  6.67s/it]


 14%|█▎        | 526/3844 [1:04:41<6:09:13,  6.68s/it]

 14%|█▎        | 527/3844 [1:04:47<6:02:43,  6.56s/it]
{'loss': 1.1671, 'grad_norm': 0.26431126127451693, 'learning_rate': 1.940617867534499e-05, 'epoch': 0.14}

 14%|█▎        | 528/3844 [1:04:54<6:12:05,  6.73s/it]


 14%|█▍        | 530/3844 [1:05:06<6:00:25,  6.53s/it]

 14%|█▍        | 531/3844 [1:05:14<6:21:06,  6.90s/it]
{'loss': 1.2043, 'grad_norm': 0.2794107806250578, 'learning_rate': 1.9394682470525183e-05, 'epoch': 0.14}

 14%|█▍        | 532/3844 [1:05:23<6:58:08,  7.58s/it]


 14%|█▍        | 534/3844 [1:05:39<6:51:20,  7.46s/it]

 14%|█▍        | 535/3844 [1:05:49<7:33:51,  8.23s/it]
[2024-05-25 20:47:30,491] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 536/3844 [1:05:57<7:26:28,  8.10s/it]

 14%|█▍        | 537/3844 [1:06:03<6:51:31,  7.47s/it]

 14%|█▍        | 538/3844 [1:06:09<6:32:10,  7.12s/it]

 14%|█▍        | 539/3844 [1:06:15<6:09:50,  6.71s/it]
{'loss': 1.1618, 'grad_norm': 0.29210610751911864, 'learning_rate': 1.93713699561929e-05, 'epoch': 0.14}


 14%|█▍        | 541/3844 [1:06:33<7:20:48,  8.01s/it]

 14%|█▍        | 542/3844 [1:06:38<6:41:27,  7.29s/it]
{'loss': 1.2087, 'grad_norm': 0.2689059080658599, 'learning_rate': 1.9362517897959e-05, 'epoch': 0.14}


 14%|█▍        | 544/3844 [1:06:53<6:48:10,  7.42s/it]

 14%|█▍        | 545/3844 [1:07:03<7:23:02,  8.06s/it]
{'loss': 1.2288, 'grad_norm': 0.292542840921395, 'learning_rate': 1.9353606000924906e-05, 'epoch': 0.14}


 14%|█▍        | 547/3844 [1:07:17<6:52:29,  7.51s/it]

 14%|█▍        | 548/3844 [1:07:23<6:30:30,  7.11s/it]
{'loss': 1.1396, 'grad_norm': 0.31761978823027864, 'learning_rate': 1.9344634322049357e-05, 'epoch': 0.14}

 14%|█▍        | 549/3844 [1:07:30<6:25:09,  7.01s/it]

 14%|█▍        | 550/3844 [1:07:36<6:05:34,  6.66s/it]


 14%|█▍        | 552/3844 [1:07:51<6:41:59,  7.33s/it]

 14%|█▍        | 553/3844 [1:08:01<7:19:29,  8.01s/it]
{'loss': 1.0428, 'grad_norm': 0.285552706892281, 'learning_rate': 1.932954883130018e-05, 'epoch': 0.14}

 14%|█▍        | 554/3844 [1:08:06<6:35:57,  7.22s/it]

 14%|█▍        | 555/3844 [1:08:14<6:43:31,  7.36s/it]

 14%|█▍        | 556/3844 [1:08:20<6:18:14,  6.90s/it]


 15%|█▍        | 558/3844 [1:08:36<6:51:30,  7.51s/it]

 15%|█▍        | 559/3844 [1:08:43<6:35:41,  7.23s/it]

 15%|█▍        | 560/3844 [1:08:49<6:12:17,  6.80s/it]
{'loss': 1.3862, 'grad_norm': 0.2963659940453818, 'learning_rate': 1.9308150940672635e-05, 'epoch': 0.15}


 15%|█▍        | 562/3844 [1:09:01<5:51:13,  6.42s/it]

 15%|█▍        | 563/3844 [1:09:07<5:42:01,  6.25s/it]
{'loss': 1.2846, 'grad_norm': 0.28945509262273295, 'learning_rate': 1.9298881220330987e-05, 'epoch': 0.15}


 15%|█▍        | 565/3844 [1:09:21<5:58:28,  6.56s/it]
{'loss': 1.3399, 'grad_norm': 0.31724328947360525, 'learning_rate': 1.9292668385275916e-05, 'epoch': 0.15}


 15%|█▍        | 567/3844 [1:09:34<5:43:23,  6.29s/it]

 15%|█▍        | 568/3844 [1:09:41<6:03:28,  6.66s/it]

 15%|█▍        | 569/3844 [1:09:52<7:20:19,  8.07s/it]

 15%|█▍        | 570/3844 [1:10:02<7:38:09,  8.40s/it]

 15%|█▍        | 571/3844 [1:10:09<7:22:29,  8.11s/it]

 15%|█▍        | 572/3844 [1:10:14<6:38:26,  7.31s/it]
{'loss': 1.1499, 'grad_norm': 0.3130867222092039, 'learning_rate': 1.9270715705718808e-05, 'epoch': 0.15}


 15%|█▍        | 574/3844 [1:10:27<6:12:47,  6.84s/it]

 15%|█▍        | 575/3844 [1:10:35<6:37:57,  7.30s/it]

 15%|█▍        | 576/3844 [1:10:43<6:47:03,  7.47s/it]

 15%|█▌        | 577/3844 [1:10:50<6:42:35,  7.39s/it]

 15%|█▌        | 578/3844 [1:10:57<6:20:55,  7.00s/it]

 15%|█▌        | 579/3844 [1:11:03<6:12:42,  6.85s/it]

 15%|█▌        | 580/3844 [1:11:09<5:53:58,  6.51s/it]

 15%|█▌        | 581/3844 [1:11:15<5:55:29,  6.54s/it]
{'loss': 1.2191, 'grad_norm': 0.30815723963933617, 'learning_rate': 1.924201692480752e-05, 'epoch': 0.15}


 15%|█▌        | 583/3844 [1:11:29<6:10:31,  6.82s/it]

 15%|█▌        | 584/3844 [1:11:37<6:19:43,  6.99s/it]
{'loss': 1.2305, 'grad_norm': 0.30444068299213617, 'learning_rate': 1.9232332445323292e-05, 'epoch': 0.15}


 15%|█▌        | 586/3844 [1:11:51<6:35:19,  7.28s/it]

 15%|█▌        | 587/3844 [1:11:59<6:46:58,  7.50s/it]

 15%|█▌        | 588/3844 [1:12:05<6:15:50,  6.93s/it]

 15%|█▌        | 589/3844 [1:12:12<6:12:38,  6.87s/it]
{'loss': 1.2612, 'grad_norm': 0.28861566512747383, 'learning_rate': 1.9216060550822125e-05, 'epoch': 0.15}

 15%|█▌        | 590/3844 [1:12:20<6:45:12,  7.47s/it]


 15%|█▌        | 592/3844 [1:12:39<7:38:17,  8.46s/it]

 15%|█▌        | 593/3844 [1:12:45<6:49:05,  7.55s/it]
{'loss': 1.3076, 'grad_norm': 0.30798255450158696, 'learning_rate': 1.920292521587826e-05, 'epoch': 0.15}


 15%|█▌        | 595/3844 [1:13:03<7:36:48,  8.44s/it]
[2024-05-25 20:54:44,413] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1546, 'grad_norm': 0.31333837778179796, 'learning_rate': 1.9196318326519394e-05, 'epoch': 0.15}


 16%|█▌        | 597/3844 [1:13:19<7:37:34,  8.46s/it]
{'loss': 1.3122, 'grad_norm': 0.2774775492801675, 'learning_rate': 1.91896853142344e-05, 'epoch': 0.16}

 16%|█▌        | 598/3844 [1:13:26<7:05:00,  7.86s/it]


 16%|█▌        | 600/3844 [1:13:41<7:04:58,  7.86s/it]
 16%|█▌        | 600/3844 [1:13:41<7:04:58,  7.86s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2727, 'grad_norm': 0.27135357253147385, 'learning_rate': 1.9176340996326727e-05, 'epoch': 0.16}

 16%|█▌        | 602/3844 [1:14:21<11:45:10, 13.05s/it]
{'loss': 1.1355, 'grad_norm': 0.2826932757758365, 'learning_rate': 1.9172988619550812e-05, 'epoch': 0.16}


 16%|█▌        | 604/3844 [1:14:33<8:37:25,  9.58s/it]

 16%|█▌        | 605/3844 [1:14:39<7:39:12,  8.51s/it]
{'loss': 1.2327, 'grad_norm': 0.3028792075004215, 'learning_rate': 1.9162892413777815e-05, 'epoch': 0.16}


 16%|█▌        | 607/3844 [1:14:53<6:57:31,  7.74s/it]
{'loss': 1.132, 'grad_norm': 0.2799535721978504, 'learning_rate': 1.9156129070968937e-05, 'epoch': 0.16}


 16%|█▌        | 609/3844 [1:15:06<6:12:18,  6.91s/it]
{'loss': 1.2568, 'grad_norm': 0.3280109859596446, 'learning_rate': 1.9149339719394938e-05, 'epoch': 0.16}


 16%|█▌        | 611/3844 [1:15:23<7:04:35,  7.88s/it]
{'loss': 1.1402, 'grad_norm': 0.2875240613011424, 'learning_rate': 1.9142524378341548e-05, 'epoch': 0.16}


 16%|█▌        | 613/3844 [1:15:38<6:37:11,  7.38s/it]

 16%|█▌        | 614/3844 [1:15:45<6:34:46,  7.33s/it]

 16%|█▌        | 615/3844 [1:15:55<7:14:59,  8.08s/it]

 16%|█▌        | 616/3844 [1:16:05<7:41:29,  8.58s/it]

 16%|█▌        | 617/3844 [1:16:12<7:15:13,  8.09s/it]
{'loss': 1.24, 'grad_norm': 0.31463278878956796, 'learning_rate': 1.9121922612269405e-05, 'epoch': 0.16}

 16%|█▌        | 618/3844 [1:16:19<6:58:11,  7.78s/it]


 16%|█▌        | 620/3844 [1:16:33<6:50:29,  7.64s/it]

 16%|█▌        | 621/3844 [1:16:39<6:20:42,  7.09s/it]

 16%|█▌        | 622/3844 [1:16:48<6:47:21,  7.59s/it]

 16%|█▌        | 623/3844 [1:16:59<7:52:36,  8.80s/it]

 16%|█▌        | 624/3844 [1:17:05<7:01:07,  7.85s/it]

 16%|█▋        | 625/3844 [1:17:11<6:36:01,  7.38s/it]

 16%|█▋        | 626/3844 [1:17:19<6:45:58,  7.57s/it]

 16%|█▋        | 627/3844 [1:17:25<6:21:43,  7.12s/it]

 16%|█▋        | 628/3844 [1:17:33<6:34:36,  7.36s/it]

 16%|█▋        | 629/3844 [1:17:41<6:43:03,  7.52s/it]

 16%|█▋        | 630/3844 [1:17:50<6:57:21,  7.79s/it]
{'loss': 1.2413, 'grad_norm': 0.28835898836414764, 'learning_rate': 1.9076486144292216e-05, 'epoch': 0.16}

 16%|█▋        | 631/3844 [1:17:57<6:49:06,  7.64s/it]


 16%|█▋        | 633/3844 [1:18:08<5:49:34,  6.53s/it]
{'loss': 1.2528, 'grad_norm': 0.32443056208395066, 'learning_rate': 1.9065845910622207e-05, 'epoch': 0.16}


 17%|█▋        | 635/3844 [1:18:21<5:51:32,  6.57s/it]

 17%|█▋        | 636/3844 [1:18:27<5:41:36,  6.39s/it]
{'loss': 1.2302, 'grad_norm': 0.31334283364872023, 'learning_rate': 1.90551477342761e-05, 'epoch': 0.17}

 17%|█▋        | 637/3844 [1:18:34<5:58:27,  6.71s/it]


 17%|█▋        | 639/3844 [1:18:53<6:57:26,  7.81s/it]

 17%|█▋        | 640/3844 [1:19:00<6:33:46,  7.37s/it]

 17%|█▋        | 641/3844 [1:19:05<6:04:58,  6.84s/it]

 17%|█▋        | 642/3844 [1:19:12<6:05:53,  6.86s/it]

 17%|█▋        | 643/3844 [1:19:21<6:46:13,  7.61s/it]

 17%|█▋        | 644/3844 [1:19:28<6:22:14,  7.17s/it]

 17%|█▋        | 645/3844 [1:19:36<6:38:51,  7.48s/it]

 17%|█▋        | 646/3844 [1:19:43<6:38:58,  7.49s/it]

 17%|█▋        | 647/3844 [1:19:50<6:20:54,  7.15s/it]
{'loss': 1.0676, 'grad_norm': 0.2853033577545167, 'learning_rate': 1.9015426464853242e-05, 'epoch': 0.17}

 17%|█▋        | 648/3844 [1:19:56<6:13:16,  7.01s/it]


 17%|█▋        | 650/3844 [1:20:12<6:27:50,  7.29s/it]

 17%|█▋        | 651/3844 [1:20:20<6:39:30,  7.51s/it]

 17%|█▋        | 652/3844 [1:20:31<7:44:32,  8.73s/it]
{'loss': 1.1852, 'grad_norm': 0.2933241950611905, 'learning_rate': 1.899711504568217e-05, 'epoch': 0.17}

 17%|█▋        | 653/3844 [1:20:37<6:50:12,  7.71s/it]


 17%|█▋        | 655/3844 [1:20:57<8:12:39,  9.27s/it]

 17%|█▋        | 656/3844 [1:21:04<7:25:11,  8.38s/it]

 17%|█▋        | 657/3844 [1:21:12<7:20:40,  8.30s/it]

 17%|█▋        | 658/3844 [1:21:18<6:44:05,  7.61s/it]

 17%|█▋        | 659/3844 [1:21:26<6:46:59,  7.67s/it]

 17%|█▋        | 660/3844 [1:21:32<6:20:26,  7.17s/it]

 17%|█▋        | 661/3844 [1:21:37<5:58:03,  6.75s/it]

 17%|█▋        | 662/3844 [1:21:44<5:52:32,  6.65s/it]
{'loss': 1.4032, 'grad_norm': 0.3098256035326109, 'learning_rate': 1.8960013340761335e-05, 'epoch': 0.17}


 17%|█▋        | 664/3844 [1:22:02<7:10:42,  8.13s/it]

 17%|█▋        | 665/3844 [1:22:10<7:09:10,  8.10s/it]

 17%|█▋        | 666/3844 [1:22:16<6:37:33,  7.51s/it]
{'loss': 1.2322, 'grad_norm': 0.30728551219725997, 'learning_rate': 1.8944994348967247e-05, 'epoch': 0.17}

 17%|█▋        | 667/3844 [1:22:23<6:19:31,  7.17s/it]


 17%|█▋        | 669/3844 [1:22:42<7:18:30,  8.29s/it]

 17%|█▋        | 670/3844 [1:22:52<7:42:33,  8.74s/it]

 17%|█▋        | 671/3844 [1:22:58<7:03:27,  8.01s/it]

 17%|█▋        | 672/3844 [1:23:09<7:50:43,  8.90s/it]
{'loss': 1.1815, 'grad_norm': 0.30944335819592295, 'learning_rate': 1.8922275347413536e-05, 'epoch': 0.17}


 18%|█▊        | 674/3844 [1:23:30<8:24:22,  9.55s/it]

 18%|█▊        | 675/3844 [1:23:36<7:30:56,  8.54s/it]
{'loss': 1.3574, 'grad_norm': 0.2982614647123238, 'learning_rate': 1.8910830272637243e-05, 'epoch': 0.18}


 18%|█▊        | 677/3844 [1:23:48<6:14:38,  7.10s/it]

 18%|█▊        | 678/3844 [1:23:54<6:00:23,  6.83s/it]

 18%|█▊        | 679/3844 [1:24:02<6:09:42,  7.01s/it]
{'loss': 1.2003, 'grad_norm': 0.337644112738148, 'learning_rate': 1.88954815937518e-05, 'epoch': 0.18}


 18%|█▊        | 681/3844 [1:24:16<6:14:17,  7.10s/it]

 18%|█▊        | 682/3844 [1:24:22<5:55:35,  6.75s/it]

 18%|█▊        | 683/3844 [1:24:30<6:22:12,  7.25s/it]
{'loss': 1.1626, 'grad_norm': 0.3059674849825183, 'learning_rate': 1.8880031841443154e-05, 'epoch': 0.18}


 18%|█▊        | 685/3844 [1:24:46<6:25:02,  7.31s/it]

 18%|█▊        | 686/3844 [1:24:54<6:42:39,  7.65s/it]

 18%|█▊        | 687/3844 [1:25:03<7:10:49,  8.19s/it]

 18%|█▊        | 688/3844 [1:25:11<7:05:48,  8.10s/it]

 18%|█▊        | 689/3844 [1:25:17<6:33:02,  7.47s/it]
{'loss': 1.1534, 'grad_norm': 0.32587569235136216, 'learning_rate': 1.8856668084630397e-05, 'epoch': 0.18}


 18%|█▊        | 691/3844 [1:25:32<6:24:13,  7.31s/it]

 18%|█▊        | 692/3844 [1:25:38<6:00:43,  6.87s/it]
{'loss': 1.2265, 'grad_norm': 0.29354145980629404, 'learning_rate': 1.8844901260170052e-05, 'epoch': 0.18}

 18%|█▊        | 693/3844 [1:25:43<5:40:42,  6.49s/it]

 18%|█▊        | 694/3844 [1:25:49<5:24:32,  6.18s/it]

 18%|█▊        | 695/3844 [1:25:57<6:00:52,  6.88s/it]


 18%|█▊        | 697/3844 [1:26:17<7:34:18,  8.66s/it]
{'loss': 1.0764, 'grad_norm': 0.30757833275613006, 'learning_rate': 1.8825164299928748e-05, 'epoch': 0.18}


 18%|█▊        | 699/3844 [1:26:30<6:38:12,  7.60s/it]

 18%|█▊        | 700/3844 [1:26:36<6:10:40,  7.07s/it]

 18%|█▊        | 701/3844 [1:26:43<6:23:51,  7.33s/it]

 18%|█▊        | 702/3844 [1:26:52<6:37:44,  7.60s/it]

 18%|█▊        | 703/3844 [1:27:02<7:20:26,  8.41s/it]

 18%|█▊        | 704/3844 [1:27:12<7:38:30,  8.76s/it]

 18%|█▊        | 705/3844 [1:27:23<8:14:05,  9.44s/it]
{'loss': 1.1406, 'grad_norm': 0.30656974696411, 'learning_rate': 1.8793259418547632e-05, 'epoch': 0.18}


 18%|█▊        | 707/3844 [1:27:38<7:40:13,  8.80s/it]

 18%|█▊        | 708/3844 [1:27:45<7:00:22,  8.04s/it]

 18%|█▊        | 709/3844 [1:27:52<6:55:35,  7.95s/it]
{'loss': 1.1993, 'grad_norm': 0.3069081886564545, 'learning_rate': 1.877715701903305e-05, 'epoch': 0.18}


 18%|█▊        | 711/3844 [1:28:08<6:39:45,  7.66s/it]
{'loss': 1.2625, 'grad_norm': 0.31341621944921977, 'learning_rate': 1.8769068409428922e-05, 'epoch': 0.18}

 19%|█▊        | 712/3844 [1:28:13<6:07:49,  7.05s/it]


 19%|█▊        | 714/3844 [1:28:28<6:10:46,  7.11s/it]

 19%|█▊        | 715/3844 [1:28:34<5:57:55,  6.86s/it]

 19%|█▊        | 716/3844 [1:28:40<5:37:49,  6.48s/it]

 19%|█▊        | 717/3844 [1:28:46<5:33:24,  6.40s/it]
{'loss': 1.1145, 'grad_norm': 0.29376807611556827, 'learning_rate': 1.8744653217157877e-05, 'epoch': 0.19}


 19%|█▊        | 719/3844 [1:28:58<5:19:53,  6.14s/it]

 19%|█▊        | 720/3844 [1:29:07<6:05:12,  7.01s/it]
{'loss': 1.0891, 'grad_norm': 0.2907876301977113, 'learning_rate': 1.873236174717295e-05, 'epoch': 0.19}


 19%|█▉        | 722/3844 [1:29:22<6:24:53,  7.40s/it]

 19%|█▉        | 723/3844 [1:29:29<6:06:53,  7.05s/it]

 19%|█▉        | 724/3844 [1:29:36<6:08:04,  7.08s/it]

 19%|█▉        | 725/3844 [1:29:44<6:19:52,  7.31s/it]

 19%|█▉        | 726/3844 [1:29:52<6:35:35,  7.61s/it]

 19%|█▉        | 727/3844 [1:29:58<6:16:07,  7.24s/it]

 19%|█▉        | 728/3844 [1:30:08<6:52:19,  7.94s/it]

 19%|█▉        | 729/3844 [1:30:15<6:31:35,  7.54s/it]

 19%|█▉        | 730/3844 [1:30:20<6:01:14,  6.96s/it]

 19%|█▉        | 731/3844 [1:30:28<6:10:05,  7.13s/it]
{'loss': 1.144, 'grad_norm': 0.3149882802153046, 'learning_rate': 1.8686816128716244e-05, 'epoch': 0.19}


 19%|█▉        | 733/3844 [1:30:42<6:06:51,  7.08s/it]

 19%|█▉        | 734/3844 [1:30:50<6:24:43,  7.42s/it]

 19%|█▉        | 735/3844 [1:30:56<6:08:44,  7.12s/it]

 19%|█▉        | 736/3844 [1:31:02<5:48:08,  6.72s/it]

 19%|█▉        | 737/3844 [1:31:08<5:38:06,  6.53s/it]

 19%|█▉        | 738/3844 [1:31:14<5:25:23,  6.29s/it]

 19%|█▉        | 739/3844 [1:31:20<5:19:41,  6.18s/it]

 19%|█▉        | 740/3844 [1:31:27<5:28:09,  6.34s/it]

 19%|█▉        | 741/3844 [1:31:33<5:19:21,  6.18s/it]
{'loss': 1.2948, 'grad_norm': 0.32348574732661123, 'learning_rate': 1.864476318577823e-05, 'epoch': 0.19}


 19%|█▉        | 743/3844 [1:31:47<5:37:57,  6.54s/it]

 19%|█▉        | 744/3844 [1:31:54<5:48:12,  6.74s/it]
{'loss': 1.2533, 'grad_norm': 0.3077648403333945, 'learning_rate': 1.8632027455144858e-05, 'epoch': 0.19}


 19%|█▉        | 746/3844 [1:32:11<6:28:08,  7.52s/it]

 19%|█▉        | 747/3844 [1:32:18<6:23:34,  7.43s/it]
{'loss': 1.1178, 'grad_norm': 0.30380122968162926, 'learning_rate': 1.8619236554505743e-05, 'epoch': 0.19}


 19%|█▉        | 749/3844 [1:32:33<6:11:21,  7.20s/it]

 20%|█▉        | 750/3844 [1:32:38<5:42:16,  6.64s/it]

 20%|█▉        | 751/3844 [1:32:44<5:39:00,  6.58s/it]

 20%|█▉        | 752/3844 [1:32:55<6:41:46,  7.80s/it]
{'loss': 1.1577, 'grad_norm': 0.31920394327230506, 'learning_rate': 1.8597796008959395e-05, 'epoch': 0.2}

 20%|█▉        | 753/3844 [1:33:02<6:21:43,  7.41s/it]

 20%|█▉        | 754/3844 [1:33:11<7:00:04,  8.16s/it]


 20%|█▉        | 756/3844 [1:33:26<6:36:57,  7.71s/it]

 20%|█▉        | 757/3844 [1:33:32<6:06:29,  7.12s/it]

 20%|█▉        | 758/3844 [1:33:40<6:23:50,  7.46s/it]

 20%|█▉        | 759/3844 [1:33:49<6:39:55,  7.78s/it]

 20%|█▉        | 760/3844 [1:33:55<6:15:57,  7.31s/it]

 20%|█▉        | 761/3844 [1:34:07<7:27:00,  8.70s/it]

 20%|█▉        | 762/3844 [1:34:14<7:05:59,  8.29s/it]
{'loss': 1.0107, 'grad_norm': 0.30579178600506046, 'learning_rate': 1.8554457374809632e-05, 'epoch': 0.2}


 20%|█▉        | 764/3844 [1:34:31<6:59:58,  8.18s/it]
{'loss': 1.152, 'grad_norm': 0.310883205487408, 'learning_rate': 1.8545716650151914e-05, 'epoch': 0.2}


 20%|█▉        | 766/3844 [1:34:49<7:13:11,  8.44s/it]
{'loss': 1.2642, 'grad_norm': 0.3382560996087573, 'learning_rate': 1.8536951650657586e-05, 'epoch': 0.2}


 20%|█▉        | 768/3844 [1:35:03<6:33:57,  7.68s/it]

 20%|██        | 769/3844 [1:35:10<6:23:24,  7.48s/it]
{'loss': 1.0509, 'grad_norm': 0.3330547425347151, 'learning_rate': 1.8523758690580072e-05, 'epoch': 0.2}


 20%|██        | 771/3844 [1:35:26<6:42:12,  7.85s/it]

 20%|██        | 772/3844 [1:35:35<6:48:45,  7.98s/it]

 20%|██        | 773/3844 [1:35:43<6:54:37,  8.10s/it]
{'loss': 1.1922, 'grad_norm': 0.3018301013483412, 'learning_rate': 1.8506083348171483e-05, 'epoch': 0.2}


 20%|██        | 775/3844 [1:35:57<6:26:40,  7.56s/it]

 20%|██        | 776/3844 [1:36:04<6:18:37,  7.40s/it]

 20%|██        | 777/3844 [1:36:13<6:40:32,  7.84s/it]

 20%|██        | 778/3844 [1:36:19<6:07:52,  7.20s/it]

 20%|██        | 779/3844 [1:36:25<5:53:33,  6.92s/it]

 20%|██        | 780/3844 [1:36:31<5:40:08,  6.66s/it]

 20%|██        | 781/3844 [1:36:37<5:34:23,  6.55s/it]
{'loss': 1.2156, 'grad_norm': 0.3345349134802005, 'learning_rate': 1.8470442918433464e-05, 'epoch': 0.2}


 20%|██        | 783/3844 [1:36:51<5:44:30,  6.75s/it]
{'loss': 1.2627, 'grad_norm': 0.2958320159331604, 'learning_rate': 1.846147259501506e-05, 'epoch': 0.2}

 20%|██        | 784/3844 [1:37:00<6:13:39,  7.33s/it]


 20%|██        | 786/3844 [1:37:15<6:26:29,  7.58s/it]

 20%|██        | 787/3844 [1:37:21<6:01:47,  7.10s/it]

 20%|██        | 788/3844 [1:37:31<6:50:30,  8.06s/it]

 21%|██        | 789/3844 [1:37:39<6:49:21,  8.04s/it]
{'loss': 1.2055, 'grad_norm': 0.3327669985470592, 'learning_rate': 1.8434417513819883e-05, 'epoch': 0.21}


 21%|██        | 791/3844 [1:37:52<6:10:52,  7.29s/it]
{'loss': 1.2356, 'grad_norm': 0.3004036881223668, 'learning_rate': 1.8425351201832922e-05, 'epoch': 0.21}

 21%|██        | 792/3844 [1:37:58<5:43:16,  6.75s/it]


 21%|██        | 794/3844 [1:38:12<6:01:50,  7.12s/it]

 21%|██        | 795/3844 [1:38:25<7:22:12,  8.70s/it]

 21%|██        | 796/3844 [1:38:35<7:45:37,  9.17s/it]
{'loss': 1.2018, 'grad_norm': 0.32131300569679, 'learning_rate': 1.8402580771807593e-05, 'epoch': 0.21}


 21%|██        | 798/3844 [1:38:49<6:53:57,  8.15s/it]
{'loss': 1.3077, 'grad_norm': 0.2917950550806005, 'learning_rate': 1.839343080768971e-05, 'epoch': 0.21}


 21%|██        | 800/3844 [1:39:05<6:45:30,  7.99s/it]

 21%|██        | 801/3844 [1:39:13<6:54:05,  8.16s/it]
{'loss': 1.2018, 'grad_norm': 0.3460580232031571, 'learning_rate': 1.8379661165424253e-05, 'epoch': 0.21}

 21%|██        | 802/3844 [1:39:22<7:01:03,  8.30s/it]


 21%|██        | 804/3844 [1:39:43<8:17:45,  9.82s/it]

 21%|██        | 805/3844 [1:39:51<7:50:13,  9.28s/it]

 21%|██        | 806/3844 [1:39:57<7:03:06,  8.36s/it]

 21%|██        | 807/3844 [1:40:09<7:54:13,  9.37s/it]
{'loss': 1.1808, 'grad_norm': 0.30507654179893384, 'learning_rate': 1.835196129808182e-05, 'epoch': 0.21}


 21%|██        | 809/3844 [1:40:27<7:50:55,  9.31s/it]

 21%|██        | 810/3844 [1:40:33<7:02:16,  8.35s/it]

 21%|██        | 811/3844 [1:40:41<6:50:02,  8.11s/it]

 21%|██        | 812/3844 [1:40:51<7:23:37,  8.78s/it]
{'loss': 1.0728, 'grad_norm': 0.32431253237815616, 'learning_rate': 1.8328714939739475e-05, 'epoch': 0.21}


 21%|██        | 814/3844 [1:41:05<6:33:01,  7.78s/it]

 21%|██        | 815/3844 [1:41:17<7:37:31,  9.06s/it]
{'loss': 1.0544, 'grad_norm': 0.31151610998004337, 'learning_rate': 1.8314696123025456e-05, 'epoch': 0.21}

 21%|██        | 816/3844 [1:41:24<7:01:40,  8.36s/it]

 21%|██▏       | 817/3844 [1:41:30<6:28:39,  7.70s/it]

 21%|██▏       | 818/3844 [1:41:36<6:02:49,  7.19s/it]

 21%|██▏       | 819/3844 [1:41:42<5:42:00,  6.78s/it]

 21%|██▏       | 820/3844 [1:41:48<5:30:20,  6.55s/it]

 21%|██▏       | 821/3844 [1:41:54<5:30:39,  6.56s/it]


 21%|██▏       | 823/3844 [1:42:11<6:27:10,  7.69s/it]

 21%|██▏       | 824/3844 [1:42:17<6:08:42,  7.33s/it]
{'loss': 1.2799, 'grad_norm': 0.31922805263450993, 'learning_rate': 1.8272321181928127e-05, 'epoch': 0.21}

 21%|██▏       | 825/3844 [1:42:24<5:56:58,  7.09s/it]


 22%|██▏       | 827/3844 [1:42:37<5:41:17,  6.79s/it]

 22%|██▏       | 828/3844 [1:42:46<6:11:28,  7.39s/it]

 22%|██▏       | 829/3844 [1:42:51<5:42:47,  6.82s/it]

 22%|██▏       | 830/3844 [1:42:57<5:29:48,  6.57s/it]
{'loss': 1.1514, 'grad_norm': 0.3152753380379978, 'learning_rate': 1.8243806715685296e-05, 'epoch': 0.22}


 22%|██▏       | 832/3844 [1:43:11<5:41:30,  6.80s/it]

 22%|██▏       | 833/3844 [1:43:17<5:25:28,  6.49s/it]

 22%|██▏       | 834/3844 [1:43:24<5:30:19,  6.58s/it]

 22%|██▏       | 835/3844 [1:43:29<5:17:30,  6.33s/it]

 22%|██▏       | 836/3844 [1:43:35<5:08:32,  6.15s/it]

 22%|██▏       | 837/3844 [1:43:41<5:02:03,  6.03s/it]

 22%|██▏       | 838/3844 [1:43:49<5:37:16,  6.73s/it]

 22%|██▏       | 839/3844 [1:43:55<5:28:30,  6.56s/it]
{'loss': 1.113, 'grad_norm': 0.33448831851775546, 'learning_rate': 1.8200640080496485e-05, 'epoch': 0.22}


 22%|██▏       | 841/3844 [1:44:12<5:54:12,  7.08s/it]

 22%|██▏       | 842/3844 [1:44:17<5:32:57,  6.65s/it]

 22%|██▏       | 843/3844 [1:44:23<5:20:46,  6.41s/it]

 22%|██▏       | 844/3844 [1:44:31<5:39:21,  6.79s/it]

 22%|██▏       | 845/3844 [1:44:37<5:34:08,  6.69s/it]

 22%|██▏       | 846/3844 [1:44:45<5:46:52,  6.94s/it]
{'loss': 1.2429, 'grad_norm': 0.3331739740900435, 'learning_rate': 1.816673977962493e-05, 'epoch': 0.22}


 22%|██▏       | 848/3844 [1:45:01<6:08:26,  7.38s/it]

 22%|██▏       | 849/3844 [1:45:07<5:50:23,  7.02s/it]

 22%|██▏       | 850/3844 [1:45:15<6:08:05,  7.38s/it]

 22%|██▏       | 851/3844 [1:45:22<5:53:34,  7.09s/it]

 22%|██▏       | 852/3844 [1:45:35<7:25:07,  8.93s/it]

 22%|██▏       | 853/3844 [1:45:41<6:49:09,  8.21s/it]

 22%|██▏       | 854/3844 [1:45:49<6:46:48,  8.16s/it]

 22%|██▏       | 855/3844 [1:45:57<6:45:35,  8.14s/it]

 22%|██▏       | 856/3844 [1:46:07<7:08:35,  8.61s/it]

 22%|██▏       | 857/3844 [1:46:15<7:02:57,  8.50s/it]

 22%|██▏       | 858/3844 [1:46:27<7:57:23,  9.59s/it]
{'loss': 1.2329, 'grad_norm': 0.32459165957113484, 'learning_rate': 1.8107964486284913e-05, 'epoch': 0.22}


 22%|██▏       | 860/3844 [1:46:43<7:17:16,  8.79s/it]

 22%|██▏       | 861/3844 [1:46:50<6:45:25,  8.15s/it]
{'loss': 1.0941, 'grad_norm': 0.34478095219369903, 'learning_rate': 1.8093140875951222e-05, 'epoch': 0.22}

 22%|██▏       | 862/3844 [1:46:56<6:20:39,  7.66s/it]

 22%|██▏       | 863/3844 [1:47:03<5:58:54,  7.22s/it]


 23%|██▎       | 865/3844 [1:47:17<5:52:24,  7.10s/it]

 23%|██▎       | 866/3844 [1:47:23<5:38:59,  6.83s/it]

 23%|██▎       | 867/3844 [1:47:29<5:31:22,  6.68s/it]

 23%|██▎       | 868/3844 [1:47:35<5:21:34,  6.48s/it]

 23%|██▎       | 869/3844 [1:47:41<5:08:48,  6.23s/it]

 23%|██▎       | 870/3844 [1:47:47<5:06:45,  6.19s/it]

 23%|██▎       | 871/3844 [1:47:53<5:01:55,  6.09s/it]
{'loss': 1.1517, 'grad_norm': 0.3281359826916972, 'learning_rate': 1.8043355800866908e-05, 'epoch': 0.23}

 23%|██▎       | 872/3844 [1:48:02<5:49:21,  7.05s/it]


 23%|██▎       | 874/3844 [1:48:15<5:36:50,  6.81s/it]

 23%|██▎       | 875/3844 [1:48:22<5:28:11,  6.63s/it]

 23%|██▎       | 876/3844 [1:48:29<5:38:34,  6.84s/it]

 23%|██▎       | 877/3844 [1:48:35<5:27:33,  6.62s/it]

 23%|██▎       | 878/3844 [1:48:43<5:53:47,  7.16s/it]

 23%|██▎       | 879/3844 [1:48:49<5:33:30,  6.75s/it]

 23%|██▎       | 880/3844 [1:48:56<5:29:12,  6.66s/it]
{'loss': 1.313, 'grad_norm': 0.334834525464149, 'learning_rate': 1.7998060761933163e-05, 'epoch': 0.23}

 23%|██▎       | 881/3844 [1:49:04<5:58:23,  7.26s/it]


 23%|██▎       | 883/3844 [1:49:22<6:39:26,  8.09s/it]
{'loss': 1.1901, 'grad_norm': 0.3365736225484974, 'learning_rate': 1.7982860050379996e-05, 'epoch': 0.23}


 23%|██▎       | 885/3844 [1:49:39<6:59:54,  8.51s/it]

 23%|██▎       | 886/3844 [1:49:45<6:21:59,  7.75s/it]

 23%|██▎       | 887/3844 [1:49:53<6:28:17,  7.88s/it]

 23%|██▎       | 888/3844 [1:50:04<7:08:28,  8.70s/it]

 23%|██▎       | 889/3844 [1:50:12<6:57:01,  8.47s/it]

 23%|██▎       | 890/3844 [1:50:17<6:12:42,  7.57s/it]

 23%|██▎       | 891/3844 [1:50:23<5:45:57,  7.03s/it]

 23%|██▎       | 892/3844 [1:50:29<5:37:11,  6.85s/it]
{'loss': 1.1919, 'grad_norm': 0.3315843770062193, 'learning_rate': 1.7936952180125883e-05, 'epoch': 0.23}


 23%|██▎       | 894/3844 [1:50:43<5:33:25,  6.78s/it]

 23%|██▎       | 895/3844 [1:50:50<5:32:07,  6.76s/it]

 23%|██▎       | 896/3844 [1:50:56<5:23:36,  6.59s/it]
{'loss': 1.3271, 'grad_norm': 0.3650128332067284, 'learning_rate': 1.791640197864868e-05, 'epoch': 0.23}

 23%|██▎       | 897/3844 [1:51:06<6:17:07,  7.68s/it]


 23%|██▎       | 899/3844 [1:51:23<6:49:52,  8.35s/it]

 23%|██▎       | 900/3844 [1:51:30<6:16:37,  7.68s/it]
 23%|██▎       | 900/3844 [1:51:30<6:16:37,  7.68s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1844, 'grad_norm': 0.35194333873342365, 'learning_rate': 1.789058776377008e-05, 'epoch': 0.23}
 23%|██▎       | 901/3844 [1:52:03<12:35:01, 15.39s/it]


 23%|██▎       | 903/3844 [1:52:22<10:03:09, 12.31s/it]

 24%|██▎       | 904/3844 [1:52:30<8:56:24, 10.95s/it]
{'loss': 1.1041, 'grad_norm': 0.33927682939523157, 'learning_rate': 1.7875031963830492e-05, 'epoch': 0.24}

 24%|██▎       | 905/3844 [1:52:37<7:52:18,  9.64s/it]

 24%|██▎       | 906/3844 [1:52:42<6:55:05,  8.48s/it]

 24%|██▎       | 907/3844 [1:52:48<6:17:35,  7.71s/it]

 24%|██▎       | 908/3844 [1:53:03<7:54:15,  9.69s/it]

 24%|██▎       | 909/3844 [1:53:10<7:25:45,  9.11s/it]


 24%|██▎       | 911/3844 [1:53:24<6:23:58,  7.86s/it]

 24%|██▎       | 912/3844 [1:53:30<5:54:37,  7.26s/it]

 24%|██▍       | 913/3844 [1:53:35<5:32:08,  6.80s/it]

 24%|██▍       | 914/3844 [1:53:41<5:19:14,  6.54s/it]

 24%|██▍       | 915/3844 [1:53:47<5:12:09,  6.39s/it]
{'loss': 1.1245, 'grad_norm': 0.3283206077028503, 'learning_rate': 1.7817564173690813e-05, 'epoch': 0.24}


 24%|██▍       | 917/3844 [1:54:04<6:00:05,  7.38s/it]
{'loss': 1.1915, 'grad_norm': 0.3025299869283447, 'learning_rate': 1.7807043168471657e-05, 'epoch': 0.24}


 24%|██▍       | 919/3844 [1:54:20<6:24:44,  7.89s/it]

 24%|██▍       | 920/3844 [1:54:28<6:31:17,  8.03s/it]
{'loss': 1.2648, 'grad_norm': 0.3350046591306601, 'learning_rate': 1.779122008892768e-05, 'epoch': 0.24}


 24%|██▍       | 922/3844 [1:54:42<6:08:23,  7.56s/it]
{'loss': 1.137, 'grad_norm': 0.34974199334476375, 'learning_rate': 1.7780643698453158e-05, 'epoch': 0.24}

 24%|██▍       | 923/3844 [1:54:53<6:51:15,  8.45s/it]


 24%|██▍       | 925/3844 [1:55:06<6:05:05,  7.50s/it]
{'loss': 1.1731, 'grad_norm': 0.3455453708668665, 'learning_rate': 1.7764737681681386e-05, 'epoch': 0.24}


 24%|██▍       | 927/3844 [1:55:21<6:07:01,  7.55s/it]

 24%|██▍       | 928/3844 [1:55:29<6:15:58,  7.74s/it]
{'loss': 1.279, 'grad_norm': 0.34670658334796306, 'learning_rate': 1.7748782038025934e-05, 'epoch': 0.24}


 24%|██▍       | 930/3844 [1:55:44<6:07:03,  7.56s/it]

 24%|██▍       | 931/3844 [1:55:50<5:54:07,  7.29s/it]
{'loss': 1.2776, 'grad_norm': 0.3486031270455717, 'learning_rate': 1.773277686946434e-05, 'epoch': 0.24}


 24%|██▍       | 933/3844 [1:56:05<5:51:21,  7.24s/it]
{'loss': 1.3461, 'grad_norm': 0.3473014512470369, 'learning_rate': 1.7722079293755783e-05, 'epoch': 0.24}


 24%|██▍       | 935/3844 [1:56:18<5:37:32,  6.96s/it]

 24%|██▍       | 936/3844 [1:56:29<6:23:15,  7.91s/it]

 24%|██▍       | 937/3844 [1:56:36<6:09:58,  7.64s/it]

 24%|██▍       | 938/3844 [1:56:41<5:43:44,  7.10s/it]

 24%|██▍       | 939/3844 [1:56:48<5:37:51,  6.98s/it]

 24%|██▍       | 940/3844 [1:56:54<5:28:08,  6.78s/it]

 24%|██▍       | 941/3844 [1:57:00<5:14:34,  6.50s/it]

 25%|██▍       | 942/3844 [1:57:06<5:03:22,  6.27s/it]

 25%|██▍       | 943/3844 [1:57:14<5:30:14,  6.83s/it]

 25%|██▍       | 944/3844 [1:57:19<5:08:13,  6.38s/it]
{'loss': 1.2066, 'grad_norm': 0.33538064807998164, 'learning_rate': 1.766285135311194e-05, 'epoch': 0.25}


 25%|██▍       | 946/3844 [1:57:34<5:39:39,  7.03s/it]

 25%|██▍       | 947/3844 [1:57:43<5:59:23,  7.44s/it]

 25%|██▍       | 948/3844 [1:57:48<5:29:57,  6.84s/it]
{'loss': 1.3419, 'grad_norm': 0.3361499962714718, 'learning_rate': 1.764115039949367e-05, 'epoch': 0.25}

 25%|██▍       | 949/3844 [1:57:57<6:02:04,  7.50s/it]


 25%|██▍       | 951/3844 [1:58:11<5:47:52,  7.21s/it]
{'loss': 1.1913, 'grad_norm': 0.3295855415399548, 'learning_rate': 1.7624817694283783e-05, 'epoch': 0.25}


 25%|██▍       | 953/3844 [1:58:28<6:18:49,  7.86s/it]

 25%|██▍       | 954/3844 [1:58:34<6:04:13,  7.56s/it]

 25%|██▍       | 955/3844 [1:58:42<6:10:24,  7.69s/it]

 25%|██▍       | 956/3844 [1:58:48<5:45:23,  7.18s/it]

 25%|██▍       | 957/3844 [1:58:54<5:23:54,  6.73s/it]

 25%|██▍       | 958/3844 [1:59:01<5:22:30,  6.70s/it]

 25%|██▍       | 959/3844 [1:59:06<5:08:19,  6.41s/it]
{'loss': 1.2172, 'grad_norm': 0.34949002338477125, 'learning_rate': 1.758102584991188e-05, 'epoch': 0.25}

 25%|██▍       | 960/3844 [1:59:15<5:43:38,  7.15s/it]

 25%|██▌       | 961/3844 [1:59:23<5:55:03,  7.39s/it]


 25%|██▌       | 963/3844 [1:59:44<7:19:30,  9.15s/it]
{'loss': 1.2479, 'grad_norm': 0.3142161927633604, 'learning_rate': 1.7559000595876974e-05, 'epoch': 0.25}


 25%|██▌       | 965/3844 [1:59:58<6:30:40,  8.14s/it]

 25%|██▌       | 966/3844 [2:00:04<5:59:01,  7.48s/it]

 25%|██▌       | 967/3844 [2:00:10<5:31:28,  6.91s/it]
{'loss': 1.1966, 'grad_norm': 0.3620897953848324, 'learning_rate': 1.7536889453960882e-05, 'epoch': 0.25}

 25%|██▌       | 968/3844 [2:00:17<5:37:05,  7.03s/it]


 25%|██▌       | 970/3844 [2:00:35<6:31:23,  8.17s/it]

 25%|██▌       | 971/3844 [2:00:41<5:58:53,  7.50s/it]

 25%|██▌       | 972/3844 [2:00:48<6:01:49,  7.56s/it]

 25%|██▌       | 973/3844 [2:00:54<5:39:16,  7.09s/it]

 25%|██▌       | 974/3844 [2:01:02<5:45:54,  7.23s/it]
{'loss': 1.1562, 'grad_norm': 0.3517655044523416, 'learning_rate': 1.7497989044107683e-05, 'epoch': 0.25}

 25%|██▌       | 975/3844 [2:01:09<5:44:32,  7.21s/it]


 25%|██▌       | 977/3844 [2:01:24<5:50:25,  7.33s/it]

 25%|██▌       | 978/3844 [2:01:30<5:39:20,  7.10s/it]
{'loss': 1.1203, 'grad_norm': 0.3333276325013786, 'learning_rate': 1.747564300886952e-05, 'epoch': 0.25}


 25%|██▌       | 980/3844 [2:01:47<5:58:26,  7.51s/it]

 26%|██▌       | 981/3844 [2:01:52<5:32:33,  6.97s/it]

 26%|██▌       | 982/3844 [2:02:01<5:52:32,  7.39s/it]

 26%|██▌       | 983/3844 [2:02:07<5:33:54,  7.00s/it]
{'loss': 1.2462, 'grad_norm': 0.30866340473410836, 'learning_rate': 1.7447591046746018e-05, 'epoch': 0.26}


 26%|██▌       | 985/3844 [2:02:21<5:32:09,  6.97s/it]

 26%|██▌       | 986/3844 [2:02:28<5:30:47,  6.94s/it]

 26%|██▌       | 987/3844 [2:02:37<5:56:59,  7.50s/it]
{'loss': 1.1057, 'grad_norm': 0.32470757557818647, 'learning_rate': 1.7425054253353905e-05, 'epoch': 0.26}


 26%|██▌       | 989/3844 [2:02:53<6:08:46,  7.75s/it]
{'loss': 1.1125, 'grad_norm': 0.3534473732487704, 'learning_rate': 1.7413754203389114e-05, 'epoch': 0.26}


 26%|██▌       | 991/3844 [2:03:06<5:36:51,  7.08s/it]

 26%|██▌       | 992/3844 [2:03:12<5:28:00,  6.90s/it]

 26%|██▌       | 993/3844 [2:03:20<5:45:59,  7.28s/it]
{'loss': 1.2815, 'grad_norm': 0.3409517351316256, 'learning_rate': 1.7391090957415756e-05, 'epoch': 0.26}


 26%|██▌       | 995/3844 [2:03:33<5:15:36,  6.65s/it]

 26%|██▌       | 996/3844 [2:03:39<5:03:44,  6.40s/it]
{'loss': 1.2391, 'grad_norm': 0.3528466467011437, 'learning_rate': 1.737403839691879e-05, 'epoch': 0.26}

 26%|██▌       | 997/3844 [2:03:45<5:09:06,  6.51s/it]


 26%|██▌       | 999/3844 [2:04:00<5:35:11,  7.07s/it]
{'loss': 1.1624, 'grad_norm': 0.3328674540259706, 'learning_rate': 1.73569387066203e-05, 'epoch': 0.26}

 26%|██▌       | 1000/3844 [2:04:06<5:14:23,  6.63s/it]

 26%|██▌       | 1001/3844 [2:04:12<5:07:34,  6.49s/it]


 26%|██▌       | 1003/3844 [2:04:27<5:36:17,  7.10s/it]
{'loss': 1.1763, 'grad_norm': 0.31948496011489186, 'learning_rate': 1.7334065995469798e-05, 'epoch': 0.26}

 26%|██▌       | 1004/3844 [2:04:34<5:34:32,  7.07s/it]

 26%|██▌       | 1005/3844 [2:04:42<5:49:12,  7.38s/it]


 26%|██▌       | 1007/3844 [2:04:56<5:38:56,  7.17s/it]
{'loss': 1.1487, 'grad_norm': 0.35655300443277094, 'learning_rate': 1.7311109952219917e-05, 'epoch': 0.26}

 26%|██▌       | 1008/3844 [2:05:02<5:18:38,  6.74s/it]


 26%|██▋       | 1010/3844 [2:05:17<5:42:50,  7.26s/it]

 26%|██▋       | 1011/3844 [2:05:25<5:55:14,  7.52s/it]

 26%|██▋       | 1012/3844 [2:05:32<5:51:32,  7.45s/it]
{'loss': 1.1766, 'grad_norm': 0.33846645951310006, 'learning_rate': 1.7282298109847346e-05, 'epoch': 0.26}


 26%|██▋       | 1014/3844 [2:05:48<5:58:46,  7.61s/it]
{'loss': 1.2895, 'grad_norm': 0.33325922653556084, 'learning_rate': 1.7270737143741293e-05, 'epoch': 0.26}


 26%|██▋       | 1016/3844 [2:06:00<5:21:12,  6.81s/it]

 26%|██▋       | 1017/3844 [2:06:07<5:21:27,  6.82s/it]

 26%|██▋       | 1018/3844 [2:06:15<5:35:07,  7.12s/it]
{'loss': 1.2372, 'grad_norm': 0.3378115718181286, 'learning_rate': 1.7247553284981845e-05, 'epoch': 0.26}

 27%|██▋       | 1019/3844 [2:06:25<6:24:14,  8.16s/it]

 27%|██▋       | 1020/3844 [2:06:31<5:56:22,  7.57s/it]


 27%|██▋       | 1022/3844 [2:06:49<6:28:18,  8.26s/it]

 27%|██▋       | 1023/3844 [2:07:01<7:17:53,  9.31s/it]
[2024-05-25 21:48:42,351] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 1024/3844 [2:07:07<6:31:32,  8.33s/it]

 27%|██▋       | 1025/3844 [2:07:13<5:55:25,  7.56s/it]
{'loss': 1.2252, 'grad_norm': 0.3416254784145273, 'learning_rate': 1.7206783538592282e-05, 'epoch': 0.27}

 27%|██▋       | 1026/3844 [2:07:20<5:49:33,  7.44s/it]

 27%|██▋       | 1027/3844 [2:07:25<5:23:29,  6.89s/it]


 27%|██▋       | 1029/3844 [2:07:41<5:41:15,  7.27s/it]
{'loss': 1.2258, 'grad_norm': 0.3429221801631007, 'learning_rate': 1.7183373856445223e-05, 'epoch': 0.27}

 27%|██▋       | 1030/3844 [2:07:50<6:03:54,  7.76s/it]


 27%|██▋       | 1032/3844 [2:08:01<5:13:07,  6.68s/it]

 27%|██▋       | 1033/3844 [2:08:06<4:56:02,  6.32s/it]

 27%|██▋       | 1034/3844 [2:08:13<5:02:43,  6.46s/it]

 27%|██▋       | 1035/3844 [2:08:19<4:46:58,  6.13s/it]

 27%|██▋       | 1036/3844 [2:08:24<4:43:00,  6.05s/it]

 27%|██▋       | 1037/3844 [2:08:31<4:44:14,  6.08s/it]

 27%|██▋       | 1038/3844 [2:08:37<4:51:11,  6.23s/it]

 27%|██▋       | 1039/3844 [2:08:45<5:09:38,  6.62s/it]

 27%|██▋       | 1040/3844 [2:08:51<5:08:28,  6.60s/it]

 27%|██▋       | 1041/3844 [2:08:59<5:29:29,  7.05s/it]
{'loss': 1.0941, 'grad_norm': 0.32305961419672485, 'learning_rate': 1.7112656159281614e-05, 'epoch': 0.27}


 27%|██▋       | 1043/3844 [2:09:15<5:57:46,  7.66s/it]
{'loss': 1.1189, 'grad_norm': 0.31595216093312106, 'learning_rate': 1.7100798966245386e-05, 'epoch': 0.27}

 27%|██▋       | 1044/3844 [2:09:22<5:41:46,  7.32s/it]

 27%|██▋       | 1045/3844 [2:09:32<6:15:51,  8.06s/it]

 27%|██▋       | 1046/3844 [2:09:38<5:52:31,  7.56s/it]

 27%|██▋       | 1047/3844 [2:09:44<5:27:04,  7.02s/it]

 27%|██▋       | 1048/3844 [2:09:52<5:38:40,  7.27s/it]


 27%|██▋       | 1050/3844 [2:10:07<5:48:53,  7.49s/it]

 27%|██▋       | 1051/3844 [2:10:13<5:27:05,  7.03s/it]
{'loss': 1.4008, 'grad_norm': 0.3588748271991492, 'learning_rate': 1.7053168827525443e-05, 'epoch': 0.27}


 27%|██▋       | 1053/3844 [2:10:27<5:43:41,  7.39s/it]

 27%|██▋       | 1054/3844 [2:10:34<5:37:10,  7.25s/it]

 27%|██▋       | 1055/3844 [2:10:40<5:19:19,  6.87s/it]

 27%|██▋       | 1056/3844 [2:10:47<5:14:18,  6.76s/it]

 27%|██▋       | 1057/3844 [2:10:55<5:31:09,  7.13s/it]

 28%|██▊       | 1058/3844 [2:11:01<5:17:09,  6.83s/it]

 28%|██▊       | 1059/3844 [2:11:09<5:33:17,  7.18s/it]

 28%|██▊       | 1060/3844 [2:11:17<5:52:02,  7.59s/it]

 28%|██▊       | 1061/3844 [2:11:25<5:49:44,  7.54s/it]

 28%|██▊       | 1062/3844 [2:11:32<5:49:45,  7.54s/it]

 28%|██▊       | 1063/3844 [2:11:41<6:05:39,  7.89s/it]
{'loss': 1.2122, 'grad_norm': 0.3535392381136557, 'learning_rate': 1.6981123248232477e-05, 'epoch': 0.28}

 28%|██▊       | 1064/3844 [2:11:50<6:21:14,  8.23s/it]


 28%|██▊       | 1066/3844 [2:12:07<6:29:02,  8.40s/it]

 28%|██▊       | 1067/3844 [2:12:14<6:03:14,  7.85s/it]

 28%|██▊       | 1068/3844 [2:12:25<7:00:24,  9.09s/it]
{'loss': 1.169, 'grad_norm': 0.34639537398860226, 'learning_rate': 1.6950893134461454e-05, 'epoch': 0.28}

 28%|██▊       | 1069/3844 [2:12:32<6:22:40,  8.27s/it]


 28%|██▊       | 1071/3844 [2:12:45<5:44:59,  7.46s/it]
{'loss': 1.1106, 'grad_norm': 0.345895596803427, 'learning_rate': 1.6932695798019366e-05, 'epoch': 0.28}

 28%|██▊       | 1072/3844 [2:12:52<5:37:02,  7.30s/it]

 28%|██▊       | 1073/3844 [2:12:58<5:21:57,  6.97s/it]


 28%|██▊       | 1075/3844 [2:13:17<6:29:37,  8.44s/it]

 28%|██▊       | 1076/3844 [2:13:27<6:49:52,  8.88s/it]

 28%|██▊       | 1077/3844 [2:13:33<6:17:57,  8.20s/it]
{'loss': 1.1465, 'grad_norm': 0.3301803802611622, 'learning_rate': 1.689616831459732e-05, 'epoch': 0.28}

 28%|██▊       | 1078/3844 [2:13:40<5:59:57,  7.81s/it]


 28%|██▊       | 1080/3844 [2:13:57<6:13:07,  8.10s/it]
{'loss': 1.1337, 'grad_norm': 0.324887384462435, 'learning_rate': 1.6877838401076006e-05, 'epoch': 0.28}


 28%|██▊       | 1082/3844 [2:14:15<6:37:54,  8.64s/it]

 28%|██▊       | 1083/3844 [2:14:20<5:53:11,  7.68s/it]

 28%|██▊       | 1084/3844 [2:14:29<5:59:55,  7.82s/it]

 28%|██▊       | 1085/3844 [2:14:36<5:58:04,  7.79s/it]
{'loss': 1.1671, 'grad_norm': 0.3458889079377147, 'learning_rate': 1.6847190917784894e-05, 'epoch': 0.28}


 28%|██▊       | 1087/3844 [2:14:50<5:27:36,  7.13s/it]

 28%|██▊       | 1088/3844 [2:14:57<5:30:58,  7.21s/it]

 28%|██▊       | 1089/3844 [2:15:03<5:11:32,  6.78s/it]

 28%|██▊       | 1090/3844 [2:15:09<5:02:05,  6.58s/it]

 28%|██▊       | 1091/3844 [2:15:18<5:27:54,  7.15s/it]

 28%|██▊       | 1092/3844 [2:15:25<5:35:19,  7.31s/it]

 28%|██▊       | 1093/3844 [2:15:35<6:07:17,  8.01s/it]

 28%|██▊       | 1094/3844 [2:15:43<6:11:26,  8.10s/it]

 28%|██▊       | 1095/3844 [2:15:49<5:38:13,  7.38s/it]

 29%|██▊       | 1096/3844 [2:15:55<5:18:01,  6.94s/it]

 29%|██▊       | 1097/3844 [2:16:01<5:09:47,  6.77s/it]

 29%|██▊       | 1098/3844 [2:16:07<4:53:08,  6.41s/it]

 29%|██▊       | 1099/3844 [2:16:13<4:46:20,  6.26s/it]

 29%|██▊       | 1100/3844 [2:16:19<4:48:56,  6.32s/it]
{'loss': 1.0567, 'grad_norm': 0.3370096548389319, 'learning_rate': 1.675452127991077e-05, 'epoch': 0.29}

 29%|██▊       | 1101/3844 [2:16:26<5:01:07,  6.59s/it]

 29%|██▊       | 1102/3844 [2:16:32<4:55:28,  6.47s/it]


 29%|██▊       | 1104/3844 [2:16:47<5:10:19,  6.80s/it]

 29%|██▊       | 1105/3844 [2:16:58<6:03:26,  7.96s/it]

 29%|██▉       | 1106/3844 [2:17:07<6:28:22,  8.51s/it]
{'loss': 1.1953, 'grad_norm': 0.3491780703443769, 'learning_rate': 1.6717150401713903e-05, 'epoch': 0.29}


 29%|██▉       | 1108/3844 [2:17:22<5:57:03,  7.83s/it]

 29%|██▉       | 1109/3844 [2:17:29<5:47:44,  7.63s/it]
{'loss': 1.203, 'grad_norm': 0.33627386271975085, 'learning_rate': 1.6698400505699187e-05, 'epoch': 0.29}


 29%|██▉       | 1111/3844 [2:17:42<5:17:33,  6.97s/it]

 29%|██▉       | 1112/3844 [2:17:47<4:55:50,  6.50s/it]

 29%|██▉       | 1113/3844 [2:17:54<4:57:39,  6.54s/it]

 29%|██▉       | 1114/3844 [2:18:03<5:37:06,  7.41s/it]

 29%|██▉       | 1115/3844 [2:18:11<5:38:37,  7.45s/it]
{'loss': 1.3173, 'grad_norm': 0.3316463403631882, 'learning_rate': 1.666077239901888e-05, 'epoch': 0.29}

 29%|██▉       | 1116/3844 [2:18:25<7:04:33,  9.34s/it]

 29%|██▉       | 1117/3844 [2:18:30<6:15:43,  8.27s/it]


 29%|██▉       | 1119/3844 [2:18:49<6:34:20,  8.68s/it]
{'loss': 1.2602, 'grad_norm': 0.34114888411511707, 'learning_rate': 1.6635592332725233e-05, 'epoch': 0.29}

 29%|██▉       | 1120/3844 [2:18:56<6:07:46,  8.10s/it]

 29%|██▉       | 1121/3844 [2:19:03<5:47:15,  7.65s/it]


 29%|██▉       | 1123/3844 [2:19:17<5:32:42,  7.34s/it]

 29%|██▉       | 1124/3844 [2:19:23<5:13:28,  6.91s/it]

 29%|██▉       | 1125/3844 [2:19:32<5:41:37,  7.54s/it]
{'loss': 1.0893, 'grad_norm': 0.3480958584987558, 'learning_rate': 1.659768095578464e-05, 'epoch': 0.29}

 29%|██▉       | 1126/3844 [2:19:38<5:25:47,  7.19s/it]


 29%|██▉       | 1128/3844 [2:19:52<5:05:24,  6.75s/it]
{'loss': 1.2603, 'grad_norm': 0.3661202486248471, 'learning_rate': 1.657866195488593e-05, 'epoch': 0.29}


 29%|██▉       | 1130/3844 [2:20:05<5:02:20,  6.68s/it]
{'loss': 1.2051, 'grad_norm': 0.3537221992710558, 'learning_rate': 1.6565959254383237e-05, 'epoch': 0.29}

 29%|██▉       | 1131/3844 [2:20:11<4:50:35,  6.43s/it]

 29%|██▉       | 1132/3844 [2:20:17<4:42:42,  6.25s/it]


 30%|██▉       | 1134/3844 [2:20:31<5:12:31,  6.92s/it]

 30%|██▉       | 1135/3844 [2:20:42<6:06:16,  8.11s/it]
{'loss': 1.1322, 'grad_norm': 0.32935903204373423, 'learning_rate': 1.653412098333317e-05, 'epoch': 0.3}


 30%|██▉       | 1137/3844 [2:21:00<6:18:33,  8.39s/it]

 30%|██▉       | 1138/3844 [2:21:08<6:10:52,  8.22s/it]

 30%|██▉       | 1139/3844 [2:21:20<7:03:27,  9.39s/it]

 30%|██▉       | 1140/3844 [2:21:26<6:22:34,  8.49s/it]

 30%|██▉       | 1141/3844 [2:21:32<5:40:56,  7.57s/it]
{'loss': 1.3179, 'grad_norm': 0.3661182043231845, 'learning_rate': 1.6495761982147007e-05, 'epoch': 0.3}

 30%|██▉       | 1142/3844 [2:21:43<6:26:52,  8.59s/it]


 30%|██▉       | 1144/3844 [2:21:58<6:05:38,  8.13s/it]
{'loss': 1.2546, 'grad_norm': 0.34153825783753083, 'learning_rate': 1.647652014550571e-05, 'epoch': 0.3}


 30%|██▉       | 1146/3844 [2:22:11<5:27:48,  7.29s/it]
{'loss': 1.1292, 'grad_norm': 0.3488618368039351, 'learning_rate': 1.646366925043158e-05, 'epoch': 0.3}


 30%|██▉       | 1148/3844 [2:22:25<5:22:40,  7.18s/it]
{'loss': 1.1879, 'grad_norm': 0.32632489911166085, 'learning_rate': 1.6450799994753968e-05, 'epoch': 0.3}


 30%|██▉       | 1150/3844 [2:22:37<4:52:11,  6.51s/it]

 30%|██▉       | 1151/3844 [2:22:42<4:36:38,  6.16s/it]

 30%|██▉       | 1152/3844 [2:22:48<4:29:52,  6.02s/it]

 30%|██▉       | 1153/3844 [2:22:58<5:25:32,  7.26s/it]
{'loss': 1.0769, 'grad_norm': 0.32014907809146537, 'learning_rate': 1.6418546767947908e-05, 'epoch': 0.3}


 30%|███       | 1155/3844 [2:23:12<5:13:38,  7.00s/it]

 30%|███       | 1156/3844 [2:23:18<4:54:53,  6.58s/it]

 30%|███       | 1157/3844 [2:23:27<5:30:25,  7.38s/it]

 30%|███       | 1158/3844 [2:23:34<5:28:18,  7.33s/it]

 30%|███       | 1159/3844 [2:23:44<5:57:47,  8.00s/it]

 30%|███       | 1160/3844 [2:23:52<5:59:30,  8.04s/it]
{'loss': 1.1928, 'grad_norm': 0.3919698995134766, 'learning_rate': 1.6373200938800935e-05, 'epoch': 0.3}


 30%|███       | 1162/3844 [2:24:04<5:13:41,  7.02s/it]

 30%|███       | 1163/3844 [2:24:12<5:23:44,  7.25s/it]
{'loss': 1.0585, 'grad_norm': 0.35011628645637743, 'learning_rate': 1.6353699031347666e-05, 'epoch': 0.3}

 30%|███       | 1164/3844 [2:24:19<5:16:32,  7.09s/it]

 30%|███       | 1165/3844 [2:24:25<5:04:47,  6.83s/it]


 30%|███       | 1167/3844 [2:24:39<5:09:38,  6.94s/it]

 30%|███       | 1168/3844 [2:24:48<5:33:07,  7.47s/it]

 30%|███       | 1169/3844 [2:24:54<5:15:15,  7.07s/it]

 30%|███       | 1170/3844 [2:25:00<4:55:55,  6.64s/it]

 30%|███       | 1171/3844 [2:25:06<4:49:59,  6.51s/it]
{'loss': 1.1897, 'grad_norm': 0.3515586991435819, 'learning_rate': 1.6301495753761734e-05, 'epoch': 0.3}


 31%|███       | 1173/3844 [2:25:20<4:55:56,  6.65s/it]

 31%|███       | 1174/3844 [2:25:27<5:11:33,  7.00s/it]

 31%|███       | 1175/3844 [2:25:34<5:04:15,  6.84s/it]

 31%|███       | 1176/3844 [2:25:42<5:18:25,  7.16s/it]

 31%|███       | 1177/3844 [2:25:48<5:11:56,  7.02s/it]

 31%|███       | 1178/3844 [2:25:56<5:13:29,  7.06s/it]

 31%|███       | 1179/3844 [2:26:02<4:59:30,  6.74s/it]

 31%|███       | 1180/3844 [2:26:07<4:45:20,  6.43s/it]
{'loss': 1.2247, 'grad_norm': 0.36800247717784923, 'learning_rate': 1.62424248500937e-05, 'epoch': 0.31}


 31%|███       | 1182/3844 [2:26:21<4:56:53,  6.69s/it]

 31%|███       | 1183/3844 [2:26:27<4:47:35,  6.48s/it]

 31%|███       | 1184/3844 [2:26:34<4:44:34,  6.42s/it]

 31%|███       | 1185/3844 [2:26:40<4:47:39,  6.49s/it]

 31%|███       | 1186/3844 [2:26:48<4:59:09,  6.75s/it]

 31%|███       | 1187/3844 [2:26:53<4:45:28,  6.45s/it]

 31%|███       | 1188/3844 [2:27:05<6:01:27,  8.17s/it]

 31%|███       | 1189/3844 [2:27:13<5:57:05,  8.07s/it]

 31%|███       | 1190/3844 [2:27:19<5:30:03,  7.46s/it]

 31%|███       | 1191/3844 [2:27:28<5:40:29,  7.70s/it]

 31%|███       | 1192/3844 [2:27:40<6:46:03,  9.19s/it]

 31%|███       | 1193/3844 [2:27:47<6:07:51,  8.33s/it]

 31%|███       | 1194/3844 [2:27:56<6:23:53,  8.69s/it]

 31%|███       | 1195/3844 [2:28:02<5:50:46,  7.95s/it]

 31%|███       | 1196/3844 [2:28:11<5:53:41,  8.01s/it]
{'loss': 1.1929, 'grad_norm': 0.3251504181349078, 'learning_rate': 1.613652549489921e-05, 'epoch': 0.31}


 31%|███       | 1198/3844 [2:28:24<5:20:25,  7.27s/it]
{'loss': 1.1428, 'grad_norm': 0.344931536353774, 'learning_rate': 1.6123209238673515e-05, 'epoch': 0.31}


 31%|███       | 1200/3844 [2:28:36<4:52:47,  6.64s/it]
 31%|███       | 1200/3844 [2:28:36<4:52:47,  6.64s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1002, 'grad_norm': 0.3487878631613872, 'learning_rate': 1.6103202253360718e-05, 'epoch': 0.31}

 31%|███▏      | 1202/3844 [2:29:16<8:59:56, 12.26s/it]
{'loss': 1.1412, 'grad_norm': 0.36447524979450485, 'learning_rate': 1.6096524583604088e-05, 'epoch': 0.31}

 31%|███▏      | 1203/3844 [2:29:21<7:33:42, 10.31s/it]


 31%|███▏      | 1205/3844 [2:29:36<6:30:37,  8.88s/it]

 31%|███▏      | 1206/3844 [2:29:42<5:49:45,  7.96s/it]

 31%|███▏      | 1207/3844 [2:29:52<6:19:06,  8.63s/it]
{'loss': 1.1479, 'grad_norm': 0.34633357461348296, 'learning_rate': 1.6063071388392775e-05, 'epoch': 0.31}


 31%|███▏      | 1209/3844 [2:30:08<6:06:01,  8.33s/it]
{'loss': 1.1685, 'grad_norm': 0.3358291817386704, 'learning_rate': 1.6049659937347647e-05, 'epoch': 0.31}


 32%|███▏      | 1211/3844 [2:30:21<5:19:30,  7.28s/it]
{'loss': 1.2202, 'grad_norm': 0.3459339227614338, 'learning_rate': 1.6036231301727775e-05, 'epoch': 0.31}


 32%|███▏      | 1213/3844 [2:30:36<5:24:59,  7.41s/it]

 32%|███▏      | 1214/3844 [2:30:42<5:09:03,  7.05s/it]

 32%|███▏      | 1215/3844 [2:30:48<4:57:35,  6.79s/it]

 32%|███▏      | 1216/3844 [2:30:57<5:16:56,  7.24s/it]
{'loss': 1.3098, 'grad_norm': 0.3466934970364959, 'learning_rate': 1.6002584780611193e-05, 'epoch': 0.32}


 32%|███▏      | 1218/3844 [2:31:10<5:01:44,  6.89s/it]

 32%|███▏      | 1219/3844 [2:31:17<4:59:28,  6.85s/it]

 32%|███▏      | 1220/3844 [2:31:22<4:46:22,  6.55s/it]

 32%|███▏      | 1221/3844 [2:31:30<5:02:56,  6.93s/it]

 32%|███▏      | 1222/3844 [2:31:36<4:49:17,  6.62s/it]
{'loss': 1.2657, 'grad_norm': 0.39209778334988965, 'learning_rate': 1.596206833860264e-05, 'epoch': 0.32}


 32%|███▏      | 1224/3844 [2:31:50<4:52:41,  6.70s/it]

 32%|███▏      | 1225/3844 [2:32:01<5:48:36,  7.99s/it]

 32%|███▏      | 1226/3844 [2:32:07<5:20:02,  7.33s/it]
{'loss': 1.2297, 'grad_norm': 0.35833697878964454, 'learning_rate': 1.5934972634507616e-05, 'epoch': 0.32}


 32%|███▏      | 1228/3844 [2:32:26<6:15:27,  8.61s/it]

 32%|███▏      | 1229/3844 [2:32:32<5:42:13,  7.85s/it]
{'loss': 1.3151, 'grad_norm': 0.3275435674843147, 'learning_rate': 1.59146065852636e-05, 'epoch': 0.32}

 32%|███▏      | 1230/3844 [2:32:38<5:17:01,  7.28s/it]

 32%|███▏      | 1231/3844 [2:32:44<5:02:35,  6.95s/it]


 32%|███▏      | 1233/3844 [2:32:59<5:16:11,  7.27s/it]
{'loss': 1.0569, 'grad_norm': 0.4002065240121803, 'learning_rate': 1.588739307218361e-05, 'epoch': 0.32}

 32%|███▏      | 1234/3844 [2:33:05<5:09:55,  7.12s/it]

 32%|███▏      | 1235/3844 [2:33:12<4:56:10,  6.81s/it]

 32%|███▏      | 1236/3844 [2:33:20<5:14:49,  7.24s/it]

 32%|███▏      | 1237/3844 [2:33:28<5:23:07,  7.44s/it]

 32%|███▏      | 1238/3844 [2:33:33<4:58:00,  6.86s/it]

 32%|███▏      | 1239/3844 [2:33:40<4:55:07,  6.80s/it]

 32%|███▏      | 1240/3844 [2:33:49<5:29:34,  7.59s/it]


 32%|███▏      | 1242/3844 [2:34:07<6:06:55,  8.46s/it]
{'loss': 1.1482, 'grad_norm': 0.3344955410212098, 'learning_rate': 1.5825918556955408e-05, 'epoch': 0.32}


 32%|███▏      | 1244/3844 [2:34:21<5:35:43,  7.75s/it]

 32%|███▏      | 1245/3844 [2:34:27<5:12:31,  7.21s/it]

 32%|███▏      | 1246/3844 [2:34:35<5:21:22,  7.42s/it]
{'loss': 1.0724, 'grad_norm': 0.3706944969100161, 'learning_rate': 1.579848877116201e-05, 'epoch': 0.32}


 32%|███▏      | 1248/3844 [2:34:53<6:01:45,  8.36s/it]

 32%|███▏      | 1249/3844 [2:34:59<5:24:23,  7.50s/it]
{'loss': 1.1885, 'grad_norm': 0.3466960583589279, 'learning_rate': 1.5777873178133437e-05, 'epoch': 0.32}


 33%|███▎      | 1251/3844 [2:35:11<4:48:15,  6.67s/it]
{'loss': 1.1795, 'grad_norm': 0.3629067517249543, 'learning_rate': 1.5764108925634083e-05, 'epoch': 0.33}

 33%|███▎      | 1252/3844 [2:35:20<5:18:11,  7.37s/it]


 33%|███▎      | 1254/3844 [2:35:31<4:40:44,  6.50s/it]

 33%|███▎      | 1255/3844 [2:35:38<4:51:58,  6.77s/it]
{'loss': 1.1103, 'grad_norm': 0.3770210102198826, 'learning_rate': 1.5736531339452156e-05, 'epoch': 0.33}


 33%|███▎      | 1257/3844 [2:35:53<5:04:34,  7.06s/it]

 33%|███▎      | 1258/3844 [2:35:59<4:47:18,  6.67s/it]

 33%|███▎      | 1259/3844 [2:36:04<4:36:02,  6.41s/it]
{'loss': 1.2043, 'grad_norm': 0.3853537889080449, 'learning_rate': 1.5708888572891437e-05, 'epoch': 0.33}

 33%|███▎      | 1260/3844 [2:36:11<4:43:54,  6.59s/it]


 33%|███▎      | 1262/3844 [2:36:27<5:05:42,  7.10s/it]

 33%|███▎      | 1263/3844 [2:36:33<4:55:36,  6.87s/it]
{'loss': 1.2148, 'grad_norm': 0.36547486149451053, 'learning_rate': 1.568118094003825e-05, 'epoch': 0.33}


 33%|███▎      | 1265/3844 [2:36:49<5:19:27,  7.43s/it]
{'loss': 1.1944, 'grad_norm': 0.34800468234466736, 'learning_rate': 1.566730289710558e-05, 'epoch': 0.33}


 33%|███▎      | 1267/3844 [2:37:01<4:46:53,  6.68s/it]
{'loss': 1.1943, 'grad_norm': 0.38802889284536757, 'learning_rate': 1.5653408755715935e-05, 'epoch': 0.33}

 33%|███▎      | 1268/3844 [2:37:11<5:32:48,  7.75s/it]


 33%|███▎      | 1270/3844 [2:37:27<5:24:54,  7.57s/it]
{'loss': 1.1673, 'grad_norm': 0.38869648690797, 'learning_rate': 1.563253744537225e-05, 'epoch': 0.33}


 33%|███▎      | 1272/3844 [2:37:41<5:23:14,  7.54s/it]
{'loss': 1.199, 'grad_norm': 0.40569970867951555, 'learning_rate': 1.5618603230610197e-05, 'epoch': 0.33}


 33%|███▎      | 1274/3844 [2:37:53<4:53:34,  6.85s/it]
{'loss': 1.1698, 'grad_norm': 0.3568633955067065, 'learning_rate': 1.560465305572672e-05, 'epoch': 0.33}

 33%|███▎      | 1275/3844 [2:38:00<4:54:15,  6.87s/it]

 33%|███▎      | 1276/3844 [2:38:06<4:38:55,  6.52s/it]


 33%|███▎      | 1278/3844 [2:38:21<5:04:27,  7.12s/it]

 33%|███▎      | 1279/3844 [2:38:29<5:06:51,  7.18s/it]

 33%|███▎      | 1280/3844 [2:38:35<4:59:49,  7.02s/it]

 33%|███▎      | 1281/3844 [2:38:41<4:45:29,  6.68s/it]

 33%|███▎      | 1282/3844 [2:38:47<4:34:22,  6.43s/it]

 33%|███▎      | 1283/3844 [2:38:53<4:33:12,  6.40s/it]

 33%|███▎      | 1284/3844 [2:39:01<4:46:38,  6.72s/it]

 33%|███▎      | 1285/3844 [2:39:07<4:42:04,  6.61s/it]
{'loss': 1.2087, 'grad_norm': 0.331055850115537, 'learning_rate': 1.5527643579783134e-05, 'epoch': 0.33}

 33%|███▎      | 1286/3844 [2:39:15<4:51:17,  6.83s/it]


 34%|███▎      | 1288/3844 [2:39:27<4:35:06,  6.46s/it]
{'loss': 1.2538, 'grad_norm': 0.3484795280481608, 'learning_rate': 1.5506558282465993e-05, 'epoch': 0.34}


 34%|███▎      | 1290/3844 [2:39:41<4:50:56,  6.83s/it]
{'loss': 1.1587, 'grad_norm': 0.3862334372081287, 'learning_rate': 1.549248185695548e-05, 'epoch': 0.34}


 34%|███▎      | 1292/3844 [2:39:55<4:49:16,  6.80s/it]
{'loss': 1.2345, 'grad_norm': 0.3666105083897945, 'learning_rate': 1.5478389829582057e-05, 'epoch': 0.34}


 34%|███▎      | 1294/3844 [2:40:09<4:54:38,  6.93s/it]

 34%|███▎      | 1295/3844 [2:40:15<4:37:00,  6.52s/it]
{'loss': 1.3239, 'grad_norm': 0.35286644417162955, 'learning_rate': 1.5457222622605813e-05, 'epoch': 0.34}


 34%|███▎      | 1297/3844 [2:40:29<4:56:26,  6.98s/it]
{'loss': 1.0464, 'grad_norm': 0.37586770113195334, 'learning_rate': 1.5443091765801305e-05, 'epoch': 0.34}

 34%|███▍      | 1298/3844 [2:40:35<4:38:04,  6.55s/it]


 34%|███▍      | 1300/3844 [2:40:47<4:29:43,  6.36s/it]

 34%|███▍      | 1301/3844 [2:40:55<4:51:49,  6.89s/it]

 34%|███▍      | 1302/3844 [2:41:03<5:09:06,  7.30s/it]

 34%|███▍      | 1303/3844 [2:41:09<4:52:32,  6.91s/it]
{'loss': 1.1731, 'grad_norm': 0.371667082764296, 'learning_rate': 1.5400606586769985e-05, 'epoch': 0.34}

 34%|███▍      | 1304/3844 [2:41:16<4:55:45,  6.99s/it]

 34%|███▍      | 1305/3844 [2:41:22<4:40:45,  6.63s/it]


 34%|███▍      | 1307/3844 [2:41:37<5:03:12,  7.17s/it]
{'loss': 1.1209, 'grad_norm': 0.3917704226771498, 'learning_rate': 1.5372206362619407e-05, 'epoch': 0.34}

 34%|███▍      | 1308/3844 [2:41:45<5:06:40,  7.26s/it]

 34%|███▍      | 1309/3844 [2:41:50<4:47:31,  6.81s/it]


 34%|███▍      | 1311/3844 [2:42:05<5:01:59,  7.15s/it]
{'loss': 1.1007, 'grad_norm': 0.3388181836905857, 'learning_rate': 1.5343745097671708e-05, 'epoch': 0.34}


 34%|███▍      | 1313/3844 [2:42:20<4:53:33,  6.96s/it]

 34%|███▍      | 1314/3844 [2:42:25<4:32:35,  6.46s/it]

 34%|███▍      | 1315/3844 [2:42:33<4:55:19,  7.01s/it]

 34%|███▍      | 1316/3844 [2:42:39<4:45:27,  6.78s/it]
{'loss': 1.1204, 'grad_norm': 0.3671273748015705, 'learning_rate': 1.53080831705908e-05, 'epoch': 0.34}

 34%|███▍      | 1317/3844 [2:42:46<4:43:51,  6.74s/it]

 34%|███▍      | 1318/3844 [2:42:54<4:59:55,  7.12s/it]

 34%|███▍      | 1319/3844 [2:43:00<4:42:27,  6.71s/it]

 34%|███▍      | 1320/3844 [2:43:06<4:36:24,  6.57s/it]

 34%|███▍      | 1321/3844 [2:43:16<5:22:58,  7.68s/it]


 34%|███▍      | 1323/3844 [2:43:34<5:55:27,  8.46s/it]

 34%|███▍      | 1324/3844 [2:43:41<5:43:40,  8.18s/it]
{'loss': 1.0343, 'grad_norm': 0.3888716031176403, 'learning_rate': 1.5250828336697878e-05, 'epoch': 0.34}


 34%|███▍      | 1326/3844 [2:43:58<5:42:17,  8.16s/it]

 35%|███▍      | 1327/3844 [2:44:04<5:13:39,  7.48s/it]

 35%|███▍      | 1328/3844 [2:44:09<4:53:30,  7.00s/it]
{'loss': 1.1373, 'grad_norm': 0.36678334885435443, 'learning_rate': 1.5222111264454524e-05, 'epoch': 0.35}

 35%|███▍      | 1329/3844 [2:44:18<5:16:21,  7.55s/it]


 35%|███▍      | 1331/3844 [2:44:32<4:52:47,  6.99s/it]
{'loss': 1.2233, 'grad_norm': 0.38719814821062243, 'learning_rate': 1.5200534503568124e-05, 'epoch': 0.35}


 35%|███▍      | 1333/3844 [2:44:47<5:01:46,  7.21s/it]
{'loss': 1.2067, 'grad_norm': 0.33404313625393905, 'learning_rate': 1.518613152209269e-05, 'epoch': 0.35}


 35%|███▍      | 1335/3844 [2:45:01<4:57:01,  7.10s/it]
{'loss': 1.0738, 'grad_norm': 0.365039628032519, 'learning_rate': 1.5171713808968584e-05, 'epoch': 0.35}


 35%|███▍      | 1337/3844 [2:45:13<4:33:51,  6.55s/it]
{'loss': 1.1212, 'grad_norm': 0.3086925508498483, 'learning_rate': 1.5157281405150554e-05, 'epoch': 0.35}

 35%|███▍      | 1338/3844 [2:45:23<5:13:49,  7.51s/it]

 35%|███▍      | 1339/3844 [2:45:33<5:44:51,  8.26s/it]

 35%|███▍      | 1340/3844 [2:45:38<5:09:31,  7.42s/it]

 35%|███▍      | 1341/3844 [2:45:44<4:50:43,  6.97s/it]

 35%|███▍      | 1342/3844 [2:45:50<4:36:52,  6.64s/it]


 35%|███▍      | 1344/3844 [2:46:02<4:19:18,  6.22s/it]

 35%|███▍      | 1345/3844 [2:46:12<5:06:34,  7.36s/it]

 35%|███▌      | 1346/3844 [2:46:17<4:42:03,  6.77s/it]
{'loss': 1.312, 'grad_norm': 0.35460157549204263, 'learning_rate': 1.5092154890824935e-05, 'epoch': 0.35}

 35%|███▌      | 1347/3844 [2:46:23<4:28:45,  6.46s/it]

 35%|███▌      | 1348/3844 [2:46:29<4:20:38,  6.27s/it]

 35%|███▌      | 1349/3844 [2:46:34<4:12:37,  6.08s/it]

 35%|███▌      | 1350/3844 [2:46:43<4:40:31,  6.75s/it]

 35%|███▌      | 1351/3844 [2:46:48<4:27:36,  6.44s/it]

 35%|███▌      | 1352/3844 [2:46:54<4:20:25,  6.27s/it]


 35%|███▌      | 1354/3844 [2:47:05<4:04:52,  5.90s/it]

 35%|███▌      | 1355/3844 [2:47:11<4:05:25,  5.92s/it]
{'loss': 1.074, 'grad_norm': 0.37647890386897226, 'learning_rate': 1.5026735467658258e-05, 'epoch': 0.35}


 35%|███▌      | 1357/3844 [2:47:25<4:24:25,  6.38s/it]
{'loss': 1.1463, 'grad_norm': 0.34673233259664893, 'learning_rate': 1.5012158418534726e-05, 'epoch': 0.35}

 35%|███▌      | 1358/3844 [2:47:37<5:25:39,  7.86s/it]

 35%|███▌      | 1359/3844 [2:47:42<4:59:46,  7.24s/it]


 35%|███▌      | 1361/3844 [2:47:58<5:15:36,  7.63s/it]
{'loss': 1.1257, 'grad_norm': 0.3754224227453569, 'learning_rate': 1.4982961649345736e-05, 'epoch': 0.35}


 35%|███▌      | 1363/3844 [2:48:11<4:55:51,  7.16s/it]

 35%|███▌      | 1364/3844 [2:48:18<4:49:02,  6.99s/it]
{'loss': 1.0872, 'grad_norm': 0.36454684829478345, 'learning_rate': 1.4961026898679703e-05, 'epoch': 0.35}


 36%|███▌      | 1366/3844 [2:48:32<4:49:57,  7.02s/it]

 36%|███▌      | 1367/3844 [2:48:37<4:29:57,  6.54s/it]

 36%|███▌      | 1368/3844 [2:48:43<4:21:34,  6.34s/it]

 36%|███▌      | 1369/3844 [2:48:51<4:45:40,  6.93s/it]

 36%|███▌      | 1370/3844 [2:48:58<4:38:58,  6.77s/it]
{'loss': 1.2027, 'grad_norm': 0.35452903885659287, 'learning_rate': 1.4917062415286689e-05, 'epoch': 0.36}

 36%|███▌      | 1371/3844 [2:49:04<4:36:50,  6.72s/it]


 36%|███▌      | 1373/3844 [2:49:19<4:48:16,  7.00s/it]

 36%|███▌      | 1374/3844 [2:49:26<4:42:50,  6.87s/it]
{'loss': 1.0608, 'grad_norm': 0.3782832865394394, 'learning_rate': 1.4887682853636005e-05, 'epoch': 0.36}

 36%|███▌      | 1375/3844 [2:49:33<4:41:13,  6.83s/it]

 36%|███▌      | 1376/3844 [2:49:41<4:56:45,  7.21s/it]


 36%|███▌      | 1378/3844 [2:49:56<4:59:21,  7.28s/it]
{'loss': 1.2408, 'grad_norm': 0.3586354017109432, 'learning_rate': 1.4858247756505608e-05, 'epoch': 0.36}

 36%|███▌      | 1379/3844 [2:50:03<5:01:25,  7.34s/it]


 36%|███▌      | 1381/3844 [2:50:16<4:45:05,  6.95s/it]
{'loss': 1.3547, 'grad_norm': 0.38101907767488097, 'learning_rate': 1.4836135189669038e-05, 'epoch': 0.36}


 36%|███▌      | 1383/3844 [2:50:32<5:10:13,  7.56s/it]

 36%|███▌      | 1384/3844 [2:50:40<5:17:20,  7.74s/it]
{'loss': 1.2323, 'grad_norm': 0.3498247104398588, 'learning_rate': 1.4813991713569545e-05, 'epoch': 0.36}


 36%|███▌      | 1386/3844 [2:50:55<5:11:01,  7.59s/it]

 36%|███▌      | 1387/3844 [2:51:02<4:57:47,  7.27s/it]

 36%|███▌      | 1388/3844 [2:51:10<5:08:56,  7.55s/it]
{'loss': 1.1852, 'grad_norm': 0.3637627749297791, 'learning_rate': 1.4784419242335615e-05, 'epoch': 0.36}


 36%|███▌      | 1390/3844 [2:51:26<5:12:32,  7.64s/it]
{'loss': 1.2541, 'grad_norm': 0.36451816220876637, 'learning_rate': 1.4769612599882146e-05, 'epoch': 0.36}


 36%|███▌      | 1392/3844 [2:51:46<6:08:46,  9.02s/it]
{'loss': 1.221, 'grad_norm': 0.345720025080731, 'learning_rate': 1.4754792408937508e-05, 'epoch': 0.36}


 36%|███▋      | 1394/3844 [2:52:00<5:33:12,  8.16s/it]

 36%|███▋      | 1395/3844 [2:52:06<5:08:57,  7.57s/it]
{'loss': 1.1803, 'grad_norm': 0.38246340641513804, 'learning_rate': 1.4732536811199518e-05, 'epoch': 0.36}


 36%|███▋      | 1397/3844 [2:52:22<5:17:57,  7.80s/it]
{'loss': 1.2825, 'grad_norm': 0.3843253286290869, 'learning_rate': 1.4717682933289674e-05, 'epoch': 0.36}

 36%|███▋      | 1398/3844 [2:52:28<5:06:15,  7.51s/it]

 36%|███▋      | 1399/3844 [2:52:35<4:57:13,  7.29s/it]


 36%|███▋      | 1401/3844 [2:52:50<5:03:26,  7.45s/it]

 36%|███▋      | 1402/3844 [2:52:56<4:45:43,  7.02s/it]

 36%|███▋      | 1403/3844 [2:53:02<4:34:18,  6.74s/it]

 37%|███▋      | 1404/3844 [2:53:10<4:43:43,  6.98s/it]

 37%|███▋      | 1405/3844 [2:53:16<4:31:19,  6.67s/it]
{'loss': 1.2982, 'grad_norm': 0.3961813023577944, 'learning_rate': 1.4658133834352428e-05, 'epoch': 0.37}

 37%|███▋      | 1406/3844 [2:53:23<4:45:28,  7.03s/it]


 37%|███▋      | 1408/3844 [2:53:38<5:01:20,  7.42s/it]
{'loss': 1.0684, 'grad_norm': 0.3486356302322608, 'learning_rate': 1.4635748195486984e-05, 'epoch': 0.37}


 37%|███▋      | 1410/3844 [2:53:52<4:52:45,  7.22s/it]
{'loss': 1.1619, 'grad_norm': 0.35915133800003884, 'learning_rate': 1.4620807967111318e-05, 'epoch': 0.37}


 37%|███▋      | 1412/3844 [2:54:04<4:27:38,  6.60s/it]
{'loss': 1.2116, 'grad_norm': 0.40745924299882547, 'learning_rate': 1.460585461293673e-05, 'epoch': 0.37}

 37%|███▋      | 1413/3844 [2:54:15<5:13:02,  7.73s/it]


 37%|███▋      | 1415/3844 [2:54:26<4:29:53,  6.67s/it]

 37%|███▋      | 1416/3844 [2:54:34<4:38:53,  6.89s/it]

 37%|███▋      | 1417/3844 [2:54:42<4:56:34,  7.33s/it]
{'loss': 1.1837, 'grad_norm': 0.3720145247038687, 'learning_rate': 1.4568414080971972e-05, 'epoch': 0.37}


 37%|███▋      | 1419/3844 [2:54:56<4:48:58,  7.15s/it]

 37%|███▋      | 1420/3844 [2:55:02<4:32:39,  6.75s/it]
{'loss': 1.2291, 'grad_norm': 0.3775420371920439, 'learning_rate': 1.4545910788331434e-05, 'epoch': 0.37}

 37%|███▋      | 1421/3844 [2:55:12<5:20:37,  7.94s/it]

 37%|███▋      | 1422/3844 [2:55:19<5:01:16,  7.46s/it]

 37%|███▋      | 1423/3844 [2:55:26<4:52:08,  7.24s/it]

 37%|███▋      | 1424/3844 [2:55:33<4:53:53,  7.29s/it]

 37%|███▋      | 1425/3844 [2:55:39<4:44:19,  7.05s/it]


 37%|███▋      | 1427/3844 [2:55:58<5:30:43,  8.21s/it]
{'loss': 1.0828, 'grad_norm': 0.39103318982838364, 'learning_rate': 1.4493290365309904e-05, 'epoch': 0.37}

 37%|███▋      | 1428/3844 [2:56:05<5:24:05,  8.05s/it]

 37%|███▋      | 1429/3844 [2:56:11<5:00:18,  7.46s/it]


 37%|███▋      | 1431/3844 [2:56:28<5:10:13,  7.71s/it]

 37%|███▋      | 1432/3844 [2:56:36<5:14:17,  7.82s/it]
{'loss': 1.1928, 'grad_norm': 0.37145497928934645, 'learning_rate': 1.4455608515331677e-05, 'epoch': 0.37}

 37%|███▋      | 1433/3844 [2:56:47<5:48:56,  8.68s/it]

 37%|███▋      | 1434/3844 [2:56:54<5:25:57,  8.11s/it]

 37%|███▋      | 1435/3844 [2:57:01<5:14:36,  7.84s/it]


 37%|███▋      | 1437/3844 [2:57:14<4:49:15,  7.21s/it]

 37%|███▋      | 1438/3844 [2:57:20<4:38:15,  6.94s/it]

 37%|███▋      | 1439/3844 [2:57:30<5:15:29,  7.87s/it]
{'loss': 1.077, 'grad_norm': 0.374411359110173, 'learning_rate': 1.4402721182022823e-05, 'epoch': 0.37}


 37%|███▋      | 1441/3844 [2:57:43<4:36:56,  6.92s/it]
{'loss': 1.162, 'grad_norm': 0.3641488977343418, 'learning_rate': 1.4387582295637809e-05, 'epoch': 0.37}

 38%|███▊      | 1442/3844 [2:57:51<4:55:37,  7.38s/it]

 38%|███▊      | 1443/3844 [2:57:57<4:38:23,  6.96s/it]


 38%|███▊      | 1445/3844 [2:58:16<5:27:43,  8.20s/it]

 38%|███▊      | 1446/3844 [2:58:22<5:05:21,  7.64s/it]

 38%|███▊      | 1447/3844 [2:58:29<4:49:13,  7.24s/it]

 38%|███▊      | 1448/3844 [2:58:36<4:55:08,  7.39s/it]

 38%|███▊      | 1449/3844 [2:58:42<4:39:55,  7.01s/it]
{'loss': 1.2381, 'grad_norm': 0.3346415748892945, 'learning_rate': 1.4326902547645815e-05, 'epoch': 0.38}


 38%|███▊      | 1451/3844 [2:58:58<4:54:12,  7.38s/it]

 38%|███▊      | 1452/3844 [2:59:04<4:42:10,  7.08s/it]
{'loss': 1.0913, 'grad_norm': 0.3395859729253355, 'learning_rate': 1.4304096793806959e-05, 'epoch': 0.38}


 38%|███▊      | 1454/3844 [2:59:17<4:19:37,  6.52s/it]
{'loss': 1.2178, 'grad_norm': 0.38578062766744936, 'learning_rate': 1.4288877666221918e-05, 'epoch': 0.38}


 38%|███▊      | 1456/3844 [2:59:29<4:08:30,  6.24s/it]
{'loss': 1.2336, 'grad_norm': 0.40705677756362063, 'learning_rate': 1.4273646355714265e-05, 'epoch': 0.38}

 38%|███▊      | 1457/3844 [2:59:35<4:09:35,  6.27s/it]


 38%|███▊      | 1459/3844 [2:59:52<4:48:29,  7.26s/it]
{'loss': 1.0735, 'grad_norm': 0.3637844256239024, 'learning_rate': 1.4250776641626346e-05, 'epoch': 0.38}

 38%|███▊      | 1460/3844 [2:59:58<4:29:29,  6.78s/it]

 38%|███▊      | 1461/3844 [3:00:05<4:34:41,  6.92s/it]

 38%|███▊      | 1462/3844 [3:00:14<4:58:21,  7.52s/it]


 38%|███▊      | 1464/3844 [3:00:32<5:28:20,  8.28s/it]

 38%|███▊      | 1465/3844 [3:00:38<5:03:06,  7.64s/it]

 38%|███▊      | 1466/3844 [3:00:45<4:46:21,  7.23s/it]
{'loss': 1.1759, 'grad_norm': 0.36705115250568243, 'learning_rate': 1.4197308574845488e-05, 'epoch': 0.38}

 38%|███▊      | 1467/3844 [3:00:53<5:06:48,  7.74s/it]

 38%|███▊      | 1468/3844 [3:01:02<5:10:29,  7.84s/it]


 38%|███▊      | 1470/3844 [3:01:16<4:56:21,  7.49s/it]
{'loss': 1.1296, 'grad_norm': 0.3590687401535148, 'learning_rate': 1.4166689698987335e-05, 'epoch': 0.38}


 38%|███▊      | 1472/3844 [3:01:28<4:27:08,  6.76s/it]
{'loss': 1.2056, 'grad_norm': 0.3818445035181248, 'learning_rate': 1.4151362485550033e-05, 'epoch': 0.38}

 38%|███▊      | 1473/3844 [3:01:35<4:33:15,  6.92s/it]


 38%|███▊      | 1475/3844 [3:01:52<5:02:40,  7.67s/it]
{'loss': 1.2897, 'grad_norm': 0.36602819239427253, 'learning_rate': 1.4128349568448294e-05, 'epoch': 0.38}


 38%|███▊      | 1477/3844 [3:02:07<4:51:21,  7.39s/it]
{'loss': 1.2343, 'grad_norm': 0.3605550512746925, 'learning_rate': 1.4112992955970925e-05, 'epoch': 0.38}


 38%|███▊      | 1479/3844 [3:02:22<4:52:48,  7.43s/it]
{'loss': 1.2064, 'grad_norm': 0.3773768820807262, 'learning_rate': 1.4097624660186462e-05, 'epoch': 0.38}


 39%|███▊      | 1481/3844 [3:02:41<5:27:30,  8.32s/it]
{'loss': 1.1096, 'grad_norm': 0.3464811042127675, 'learning_rate': 1.4082244724749856e-05, 'epoch': 0.39}


 39%|███▊      | 1483/3844 [3:02:57<5:23:16,  8.22s/it]

 39%|███▊      | 1484/3844 [3:03:03<4:59:05,  7.60s/it]
{'loss': 1.1215, 'grad_norm': 0.36599364416514973, 'learning_rate': 1.4059153092823729e-05, 'epoch': 0.39}

 39%|███▊      | 1485/3844 [3:03:11<5:12:34,  7.95s/it]


 39%|███▊      | 1487/3844 [3:03:27<5:03:36,  7.73s/it]

 39%|███▊      | 1488/3844 [3:03:32<4:38:31,  7.09s/it]
{'loss': 1.113, 'grad_norm': 0.3943487514287335, 'learning_rate': 1.4028323919503764e-05, 'epoch': 0.39}


 39%|███▉      | 1490/3844 [3:03:47<4:44:52,  7.26s/it]
{'loss': 1.1602, 'grad_norm': 0.3428759388717095, 'learning_rate': 1.401289214674618e-05, 'epoch': 0.39}


 39%|███▉      | 1492/3844 [3:04:02<4:57:01,  7.58s/it]

 39%|███▉      | 1493/3844 [3:04:12<5:25:38,  8.31s/it]
{'loss': 1.2162, 'grad_norm': 0.34708356335858404, 'learning_rate': 1.3989723128263033e-05, 'epoch': 0.39}

 39%|███▉      | 1494/3844 [3:04:20<5:15:32,  8.06s/it]


 39%|███▉      | 1496/3844 [3:04:34<5:04:22,  7.78s/it]
{'loss': 1.1589, 'grad_norm': 0.3367479449695531, 'learning_rate': 1.3966528610202983e-05, 'epoch': 0.39}


 39%|███▉      | 1498/3844 [3:04:51<5:06:57,  7.85s/it]

 39%|███▉      | 1499/3844 [3:04:59<5:12:27,  7.99s/it]

 39%|███▉      | 1500/3844 [3:05:10<5:52:04,  9.01s/it]
 39%|███▉      | 1500/3844 [3:05:10<5:52:04,  9.01s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 39%|███▉      | 1501/3844 [3:05:42<10:22:16, 15.94s/it]
{'loss': 1.36, 'grad_norm': 0.3686010895754513, 'learning_rate': 1.3927814817086413e-05, 'epoch': 0.39}


 39%|███▉      | 1503/3844 [3:05:55<7:04:04, 10.87s/it]

 39%|███▉      | 1504/3844 [3:06:00<6:02:56,  9.31s/it]

 39%|███▉      | 1505/3844 [3:06:06<5:23:49,  8.31s/it]

 39%|███▉      | 1506/3844 [3:06:13<5:00:11,  7.70s/it]
{'loss': 1.1331, 'grad_norm': 0.3720653429397469, 'learning_rate': 1.3889031290976465e-05, 'epoch': 0.39}

 39%|███▉      | 1507/3844 [3:06:22<5:17:48,  8.16s/it]


 39%|███▉      | 1509/3844 [3:06:35<4:42:27,  7.26s/it]

 39%|███▉      | 1510/3844 [3:06:43<4:49:36,  7.45s/it]
{'loss': 1.1875, 'grad_norm': 0.365664496185989, 'learning_rate': 1.3857954724996639e-05, 'epoch': 0.39}

 39%|███▉      | 1511/3844 [3:06:53<5:27:54,  8.43s/it]


 39%|███▉      | 1513/3844 [3:07:07<4:53:47,  7.56s/it]
{'loss': 1.2089, 'grad_norm': 0.373213308021042, 'learning_rate': 1.3834618514223843e-05, 'epoch': 0.39}


 39%|███▉      | 1515/3844 [3:07:19<4:26:02,  6.85s/it]
{'loss': 1.3366, 'grad_norm': 0.362110215425748, 'learning_rate': 1.3819047415465196e-05, 'epoch': 0.39}

 39%|███▉      | 1516/3844 [3:07:26<4:26:14,  6.86s/it]


 39%|███▉      | 1518/3844 [3:07:41<4:34:26,  7.08s/it]
{'loss': 1.1425, 'grad_norm': 0.3674884403597916, 'learning_rate': 1.3795670440539609e-05, 'epoch': 0.39}


 40%|███▉      | 1520/3844 [3:07:59<5:18:03,  8.21s/it]
{'loss': 1.0182, 'grad_norm': 0.37398132438766746, 'learning_rate': 1.3780072303957198e-05, 'epoch': 0.4}

 40%|███▉      | 1521/3844 [3:08:06<5:08:11,  7.96s/it]


 40%|███▉      | 1523/3844 [3:08:21<4:59:27,  7.74s/it]
{'loss': 1.0972, 'grad_norm': 0.4022078342291183, 'learning_rate': 1.3756654979905116e-05, 'epoch': 0.4}


 40%|███▉      | 1525/3844 [3:08:37<5:10:05,  8.02s/it]

 40%|███▉      | 1526/3844 [3:08:43<4:49:57,  7.51s/it]
{'loss': 1.3224, 'grad_norm': 0.37937034751970533, 'learning_rate': 1.3733213645888062e-05, 'epoch': 0.4}

 40%|███▉      | 1527/3844 [3:08:52<5:04:24,  7.88s/it]


 40%|███▉      | 1529/3844 [3:09:05<4:39:53,  7.25s/it]
{'loss': 1.0835, 'grad_norm': 0.37128756292703635, 'learning_rate': 1.370974845172699e-05, 'epoch': 0.4}

 40%|███▉      | 1530/3844 [3:09:12<4:33:55,  7.10s/it]


 40%|███▉      | 1532/3844 [3:09:27<4:36:27,  7.17s/it]
{'loss': 1.1814, 'grad_norm': 0.38203534184454735, 'learning_rate': 1.368625954739534e-05, 'epoch': 0.4}


 40%|███▉      | 1534/3844 [3:09:41<4:38:35,  7.24s/it]

 40%|███▉      | 1535/3844 [3:09:47<4:25:04,  6.89s/it]
{'loss': 1.0179, 'grad_norm': 0.35739629756760866, 'learning_rate': 1.3662747083018095e-05, 'epoch': 0.4}


 40%|███▉      | 1537/3844 [3:10:01<4:22:55,  6.84s/it]
{'loss': 1.2146, 'grad_norm': 0.3835326608879642, 'learning_rate': 1.3647059092056374e-05, 'epoch': 0.4}


 40%|████      | 1539/3844 [3:10:17<4:48:59,  7.52s/it]

 40%|████      | 1540/3844 [3:10:27<5:22:09,  8.39s/it]

 40%|████      | 1541/3844 [3:10:35<5:10:20,  8.09s/it]

 40%|████      | 1542/3844 [3:10:41<4:52:13,  7.62s/it]
{'loss': 1.0514, 'grad_norm': 0.4250563059039314, 'learning_rate': 1.3607793888158887e-05, 'epoch': 0.4}


 40%|████      | 1544/3844 [3:10:53<4:17:10,  6.71s/it]

 40%|████      | 1545/3844 [3:10:59<4:10:25,  6.54s/it]

 40%|████      | 1546/3844 [3:11:07<4:21:57,  6.84s/it]

 40%|████      | 1547/3844 [3:11:13<4:12:05,  6.58s/it]

 40%|████      | 1548/3844 [3:11:25<5:12:00,  8.15s/it]

 40%|████      | 1549/3844 [3:11:31<4:53:50,  7.68s/it]

 40%|████      | 1550/3844 [3:11:39<4:56:21,  7.75s/it]
{'loss': 1.2041, 'grad_norm': 0.3640180879142419, 'learning_rate': 1.3544836625309229e-05, 'epoch': 0.4}


 40%|████      | 1552/3844 [3:11:57<5:23:24,  8.47s/it]
{'loss': 1.1447, 'grad_norm': 0.3798756933614604, 'learning_rate': 1.352907202422601e-05, 'epoch': 0.4}

 40%|████      | 1553/3844 [3:12:08<5:48:46,  9.13s/it]


 40%|████      | 1555/3844 [3:12:21<5:05:15,  8.00s/it]

 40%|████      | 1556/3844 [3:12:27<4:42:16,  7.40s/it]
{'loss': 1.1806, 'grad_norm': 0.4295575568174965, 'learning_rate': 1.3497512792979013e-05, 'epoch': 0.4}

 41%|████      | 1557/3844 [3:12:36<4:58:58,  7.84s/it]

 41%|████      | 1558/3844 [3:12:42<4:33:16,  7.17s/it]

 41%|████      | 1559/3844 [3:12:50<4:41:52,  7.40s/it]

 41%|████      | 1560/3844 [3:12:56<4:31:50,  7.14s/it]

 41%|████      | 1561/3844 [3:13:02<4:19:58,  6.83s/it]

 41%|████      | 1562/3844 [3:13:08<4:07:43,  6.51s/it]

 41%|████      | 1563/3844 [3:13:16<4:18:36,  6.80s/it]

 41%|████      | 1564/3844 [3:13:22<4:18:34,  6.80s/it]

 41%|████      | 1565/3844 [3:13:32<4:50:18,  7.64s/it]

 41%|████      | 1566/3844 [3:13:40<4:56:03,  7.80s/it]

 41%|████      | 1567/3844 [3:13:46<4:34:34,  7.24s/it]


 41%|████      | 1569/3844 [3:14:00<4:26:20,  7.02s/it]

 41%|████      | 1570/3844 [3:14:05<4:09:35,  6.59s/it]

 41%|████      | 1571/3844 [3:14:11<4:03:30,  6.43s/it]

 41%|████      | 1572/3844 [3:14:19<4:19:12,  6.85s/it]
{'loss': 1.3035, 'grad_norm': 0.3647012868758942, 'learning_rate': 1.3370882061557635e-05, 'epoch': 0.41}

 41%|████      | 1573/3844 [3:14:28<4:42:23,  7.46s/it]


 41%|████      | 1575/3844 [3:14:43<4:47:09,  7.59s/it]

 41%|████      | 1576/3844 [3:14:55<5:40:26,  9.01s/it]

 41%|████      | 1577/3844 [3:15:03<5:26:15,  8.63s/it]
{'loss': 1.1101, 'grad_norm': 0.35520810396367725, 'learning_rate': 1.3331183199365388e-05, 'epoch': 0.41}


 41%|████      | 1579/3844 [3:15:19<5:22:06,  8.53s/it]
{'loss': 1.1754, 'grad_norm': 0.36965320072591434, 'learning_rate': 1.3315287055612687e-05, 'epoch': 0.41}

 41%|████      | 1580/3844 [3:15:25<4:52:17,  7.75s/it]

 41%|████      | 1581/3844 [3:15:32<4:44:12,  7.54s/it]


 41%|████      | 1583/3844 [3:15:43<4:06:33,  6.54s/it]
{'loss': 1.1865, 'grad_norm': 0.3646973820721462, 'learning_rate': 1.3283466561221928e-05, 'epoch': 0.41}

 41%|████      | 1584/3844 [3:15:50<4:08:12,  6.59s/it]


 41%|████▏     | 1586/3844 [3:16:05<4:33:33,  7.27s/it]

 41%|████▏     | 1587/3844 [3:16:13<4:37:52,  7.39s/it]
{'loss': 1.2227, 'grad_norm': 0.3358027243070158, 'learning_rate': 1.3251608758991018e-05, 'epoch': 0.41}

 41%|████▏     | 1588/3844 [3:16:20<4:38:10,  7.40s/it]

 41%|████▏     | 1589/3844 [3:16:28<4:43:41,  7.55s/it]

 41%|████▏     | 1590/3844 [3:16:36<4:45:59,  7.61s/it]

 41%|████▏     | 1591/3844 [3:16:45<4:57:29,  7.92s/it]


 41%|████▏     | 1593/3844 [3:17:01<4:57:08,  7.92s/it]

 41%|████▏     | 1594/3844 [3:17:08<4:41:54,  7.52s/it]

 41%|████▏     | 1595/3844 [3:17:16<4:43:31,  7.56s/it]
{'loss': 1.1442, 'grad_norm': 0.3731866509928999, 'learning_rate': 1.3187782679344376e-05, 'epoch': 0.41}


 42%|████▏     | 1597/3844 [3:17:28<4:16:14,  6.84s/it]

 42%|████▏     | 1598/3844 [3:17:34<4:06:44,  6.59s/it]

 42%|████▏     | 1599/3844 [3:17:40<3:55:41,  6.30s/it]

 42%|████▏     | 1600/3844 [3:17:49<4:31:56,  7.27s/it]
{'loss': 1.0826, 'grad_norm': 0.36427183905159477, 'learning_rate': 1.3147817622171471e-05, 'epoch': 0.42}


 42%|████▏     | 1602/3844 [3:18:02<4:12:05,  6.75s/it]

 42%|████▏     | 1603/3844 [3:18:08<4:01:50,  6.47s/it]
{'loss': 1.2445, 'grad_norm': 0.38477514864049456, 'learning_rate': 1.3123811717517181e-05, 'epoch': 0.42}


 42%|████▏     | 1605/3844 [3:18:26<4:46:50,  7.69s/it]
{'loss': 1.0993, 'grad_norm': 0.3858891080885567, 'learning_rate': 1.3107796679791231e-05, 'epoch': 0.42}

 42%|████▏     | 1606/3844 [3:18:32<4:29:03,  7.21s/it]

 42%|████▏     | 1607/3844 [3:18:42<4:59:47,  8.04s/it]


 42%|████▏     | 1609/3844 [3:18:54<4:16:14,  6.88s/it]
{'loss': 1.1756, 'grad_norm': 0.3997058267686165, 'learning_rate': 1.3075740165972323e-05, 'epoch': 0.42}


 42%|████▏     | 1611/3844 [3:19:08<4:14:35,  6.84s/it]

 42%|████▏     | 1612/3844 [3:19:16<4:24:47,  7.12s/it]

 42%|████▏     | 1613/3844 [3:19:22<4:11:51,  6.77s/it]

 42%|████▏     | 1614/3844 [3:19:30<4:24:45,  7.12s/it]

 42%|████▏     | 1615/3844 [3:19:37<4:33:50,  7.37s/it]
{'loss': 1.1077, 'grad_norm': 0.3796465196084994, 'learning_rate': 1.3027589982457229e-05, 'epoch': 0.42}


 42%|████▏     | 1617/3844 [3:19:55<5:02:34,  8.15s/it]

 42%|████▏     | 1618/3844 [3:20:06<5:29:13,  8.87s/it]
{'loss': 1.175, 'grad_norm': 0.36777596173965893, 'learning_rate': 1.3003485788311296e-05, 'epoch': 0.42}


 42%|████▏     | 1620/3844 [3:20:20<4:47:39,  7.76s/it]

 42%|████▏     | 1621/3844 [3:20:28<4:47:07,  7.75s/it]
{'loss': 1.283, 'grad_norm': 0.3657983246957496, 'learning_rate': 1.2979362397941873e-05, 'epoch': 0.42}


 42%|████▏     | 1623/3844 [3:20:43<4:50:06,  7.84s/it]

 42%|████▏     | 1624/3844 [3:20:53<5:14:47,  8.51s/it]

 42%|████▏     | 1625/3844 [3:20:59<4:46:59,  7.76s/it]

 42%|████▏     | 1626/3844 [3:21:06<4:28:42,  7.27s/it]
{'loss': 1.2167, 'grad_norm': 0.40236886684114254, 'learning_rate': 1.2939114507867871e-05, 'epoch': 0.42}

 42%|████▏     | 1627/3844 [3:21:13<4:31:32,  7.35s/it]

 42%|████▏     | 1628/3844 [3:21:20<4:30:06,  7.31s/it]


 42%|████▏     | 1630/3844 [3:21:34<4:16:46,  6.96s/it]

 42%|████▏     | 1631/3844 [3:21:41<4:24:46,  7.18s/it]

 42%|████▏     | 1632/3844 [3:21:50<4:35:41,  7.48s/it]
{'loss': 1.1048, 'grad_norm': 0.37410427318993866, 'learning_rate': 1.2890748225166392e-05, 'epoch': 0.42}


 43%|████▎     | 1634/3844 [3:22:06<4:50:57,  7.90s/it]

 43%|████▎     | 1635/3844 [3:22:16<5:14:44,  8.55s/it]

 43%|████▎     | 1636/3844 [3:22:26<5:26:43,  8.88s/it]

 43%|████▎     | 1637/3844 [3:22:34<5:21:22,  8.74s/it]
{'loss': 1.0382, 'grad_norm': 0.3442673956645037, 'learning_rate': 1.2850386483666867e-05, 'epoch': 0.43}

 43%|████▎     | 1638/3844 [3:22:41<4:57:21,  8.09s/it]

 43%|████▎     | 1639/3844 [3:22:47<4:37:59,  7.56s/it]


 43%|████▎     | 1641/3844 [3:22:58<4:01:50,  6.59s/it]

 43%|████▎     | 1642/3844 [3:23:06<4:14:10,  6.93s/it]
{'loss': 1.0191, 'grad_norm': 0.376511589753907, 'learning_rate': 1.2809974137444087e-05, 'epoch': 0.43}


 43%|████▎     | 1644/3844 [3:23:24<5:02:34,  8.25s/it]

 43%|████▎     | 1645/3844 [3:23:34<5:17:09,  8.65s/it]
{'loss': 1.2025, 'grad_norm': 0.35460040459741327, 'learning_rate': 1.2785702737872156e-05, 'epoch': 0.43}

 43%|████▎     | 1646/3844 [3:23:41<4:58:38,  8.15s/it]

 43%|████▎     | 1647/3844 [3:23:49<4:58:44,  8.16s/it]

 43%|████▎     | 1648/3844 [3:23:55<4:37:15,  7.58s/it]

 43%|████▎     | 1649/3844 [3:24:01<4:13:06,  6.92s/it]

 43%|████▎     | 1650/3844 [3:24:10<4:44:27,  7.78s/it]

 43%|████▎     | 1651/3844 [3:24:17<4:26:04,  7.28s/it]

 43%|████▎     | 1652/3844 [3:24:27<5:01:44,  8.26s/it]

 43%|████▎     | 1653/3844 [3:24:33<4:37:45,  7.61s/it]


 43%|████▎     | 1655/3844 [3:24:48<4:33:08,  7.49s/it]

 43%|████▎     | 1656/3844 [3:24:54<4:24:16,  7.25s/it]
{'loss': 1.1637, 'grad_norm': 0.39104764258974406, 'learning_rate': 1.2696556461348517e-05, 'epoch': 0.43}

 43%|████▎     | 1657/3844 [3:25:01<4:16:55,  7.05s/it]

 43%|████▎     | 1658/3844 [3:25:09<4:29:39,  7.40s/it]

 43%|████▎     | 1659/3844 [3:25:15<4:16:46,  7.05s/it]


 43%|████▎     | 1661/3844 [3:25:28<4:00:10,  6.60s/it]
{'loss': 1.1333, 'grad_norm': 0.37102203597818395, 'learning_rate': 1.265595836795802e-05, 'epoch': 0.43}


 43%|████▎     | 1663/3844 [3:25:44<4:30:33,  7.44s/it]

 43%|████▎     | 1664/3844 [3:25:54<4:54:12,  8.10s/it]
{'loss': 1.0692, 'grad_norm': 0.35692503890633326, 'learning_rate': 1.263157683235955e-05, 'epoch': 0.43}


 43%|████▎     | 1666/3844 [3:26:06<4:19:03,  7.14s/it]

 43%|████▎     | 1667/3844 [3:26:14<4:26:22,  7.34s/it]
{'loss': 1.0912, 'grad_norm': 0.378763700126184, 'learning_rate': 1.2607178477524853e-05, 'epoch': 0.43}

 43%|████▎     | 1668/3844 [3:26:19<4:05:03,  6.76s/it]


 43%|████▎     | 1670/3844 [3:26:34<4:23:19,  7.27s/it]
{'loss': 1.1563, 'grad_norm': 0.38059982125094516, 'learning_rate': 1.2582763459391485e-05, 'epoch': 0.43}


 43%|████▎     | 1672/3844 [3:26:54<5:07:26,  8.49s/it]

 44%|████▎     | 1673/3844 [3:27:00<4:45:52,  7.90s/it]

 44%|████▎     | 1674/3844 [3:27:06<4:22:40,  7.26s/it]

 44%|████▎     | 1675/3844 [3:27:12<4:06:47,  6.83s/it]
{'loss': 1.1726, 'grad_norm': 0.3707848912489325, 'learning_rate': 1.254203515682114e-05, 'epoch': 0.44}

 44%|████▎     | 1676/3844 [3:27:17<3:51:30,  6.41s/it]


 44%|████▎     | 1678/3844 [3:27:34<4:17:05,  7.12s/it]
{'loss': 1.3414, 'grad_norm': 0.38175976894355296, 'learning_rate': 1.251757646639288e-05, 'epoch': 0.44}


 44%|████▎     | 1680/3844 [3:27:50<4:36:30,  7.67s/it]
{'loss': 1.2573, 'grad_norm': 0.3866994512229705, 'learning_rate': 1.2501261723884665e-05, 'epoch': 0.44}


 44%|████▍     | 1682/3844 [3:28:04<4:24:31,  7.34s/it]

 44%|████▍     | 1683/3844 [3:28:14<4:50:13,  8.06s/it]
{'loss': 1.0646, 'grad_norm': 0.3456757539845972, 'learning_rate': 1.2476776302646793e-05, 'epoch': 0.44}

 44%|████▍     | 1684/3844 [3:28:19<4:20:52,  7.25s/it]

 44%|████▍     | 1685/3844 [3:28:25<4:06:27,  6.85s/it]


 44%|████▍     | 1687/3844 [3:28:42<4:41:43,  7.84s/it]

 44%|████▍     | 1688/3844 [3:28:48<4:28:42,  7.48s/it]
{'loss': 1.2905, 'grad_norm': 0.4012515462290932, 'learning_rate': 1.2435932167116502e-05, 'epoch': 0.44}

 44%|████▍     | 1689/3844 [3:29:00<5:09:01,  8.60s/it]


 44%|████▍     | 1691/3844 [3:29:13<4:34:24,  7.65s/it]

 44%|████▍     | 1692/3844 [3:29:18<4:13:10,  7.06s/it]
{'loss': 1.0986, 'grad_norm': 0.3676051536119989, 'learning_rate': 1.2403225686270383e-05, 'epoch': 0.44}

 44%|████▍     | 1693/3844 [3:29:25<4:07:12,  6.90s/it]


 44%|████▍     | 1695/3844 [3:29:38<4:00:50,  6.72s/it]

 44%|████▍     | 1696/3844 [3:29:44<3:53:48,  6.53s/it]

 44%|████▍     | 1697/3844 [3:29:54<4:30:06,  7.55s/it]

 44%|████▍     | 1698/3844 [3:30:02<4:36:09,  7.72s/it]
{'loss': 1.1848, 'grad_norm': 0.390110516533742, 'learning_rate': 1.2354114881998714e-05, 'epoch': 0.44}

 44%|████▍     | 1699/3844 [3:30:09<4:28:33,  7.51s/it]

 44%|████▍     | 1700/3844 [3:30:17<4:32:12,  7.62s/it]

 44%|████▍     | 1701/3844 [3:30:23<4:19:11,  7.26s/it]


 44%|████▍     | 1703/3844 [3:30:36<4:06:11,  6.90s/it]

 44%|████▍     | 1704/3844 [3:30:43<4:00:28,  6.74s/it]

 44%|████▍     | 1705/3844 [3:30:48<3:49:57,  6.45s/it]

 44%|████▍     | 1706/3844 [3:30:54<3:39:32,  6.16s/it]
{'loss': 1.3418, 'grad_norm': 0.38907332688396357, 'learning_rate': 1.2288540408144998e-05, 'epoch': 0.44}


 44%|████▍     | 1708/3844 [3:31:09<4:00:14,  6.75s/it]
{'loss': 1.2339, 'grad_norm': 0.38608479460279715, 'learning_rate': 1.2272130421230819e-05, 'epoch': 0.44}

 44%|████▍     | 1709/3844 [3:31:19<4:43:44,  7.97s/it]

 44%|████▍     | 1710/3844 [3:31:30<5:10:43,  8.74s/it]
{'loss': 1.2516, 'grad_norm': 0.3704532949576308, 'learning_rate': 1.2255713980136498e-05, 'epoch': 0.44}


 45%|████▍     | 1712/3844 [3:31:44<4:37:37,  7.81s/it]

 45%|████▍     | 1713/3844 [3:31:50<4:18:08,  7.27s/it]
{'loss': 1.1281, 'grad_norm': 0.3910102670600781, 'learning_rate': 1.2231077318920379e-05, 'epoch': 0.45}

 45%|████▍     | 1714/3844 [3:31:56<3:58:23,  6.72s/it]


 45%|████▍     | 1716/3844 [3:32:09<3:59:49,  6.76s/it]
{'loss': 1.1208, 'grad_norm': 0.3708614824559299, 'learning_rate': 1.2206426398186534e-05, 'epoch': 0.45}


 45%|████▍     | 1718/3844 [3:32:22<3:56:58,  6.69s/it]
{'loss': 1.2276, 'grad_norm': 0.39973462932775833, 'learning_rate': 1.2189984606875986e-05, 'epoch': 0.45}

 45%|████▍     | 1719/3844 [3:32:29<4:00:09,  6.78s/it]

 45%|████▍     | 1720/3844 [3:32:38<4:16:57,  7.26s/it]

 45%|████▍     | 1721/3844 [3:32:44<4:04:05,  6.90s/it]

 45%|████▍     | 1722/3844 [3:32:51<4:12:29,  7.14s/it]


 45%|████▍     | 1724/3844 [3:33:06<4:17:20,  7.28s/it]
{'loss': 1.2575, 'grad_norm': 0.41116279303232955, 'learning_rate': 1.2140622094822054e-05, 'epoch': 0.45}


 45%|████▍     | 1726/3844 [3:33:20<4:08:47,  7.05s/it]

 45%|████▍     | 1727/3844 [3:33:27<4:04:26,  6.93s/it]

 45%|████▍     | 1728/3844 [3:33:33<3:54:02,  6.64s/it]
{'loss': 1.1558, 'grad_norm': 0.41716010497259365, 'learning_rate': 1.2107683272455765e-05, 'epoch': 0.45}


 45%|████▌     | 1730/3844 [3:33:49<4:18:38,  7.34s/it]
{'loss': 1.082, 'grad_norm': 0.41463672824395165, 'learning_rate': 1.2091204857295956e-05, 'epoch': 0.45}

 45%|████▌     | 1731/3844 [3:33:56<4:13:16,  7.19s/it]

 45%|████▌     | 1732/3844 [3:34:03<4:16:04,  7.27s/it]


 45%|████▌     | 1734/3844 [3:34:17<4:04:56,  6.97s/it]

 45%|████▌     | 1735/3844 [3:34:23<3:58:26,  6.78s/it]

 45%|████▌     | 1736/3844 [3:34:29<3:49:50,  6.54s/it]
{'loss': 1.1616, 'grad_norm': 0.3716430188378086, 'learning_rate': 1.2041734157660179e-05, 'epoch': 0.45}

 45%|████▌     | 1737/3844 [3:34:36<3:55:11,  6.70s/it]

 45%|████▌     | 1738/3844 [3:34:46<4:31:11,  7.73s/it]


 45%|████▌     | 1740/3844 [3:35:02<4:32:18,  7.77s/it]
{'loss': 1.2072, 'grad_norm': 0.39539955529584914, 'learning_rate': 1.200872461456667e-05, 'epoch': 0.45}

 45%|████▌     | 1741/3844 [3:35:08<4:11:17,  7.17s/it]

 45%|████▌     | 1742/3844 [3:35:14<3:55:36,  6.73s/it]


 45%|████▌     | 1744/3844 [3:35:27<3:54:20,  6.70s/it]

 45%|████▌     | 1745/3844 [3:35:33<3:43:32,  6.39s/it]

 45%|████▌     | 1746/3844 [3:35:41<3:59:41,  6.85s/it]

 45%|████▌     | 1747/3844 [3:35:51<4:35:11,  7.87s/it]

 45%|████▌     | 1748/3844 [3:35:56<4:11:39,  7.20s/it]
{'loss': 1.0945, 'grad_norm': 0.34613160425986667, 'learning_rate': 1.194263743231106e-05, 'epoch': 0.45}

 45%|████▌     | 1749/3844 [3:36:04<4:13:25,  7.26s/it]

 46%|████▌     | 1750/3844 [3:36:09<3:55:00,  6.73s/it]

 46%|████▌     | 1751/3844 [3:36:16<3:50:10,  6.60s/it]

 46%|████▌     | 1752/3844 [3:36:24<4:09:57,  7.17s/it]


 46%|████▌     | 1754/3844 [3:36:38<4:06:02,  7.06s/it]
{'loss': 1.1367, 'grad_norm': 0.42196363069048604, 'learning_rate': 1.189301394002718e-05, 'epoch': 0.46}

 46%|████▌     | 1755/3844 [3:36:44<3:52:37,  6.68s/it]

 46%|████▌     | 1756/3844 [3:36:50<3:42:21,  6.39s/it]

 46%|████▌     | 1757/3844 [3:36:56<3:36:21,  6.22s/it]


 46%|████▌     | 1759/3844 [3:37:09<3:46:18,  6.51s/it]
{'loss': 1.2558, 'grad_norm': 0.4042997481183428, 'learning_rate': 1.1851624007270703e-05, 'epoch': 0.46}


 46%|████▌     | 1761/3844 [3:37:25<4:10:41,  7.22s/it]
{'loss': 1.1993, 'grad_norm': 0.3779785164742261, 'learning_rate': 1.1835058788542078e-05, 'epoch': 0.46}

 46%|████▌     | 1762/3844 [3:37:31<4:06:44,  7.11s/it]


 46%|████▌     | 1764/3844 [3:37:45<4:01:38,  6.97s/it]
{'loss': 1.0641, 'grad_norm': 0.3538428450204004, 'learning_rate': 1.1810201201456134e-05, 'epoch': 0.46}


 46%|████▌     | 1766/3844 [3:37:59<4:03:43,  7.04s/it]
{'loss': 1.146, 'grad_norm': 0.4197840172962207, 'learning_rate': 1.1793623039385544e-05, 'epoch': 0.46}


 46%|████▌     | 1768/3844 [3:38:15<4:11:56,  7.28s/it]

 46%|████▌     | 1769/3844 [3:38:23<4:14:49,  7.37s/it]

 46%|████▌     | 1770/3844 [3:38:29<4:00:24,  6.95s/it]

 46%|████▌     | 1771/3844 [3:38:35<3:53:04,  6.75s/it]

 46%|████▌     | 1772/3844 [3:38:40<3:40:22,  6.38s/it]
{'loss': 1.2674, 'grad_norm': 0.37732310618715437, 'learning_rate': 1.17438581719779e-05, 'epoch': 0.46}


 46%|████▌     | 1774/3844 [3:38:59<4:40:02,  8.12s/it]
{'loss': 1.054, 'grad_norm': 0.3609695651921333, 'learning_rate': 1.1727259912844908e-05, 'epoch': 0.46}

 46%|████▌     | 1775/3844 [3:39:06<4:24:17,  7.66s/it]


 46%|████▌     | 1777/3844 [3:39:19<4:03:53,  7.08s/it]
{'loss': 1.1739, 'grad_norm': 0.3695586719945952, 'learning_rate': 1.1702353339328815e-05, 'epoch': 0.46}

 46%|████▋     | 1778/3844 [3:39:26<4:01:12,  7.01s/it]

 46%|████▋     | 1779/3844 [3:39:32<3:51:51,  6.74s/it]

 46%|████▋     | 1780/3844 [3:39:42<4:25:27,  7.72s/it]


 46%|████▋     | 1782/3844 [3:39:59<4:34:39,  7.99s/it]
{'loss': 1.1798, 'grad_norm': 0.4136046227205628, 'learning_rate': 1.1660818283718706e-05, 'epoch': 0.46}

 46%|████▋     | 1783/3844 [3:40:06<4:25:19,  7.72s/it]


 46%|████▋     | 1785/3844 [3:40:19<4:02:21,  7.06s/it]
{'loss': 1.1604, 'grad_norm': 0.3979732030211781, 'learning_rate': 1.1635883050067088e-05, 'epoch': 0.46}

 46%|████▋     | 1786/3844 [3:40:28<4:20:01,  7.58s/it]


 47%|████▋     | 1788/3844 [3:40:43<4:22:12,  7.65s/it]
{'loss': 1.1041, 'grad_norm': 0.4094559473412068, 'learning_rate': 1.1610937360971748e-05, 'epoch': 0.47}


 47%|████▋     | 1790/3844 [3:40:57<4:15:23,  7.46s/it]
{'loss': 1.1136, 'grad_norm': 0.36401502296835675, 'learning_rate': 1.1594301171725994e-05, 'epoch': 0.47}


 47%|████▋     | 1792/3844 [3:41:11<4:00:30,  7.03s/it]
{'loss': 1.1743, 'grad_norm': 0.37191318836624443, 'learning_rate': 1.1577660453731858e-05, 'epoch': 0.47}

 47%|████▋     | 1793/3844 [3:41:17<3:49:28,  6.71s/it]

 47%|████▋     | 1794/3844 [3:41:26<4:21:07,  7.64s/it]

 47%|████▋     | 1795/3844 [3:41:34<4:24:13,  7.74s/it]

 47%|████▋     | 1796/3844 [3:41:40<4:05:51,  7.20s/it]


 47%|████▋     | 1798/3844 [3:41:56<4:11:10,  7.37s/it]

 47%|████▋     | 1799/3844 [3:42:01<3:54:32,  6.88s/it]

 47%|████▋     | 1800/3844 [3:42:07<3:43:53,  6.57s/it]
 47%|████▋     | 1800/3844 [3:42:07<3:43:53,  6.57s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 47%|████▋     | 1801/3844 [3:42:37<7:46:40, 13.71s/it]

 47%|████▋     | 1802/3844 [3:42:45<6:40:44, 11.77s/it]
{'loss': 1.1538, 'grad_norm': 0.38070809088070945, 'learning_rate': 1.1494390587406423e-05, 'epoch': 0.47}


 47%|████▋     | 1804/3844 [3:42:59<5:21:44,  9.46s/it]

 47%|████▋     | 1805/3844 [3:43:07<5:09:26,  9.11s/it]

 47%|████▋     | 1806/3844 [3:43:15<4:59:48,  8.83s/it]

 47%|████▋     | 1807/3844 [3:43:25<5:08:44,  9.09s/it]

 47%|████▋     | 1808/3844 [3:43:31<4:40:11,  8.26s/it]

 47%|████▋     | 1809/3844 [3:43:43<5:16:53,  9.34s/it]
{'loss': 1.0847, 'grad_norm': 0.37465131654345735, 'learning_rate': 1.1436038185646477e-05, 'epoch': 0.47}

 47%|████▋     | 1810/3844 [3:43:50<4:53:15,  8.65s/it]


 47%|████▋     | 1812/3844 [3:44:03<4:19:24,  7.66s/it]

 47%|████▋     | 1813/3844 [3:44:11<4:20:55,  7.71s/it]
{'loss': 1.0598, 'grad_norm': 0.39942797655892454, 'learning_rate': 1.1402671390282046e-05, 'epoch': 0.47}

 47%|████▋     | 1814/3844 [3:44:16<3:56:18,  6.98s/it]


 47%|████▋     | 1816/3844 [3:44:27<3:30:56,  6.24s/it]
{'loss': 1.325, 'grad_norm': 0.3979005372055047, 'learning_rate': 1.1377635813955835e-05, 'epoch': 0.47}


 47%|████▋     | 1818/3844 [3:44:42<3:45:41,  6.68s/it]
{'loss': 1.2315, 'grad_norm': 0.4113978344026495, 'learning_rate': 1.136094052824535e-05, 'epoch': 0.47}


 47%|████▋     | 1820/3844 [3:45:03<4:48:14,  8.54s/it]

 47%|████▋     | 1821/3844 [3:45:09<4:25:55,  7.89s/it]
{'loss': 1.1363, 'grad_norm': 0.38232516522260357, 'learning_rate': 1.1335890366001036e-05, 'epoch': 0.47}


 47%|████▋     | 1823/3844 [3:45:25<4:27:31,  7.94s/it]

 47%|████▋     | 1824/3844 [3:45:35<4:42:54,  8.40s/it]

 47%|████▋     | 1825/3844 [3:45:43<4:39:53,  8.32s/it]
{'loss': 1.2029, 'grad_norm': 0.40963552104670997, 'learning_rate': 1.1302476895872689e-05, 'epoch': 0.47}

 48%|████▊     | 1826/3844 [3:45:48<4:09:10,  7.41s/it]


 48%|████▊     | 1828/3844 [3:46:05<4:24:05,  7.86s/it]
{'loss': 1.1371, 'grad_norm': 0.36522543363691934, 'learning_rate': 1.1277407060548374e-05, 'epoch': 0.48}


 48%|████▊     | 1830/3844 [3:46:19<4:12:08,  7.51s/it]
{'loss': 1.1345, 'grad_norm': 0.40023776859070653, 'learning_rate': 1.1260689291377767e-05, 'epoch': 0.48}

 48%|████▊     | 1831/3844 [3:46:26<4:02:45,  7.24s/it]

 48%|████▊     | 1832/3844 [3:46:32<3:55:08,  7.01s/it]

 48%|████▊     | 1833/3844 [3:46:42<4:20:47,  7.78s/it]

 48%|████▊     | 1834/3844 [3:46:51<4:29:32,  8.05s/it]

 48%|████▊     | 1835/3844 [3:46:57<4:08:55,  7.43s/it]

 48%|████▊     | 1836/3844 [3:47:06<4:28:30,  8.02s/it]


 48%|████▊     | 1838/3844 [3:47:19<4:03:46,  7.29s/it]
{'loss': 1.1358, 'grad_norm': 0.37232629587603755, 'learning_rate': 1.1193782878775622e-05, 'epoch': 0.48}


 48%|████▊     | 1840/3844 [3:47:36<4:26:17,  7.97s/it]

 48%|████▊     | 1841/3844 [3:47:45<4:40:43,  8.41s/it]
{'loss': 1.1386, 'grad_norm': 0.38746766203737915, 'learning_rate': 1.1168678822644512e-05, 'epoch': 0.48}


 48%|████▊     | 1843/3844 [3:48:02<4:35:50,  8.27s/it]

 48%|████▊     | 1844/3844 [3:48:09<4:26:43,  8.00s/it]

 48%|████▊     | 1845/3844 [3:48:16<4:10:30,  7.52s/it]
{'loss': 1.2929, 'grad_norm': 0.3803137959676886, 'learning_rate': 1.1135195156483741e-05, 'epoch': 0.48}

 48%|████▊     | 1846/3844 [3:48:22<4:00:35,  7.22s/it]

 48%|████▊     | 1847/3844 [3:48:29<3:53:56,  7.03s/it]

 48%|████▊     | 1848/3844 [3:48:34<3:39:12,  6.59s/it]

 48%|████▊     | 1849/3844 [3:48:42<3:50:38,  6.94s/it]


 48%|████▊     | 1851/3844 [3:49:00<4:15:08,  7.68s/it]

 48%|████▊     | 1852/3844 [3:49:06<3:58:06,  7.17s/it]

 48%|████▊     | 1853/3844 [3:49:12<3:47:55,  6.87s/it]

 48%|████▊     | 1854/3844 [3:49:19<3:53:35,  7.04s/it]

 48%|████▊     | 1855/3844 [3:49:26<3:52:01,  7.00s/it]

 48%|████▊     | 1856/3844 [3:49:32<3:37:07,  6.55s/it]
{'loss': 1.2037, 'grad_norm': 0.3964853074675836, 'learning_rate': 1.1043049711681155e-05, 'epoch': 0.48}

 48%|████▊     | 1857/3844 [3:49:39<3:42:33,  6.72s/it]


 48%|████▊     | 1859/3844 [3:49:57<4:38:01,  8.40s/it]

 48%|████▊     | 1860/3844 [3:50:04<4:14:52,  7.71s/it]
{'loss': 1.2668, 'grad_norm': 0.38203216141057394, 'learning_rate': 1.1009519639165162e-05, 'epoch': 0.48}


 48%|████▊     | 1862/3844 [3:50:18<4:02:04,  7.33s/it]

 48%|████▊     | 1863/3844 [3:50:26<4:08:57,  7.54s/it]

 48%|████▊     | 1864/3844 [3:50:31<3:50:45,  6.99s/it]

 49%|████▊     | 1865/3844 [3:50:38<3:47:57,  6.91s/it]

 49%|████▊     | 1866/3844 [3:50:44<3:38:33,  6.63s/it]
{'loss': 1.3018, 'grad_norm': 0.3674289787420261, 'learning_rate': 1.0959203142298982e-05, 'epoch': 0.49}

 49%|████▊     | 1867/3844 [3:50:50<3:28:05,  6.32s/it]


 49%|████▊     | 1869/3844 [3:51:02<3:23:59,  6.20s/it]
{'loss': 1.2285, 'grad_norm': 0.3764268126232334, 'learning_rate': 1.0934035617604688e-05, 'epoch': 0.49}


 49%|████▊     | 1871/3844 [3:51:16<3:36:58,  6.60s/it]
{'loss': 1.0022, 'grad_norm': 0.38310053177105025, 'learning_rate': 1.0917253941368915e-05, 'epoch': 0.49}

 49%|████▊     | 1872/3844 [3:51:22<3:36:33,  6.59s/it]


 49%|████▉     | 1874/3844 [3:51:38<3:51:36,  7.05s/it]
{'loss': 1.2243, 'grad_norm': 0.4083439612911343, 'learning_rate': 1.0892076556530324e-05, 'epoch': 0.49}

 49%|████▉     | 1875/3844 [3:51:45<3:47:35,  6.94s/it]

 49%|████▉     | 1876/3844 [3:51:52<3:55:21,  7.18s/it]


 49%|████▉     | 1878/3844 [3:52:08<4:07:04,  7.54s/it]
{'loss': 1.15, 'grad_norm': 0.38858477164676714, 'learning_rate': 1.085849786883125e-05, 'epoch': 0.49}

 49%|████▉     | 1879/3844 [3:52:13<3:48:30,  6.98s/it]


 49%|████▉     | 1881/3844 [3:52:34<4:45:51,  8.74s/it]
{'loss': 1.1783, 'grad_norm': 0.3724355729202189, 'learning_rate': 1.0833307430771747e-05, 'epoch': 0.49}

 49%|████▉     | 1882/3844 [3:52:43<4:51:17,  8.91s/it]

 49%|████▉     | 1883/3844 [3:52:51<4:43:06,  8.66s/it]

 49%|████▉     | 1884/3844 [3:52:59<4:31:29,  8.31s/it]

 49%|████▉     | 1885/3844 [3:53:09<4:49:26,  8.87s/it]

 49%|████▉     | 1886/3844 [3:53:15<4:25:09,  8.13s/it]

 49%|████▉     | 1887/3844 [3:53:21<3:58:14,  7.30s/it]

 49%|████▉     | 1888/3844 [3:53:27<3:50:45,  7.08s/it]

 49%|████▉     | 1889/3844 [3:53:39<4:33:25,  8.39s/it]

 49%|████▉     | 1890/3844 [3:53:45<4:15:55,  7.86s/it]

 49%|████▉     | 1891/3844 [3:53:58<4:58:32,  9.17s/it]

 49%|████▉     | 1892/3844 [3:54:03<4:23:02,  8.09s/it]

 49%|████▉     | 1893/3844 [3:54:09<4:01:25,  7.42s/it]


 49%|████▉     | 1895/3844 [3:54:22<3:48:17,  7.03s/it]
{'loss': 1.0632, 'grad_norm': 0.36737295629397454, 'learning_rate': 1.0715684235269955e-05, 'epoch': 0.49}


 49%|████▉     | 1897/3844 [3:54:34<3:30:15,  6.48s/it]
{'loss': 1.2863, 'grad_norm': 0.3762598363886968, 'learning_rate': 1.0698872407841487e-05, 'epoch': 0.49}

 49%|████▉     | 1898/3844 [3:54:41<3:38:16,  6.73s/it]

 49%|████▉     | 1899/3844 [3:54:48<3:33:53,  6.60s/it]

 49%|████▉     | 1900/3844 [3:54:54<3:26:22,  6.37s/it]

 49%|████▉     | 1901/3844 [3:54:59<3:20:55,  6.20s/it]

 49%|████▉     | 1902/3844 [3:55:05<3:18:33,  6.13s/it]

 50%|████▉     | 1903/3844 [3:55:11<3:14:41,  6.02s/it]


 50%|████▉     | 1905/3844 [3:55:26<3:40:23,  6.82s/it]

 50%|████▉     | 1906/3844 [3:55:32<3:30:20,  6.51s/it]
{'loss': 1.1048, 'grad_norm': 0.3847451548967571, 'learning_rate': 1.0623195307080824e-05, 'epoch': 0.5}

 50%|████▉     | 1907/3844 [3:55:39<3:30:04,  6.51s/it]

 50%|████▉     | 1908/3844 [3:55:46<3:36:04,  6.70s/it]

 50%|████▉     | 1909/3844 [3:55:51<3:26:50,  6.41s/it]

 50%|████▉     | 1910/3844 [3:55:57<3:18:19,  6.15s/it]

 50%|████▉     | 1911/3844 [3:56:03<3:15:08,  6.06s/it]

 50%|████▉     | 1912/3844 [3:56:11<3:38:24,  6.78s/it]

 50%|████▉     | 1913/3844 [3:56:17<3:32:08,  6.59s/it]

 50%|████▉     | 1914/3844 [3:56:27<3:59:34,  7.45s/it]

 50%|████▉     | 1915/3844 [3:56:35<4:03:52,  7.59s/it]


 50%|████▉     | 1917/3844 [3:56:53<4:19:43,  8.09s/it]
{'loss': 1.0119, 'grad_norm': 0.3818399042764559, 'learning_rate': 1.0530652829520837e-05, 'epoch': 0.5}

 50%|████▉     | 1918/3844 [3:57:01<4:26:02,  8.29s/it]

 50%|████▉     | 1919/3844 [3:57:08<4:10:33,  7.81s/it]

 50%|████▉     | 1920/3844 [3:57:13<3:47:30,  7.09s/it]


 50%|█████     | 1922/3844 [3:57:27<3:39:46,  6.86s/it]
{'loss': 1.0841, 'grad_norm': 0.41012991513190133, 'learning_rate': 1.0488572515348804e-05, 'epoch': 0.5}

 50%|█████     | 1923/3844 [3:57:36<4:03:46,  7.61s/it]

 50%|█████     | 1924/3844 [3:57:43<4:00:45,  7.52s/it]

 50%|█████     | 1925/3844 [3:57:53<4:23:32,  8.24s/it]

 50%|█████     | 1926/3844 [3:58:01<4:19:27,  8.12s/it]


 50%|█████     | 1928/3844 [3:58:16<4:09:36,  7.82s/it]
{'loss': 1.0602, 'grad_norm': 0.3755673605216431, 'learning_rate': 1.0438064754497512e-05, 'epoch': 0.5}

 50%|█████     | 1929/3844 [3:58:23<4:00:08,  7.52s/it]

 50%|█████     | 1930/3844 [3:58:31<4:02:44,  7.61s/it]

 50%|█████     | 1931/3844 [3:58:37<3:52:29,  7.29s/it]


 50%|█████     | 1933/3844 [3:58:54<4:11:25,  7.89s/it]
{'loss': 1.1351, 'grad_norm': 0.3865759805407422, 'learning_rate': 1.0395966344020482e-05, 'epoch': 0.5}

 50%|█████     | 1934/3844 [3:59:00<3:49:29,  7.21s/it]

 50%|█████     | 1935/3844 [3:59:07<3:47:46,  7.16s/it]

 50%|█████     | 1936/3844 [3:59:13<3:38:13,  6.86s/it]


 50%|█████     | 1938/3844 [3:59:34<4:37:00,  8.72s/it]
{'loss': 1.3218, 'grad_norm': 0.41505772138421276, 'learning_rate': 1.0353860903701227e-05, 'epoch': 0.5}

 50%|█████     | 1939/3844 [3:59:43<4:39:27,  8.80s/it]

 50%|█████     | 1940/3844 [3:59:50<4:17:13,  8.11s/it]

 50%|█████     | 1941/3844 [4:00:01<4:46:22,  9.03s/it]

 51%|█████     | 1942/3844 [4:00:08<4:20:53,  8.23s/it]


 51%|█████     | 1944/3844 [4:00:21<3:52:22,  7.34s/it]
{'loss': 1.0588, 'grad_norm': 0.3972287794423387, 'learning_rate': 1.0303326148449093e-05, 'epoch': 0.51}

 51%|█████     | 1945/3844 [4:00:28<3:53:26,  7.38s/it]


 51%|█████     | 1947/3844 [4:00:43<3:50:22,  7.29s/it]
{'loss': 1.0997, 'grad_norm': 0.3878122603823215, 'learning_rate': 1.0278055782094391e-05, 'epoch': 0.51}


 51%|█████     | 1949/3844 [4:00:54<3:24:49,  6.49s/it]
{'loss': 1.1902, 'grad_norm': 0.4073228495418346, 'learning_rate': 1.0261207873919818e-05, 'epoch': 0.51}

 51%|█████     | 1950/3844 [4:01:04<3:50:18,  7.30s/it]

 51%|█████     | 1951/3844 [4:01:11<3:54:26,  7.43s/it]


 51%|█████     | 1953/3844 [4:01:23<3:26:51,  6.56s/it]
{'loss': 1.1671, 'grad_norm': 0.3657358634974404, 'learning_rate': 1.022750987948101e-05, 'epoch': 0.51}


 51%|█████     | 1955/3844 [4:01:40<4:02:31,  7.70s/it]
{'loss': 1.0214, 'grad_norm': 0.3725348241321971, 'learning_rate': 1.0210659888938794e-05, 'epoch': 0.51}

 51%|█████     | 1956/3844 [4:01:46<3:46:23,  7.19s/it]

 51%|█████     | 1957/3844 [4:01:53<3:40:36,  7.01s/it]

 51%|█████     | 1958/3844 [4:01:59<3:31:52,  6.74s/it]

 51%|█████     | 1959/3844 [4:02:08<3:52:53,  7.41s/it]

 51%|█████     | 1960/3844 [4:02:16<4:01:28,  7.69s/it]

 51%|█████     | 1961/3844 [4:02:24<3:58:55,  7.61s/it]


 51%|█████     | 1963/3844 [4:02:39<3:52:53,  7.43s/it]

 51%|█████     | 1964/3844 [4:02:45<3:43:08,  7.12s/it]

 51%|█████     | 1965/3844 [4:02:57<4:29:21,  8.60s/it]

 51%|█████     | 1966/3844 [4:03:03<4:04:25,  7.81s/it]

 51%|█████     | 1967/3844 [4:03:11<4:04:20,  7.81s/it]
{'loss': 1.193, 'grad_norm': 0.37886942306492805, 'learning_rate': 1.0109549054686283e-05, 'epoch': 0.51}


 51%|█████     | 1969/3844 [4:03:23<3:34:46,  6.87s/it]
{'loss': 1.1017, 'grad_norm': 0.4094098731331926, 'learning_rate': 1.0092695880588663e-05, 'epoch': 0.51}

 51%|█████     | 1970/3844 [4:03:28<3:21:56,  6.47s/it]

 51%|█████▏    | 1971/3844 [4:03:34<3:15:49,  6.27s/it]

 51%|█████▏    | 1972/3844 [4:03:42<3:29:08,  6.70s/it]

 51%|█████▏    | 1973/3844 [4:03:48<3:23:32,  6.53s/it]

 51%|█████▏    | 1974/3844 [4:03:53<3:12:20,  6.17s/it]

 51%|█████▏    | 1975/3844 [4:03:59<3:11:33,  6.15s/it]

 51%|█████▏    | 1976/3844 [4:04:07<3:26:45,  6.64s/it]

 51%|█████▏    | 1977/3844 [4:04:14<3:29:34,  6.74s/it]
[2024-05-25 23:46:05,199] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████▏    | 1978/3844 [4:04:24<3:55:06,  7.56s/it]

 51%|█████▏    | 1979/3844 [4:04:30<3:45:12,  7.25s/it]

 52%|█████▏    | 1980/3844 [4:04:38<3:49:45,  7.40s/it]

 52%|█████▏    | 1981/3844 [4:04:46<3:59:03,  7.70s/it]

 52%|█████▏    | 1982/3844 [4:04:54<3:55:09,  7.58s/it]

 52%|█████▏    | 1983/3844 [4:05:00<3:45:54,  7.28s/it]


 52%|█████▏    | 1985/3844 [4:05:13<3:31:30,  6.83s/it]
{'loss': 1.1162, 'grad_norm': 0.4208014135368109, 'learning_rate': 9.957865030072393e-06, 'epoch': 0.52}

 52%|█████▏    | 1986/3844 [4:05:20<3:38:22,  7.05s/it]


 52%|█████▏    | 1988/3844 [4:05:35<3:39:13,  7.09s/it]
{'loss': 1.1165, 'grad_norm': 0.3679469131601636, 'learning_rate': 9.932584359304571e-06, 'epoch': 0.52}

 52%|█████▏    | 1989/3844 [4:05:42<3:38:36,  7.07s/it]

 52%|█████▏    | 1990/3844 [4:05:48<3:32:52,  6.89s/it]

 52%|█████▏    | 1991/3844 [4:05:55<3:34:52,  6.96s/it]

 52%|█████▏    | 1992/3844 [4:06:04<3:48:30,  7.40s/it]

 52%|█████▏    | 1993/3844 [4:06:10<3:37:36,  7.05s/it]

 52%|█████▏    | 1994/3844 [4:06:16<3:22:25,  6.57s/it]

 52%|█████▏    | 1995/3844 [4:06:24<3:43:30,  7.25s/it]

 52%|█████▏    | 1996/3844 [4:06:33<3:58:36,  7.75s/it]

 52%|█████▏    | 1997/3844 [4:06:40<3:45:35,  7.33s/it]

 52%|█████▏    | 1998/3844 [4:06:46<3:37:06,  7.06s/it]

 52%|█████▏    | 1999/3844 [4:06:52<3:22:03,  6.57s/it]


 52%|█████▏    | 2001/3844 [4:07:07<3:38:45,  7.12s/it]
{'loss': 1.1264, 'grad_norm': 0.36339867562216954, 'learning_rate': 9.823041839472215e-06, 'epoch': 0.52}

 52%|█████▏    | 2002/3844 [4:07:16<3:57:30,  7.74s/it]
[2024-05-25 23:49:07,648] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 2003/3844 [4:07:26<4:16:58,  8.38s/it]


 52%|█████▏    | 2005/3844 [4:07:41<4:04:14,  7.97s/it]
{'loss': 1.1884, 'grad_norm': 0.3688901461222517, 'learning_rate': 9.789340111061207e-06, 'epoch': 0.52}

 52%|█████▏    | 2006/3844 [4:07:47<3:44:10,  7.32s/it]

 52%|█████▏    | 2007/3844 [4:07:54<3:44:27,  7.33s/it]


 52%|█████▏    | 2009/3844 [4:08:07<3:29:50,  6.86s/it]

 52%|█████▏    | 2010/3844 [4:08:15<3:39:49,  7.19s/it]
{'loss': 1.0913, 'grad_norm': 0.37808054548915265, 'learning_rate': 9.747216361402385e-06, 'epoch': 0.52}
[2024-05-25 23:50:07,502] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 52%|█████▏    | 2012/3844 [4:08:33<4:04:42,  8.01s/it]

 52%|█████▏    | 2013/3844 [4:08:39<3:45:24,  7.39s/it]
{'loss': 1.0655, 'grad_norm': 0.39041346577677594, 'learning_rate': 9.721944217905612e-06, 'epoch': 0.52}

 52%|█████▏    | 2014/3844 [4:08:46<3:37:44,  7.14s/it]


 52%|█████▏    | 2016/3844 [4:09:01<3:43:10,  7.33s/it]

 52%|█████▏    | 2017/3844 [4:09:07<3:30:34,  6.92s/it]
{'loss': 1.2061, 'grad_norm': 0.3896415436932244, 'learning_rate': 9.688250818935602e-06, 'epoch': 0.52}


 53%|█████▎    | 2019/3844 [4:09:19<3:14:18,  6.39s/it]
{'loss': 1.2105, 'grad_norm': 0.3806210433495407, 'learning_rate': 9.671405423849135e-06, 'epoch': 0.53}

 53%|█████▎    | 2020/3844 [4:09:25<3:05:49,  6.11s/it]


 53%|█████▎    | 2022/3844 [4:09:37<3:11:53,  6.32s/it]
{'loss': 1.2448, 'grad_norm': 0.42147530033713027, 'learning_rate': 9.646139096298772e-06, 'epoch': 0.53}


 53%|█████▎    | 2024/3844 [4:09:49<3:07:10,  6.17s/it]
{'loss': 1.1816, 'grad_norm': 0.40502002597486064, 'learning_rate': 9.629296124428945e-06, 'epoch': 0.53}

 53%|█████▎    | 2025/3844 [4:09:55<2:59:26,  5.92s/it]

 53%|█████▎    | 2026/3844 [4:10:03<3:16:04,  6.47s/it]

 53%|█████▎    | 2027/3844 [4:10:08<3:09:01,  6.24s/it]

 53%|█████▎    | 2028/3844 [4:10:18<3:43:01,  7.37s/it]


 53%|█████▎    | 2030/3844 [4:10:33<3:48:29,  7.56s/it]
{'loss': 1.158, 'grad_norm': 0.3700535978506953, 'learning_rate': 9.578773718275696e-06, 'epoch': 0.53}

 53%|█████▎    | 2031/3844 [4:10:40<3:44:04,  7.42s/it]

 53%|█████▎    | 2032/3844 [4:10:47<3:38:47,  7.24s/it]

 53%|█████▎    | 2033/3844 [4:10:57<3:59:58,  7.95s/it]


 53%|█████▎    | 2035/3844 [4:11:13<4:07:33,  8.21s/it]
{'loss': 1.1138, 'grad_norm': 0.39019461362270463, 'learning_rate': 9.536679884464462e-06, 'epoch': 0.53}

 53%|█████▎    | 2036/3844 [4:11:20<3:58:45,  7.92s/it]

 53%|█████▎    | 2037/3844 [4:11:27<3:44:27,  7.45s/it]

 53%|█████▎    | 2038/3844 [4:11:32<3:27:44,  6.90s/it]

 53%|█████▎    | 2039/3844 [4:11:38<3:18:17,  6.59s/it]

 53%|█████▎    | 2040/3844 [4:11:44<3:07:58,  6.25s/it]


 53%|█████▎    | 2042/3844 [4:11:57<3:17:37,  6.58s/it]
{'loss': 1.1023, 'grad_norm': 0.4127888304196037, 'learning_rate': 9.477762503536402e-06, 'epoch': 0.53}

 53%|█████▎    | 2043/3844 [4:12:06<3:34:05,  7.13s/it]

 53%|█████▎    | 2044/3844 [4:12:14<3:40:00,  7.33s/it]

 53%|█████▎    | 2045/3844 [4:12:22<3:45:52,  7.53s/it]

 53%|█████▎    | 2046/3844 [4:12:29<3:41:36,  7.40s/it]

 53%|█████▎    | 2047/3844 [4:12:36<3:43:06,  7.45s/it]

 53%|█████▎    | 2048/3844 [4:12:42<3:27:48,  6.94s/it]


 53%|█████▎    | 2050/3844 [4:12:57<3:42:09,  7.43s/it]

 53%|█████▎    | 2051/3844 [4:13:03<3:30:07,  7.03s/it]

 53%|█████▎    | 2052/3844 [4:13:11<3:38:58,  7.33s/it]

 53%|█████▎    | 2053/3844 [4:13:18<3:28:22,  6.98s/it]
{'loss': 1.2135, 'grad_norm': 0.3705035273208799, 'learning_rate': 9.385215552097475e-06, 'epoch': 0.53}

 53%|█████▎    | 2054/3844 [4:13:26<3:40:27,  7.39s/it]

 53%|█████▎    | 2055/3844 [4:13:32<3:29:15,  7.02s/it]


 54%|█████▎    | 2057/3844 [4:13:45<3:26:52,  6.95s/it]

 54%|█████▎    | 2058/3844 [4:13:51<3:18:21,  6.66s/it]
{'loss': 1.078, 'grad_norm': 0.4178382676361057, 'learning_rate': 9.343165741529927e-06, 'epoch': 0.54}


 54%|█████▎    | 2060/3844 [4:14:08<3:37:21,  7.31s/it]
{'loss': 1.1705, 'grad_norm': 0.3711628945788286, 'learning_rate': 9.326349040638564e-06, 'epoch': 0.54}


 54%|█████▎    | 2062/3844 [4:14:22<3:29:26,  7.05s/it]

 54%|█████▎    | 2063/3844 [4:14:34<4:13:43,  8.55s/it]
{'loss': 1.0067, 'grad_norm': 0.35324662105314425, 'learning_rate': 9.301127592158516e-06, 'epoch': 0.54}

 54%|█████▎    | 2064/3844 [4:14:42<4:14:26,  8.58s/it]


 54%|█████▎    | 2066/3844 [4:15:02<4:41:15,  9.49s/it]
{'loss': 1.0667, 'grad_norm': 0.3835558675654984, 'learning_rate': 9.275910610392104e-06, 'epoch': 0.54}

 54%|█████▍    | 2067/3844 [4:15:10<4:30:30,  9.13s/it]

 54%|█████▍    | 2068/3844 [4:15:16<4:05:09,  8.28s/it]

 54%|█████▍    | 2069/3844 [4:15:23<3:50:17,  7.78s/it]

 54%|█████▍    | 2070/3844 [4:15:29<3:32:07,  7.17s/it]

 54%|█████▍    | 2071/3844 [4:15:34<3:18:25,  6.71s/it]

 54%|█████▍    | 2072/3844 [4:15:42<3:31:19,  7.16s/it]

 54%|█████▍    | 2073/3844 [4:15:48<3:19:36,  6.76s/it]


 54%|█████▍    | 2075/3844 [4:16:02<3:19:49,  6.78s/it]
{'loss': 0.992, 'grad_norm': 0.3995039133221986, 'learning_rate': 9.20028807692249e-06, 'epoch': 0.54}

 54%|█████▍    | 2076/3844 [4:16:09<3:18:50,  6.75s/it]


 54%|█████▍    | 2078/3844 [4:16:20<3:00:52,  6.14s/it]
{'loss': 1.086, 'grad_norm': 0.4038437715182768, 'learning_rate': 9.175090573406412e-06, 'epoch': 0.54}


 54%|█████▍    | 2080/3844 [4:16:34<3:18:36,  6.76s/it]
{'loss': 1.127, 'grad_norm': 0.37957052633147853, 'learning_rate': 9.158295156820403e-06, 'epoch': 0.54}

 54%|█████▍    | 2081/3844 [4:16:45<3:56:00,  8.03s/it]

 54%|█████▍    | 2082/3844 [4:16:55<4:12:58,  8.61s/it]


 54%|█████▍    | 2084/3844 [4:17:12<4:09:04,  8.49s/it]

 54%|█████▍    | 2085/3844 [4:17:17<3:44:01,  7.64s/it]

 54%|█████▍    | 2086/3844 [4:17:24<3:32:07,  7.24s/it]
{'loss': 1.1876, 'grad_norm': 0.38984763584460946, 'learning_rate': 9.107923443469677e-06, 'epoch': 0.54}

 54%|█████▍    | 2087/3844 [4:17:32<3:41:17,  7.56s/it]

 54%|█████▍    | 2088/3844 [4:17:38<3:29:00,  7.14s/it]

 54%|█████▍    | 2089/3844 [4:17:44<3:18:21,  6.78s/it]

 54%|█████▍    | 2090/3844 [4:17:55<3:50:40,  7.89s/it]

 54%|█████▍    | 2091/3844 [4:18:01<3:34:00,  7.32s/it]


 54%|█████▍    | 2093/3844 [4:18:14<3:21:41,  6.91s/it]

 54%|█████▍    | 2094/3844 [4:18:22<3:31:38,  7.26s/it]
{'loss': 1.09, 'grad_norm': 0.4172572968067096, 'learning_rate': 9.04079685770102e-06, 'epoch': 0.54}

 55%|█████▍    | 2095/3844 [4:18:31<3:47:37,  7.81s/it]

 55%|█████▍    | 2096/3844 [4:18:37<3:33:44,  7.34s/it]


 55%|█████▍    | 2098/3844 [4:18:56<4:05:04,  8.42s/it]

 55%|█████▍    | 2099/3844 [4:19:02<3:45:52,  7.77s/it]
{'loss': 1.2128, 'grad_norm': 0.38156933896761974, 'learning_rate': 8.998864686114103e-06, 'epoch': 0.55}

 55%|█████▍    | 2100/3844 [4:19:08<3:32:56,  7.33s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.2307, 'grad_norm': 0.3905142374698333, 'learning_rate': 8.98209675246235e-06, 'epoch': 0.55}
 55%|█████▍    | 2101/3844 [4:19:47<8:05:13, 16.70s/it]

 55%|█████▍    | 2102/3844 [4:19:54<6:44:00, 13.92s/it]

 55%|█████▍    | 2103/3844 [4:20:05<6:15:06, 12.93s/it]

 55%|█████▍    | 2104/3844 [4:20:14<5:45:51, 11.93s/it]

 55%|█████▍    | 2105/3844 [4:20:21<4:56:39, 10.24s/it]

 55%|█████▍    | 2106/3844 [4:20:27<4:20:51,  9.01s/it]

 55%|█████▍    | 2107/3844 [4:20:33<4:00:12,  8.30s/it]


 55%|█████▍    | 2109/3844 [4:20:48<3:42:32,  7.70s/it]

 55%|█████▍    | 2110/3844 [4:20:54<3:25:56,  7.13s/it]

 55%|█████▍    | 2111/3844 [4:21:00<3:15:33,  6.77s/it]

 55%|█████▍    | 2112/3844 [4:21:06<3:09:54,  6.58s/it]
{'loss': 1.3017, 'grad_norm': 0.3938626107751579, 'learning_rate': 8.889926078566228e-06, 'epoch': 0.55}

 55%|█████▍    | 2113/3844 [4:21:12<3:10:25,  6.60s/it]

 55%|█████▍    | 2114/3844 [4:21:22<3:32:36,  7.37s/it]

 55%|█████▌    | 2115/3844 [4:21:31<3:50:13,  7.99s/it]

 55%|█████▌    | 2116/3844 [4:21:37<3:31:09,  7.33s/it]

 55%|█████▌    | 2117/3844 [4:21:43<3:19:15,  6.92s/it]

 55%|█████▌    | 2118/3844 [4:21:49<3:11:55,  6.67s/it]

 55%|█████▌    | 2119/3844 [4:21:54<2:59:42,  6.25s/it]


 55%|█████▌    | 2121/3844 [4:22:06<2:54:34,  6.08s/it]
{'loss': 1.0822, 'grad_norm': 0.40350136479151066, 'learning_rate': 8.814584300096854e-06, 'epoch': 0.55}


 55%|█████▌    | 2123/3844 [4:22:20<3:08:09,  6.56s/it]
{'loss': 1.1772, 'grad_norm': 0.38265428614564084, 'learning_rate': 8.797850790112561e-06, 'epoch': 0.55}

 55%|█████▌    | 2124/3844 [4:22:25<2:57:25,  6.19s/it]

 55%|█████▌    | 2125/3844 [4:22:33<3:12:02,  6.70s/it]


 55%|█████▌    | 2127/3844 [4:22:46<3:12:16,  6.72s/it]

 55%|█████▌    | 2128/3844 [4:22:52<3:06:36,  6.53s/it]
{'loss': 1.1914, 'grad_norm': 0.3877179295705846, 'learning_rate': 8.756032058888243e-06, 'epoch': 0.55}

 55%|█████▌    | 2129/3844 [4:23:01<3:28:57,  7.31s/it]

 55%|█████▌    | 2130/3844 [4:23:08<3:19:03,  6.97s/it]


 55%|█████▌    | 2132/3844 [4:23:24<3:33:11,  7.47s/it]
{'loss': 1.0996, 'grad_norm': 0.4223759535223226, 'learning_rate': 8.722592939451626e-06, 'epoch': 0.55}


 56%|█████▌    | 2134/3844 [4:23:42<4:00:27,  8.44s/it]
{'loss': 1.1161, 'grad_norm': 0.3937681480402525, 'learning_rate': 8.705878798864666e-06, 'epoch': 0.56}

 56%|█████▌    | 2135/3844 [4:23:49<3:45:55,  7.93s/it]

 56%|█████▌    | 2136/3844 [4:23:55<3:34:13,  7.53s/it]


 56%|█████▌    | 2138/3844 [4:24:08<3:17:46,  6.96s/it]
{'loss': 1.2741, 'grad_norm': 0.3980301953298394, 'learning_rate': 8.672461593342999e-06, 'epoch': 0.56}

 56%|█████▌    | 2139/3844 [4:24:15<3:19:17,  7.01s/it]


 56%|█████▌    | 2141/3844 [4:24:32<3:48:33,  8.05s/it]

 56%|█████▌    | 2142/3844 [4:24:40<3:46:46,  7.99s/it]

 56%|█████▌    | 2143/3844 [4:24:52<4:19:33,  9.16s/it]
{'loss': 1.1426, 'grad_norm': 0.3645282525086789, 'learning_rate': 8.63071134270168e-06, 'epoch': 0.56}

 56%|█████▌    | 2144/3844 [4:24:59<4:04:39,  8.64s/it]

 56%|█████▌    | 2145/3844 [4:25:08<3:59:51,  8.47s/it]


 56%|█████▌    | 2147/3844 [4:25:22<3:43:08,  7.89s/it]

 56%|█████▌    | 2148/3844 [4:25:30<3:42:08,  7.86s/it]
{'loss': 1.2501, 'grad_norm': 0.37360728339066585, 'learning_rate': 8.588985401912357e-06, 'epoch': 0.56}


 56%|█████▌    | 2150/3844 [4:25:44<3:29:09,  7.41s/it]
{'loss': 1.2196, 'grad_norm': 0.4054549115512877, 'learning_rate': 8.572301998306313e-06, 'epoch': 0.56}

 56%|█████▌    | 2151/3844 [4:25:49<3:11:38,  6.79s/it]


 56%|█████▌    | 2153/3844 [4:26:04<3:20:27,  7.11s/it]

 56%|█████▌    | 2154/3844 [4:26:12<3:27:35,  7.37s/it]

 56%|█████▌    | 2155/3844 [4:26:20<3:33:56,  7.60s/it]
{'loss': 1.0859, 'grad_norm': 0.3880598154262384, 'learning_rate': 8.530611335733024e-06, 'epoch': 0.56}

 56%|█████▌    | 2156/3844 [4:26:28<3:30:28,  7.48s/it]


 56%|█████▌    | 2158/3844 [4:26:38<3:00:14,  6.41s/it]
{'loss': 1.1349, 'grad_norm': 0.37737553494495935, 'learning_rate': 8.505609412593579e-06, 'epoch': 0.56}


 56%|█████▌    | 2160/3844 [4:26:52<3:11:29,  6.82s/it]
{'loss': 1.2764, 'grad_norm': 0.37283532498993527, 'learning_rate': 8.488946760150791e-06, 'epoch': 0.56}

 56%|█████▌    | 2161/3844 [4:27:00<3:16:10,  6.99s/it]


 56%|█████▋    | 2163/3844 [4:27:14<3:22:09,  7.22s/it]
{'loss': 1.0165, 'grad_norm': 0.3895524200813466, 'learning_rate': 8.46396084429106e-06, 'epoch': 0.56}

 56%|█████▋    | 2164/3844 [4:27:23<3:38:40,  7.81s/it]

 56%|█████▋    | 2165/3844 [4:27:30<3:27:05,  7.40s/it]

 56%|█████▋    | 2166/3844 [4:27:36<3:16:30,  7.03s/it]


 56%|█████▋    | 2168/3844 [4:27:51<3:19:33,  7.14s/it]
{'loss': 1.0272, 'grad_norm': 0.38956821265397773, 'learning_rate': 8.422339546268145e-06, 'epoch': 0.56}

 56%|█████▋    | 2169/3844 [4:27:58<3:21:18,  7.21s/it]


 56%|█████▋    | 2171/3844 [4:28:15<3:41:05,  7.93s/it]

 57%|█████▋    | 2172/3844 [4:28:25<3:59:04,  8.58s/it]
{'loss': 1.2164, 'grad_norm': 0.379855854868816, 'learning_rate': 8.389062639028256e-06, 'epoch': 0.56}

 57%|█████▋    | 2173/3844 [4:28:32<3:44:59,  8.08s/it]

 57%|█████▋    | 2174/3844 [4:28:37<3:25:04,  7.37s/it]


 57%|█████▋    | 2176/3844 [4:28:53<3:22:56,  7.30s/it]

 57%|█████▋    | 2177/3844 [4:28:58<3:09:50,  6.83s/it]
{'loss': 1.2372, 'grad_norm': 0.44032117648175984, 'learning_rate': 8.347492289276892e-06, 'epoch': 0.57}

 57%|█████▋    | 2178/3844 [4:29:09<3:42:08,  8.00s/it]


 57%|█████▋    | 2180/3844 [4:29:25<3:41:00,  7.97s/it]
{'loss': 1.2003, 'grad_norm': 0.4000794727466582, 'learning_rate': 8.322564114463552e-06, 'epoch': 0.57}

 57%|█████▋    | 2181/3844 [4:29:36<4:08:57,  8.98s/it]

 57%|█████▋    | 2182/3844 [4:29:43<3:56:09,  8.53s/it]


 57%|█████▋    | 2184/3844 [4:29:59<3:47:09,  8.21s/it]
{'loss': 1.188, 'grad_norm': 0.4011132057610295, 'learning_rate': 8.2893432527171e-06, 'epoch': 0.57}

 57%|█████▋    | 2185/3844 [4:30:04<3:21:39,  7.29s/it]


 57%|█████▋    | 2187/3844 [4:30:21<3:39:00,  7.93s/it]
{'loss': 1.0239, 'grad_norm': 0.3830349402297832, 'learning_rate': 8.264440341337843e-06, 'epoch': 0.57}


 57%|█████▋    | 2189/3844 [4:30:34<3:23:43,  7.39s/it]
{'loss': 1.1707, 'grad_norm': 0.38736649521183825, 'learning_rate': 8.24784455310103e-06, 'epoch': 0.57}


 57%|█████▋    | 2191/3844 [4:30:53<3:41:06,  8.03s/it]
{'loss': 1.2231, 'grad_norm': 0.40914179885269353, 'learning_rate': 8.23125374201112e-06, 'epoch': 0.57}


 57%|█████▋    | 2193/3844 [4:31:06<3:25:34,  7.47s/it]
{'loss': 1.3197, 'grad_norm': 0.37790488192932603, 'learning_rate': 8.214667955195732e-06, 'epoch': 0.57}


 57%|█████▋    | 2195/3844 [4:31:20<3:18:14,  7.21s/it]
{'loss': 0.9926, 'grad_norm': 0.37141006217272454, 'learning_rate': 8.198087239768202e-06, 'epoch': 0.57}

 57%|█████▋    | 2196/3844 [4:31:28<3:20:25,  7.30s/it]


 57%|█████▋    | 2198/3844 [4:31:41<3:09:59,  6.93s/it]
{'loss': 1.2717, 'grad_norm': 0.3903700309725032, 'learning_rate': 8.173225778504094e-06, 'epoch': 0.57}

 57%|█████▋    | 2199/3844 [4:31:49<3:21:24,  7.35s/it]


 57%|█████▋    | 2201/3844 [4:32:05<3:30:29,  7.69s/it]
{'loss': 1.2058, 'grad_norm': 0.3937940875652016, 'learning_rate': 8.1483759927293e-06, 'epoch': 0.57}

 57%|█████▋    | 2202/3844 [4:32:12<3:22:40,  7.41s/it]


 57%|█████▋    | 2204/3844 [4:32:25<3:11:46,  7.02s/it]
{'loss': 1.2956, 'grad_norm': 0.38660264261137345, 'learning_rate': 8.123538041266622e-06, 'epoch': 0.57}

 57%|█████▋    | 2205/3844 [4:32:31<3:06:39,  6.83s/it]

 57%|█████▋    | 2206/3844 [4:32:37<2:58:32,  6.54s/it]

 57%|█████▋    | 2207/3844 [4:32:45<3:13:30,  7.09s/it]

 57%|█████▋    | 2208/3844 [4:32:51<3:04:28,  6.77s/it]

 57%|█████▋    | 2209/3844 [4:33:00<3:21:37,  7.40s/it]

 57%|█████▋    | 2210/3844 [4:33:06<3:07:09,  6.87s/it]

 58%|█████▊    | 2211/3844 [4:33:13<3:12:01,  7.06s/it]

 58%|█████▊    | 2212/3844 [4:33:19<3:00:31,  6.64s/it]

 58%|█████▊    | 2213/3844 [4:33:30<3:37:26,  8.00s/it]

 58%|█████▊    | 2214/3844 [4:33:38<3:36:04,  7.95s/it]

 58%|█████▊    | 2215/3844 [4:33:47<3:46:27,  8.34s/it]

 58%|█████▊    | 2216/3844 [4:33:53<3:26:31,  7.61s/it]


 58%|█████▊    | 2218/3844 [4:34:11<3:48:10,  8.42s/it]

 58%|█████▊    | 2219/3844 [4:34:19<3:45:54,  8.34s/it]

 58%|█████▊    | 2220/3844 [4:34:25<3:24:27,  7.55s/it]
{'loss': 1.2107, 'grad_norm': 0.37156378829097064, 'learning_rate': 7.991275385433333e-06, 'epoch': 0.58}

 58%|█████▊    | 2221/3844 [4:34:30<3:05:55,  6.87s/it]


 58%|█████▊    | 2223/3844 [4:34:45<3:13:29,  7.16s/it]
{'loss': 1.1927, 'grad_norm': 0.4208680496712693, 'learning_rate': 7.9665160678663e-06, 'epoch': 0.58}


 58%|█████▊    | 2225/3844 [4:34:59<3:11:15,  7.09s/it]

 58%|█████▊    | 2226/3844 [4:35:05<3:01:07,  6.72s/it]
{'loss': 1.2272, 'grad_norm': 0.4109059928979473, 'learning_rate': 7.941769746935431e-06, 'epoch': 0.58}

 58%|█████▊    | 2227/3844 [4:35:10<2:48:51,  6.27s/it]

 58%|█████▊    | 2228/3844 [4:35:16<2:46:30,  6.18s/it]


 58%|█████▊    | 2230/3844 [4:35:33<3:21:38,  7.50s/it]

 58%|█████▊    | 2231/3844 [4:35:45<3:57:19,  8.83s/it]
{'loss': 0.9244, 'grad_norm': 0.3827299023538143, 'learning_rate': 7.900555189667516e-06, 'epoch': 0.58}

 58%|█████▊    | 2232/3844 [4:35:52<3:39:38,  8.18s/it]


 58%|█████▊    | 2234/3844 [4:36:05<3:19:47,  7.45s/it]
{'loss': 1.2086, 'grad_norm': 0.3943362451302358, 'learning_rate': 7.875844299438422e-06, 'epoch': 0.58}

 58%|█████▊    | 2235/3844 [4:36:12<3:11:58,  7.16s/it]

 58%|█████▊    | 2236/3844 [4:36:20<3:23:51,  7.61s/it]

 58%|█████▊    | 2237/3844 [4:36:26<3:09:19,  7.07s/it]

 58%|█████▊    | 2238/3844 [4:36:32<2:57:46,  6.64s/it]

 58%|█████▊    | 2239/3844 [4:36:39<3:00:20,  6.74s/it]

 58%|█████▊    | 2240/3844 [4:36:45<3:00:29,  6.75s/it]

 58%|█████▊    | 2241/3844 [4:36:51<2:53:26,  6.49s/it]


 58%|█████▊    | 2243/3844 [4:37:05<3:01:54,  6.82s/it]
{'loss': 1.0211, 'grad_norm': 0.39963138382957547, 'learning_rate': 7.801793716944561e-06, 'epoch': 0.58}


 58%|█████▊    | 2245/3844 [4:37:19<3:02:03,  6.83s/it]
{'loss': 1.1502, 'grad_norm': 0.39146577431991314, 'learning_rate': 7.785355053568217e-06, 'epoch': 0.58}

 58%|█████▊    | 2246/3844 [4:37:27<3:14:22,  7.30s/it]

 58%|█████▊    | 2247/3844 [4:37:36<3:25:27,  7.72s/it]


 59%|█████▊    | 2249/3844 [4:37:53<3:38:13,  8.21s/it]

 59%|█████▊    | 2250/3844 [4:38:03<3:52:12,  8.74s/it]
{'loss': 1.1643, 'grad_norm': 0.3919563602551441, 'learning_rate': 7.744286019863503e-06, 'epoch': 0.59}

 59%|█████▊    | 2251/3844 [4:38:11<3:42:08,  8.37s/it]


 59%|█████▊    | 2253/3844 [4:38:25<3:25:37,  7.75s/it]

 59%|█████▊    | 2254/3844 [4:38:31<3:16:54,  7.43s/it]
{'loss': 1.2076, 'grad_norm': 0.3811931279525954, 'learning_rate': 7.711459591855004e-06, 'epoch': 0.59}

 59%|█████▊    | 2255/3844 [4:38:38<3:07:53,  7.09s/it]

 59%|█████▊    | 2256/3844 [4:38:44<3:01:17,  6.85s/it]

 59%|█████▊    | 2257/3844 [4:38:51<3:00:09,  6.81s/it]

 59%|█████▊    | 2258/3844 [4:38:56<2:47:59,  6.36s/it]


 59%|█████▉    | 2260/3844 [4:39:09<2:53:20,  6.57s/it]

 59%|█████▉    | 2261/3844 [4:39:15<2:49:21,  6.42s/it]
{'loss': 1.1868, 'grad_norm': 0.3978523714596241, 'learning_rate': 7.654076137146179e-06, 'epoch': 0.59}

 59%|█████▉    | 2262/3844 [4:39:21<2:46:54,  6.33s/it]

 59%|█████▉    | 2263/3844 [4:39:28<2:51:54,  6.52s/it]

 59%|█████▉    | 2264/3844 [4:39:34<2:46:51,  6.34s/it]

 59%|█████▉    | 2265/3844 [4:39:41<2:45:28,  6.29s/it]


 59%|█████▉    | 2267/3844 [4:40:05<4:01:26,  9.19s/it]
{'loss': 1.2308, 'grad_norm': 0.372334603957586, 'learning_rate': 7.604955215066089e-06, 'epoch': 0.59}

 59%|█████▉    | 2268/3844 [4:40:16<4:17:46,  9.81s/it]

 59%|█████▉    | 2269/3844 [4:40:25<4:06:41,  9.40s/it]


 59%|█████▉    | 2271/3844 [4:40:41<3:56:47,  9.03s/it]

 59%|█████▉    | 2272/3844 [4:40:47<3:32:07,  8.10s/it]
{'loss': 1.1573, 'grad_norm': 0.39647010667006816, 'learning_rate': 7.5640678328835e-06, 'epoch': 0.59}

 59%|█████▉    | 2273/3844 [4:40:54<3:18:20,  7.58s/it]


 59%|█████▉    | 2275/3844 [4:41:09<3:19:04,  7.61s/it]
{'loss': 1.1621, 'grad_norm': 0.40132629480657184, 'learning_rate': 7.539556115541261e-06, 'epoch': 0.59}

 59%|█████▉    | 2276/3844 [4:41:16<3:14:33,  7.44s/it]


 59%|█████▉    | 2278/3844 [4:41:33<3:28:14,  7.98s/it]

 59%|█████▉    | 2279/3844 [4:41:41<3:30:58,  8.09s/it]

 59%|█████▉    | 2280/3844 [4:41:47<3:13:20,  7.42s/it]
{'loss': 1.1184, 'grad_norm': 0.43451410582123823, 'learning_rate': 7.498738276115336e-06, 'epoch': 0.59}


 59%|█████▉    | 2282/3844 [4:42:05<3:34:34,  8.24s/it]
{'loss': 1.1308, 'grad_norm': 0.36939971027749907, 'learning_rate': 7.48242353360712e-06, 'epoch': 0.59}

 59%|█████▉    | 2283/3844 [4:42:16<3:51:54,  8.91s/it]

 59%|█████▉    | 2284/3844 [4:42:28<4:16:54,  9.88s/it]

 59%|█████▉    | 2285/3844 [4:42:34<3:49:36,  8.84s/it]

 59%|█████▉    | 2286/3844 [4:42:42<3:41:24,  8.53s/it]

 59%|█████▉    | 2287/3844 [4:42:52<3:52:58,  8.98s/it]


 60%|█████▉    | 2289/3844 [4:43:06<3:24:47,  7.90s/it]

 60%|█████▉    | 2290/3844 [4:43:11<3:07:20,  7.23s/it]
{'loss': 1.2085, 'grad_norm': 0.421022654403662, 'learning_rate': 7.417236540608517e-06, 'epoch': 0.6}

 60%|█████▉    | 2291/3844 [4:43:17<2:54:49,  6.75s/it]

 60%|█████▉    | 2292/3844 [4:43:26<3:10:45,  7.37s/it]


 60%|█████▉    | 2294/3844 [4:43:40<3:05:00,  7.16s/it]
{'loss': 1.1486, 'grad_norm': 0.3953127535240324, 'learning_rate': 7.3846868783498016e-06, 'epoch': 0.6}

 60%|█████▉    | 2295/3844 [4:43:46<3:00:15,  6.98s/it]


 60%|█████▉    | 2297/3844 [4:44:00<2:57:21,  6.88s/it]
{'loss': 1.24, 'grad_norm': 0.4135352286592754, 'learning_rate': 7.36029411260607e-06, 'epoch': 0.6}


 60%|█████▉    | 2299/3844 [4:44:13<2:58:00,  6.91s/it]

 60%|█████▉    | 2300/3844 [4:44:23<3:21:30,  7.83s/it]
{'loss': 1.1213, 'grad_norm': 0.39202058367897574, 'learning_rate': 7.335918218053926e-06, 'epoch': 0.6}

 60%|█████▉    | 2301/3844 [4:44:30<3:14:36,  7.57s/it]

 60%|█████▉    | 2302/3844 [4:44:38<3:17:32,  7.69s/it]

 60%|█████▉    | 2303/3844 [4:44:49<3:42:55,  8.68s/it]


 60%|█████▉    | 2305/3844 [4:45:01<3:10:44,  7.44s/it]

 60%|█████▉    | 2306/3844 [4:45:08<3:01:02,  7.06s/it]

 60%|██████    | 2307/3844 [4:45:18<3:23:54,  7.96s/it]

 60%|██████    | 2308/3844 [4:45:24<3:10:03,  7.42s/it]
{'loss': 1.1217, 'grad_norm': 0.3936140530888229, 'learning_rate': 7.2709994984212515e-06, 'epoch': 0.6}


 60%|██████    | 2310/3844 [4:45:40<3:13:50,  7.58s/it]
{'loss': 1.1538, 'grad_norm': 0.3943959164966856, 'learning_rate': 7.254789083209423e-06, 'epoch': 0.6}

 60%|██████    | 2311/3844 [4:45:46<3:05:07,  7.25s/it]

 60%|██████    | 2312/3844 [4:45:57<3:33:43,  8.37s/it]

 60%|██████    | 2313/3844 [4:46:02<3:11:06,  7.49s/it]


 60%|██████    | 2315/3844 [4:46:15<3:00:00,  7.06s/it]
{'loss': 1.1968, 'grad_norm': 0.42887086671606195, 'learning_rate': 7.214297262127847e-06, 'epoch': 0.6}

 60%|██████    | 2316/3844 [4:46:21<2:49:49,  6.67s/it]


 60%|██████    | 2318/3844 [4:46:38<3:11:17,  7.52s/it]
{'loss': 1.148, 'grad_norm': 0.3798465663569047, 'learning_rate': 7.190025862555916e-06, 'epoch': 0.6}


 60%|██████    | 2320/3844 [4:46:52<3:05:39,  7.31s/it]

 60%|██████    | 2321/3844 [4:46:57<2:52:45,  6.81s/it]
{'loss': 1.128, 'grad_norm': 0.4036733054245255, 'learning_rate': 7.165772422413571e-06, 'epoch': 0.6}

 60%|██████    | 2322/3844 [4:47:07<3:12:52,  7.60s/it]

 60%|██████    | 2323/3844 [4:47:13<2:58:23,  7.04s/it]


 60%|██████    | 2325/3844 [4:47:23<2:37:23,  6.22s/it]
{'loss': 1.1806, 'grad_norm': 0.41521213822672065, 'learning_rate': 7.133462707018484e-06, 'epoch': 0.6}


 61%|██████    | 2327/3844 [4:47:42<3:09:46,  7.51s/it]

 61%|██████    | 2328/3844 [4:47:49<3:12:17,  7.61s/it]
{'loss': 1.1438, 'grad_norm': 0.4058366164493441, 'learning_rate': 7.109251774833611e-06, 'epoch': 0.61}

 61%|██████    | 2329/3844 [4:47:59<3:26:49,  8.19s/it]

 61%|██████    | 2330/3844 [4:48:05<3:11:22,  7.58s/it]


 61%|██████    | 2332/3844 [4:48:20<3:08:30,  7.48s/it]
{'loss': 1.1407, 'grad_norm': 0.37932211231562607, 'learning_rate': 7.076999298598708e-06, 'epoch': 0.61}


 61%|██████    | 2334/3844 [4:48:36<3:12:46,  7.66s/it]

 61%|██████    | 2335/3844 [4:48:48<3:43:16,  8.88s/it]
{'loss': 1.1413, 'grad_norm': 0.40959655809205614, 'learning_rate': 7.052831716840965e-06, 'epoch': 0.61}


 61%|██████    | 2337/3844 [4:49:00<3:07:33,  7.47s/it]

 61%|██████    | 2338/3844 [4:49:08<3:10:30,  7.59s/it]
{'loss': 1.0075, 'grad_norm': 0.38642875666897897, 'learning_rate': 7.028682971363798e-06, 'epoch': 0.61}

 61%|██████    | 2339/3844 [4:49:17<3:26:08,  8.22s/it]

 61%|██████    | 2340/3844 [4:49:23<3:08:49,  7.53s/it]

 61%|██████    | 2341/3844 [4:49:29<2:56:34,  7.05s/it]

 61%|██████    | 2342/3844 [4:49:35<2:46:30,  6.65s/it]

 61%|██████    | 2343/3844 [4:49:43<2:59:48,  7.19s/it]

 61%|██████    | 2344/3844 [4:49:51<3:05:23,  7.42s/it]

 61%|██████    | 2345/3844 [4:49:57<2:50:30,  6.82s/it]

 61%|██████    | 2346/3844 [4:50:03<2:48:40,  6.76s/it]

 61%|██████    | 2347/3844 [4:50:12<3:07:26,  7.51s/it]


 61%|██████    | 2349/3844 [4:50:25<2:55:43,  7.05s/it]
{'loss': 1.1219, 'grad_norm': 0.41573488004820414, 'learning_rate': 6.940301219061376e-06, 'epoch': 0.61}


 61%|██████    | 2351/3844 [4:50:42<3:15:42,  7.86s/it]
{'loss': 1.1216, 'grad_norm': 0.3537231392441494, 'learning_rate': 6.92425983402768e-06, 'epoch': 0.61}

 61%|██████    | 2352/3844 [4:50:53<3:39:49,  8.84s/it]

 61%|██████    | 2353/3844 [4:50:59<3:20:29,  8.07s/it]

 61%|██████    | 2354/3844 [4:51:07<3:16:48,  7.93s/it]

 61%|██████▏   | 2355/3844 [4:51:13<3:05:18,  7.47s/it]

 61%|██████▏   | 2356/3844 [4:51:23<3:18:58,  8.02s/it]


 61%|██████▏   | 2358/3844 [4:51:36<3:01:31,  7.33s/it]
{'loss': 1.2777, 'grad_norm': 0.4042212356929483, 'learning_rate': 6.868184088322625e-06, 'epoch': 0.61}


 61%|██████▏   | 2360/3844 [4:51:48<2:47:54,  6.79s/it]

 61%|██████▏   | 2361/3844 [4:51:54<2:39:40,  6.46s/it]
{'loss': 1.2703, 'grad_norm': 0.4037896028960503, 'learning_rate': 6.8441848728581975e-06, 'epoch': 0.61}


 61%|██████▏   | 2363/3844 [4:52:08<2:46:46,  6.76s/it]

 61%|██████▏   | 2364/3844 [4:52:14<2:39:09,  6.45s/it]

 62%|██████▏   | 2365/3844 [4:52:20<2:35:57,  6.33s/it]
{'loss': 1.1754, 'grad_norm': 0.4254542613557355, 'learning_rate': 6.812217320655623e-06, 'epoch': 0.62}


 62%|██████▏   | 2367/3844 [4:52:36<2:58:48,  7.26s/it]
{'loss': 1.1588, 'grad_norm': 0.3841670968904014, 'learning_rate': 6.796247104611755e-06, 'epoch': 0.62}

 62%|██████▏   | 2368/3844 [4:52:42<2:53:01,  7.03s/it]


 62%|██████▏   | 2370/3844 [4:52:58<3:03:25,  7.47s/it]
{'loss': 1.2369, 'grad_norm': 0.3843300244329242, 'learning_rate': 6.772308858215118e-06, 'epoch': 0.62}


 62%|██████▏   | 2372/3844 [4:53:14<3:13:03,  7.87s/it]
{'loss': 1.155, 'grad_norm': 0.3786960711878911, 'learning_rate': 6.756361478506579e-06, 'epoch': 0.62}

 62%|██████▏   | 2373/3844 [4:53:21<3:10:12,  7.76s/it]

 62%|██████▏   | 2374/3844 [4:53:29<3:10:43,  7.78s/it]

 62%|██████▏   | 2375/3844 [4:53:36<3:00:25,  7.37s/it]

 62%|██████▏   | 2376/3844 [4:53:41<2:46:31,  6.81s/it]

 62%|██████▏   | 2377/3844 [4:53:48<2:44:41,  6.74s/it]

 62%|██████▏   | 2378/3844 [4:53:59<3:14:26,  7.96s/it]

 62%|██████▏   | 2379/3844 [4:54:07<3:16:05,  8.03s/it]

 62%|██████▏   | 2380/3844 [4:54:13<3:03:30,  7.52s/it]

 62%|██████▏   | 2381/3844 [4:54:21<3:07:56,  7.71s/it]

 62%|██████▏   | 2382/3844 [4:54:27<2:50:29,  7.00s/it]


 62%|██████▏   | 2384/3844 [4:54:44<3:08:58,  7.77s/it]

 62%|██████▏   | 2385/3844 [4:54:50<2:57:49,  7.31s/it]
{'loss': 1.1679, 'grad_norm': 0.4258252059851464, 'learning_rate': 6.652930119391752e-06, 'epoch': 0.62}

 62%|██████▏   | 2386/3844 [4:54:56<2:49:27,  6.97s/it]


 62%|██████▏   | 2388/3844 [4:55:12<3:03:24,  7.56s/it]
{'loss': 1.2497, 'grad_norm': 0.37961653675491097, 'learning_rate': 6.6291179384423674e-06, 'epoch': 0.62}

 62%|██████▏   | 2389/3844 [4:55:20<3:07:10,  7.72s/it]


 62%|██████▏   | 2391/3844 [4:55:32<2:43:21,  6.75s/it]

 62%|██████▏   | 2392/3844 [4:55:38<2:39:53,  6.61s/it]
{'loss': 1.1457, 'grad_norm': 0.3875486735863408, 'learning_rate': 6.597401903590543e-06, 'epoch': 0.62}

 62%|██████▏   | 2393/3844 [4:55:46<2:44:18,  6.79s/it]

 62%|██████▏   | 2394/3844 [4:55:53<2:49:00,  6.99s/it]

 62%|██████▏   | 2395/3844 [4:56:02<3:01:59,  7.54s/it]

 62%|██████▏   | 2396/3844 [4:56:08<2:50:39,  7.07s/it]

 62%|██████▏   | 2397/3844 [4:56:14<2:44:34,  6.82s/it]

 62%|██████▏   | 2398/3844 [4:56:21<2:42:05,  6.73s/it]

 62%|██████▏   | 2399/3844 [4:56:27<2:42:16,  6.74s/it]

 62%|██████▏   | 2400/3844 [4:56:34<2:42:17,  6.74s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1112, 'grad_norm': 0.429570238864017, 'learning_rate': 6.526182729395906e-06, 'epoch': 0.62}
 62%|██████▏   | 2401/3844 [4:57:05<5:35:18, 13.94s/it]

 62%|██████▏   | 2402/3844 [4:57:11<4:37:11, 11.53s/it]

 63%|██████▎   | 2403/3844 [4:57:17<4:01:42, 10.06s/it]

 63%|██████▎   | 2404/3844 [4:57:23<3:28:44,  8.70s/it]

 63%|██████▎   | 2405/3844 [4:57:31<3:21:36,  8.41s/it]

 63%|██████▎   | 2406/3844 [4:57:40<3:30:13,  8.77s/it]

 63%|██████▎   | 2407/3844 [4:57:51<3:41:36,  9.25s/it]

 63%|██████▎   | 2408/3844 [4:57:58<3:29:28,  8.75s/it]

 63%|██████▎   | 2409/3844 [4:58:07<3:31:55,  8.86s/it]

 63%|██████▎   | 2410/3844 [4:58:14<3:13:31,  8.10s/it]

 63%|██████▎   | 2411/3844 [4:58:23<3:23:47,  8.53s/it]

 63%|██████▎   | 2412/3844 [4:58:30<3:11:08,  8.01s/it]

 63%|██████▎   | 2413/3844 [4:58:36<2:56:06,  7.38s/it]

 63%|██████▎   | 2414/3844 [4:58:43<2:52:09,  7.22s/it]

 63%|██████▎   | 2415/3844 [4:58:49<2:42:06,  6.81s/it]

 63%|██████▎   | 2416/3844 [4:58:56<2:46:26,  6.99s/it]


 63%|██████▎   | 2418/3844 [4:59:10<2:43:39,  6.89s/it]
{'loss': 1.3485, 'grad_norm': 0.43401988734692, 'learning_rate': 6.392206111841116e-06, 'epoch': 0.63}


 63%|██████▎   | 2420/3844 [4:59:24<2:45:52,  6.99s/it]
{'loss': 1.0771, 'grad_norm': 0.3933804168078383, 'learning_rate': 6.3764923050439e-06, 'epoch': 0.63}

 63%|██████▎   | 2421/3844 [4:59:33<2:55:58,  7.42s/it]

 63%|██████▎   | 2422/3844 [4:59:40<2:54:05,  7.35s/it]

 63%|██████▎   | 2423/3844 [4:59:48<2:57:03,  7.48s/it]

 63%|██████▎   | 2424/3844 [4:59:59<3:24:11,  8.63s/it]

 63%|██████▎   | 2425/3844 [5:00:08<3:29:38,  8.86s/it]


 63%|██████▎   | 2427/3844 [5:00:22<3:05:59,  7.88s/it]
{'loss': 1.149, 'grad_norm': 0.4052437530603912, 'learning_rate': 6.321575330365493e-06, 'epoch': 0.63}

 63%|██████▎   | 2428/3844 [5:00:32<3:21:49,  8.55s/it]

 63%|██████▎   | 2429/3844 [5:00:43<3:35:17,  9.13s/it]

 63%|██████▎   | 2430/3844 [5:00:50<3:21:15,  8.54s/it]

 63%|██████▎   | 2431/3844 [5:00:57<3:09:06,  8.03s/it]

 63%|██████▎   | 2432/3844 [5:01:05<3:08:56,  8.03s/it]

 63%|██████▎   | 2433/3844 [5:01:11<2:58:22,  7.58s/it]

 63%|██████▎   | 2434/3844 [5:01:20<3:04:23,  7.85s/it]

 63%|██████▎   | 2435/3844 [5:01:25<2:47:25,  7.13s/it]

 63%|██████▎   | 2436/3844 [5:01:32<2:45:20,  7.05s/it]

 63%|██████▎   | 2437/3844 [5:01:38<2:35:33,  6.63s/it]


 63%|██████▎   | 2439/3844 [5:01:54<2:58:12,  7.61s/it]
{'loss': 1.1133, 'grad_norm': 0.38865371686092176, 'learning_rate': 6.227730793711044e-06, 'epoch': 0.63}

 63%|██████▎   | 2440/3844 [5:02:02<2:56:16,  7.53s/it]

 64%|██████▎   | 2441/3844 [5:02:08<2:49:36,  7.25s/it]

 64%|██████▎   | 2442/3844 [5:02:14<2:36:22,  6.69s/it]

 64%|██████▎   | 2443/3844 [5:02:20<2:34:34,  6.62s/it]

 64%|██████▎   | 2444/3844 [5:02:26<2:29:42,  6.42s/it]

 64%|██████▎   | 2445/3844 [5:02:33<2:32:18,  6.53s/it]

 64%|██████▎   | 2446/3844 [5:02:39<2:26:47,  6.30s/it]

 64%|██████▎   | 2447/3844 [5:02:47<2:41:05,  6.92s/it]

 64%|██████▎   | 2448/3844 [5:02:57<3:04:14,  7.92s/it]

 64%|██████▎   | 2449/3844 [5:03:04<2:52:46,  7.43s/it]

 64%|██████▎   | 2450/3844 [5:03:10<2:44:24,  7.08s/it]

 64%|██████▍   | 2451/3844 [5:03:16<2:40:16,  6.90s/it]

 64%|██████▍   | 2452/3844 [5:03:24<2:46:58,  7.20s/it]

 64%|██████▍   | 2453/3844 [5:03:30<2:36:26,  6.75s/it]

 64%|██████▍   | 2454/3844 [5:03:38<2:47:34,  7.23s/it]

 64%|██████▍   | 2455/3844 [5:03:44<2:34:39,  6.68s/it]

 64%|██████▍   | 2456/3844 [5:03:50<2:31:12,  6.54s/it]

 64%|██████▍   | 2457/3844 [5:03:55<2:25:06,  6.28s/it]

 64%|██████▍   | 2458/3844 [5:04:03<2:32:03,  6.58s/it]

 64%|██████▍   | 2459/3844 [5:04:10<2:34:33,  6.70s/it]

 64%|██████▍   | 2460/3844 [5:04:16<2:33:19,  6.65s/it]

 64%|██████▍   | 2461/3844 [5:04:22<2:25:28,  6.31s/it]

 64%|██████▍   | 2462/3844 [5:04:29<2:32:54,  6.64s/it]

 64%|██████▍   | 2463/3844 [5:04:37<2:40:36,  6.98s/it]

 64%|██████▍   | 2464/3844 [5:04:44<2:43:06,  7.09s/it]

 64%|██████▍   | 2465/3844 [5:04:53<2:52:10,  7.49s/it]

 64%|██████▍   | 2466/3844 [5:05:01<2:59:28,  7.81s/it]

 64%|██████▍   | 2467/3844 [5:05:07<2:46:58,  7.28s/it]

 64%|██████▍   | 2468/3844 [5:05:16<2:53:51,  7.58s/it]

 64%|██████▍   | 2469/3844 [5:05:25<3:07:20,  8.17s/it]

 64%|██████▍   | 2470/3844 [5:05:32<2:55:15,  7.65s/it]

 64%|██████▍   | 2471/3844 [5:05:37<2:41:45,  7.07s/it]

 64%|██████▍   | 2472/3844 [5:05:44<2:37:01,  6.87s/it]

 64%|██████▍   | 2473/3844 [5:05:49<2:27:15,  6.44s/it]

 64%|██████▍   | 2474/3844 [5:05:56<2:27:39,  6.47s/it]

 64%|██████▍   | 2475/3844 [5:06:01<2:18:56,  6.09s/it]

 64%|██████▍   | 2476/3844 [5:06:07<2:20:30,  6.16s/it]

 64%|██████▍   | 2477/3844 [5:06:18<2:53:06,  7.60s/it]

 64%|██████▍   | 2478/3844 [5:06:26<2:54:46,  7.68s/it]

 64%|██████▍   | 2479/3844 [5:06:32<2:43:57,  7.21s/it]

 65%|██████▍   | 2480/3844 [5:06:41<2:56:44,  7.77s/it]

 65%|██████▍   | 2481/3844 [5:06:47<2:40:56,  7.08s/it]

 65%|██████▍   | 2482/3844 [5:06:53<2:35:15,  6.84s/it]

 65%|██████▍   | 2483/3844 [5:06:59<2:32:28,  6.72s/it]

 65%|██████▍   | 2484/3844 [5:07:07<2:35:35,  6.86s/it]

 65%|██████▍   | 2485/3844 [5:07:12<2:27:28,  6.51s/it]

 65%|██████▍   | 2486/3844 [5:07:19<2:26:31,  6.47s/it]

 65%|██████▍   | 2487/3844 [5:07:25<2:24:12,  6.38s/it]

 65%|██████▍   | 2488/3844 [5:07:32<2:27:55,  6.55s/it]

 65%|██████▍   | 2489/3844 [5:07:38<2:28:37,  6.58s/it]

 65%|██████▍   | 2490/3844 [5:07:44<2:22:53,  6.33s/it]

 65%|██████▍   | 2491/3844 [5:07:54<2:45:37,  7.34s/it]

 65%|██████▍   | 2492/3844 [5:08:02<2:52:29,  7.65s/it]

 65%|██████▍   | 2493/3844 [5:08:11<2:57:42,  7.89s/it]
[2024-05-26 00:50:02,413] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▍   | 2494/3844 [5:08:21<3:12:21,  8.55s/it]

 65%|██████▍   | 2495/3844 [5:08:26<2:52:31,  7.67s/it]

 65%|██████▍   | 2496/3844 [5:08:33<2:43:49,  7.29s/it]

 65%|██████▍   | 2497/3844 [5:08:41<2:46:47,  7.43s/it]

 65%|██████▍   | 2498/3844 [5:08:48<2:49:01,  7.53s/it]

 65%|██████▌   | 2499/3844 [5:08:54<2:34:31,  6.89s/it]

 65%|██████▌   | 2500/3844 [5:09:02<2:41:27,  7.21s/it]

 65%|██████▌   | 2501/3844 [5:09:09<2:43:29,  7.30s/it]

 65%|██████▌   | 2502/3844 [5:09:16<2:42:11,  7.25s/it]

 65%|██████▌   | 2503/3844 [5:09:23<2:38:36,  7.10s/it]

 65%|██████▌   | 2504/3844 [5:09:30<2:35:30,  6.96s/it]

 65%|██████▌   | 2505/3844 [5:09:37<2:33:44,  6.89s/it]

 65%|██████▌   | 2506/3844 [5:09:42<2:26:17,  6.56s/it]


 65%|██████▌   | 2508/3844 [5:09:54<2:19:42,  6.27s/it]

 65%|██████▌   | 2509/3844 [5:10:00<2:13:42,  6.01s/it]

 65%|██████▌   | 2510/3844 [5:10:13<2:59:57,  8.09s/it]

 65%|██████▌   | 2511/3844 [5:10:20<2:52:50,  7.78s/it]

 65%|██████▌   | 2512/3844 [5:10:27<2:48:01,  7.57s/it]

 65%|██████▌   | 2513/3844 [5:10:34<2:45:59,  7.48s/it]

 65%|██████▌   | 2514/3844 [5:10:43<2:54:17,  7.86s/it]

 65%|██████▌   | 2515/3844 [5:10:54<3:17:11,  8.90s/it]

 65%|██████▌   | 2516/3844 [5:11:01<3:02:54,  8.26s/it]

 65%|██████▌   | 2517/3844 [5:11:06<2:44:49,  7.45s/it]
{'loss': 1.1738, 'grad_norm': 0.41528522572577603, 'learning_rate': 5.627569054048731e-06, 'epoch': 0.65}


 66%|██████▌   | 2519/3844 [5:11:21<2:42:59,  7.38s/it]

 66%|██████▌   | 2520/3844 [5:11:29<2:44:23,  7.45s/it]
{'loss': 1.1319, 'grad_norm': 0.3808978948456862, 'learning_rate': 5.604846700568458e-06, 'epoch': 0.66}


 66%|██████▌   | 2522/3844 [5:11:41<2:29:38,  6.79s/it]

 66%|██████▌   | 2523/3844 [5:11:46<2:19:53,  6.35s/it]

 66%|██████▌   | 2524/3844 [5:11:52<2:13:04,  6.05s/it]
{'loss': 1.214, 'grad_norm': 0.45414561631579653, 'learning_rate': 5.574593951150016e-06, 'epoch': 0.66}


 66%|██████▌   | 2526/3844 [5:12:03<2:08:40,  5.86s/it]

 66%|██████▌   | 2527/3844 [5:12:12<2:28:59,  6.79s/it]
{'loss': 1.2008, 'grad_norm': 0.3933767050630895, 'learning_rate': 5.551937368486492e-06, 'epoch': 0.66}


 66%|██████▌   | 2529/3844 [5:12:23<2:12:40,  6.05s/it]

 66%|██████▌   | 2530/3844 [5:12:29<2:14:10,  6.13s/it]

 66%|██████▌   | 2531/3844 [5:12:35<2:13:35,  6.10s/it]

 66%|██████▌   | 2532/3844 [5:12:41<2:11:11,  6.00s/it]

 66%|██████▌   | 2533/3844 [5:12:48<2:16:05,  6.23s/it]

 66%|██████▌   | 2534/3844 [5:12:56<2:28:34,  6.80s/it]
{'loss': 1.2668, 'grad_norm': 0.45250832137325364, 'learning_rate': 5.499182815980461e-06, 'epoch': 0.66}


 66%|██████▌   | 2536/3844 [5:13:07<2:14:42,  6.18s/it]

 66%|██████▌   | 2537/3844 [5:13:14<2:18:29,  6.36s/it]
{'loss': 1.1784, 'grad_norm': 0.4122086167544207, 'learning_rate': 5.476621558656485e-06, 'epoch': 0.66}


 66%|██████▌   | 2539/3844 [5:13:29<2:31:26,  6.96s/it]

 66%|██████▌   | 2540/3844 [5:13:37<2:38:28,  7.29s/it]

 66%|██████▌   | 2541/3844 [5:13:45<2:42:08,  7.47s/it]

 66%|██████▌   | 2542/3844 [5:13:51<2:31:48,  7.00s/it]

 66%|██████▌   | 2543/3844 [5:14:01<2:54:15,  8.04s/it]
{'loss': 1.1714, 'grad_norm': 0.39144096450590316, 'learning_rate': 5.431585919028031e-06, 'epoch': 0.66}


 66%|██████▌   | 2545/3844 [5:14:14<2:39:11,  7.35s/it]

 66%|██████▌   | 2546/3844 [5:14:20<2:25:39,  6.73s/it]

 66%|██████▋   | 2547/3844 [5:14:31<2:54:34,  8.08s/it]

 66%|██████▋   | 2548/3844 [5:14:40<3:02:52,  8.47s/it]

 66%|██████▋   | 2549/3844 [5:14:48<2:56:23,  8.17s/it]
{'loss': 1.2008, 'grad_norm': 0.41792775938654336, 'learning_rate': 5.38666707190501e-06, 'epoch': 0.66}


 66%|██████▋   | 2551/3844 [5:15:04<2:54:43,  8.11s/it]

 66%|██████▋   | 2552/3844 [5:15:10<2:43:03,  7.57s/it]

 66%|██████▋   | 2553/3844 [5:15:16<2:35:27,  7.23s/it]

 66%|██████▋   | 2554/3844 [5:15:25<2:43:55,  7.62s/it]

 66%|██████▋   | 2555/3844 [5:15:33<2:46:43,  7.76s/it]
{'loss': 1.1365, 'grad_norm': 0.42738262481792916, 'learning_rate': 5.341866165647572e-06, 'epoch': 0.66}


 67%|██████▋   | 2557/3844 [5:15:48<2:46:51,  7.78s/it]

 67%|██████▋   | 2558/3844 [5:15:55<2:38:26,  7.39s/it]

 67%|██████▋   | 2559/3844 [5:16:01<2:29:21,  6.97s/it]

 67%|██████▋   | 2560/3844 [5:16:08<2:30:19,  7.02s/it]

 67%|██████▋   | 2561/3844 [5:16:13<2:19:37,  6.53s/it]

 67%|██████▋   | 2562/3844 [5:16:19<2:16:34,  6.39s/it]
{'loss': 1.319, 'grad_norm': 0.4210234252833682, 'learning_rate': 5.289749033671542e-06, 'epoch': 0.67}


 67%|██████▋   | 2564/3844 [5:16:34<2:26:27,  6.87s/it]

 67%|██████▋   | 2565/3844 [5:16:40<2:19:53,  6.56s/it]

 67%|██████▋   | 2566/3844 [5:16:47<2:20:55,  6.62s/it]

 67%|██████▋   | 2567/3844 [5:16:54<2:24:55,  6.81s/it]

 67%|██████▋   | 2568/3844 [5:17:02<2:33:11,  7.20s/it]

 67%|██████▋   | 2569/3844 [5:17:09<2:28:49,  7.00s/it]

 67%|██████▋   | 2570/3844 [5:17:16<2:30:18,  7.08s/it]

 67%|██████▋   | 2571/3844 [5:17:22<2:25:24,  6.85s/it]

 67%|██████▋   | 2572/3844 [5:17:28<2:18:55,  6.55s/it]

 67%|██████▋   | 2573/3844 [5:17:35<2:21:38,  6.69s/it]

 67%|██████▋   | 2574/3844 [5:17:44<2:38:12,  7.47s/it]

 67%|██████▋   | 2575/3844 [5:17:53<2:44:59,  7.80s/it]
{'loss': 1.0887, 'grad_norm': 0.4261817117631897, 'learning_rate': 5.193396289399788e-06, 'epoch': 0.67}


 67%|██████▋   | 2577/3844 [5:18:07<2:34:14,  7.30s/it]

 67%|██████▋   | 2578/3844 [5:18:12<2:22:02,  6.73s/it]

 67%|██████▋   | 2579/3844 [5:18:17<2:12:48,  6.30s/it]

 67%|██████▋   | 2580/3844 [5:18:23<2:10:58,  6.22s/it]

 67%|██████▋   | 2581/3844 [5:18:29<2:07:10,  6.04s/it]

 67%|██████▋   | 2582/3844 [5:18:34<2:03:49,  5.89s/it]

 67%|██████▋   | 2583/3844 [5:18:43<2:21:03,  6.71s/it]

 67%|██████▋   | 2584/3844 [5:18:52<2:37:45,  7.51s/it]
{'loss': 1.1508, 'grad_norm': 0.4037117217280671, 'learning_rate': 5.127027773882699e-06, 'epoch': 0.67}


 67%|██████▋   | 2586/3844 [5:19:06<2:28:48,  7.10s/it]

 67%|██████▋   | 2587/3844 [5:19:12<2:21:51,  6.77s/it]

 67%|██████▋   | 2588/3844 [5:19:18<2:18:34,  6.62s/it]
{'loss': 1.2856, 'grad_norm': 0.40106464475985115, 'learning_rate': 5.097620402725085e-06, 'epoch': 0.67}


 67%|██████▋   | 2590/3844 [5:19:33<2:25:08,  6.94s/it]

 67%|██████▋   | 2591/3844 [5:19:40<2:22:36,  6.83s/it]

 67%|██████▋   | 2592/3844 [5:19:47<2:21:04,  6.76s/it]

 67%|██████▋   | 2593/3844 [5:19:54<2:27:16,  7.06s/it]

 67%|██████▋   | 2594/3844 [5:20:02<2:33:38,  7.38s/it]
{'loss': 1.1047, 'grad_norm': 0.39080982562207733, 'learning_rate': 5.053613892362698e-06, 'epoch': 0.67}


 68%|██████▊   | 2596/3844 [5:20:21<2:53:43,  8.35s/it]

 68%|██████▊   | 2597/3844 [5:20:28<2:42:41,  7.83s/it]

 68%|██████▊   | 2598/3844 [5:20:34<2:33:46,  7.40s/it]

 68%|██████▊   | 2599/3844 [5:20:40<2:25:05,  6.99s/it]
{'loss': 1.0233, 'grad_norm': 0.38547447545656877, 'learning_rate': 5.017038350654268e-06, 'epoch': 0.68}


 68%|██████▊   | 2601/3844 [5:21:00<3:04:44,  8.92s/it]

 68%|██████▊   | 2602/3844 [5:21:07<2:50:17,  8.23s/it]

 68%|██████▊   | 2603/3844 [5:21:12<2:32:44,  7.38s/it]

 68%|██████▊   | 2604/3844 [5:21:20<2:37:21,  7.61s/it]

 68%|██████▊   | 2605/3844 [5:21:28<2:38:31,  7.68s/it]

 68%|██████▊   | 2606/3844 [5:21:34<2:25:13,  7.04s/it]

 68%|██████▊   | 2607/3844 [5:21:40<2:22:51,  6.93s/it]
{'loss': 1.0202, 'grad_norm': 0.42149201794584723, 'learning_rate': 4.958701762088801e-06, 'epoch': 0.68}


 68%|██████▊   | 2609/3844 [5:21:54<2:26:25,  7.11s/it]

 68%|██████▊   | 2610/3844 [5:22:00<2:14:41,  6.55s/it]

 68%|██████▊   | 2611/3844 [5:22:06<2:14:06,  6.53s/it]

 68%|██████▊   | 2612/3844 [5:22:14<2:20:07,  6.82s/it]

 68%|██████▊   | 2613/3844 [5:22:19<2:10:34,  6.36s/it]

 68%|██████▊   | 2614/3844 [5:22:27<2:21:34,  6.91s/it]

 68%|██████▊   | 2615/3844 [5:22:37<2:39:22,  7.78s/it]

 68%|██████▊   | 2616/3844 [5:22:44<2:37:09,  7.68s/it]
{'loss': 1.148, 'grad_norm': 0.43019676304979965, 'learning_rate': 4.893347105165468e-06, 'epoch': 0.68}


 68%|██████▊   | 2618/3844 [5:22:59<2:38:16,  7.75s/it]

 68%|██████▊   | 2619/3844 [5:23:06<2:28:16,  7.26s/it]

 68%|██████▊   | 2620/3844 [5:23:11<2:17:14,  6.73s/it]

 68%|██████▊   | 2621/3844 [5:23:17<2:10:57,  6.42s/it]

 68%|██████▊   | 2622/3844 [5:23:26<2:25:54,  7.16s/it]

 68%|██████▊   | 2623/3844 [5:23:35<2:38:28,  7.79s/it]

 68%|██████▊   | 2624/3844 [5:23:40<2:24:56,  7.13s/it]

 68%|██████▊   | 2625/3844 [5:23:47<2:20:58,  6.94s/it]

 68%|██████▊   | 2626/3844 [5:23:55<2:30:00,  7.39s/it]

 68%|██████▊   | 2627/3844 [5:24:03<2:31:53,  7.49s/it]

 68%|██████▊   | 2628/3844 [5:24:10<2:27:26,  7.27s/it]

 68%|██████▊   | 2629/3844 [5:24:17<2:27:23,  7.28s/it]

 68%|██████▊   | 2630/3844 [5:24:24<2:22:37,  7.05s/it]

 68%|██████▊   | 2631/3844 [5:24:31<2:22:16,  7.04s/it]

 68%|██████▊   | 2632/3844 [5:24:39<2:29:28,  7.40s/it]

 68%|██████▊   | 2633/3844 [5:24:47<2:35:17,  7.69s/it]

 69%|██████▊   | 2634/3844 [5:24:57<2:44:31,  8.16s/it]

 69%|██████▊   | 2635/3844 [5:25:02<2:29:44,  7.43s/it]

 69%|██████▊   | 2636/3844 [5:25:08<2:22:01,  7.05s/it]
{'loss': 1.1456, 'grad_norm': 0.41673014839346584, 'learning_rate': 4.749171663302124e-06, 'epoch': 0.69}


 69%|██████▊   | 2638/3844 [5:25:23<2:28:28,  7.39s/it]

 69%|██████▊   | 2639/3844 [5:25:30<2:26:56,  7.32s/it]

 69%|██████▊   | 2640/3844 [5:25:38<2:25:31,  7.25s/it]

 69%|██████▊   | 2641/3844 [5:25:47<2:39:46,  7.97s/it]

 69%|██████▊   | 2642/3844 [5:25:58<2:58:49,  8.93s/it]

 69%|██████▉   | 2643/3844 [5:26:05<2:47:42,  8.38s/it]

 69%|██████▉   | 2644/3844 [5:26:13<2:40:53,  8.04s/it]

 69%|██████▉   | 2645/3844 [5:26:20<2:34:10,  7.72s/it]

 69%|██████▉   | 2646/3844 [5:26:25<2:20:57,  7.06s/it]

 69%|██████▉   | 2647/3844 [5:26:31<2:10:59,  6.57s/it]

 69%|██████▉   | 2648/3844 [5:26:39<2:22:33,  7.15s/it]

 69%|██████▉   | 2649/3844 [5:26:47<2:28:30,  7.46s/it]

 69%|██████▉   | 2650/3844 [5:26:53<2:18:53,  6.98s/it]

 69%|██████▉   | 2651/3844 [5:26:59<2:13:44,  6.73s/it]
{'loss': 1.1657, 'grad_norm': 0.4105517309127573, 'learning_rate': 4.642016659950241e-06, 'epoch': 0.69}


 69%|██████▉   | 2653/3844 [5:27:15<2:26:36,  7.39s/it]

 69%|██████▉   | 2654/3844 [5:27:24<2:35:41,  7.85s/it]

 69%|██████▉   | 2655/3844 [5:27:32<2:34:21,  7.79s/it]

 69%|██████▉   | 2656/3844 [5:27:38<2:23:32,  7.25s/it]

 69%|██████▉   | 2657/3844 [5:27:45<2:21:41,  7.16s/it]

 69%|██████▉   | 2658/3844 [5:27:51<2:14:51,  6.82s/it]

 69%|██████▉   | 2659/3844 [5:27:57<2:12:12,  6.69s/it]

 69%|██████▉   | 2660/3844 [5:28:04<2:14:43,  6.83s/it]

 69%|██████▉   | 2661/3844 [5:28:11<2:14:49,  6.84s/it]
{'loss': 1.2434, 'grad_norm': 0.40163854894703027, 'learning_rate': 4.571054552569361e-06, 'epoch': 0.69}


 69%|██████▉   | 2663/3844 [5:28:26<2:19:12,  7.07s/it]

 69%|██████▉   | 2664/3844 [5:28:32<2:15:39,  6.90s/it]

 69%|██████▉   | 2665/3844 [5:28:41<2:27:25,  7.50s/it]

 69%|██████▉   | 2666/3844 [5:28:49<2:27:54,  7.53s/it]
{'loss': 1.0514, 'grad_norm': 0.4156335329959635, 'learning_rate': 4.535717759624677e-06, 'epoch': 0.69}


 69%|██████▉   | 2668/3844 [5:29:09<2:56:07,  8.99s/it]

 69%|██████▉   | 2669/3844 [5:29:17<2:48:08,  8.59s/it]
{'loss': 1.1503, 'grad_norm': 0.4179394969304733, 'learning_rate': 4.514562208999185e-06, 'epoch': 0.69}


 69%|██████▉   | 2671/3844 [5:29:34<2:47:49,  8.58s/it]

 70%|██████▉   | 2672/3844 [5:29:41<2:42:04,  8.30s/it]

 70%|██████▉   | 2673/3844 [5:29:47<2:26:02,  7.48s/it]

 70%|██████▉   | 2674/3844 [5:29:53<2:15:59,  6.97s/it]
{'loss': 1.2906, 'grad_norm': 0.41063300347117365, 'learning_rate': 4.4793809338716986e-06, 'epoch': 0.7}


 70%|██████▉   | 2676/3844 [5:30:04<2:02:46,  6.31s/it]

 70%|██████▉   | 2677/3844 [5:30:15<2:30:01,  7.71s/it]

 70%|██████▉   | 2678/3844 [5:30:22<2:24:34,  7.44s/it]

 70%|██████▉   | 2679/3844 [5:30:27<2:12:36,  6.83s/it]

 70%|██████▉   | 2680/3844 [5:30:36<2:23:07,  7.38s/it]

 70%|██████▉   | 2681/3844 [5:30:41<2:11:28,  6.78s/it]

 70%|██████▉   | 2682/3844 [5:30:47<2:06:15,  6.52s/it]

 70%|██████▉   | 2683/3844 [5:30:53<2:02:59,  6.36s/it]

 70%|██████▉   | 2684/3844 [5:31:04<2:28:54,  7.70s/it]

 70%|██████▉   | 2685/3844 [5:31:10<2:16:43,  7.08s/it]
{'loss': 1.1637, 'grad_norm': 0.3874193001315548, 'learning_rate': 4.402328004378992e-06, 'epoch': 0.7}


 70%|██████▉   | 2687/3844 [5:31:23<2:11:37,  6.83s/it]

 70%|██████▉   | 2688/3844 [5:31:30<2:10:50,  6.79s/it]

 70%|██████▉   | 2689/3844 [5:31:35<2:05:17,  6.51s/it]

 70%|██████▉   | 2690/3844 [5:31:43<2:10:44,  6.80s/it]

 70%|███████   | 2691/3844 [5:31:49<2:07:49,  6.65s/it]

 70%|███████   | 2692/3844 [5:31:55<2:04:32,  6.49s/it]

 70%|███████   | 2693/3844 [5:32:02<2:03:01,  6.41s/it]

 70%|███████   | 2694/3844 [5:32:08<2:03:48,  6.46s/it]

 70%|███████   | 2695/3844 [5:32:15<2:07:23,  6.65s/it]

 70%|███████   | 2696/3844 [5:32:23<2:13:48,  6.99s/it]
{'loss': 1.2647, 'grad_norm': 0.37066072787259324, 'learning_rate': 4.325756066656029e-06, 'epoch': 0.7}

 70%|███████   | 2697/3844 [5:32:31<2:16:42,  7.15s/it]


 70%|███████   | 2699/3844 [5:32:45<2:18:21,  7.25s/it]
{'loss': 1.2322, 'grad_norm': 0.41845952872337955, 'learning_rate': 4.3049571549078225e-06, 'epoch': 0.7}

 70%|███████   | 2700/3844 [5:32:53<2:17:52,  7.23s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|███████   | 2701/3844 [5:33:23<4:32:02, 14.28s/it]

 70%|███████   | 2702/3844 [5:33:30<3:48:12, 11.99s/it]

 70%|███████   | 2703/3844 [5:33:36<3:16:07, 10.31s/it]

 70%|███████   | 2704/3844 [5:33:44<2:58:28,  9.39s/it]

 70%|███████   | 2705/3844 [5:33:51<2:46:50,  8.79s/it]

 70%|███████   | 2706/3844 [5:33:57<2:30:12,  7.92s/it]

 70%|███████   | 2707/3844 [5:34:05<2:32:53,  8.07s/it]

 70%|███████   | 2708/3844 [5:34:13<2:32:26,  8.05s/it]

 70%|███████   | 2709/3844 [5:34:20<2:23:57,  7.61s/it]

 70%|███████   | 2710/3844 [5:34:27<2:23:01,  7.57s/it]

 71%|███████   | 2711/3844 [5:34:34<2:17:40,  7.29s/it]

 71%|███████   | 2712/3844 [5:34:44<2:33:16,  8.12s/it]

 71%|███████   | 2713/3844 [5:34:50<2:20:21,  7.45s/it]

 71%|███████   | 2714/3844 [5:34:56<2:12:09,  7.02s/it]

 71%|███████   | 2715/3844 [5:35:02<2:05:33,  6.67s/it]

 71%|███████   | 2716/3844 [5:35:10<2:12:40,  7.06s/it]

 71%|███████   | 2717/3844 [5:35:18<2:20:52,  7.50s/it]
{'loss': 1.0865, 'grad_norm': 0.4090336430057568, 'learning_rate': 4.1809326957771356e-06, 'epoch': 0.71}

 71%|███████   | 2718/3844 [5:35:25<2:15:20,  7.21s/it]


 71%|███████   | 2720/3844 [5:35:37<2:06:15,  6.74s/it]

 71%|███████   | 2721/3844 [5:35:47<2:24:04,  7.70s/it]

 71%|███████   | 2722/3844 [5:35:55<2:26:21,  7.83s/it]

 71%|███████   | 2723/3844 [5:36:02<2:18:42,  7.42s/it]

 71%|███████   | 2724/3844 [5:36:11<2:25:33,  7.80s/it]
{'loss': 1.2138, 'grad_norm': 0.4121794760452142, 'learning_rate': 4.133060979093623e-06, 'epoch': 0.71}

 71%|███████   | 2725/3844 [5:36:19<2:27:50,  7.93s/it]


 71%|███████   | 2727/3844 [5:36:30<2:06:14,  6.78s/it]

 71%|███████   | 2728/3844 [5:36:36<1:58:53,  6.39s/it]

 71%|███████   | 2729/3844 [5:36:42<1:57:38,  6.33s/it]

 71%|███████   | 2730/3844 [5:36:48<1:58:11,  6.37s/it]

 71%|███████   | 2731/3844 [5:36:54<1:55:57,  6.25s/it]

 71%|███████   | 2732/3844 [5:37:01<1:59:05,  6.43s/it]
{'loss': 1.2355, 'grad_norm': 0.388564157872883, 'learning_rate': 4.07860052498734e-06, 'epoch': 0.71}


 71%|███████   | 2734/3844 [5:37:16<2:05:49,  6.80s/it]

 71%|███████   | 2735/3844 [5:37:22<1:59:17,  6.45s/it]

 71%|███████   | 2736/3844 [5:37:28<1:57:49,  6.38s/it]

 71%|███████   | 2737/3844 [5:37:34<1:57:32,  6.37s/it]

 71%|███████   | 2738/3844 [5:37:43<2:11:52,  7.15s/it]

 71%|███████▏  | 2739/3844 [5:37:52<2:18:44,  7.53s/it]

 71%|███████▏  | 2740/3844 [5:37:58<2:10:06,  7.07s/it]
{'loss': 1.2518, 'grad_norm': 0.41361127502137657, 'learning_rate': 4.024409193764944e-06, 'epoch': 0.71}


 71%|███████▏  | 2742/3844 [5:38:11<2:06:30,  6.89s/it]

 71%|███████▏  | 2743/3844 [5:38:22<2:28:01,  8.07s/it]

 71%|███████▏  | 2744/3844 [5:38:29<2:21:55,  7.74s/it]

 71%|███████▏  | 2745/3844 [5:38:41<2:41:25,  8.81s/it]

 71%|███████▏  | 2746/3844 [5:38:46<2:22:41,  7.80s/it]

 71%|███████▏  | 2747/3844 [5:38:58<2:47:01,  9.14s/it]

 71%|███████▏  | 2748/3844 [5:39:08<2:49:19,  9.27s/it]

 72%|███████▏  | 2749/3844 [5:39:14<2:30:32,  8.25s/it]

 72%|███████▏  | 2750/3844 [5:39:20<2:17:25,  7.54s/it]

 72%|███████▏  | 2751/3844 [5:39:28<2:22:45,  7.84s/it]

 72%|███████▏  | 2752/3844 [5:39:36<2:21:45,  7.79s/it]

 72%|███████▏  | 2753/3844 [5:39:48<2:47:37,  9.22s/it]

 72%|███████▏  | 2754/3844 [5:39:56<2:36:38,  8.62s/it]
{'loss': 1.0825, 'grad_norm': 0.40274977806242707, 'learning_rate': 3.9302293422072165e-06, 'epoch': 0.72}


 72%|███████▏  | 2756/3844 [5:40:12<2:33:17,  8.45s/it]

 72%|███████▏  | 2757/3844 [5:40:19<2:21:47,  7.83s/it]

 72%|███████▏  | 2758/3844 [5:40:26<2:20:28,  7.76s/it]

 72%|███████▏  | 2759/3844 [5:40:35<2:24:31,  7.99s/it]
{'loss': 1.1314, 'grad_norm': 0.3748542571633696, 'learning_rate': 3.896797746639283e-06, 'epoch': 0.72}


 72%|███████▏  | 2761/3844 [5:40:51<2:22:07,  7.87s/it]

 72%|███████▏  | 2762/3844 [5:41:03<2:43:56,  9.09s/it]
[2024-05-26 01:22:44,091] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 2763/3844 [5:41:10<2:34:51,  8.60s/it]

 72%|███████▏  | 2764/3844 [5:41:16<2:20:56,  7.83s/it]
{'loss': 1.2255, 'grad_norm': 0.39050989004834746, 'learning_rate': 3.863474505100792e-06, 'epoch': 0.72}


 72%|███████▏  | 2766/3844 [5:41:31<2:16:40,  7.61s/it]

 72%|███████▏  | 2767/3844 [5:41:36<2:04:37,  6.94s/it]
{'loss': 1.1564, 'grad_norm': 0.43076145340988325, 'learning_rate': 3.843532816301389e-06, 'epoch': 0.72}


 72%|███████▏  | 2769/3844 [5:41:52<2:11:08,  7.32s/it]
{'loss': 1.1249, 'grad_norm': 0.3992700870623757, 'learning_rate': 3.830260209200442e-06, 'epoch': 0.72}

 72%|███████▏  | 2770/3844 [5:41:59<2:10:22,  7.28s/it]

 72%|███████▏  | 2771/3844 [5:42:05<2:04:05,  6.94s/it]


 72%|███████▏  | 2773/3844 [5:42:21<2:15:36,  7.60s/it]

 72%|███████▏  | 2774/3844 [5:42:28<2:14:59,  7.57s/it]

 72%|███████▏  | 2775/3844 [5:42:34<2:02:58,  6.90s/it]

 72%|███████▏  | 2776/3844 [5:42:43<2:14:15,  7.54s/it]

 72%|███████▏  | 2777/3844 [5:42:49<2:08:54,  7.25s/it]

 72%|███████▏  | 2778/3844 [5:42:57<2:10:33,  7.35s/it]

 72%|███████▏  | 2779/3844 [5:43:04<2:09:49,  7.31s/it]

 72%|███████▏  | 2780/3844 [5:43:10<1:59:59,  6.77s/it]

 72%|███████▏  | 2781/3844 [5:43:16<1:57:34,  6.64s/it]
{'loss': 1.0894, 'grad_norm': 0.4308721479030179, 'learning_rate': 3.7509939217808378e-06, 'epoch': 0.72}


 72%|███████▏  | 2783/3844 [5:43:30<1:57:34,  6.65s/it]

 72%|███████▏  | 2784/3844 [5:43:38<2:09:13,  7.31s/it]

 72%|███████▏  | 2785/3844 [5:43:48<2:22:47,  8.09s/it]

 72%|███████▏  | 2786/3844 [5:43:56<2:17:58,  7.82s/it]
{'loss': 1.1106, 'grad_norm': 0.3903549922006087, 'learning_rate': 3.718154440156847e-06, 'epoch': 0.72}

 73%|███████▎  | 2787/3844 [5:44:04<2:18:26,  7.86s/it]


 73%|███████▎  | 2789/3844 [5:44:15<1:58:19,  6.73s/it]

 73%|███████▎  | 2790/3844 [5:44:21<1:53:14,  6.45s/it]

 73%|███████▎  | 2791/3844 [5:44:27<1:51:48,  6.37s/it]

 73%|███████▎  | 2792/3844 [5:44:33<1:51:26,  6.36s/it]

 73%|███████▎  | 2793/3844 [5:44:41<1:58:56,  6.79s/it]

 73%|███████▎  | 2794/3844 [5:44:52<2:23:38,  8.21s/it]
[2024-05-26 01:26:34,056] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 2795/3844 [5:45:03<2:33:05,  8.76s/it]

 73%|███████▎  | 2796/3844 [5:45:12<2:39:17,  9.12s/it]

 73%|███████▎  | 2797/3844 [5:45:19<2:25:10,  8.32s/it]

 73%|███████▎  | 2798/3844 [5:45:25<2:11:36,  7.55s/it]

 73%|███████▎  | 2799/3844 [5:45:31<2:05:02,  7.18s/it]

 73%|███████▎  | 2800/3844 [5:45:38<2:04:14,  7.14s/it]

 73%|███████▎  | 2801/3844 [5:45:44<1:57:55,  6.78s/it]

 73%|███████▎  | 2802/3844 [5:45:53<2:06:58,  7.31s/it]

 73%|███████▎  | 2803/3844 [5:45:59<2:00:29,  6.94s/it]

 73%|███████▎  | 2804/3844 [5:46:05<1:55:37,  6.67s/it]

 73%|███████▎  | 2805/3844 [5:46:11<1:53:20,  6.55s/it]

 73%|███████▎  | 2806/3844 [5:46:16<1:48:04,  6.25s/it]

 73%|███████▎  | 2807/3844 [5:46:22<1:44:17,  6.03s/it]

 73%|███████▎  | 2808/3844 [5:46:32<2:03:37,  7.16s/it]

 73%|███████▎  | 2809/3844 [5:46:40<2:10:47,  7.58s/it]

 73%|███████▎  | 2810/3844 [5:46:47<2:05:40,  7.29s/it]

 73%|███████▎  | 2811/3844 [5:46:53<1:57:57,  6.85s/it]

 73%|███████▎  | 2812/3844 [5:46:59<1:54:17,  6.64s/it]

 73%|███████▎  | 2813/3844 [5:47:06<1:58:02,  6.87s/it]
{'loss': 1.2321, 'grad_norm': 0.4104295966521846, 'learning_rate': 3.542763084615334e-06, 'epoch': 0.73}


 73%|███████▎  | 2815/3844 [5:47:24<2:13:07,  7.76s/it]

 73%|███████▎  | 2816/3844 [5:47:30<2:06:17,  7.37s/it]

 73%|███████▎  | 2817/3844 [5:47:38<2:10:30,  7.62s/it]

 73%|███████▎  | 2818/3844 [5:47:44<2:00:59,  7.08s/it]

 73%|███████▎  | 2819/3844 [5:47:51<1:59:07,  6.97s/it]

 73%|███████▎  | 2820/3844 [5:47:57<1:55:37,  6.78s/it]
{'loss': 1.2062, 'grad_norm': 0.388465525239964, 'learning_rate': 3.497833292120499e-06, 'epoch': 0.73}


 73%|███████▎  | 2822/3844 [5:48:15<2:15:39,  7.96s/it]
{'loss': 1.005, 'grad_norm': 0.3730672558872235, 'learning_rate': 3.485037697672079e-06, 'epoch': 0.73}


 73%|███████▎  | 2824/3844 [5:48:31<2:12:51,  7.82s/it]
{'loss': 1.1643, 'grad_norm': 0.3907907245872072, 'learning_rate': 3.472260609529433e-06, 'epoch': 0.73}


 74%|███████▎  | 2826/3844 [5:48:46<2:07:57,  7.54s/it]

 74%|███████▎  | 2827/3844 [5:48:55<2:15:53,  8.02s/it]
{'loss': 1.1322, 'grad_norm': 0.41104485289763343, 'learning_rate': 3.4531297560184107e-06, 'epoch': 0.74}


 74%|███████▎  | 2829/3844 [5:49:08<2:02:39,  7.25s/it]

 74%|███████▎  | 2830/3844 [5:49:15<2:00:35,  7.14s/it]
{'loss': 1.055, 'grad_norm': 0.3770890319825124, 'learning_rate': 3.4340407456167657e-06, 'epoch': 0.74}


 74%|███████▎  | 2832/3844 [5:49:33<2:18:51,  8.23s/it]

 74%|███████▎  | 2833/3844 [5:49:41<2:18:40,  8.23s/it]

 74%|███████▎  | 2834/3844 [5:49:47<2:05:29,  7.45s/it]

 74%|███████▍  | 2835/3844 [5:49:52<1:55:10,  6.85s/it]

 74%|███████▍  | 2836/3844 [5:50:01<2:03:09,  7.33s/it]

 74%|███████▍  | 2837/3844 [5:50:08<2:04:49,  7.44s/it]

 74%|███████▍  | 2838/3844 [5:50:17<2:10:35,  7.79s/it]

 74%|███████▍  | 2839/3844 [5:50:23<2:00:19,  7.18s/it]

 74%|███████▍  | 2840/3844 [5:50:33<2:14:49,  8.06s/it]

 74%|███████▍  | 2841/3844 [5:50:39<2:04:48,  7.47s/it]

 74%|███████▍  | 2842/3844 [5:50:45<1:57:00,  7.01s/it]

 74%|███████▍  | 2843/3844 [5:50:50<1:49:15,  6.55s/it]

 74%|███████▍  | 2844/3844 [5:50:56<1:43:02,  6.18s/it]

 74%|███████▍  | 2845/3844 [5:51:01<1:40:15,  6.02s/it]

 74%|███████▍  | 2846/3844 [5:51:09<1:47:32,  6.47s/it]

 74%|███████▍  | 2847/3844 [5:51:16<1:49:50,  6.61s/it]

 74%|███████▍  | 2848/3844 [5:51:21<1:44:41,  6.31s/it]

 74%|███████▍  | 2849/3844 [5:51:27<1:43:41,  6.25s/it]

 74%|███████▍  | 2850/3844 [5:51:36<1:57:26,  7.09s/it]

 74%|███████▍  | 2851/3844 [5:51:46<2:07:27,  7.70s/it]

 74%|███████▍  | 2852/3844 [5:51:52<2:03:28,  7.47s/it]
{'loss': 1.0986, 'grad_norm': 0.44628720444190145, 'learning_rate': 3.2953447661960902e-06, 'epoch': 0.74}


 74%|███████▍  | 2854/3844 [5:52:08<2:10:54,  7.93s/it]

 74%|███████▍  | 2855/3844 [5:52:17<2:13:41,  8.11s/it]

 74%|███████▍  | 2856/3844 [5:52:22<2:01:30,  7.38s/it]

 74%|███████▍  | 2857/3844 [5:52:29<1:57:52,  7.17s/it]

 74%|███████▍  | 2858/3844 [5:52:35<1:52:59,  6.88s/it]

 74%|███████▍  | 2859/3844 [5:52:44<2:03:40,  7.53s/it]

 74%|███████▍  | 2860/3844 [5:52:50<1:52:30,  6.86s/it]
{'loss': 1.1325, 'grad_norm': 0.4216181815096861, 'learning_rate': 3.2454787200892336e-06, 'epoch': 0.74}


 74%|███████▍  | 2862/3844 [5:53:03<1:49:50,  6.71s/it]

 74%|███████▍  | 2863/3844 [5:53:09<1:45:15,  6.44s/it]

 75%|███████▍  | 2864/3844 [5:53:15<1:44:12,  6.38s/it]

 75%|███████▍  | 2865/3844 [5:53:21<1:44:26,  6.40s/it]

 75%|███████▍  | 2866/3844 [5:53:28<1:44:48,  6.43s/it]

 75%|███████▍  | 2867/3844 [5:53:34<1:44:38,  6.43s/it]

 75%|███████▍  | 2868/3844 [5:53:41<1:44:42,  6.44s/it]
{'loss': 1.1898, 'grad_norm': 0.4220446608546629, 'learning_rate': 3.195919661587894e-06, 'epoch': 0.75}


 75%|███████▍  | 2870/3844 [5:53:57<2:02:19,  7.54s/it]

 75%|███████▍  | 2871/3844 [5:54:04<1:59:41,  7.38s/it]

 75%|███████▍  | 2872/3844 [5:54:11<1:56:44,  7.21s/it]

 75%|███████▍  | 2873/3844 [5:54:21<2:07:34,  7.88s/it]

 75%|███████▍  | 2874/3844 [5:54:29<2:07:28,  7.89s/it]

 75%|███████▍  | 2875/3844 [5:54:35<2:01:49,  7.54s/it]

 75%|███████▍  | 2876/3844 [5:54:43<2:03:59,  7.69s/it]

 75%|███████▍  | 2877/3844 [5:54:51<2:04:10,  7.71s/it]

 75%|███████▍  | 2878/3844 [5:55:01<2:16:36,  8.48s/it]

 75%|███████▍  | 2879/3844 [5:55:08<2:05:00,  7.77s/it]

 75%|███████▍  | 2880/3844 [5:55:14<1:56:49,  7.27s/it]

 75%|███████▍  | 2881/3844 [5:55:21<1:57:44,  7.34s/it]

 75%|███████▍  | 2882/3844 [5:55:27<1:52:16,  7.00s/it]

 75%|███████▌  | 2883/3844 [5:55:33<1:44:57,  6.55s/it]

 75%|███████▌  | 2884/3844 [5:55:39<1:42:55,  6.43s/it]

 75%|███████▌  | 2885/3844 [5:55:45<1:38:34,  6.17s/it]

 75%|███████▌  | 2886/3844 [5:55:52<1:43:47,  6.50s/it]

 75%|███████▌  | 2887/3844 [5:55:57<1:39:03,  6.21s/it]

 75%|███████▌  | 2888/3844 [5:56:04<1:40:36,  6.31s/it]
{'loss': 1.0907, 'grad_norm': 0.42301579487678004, 'learning_rate': 3.0733798330000607e-06, 'epoch': 0.75}


 75%|███████▌  | 2890/3844 [5:56:16<1:37:55,  6.16s/it]

 75%|███████▌  | 2891/3844 [5:56:23<1:42:41,  6.47s/it]

 75%|███████▌  | 2892/3844 [5:56:33<1:57:08,  7.38s/it]

 75%|███████▌  | 2893/3844 [5:56:39<1:53:38,  7.17s/it]

 75%|███████▌  | 2894/3844 [5:56:46<1:49:12,  6.90s/it]

 75%|███████▌  | 2895/3844 [5:56:51<1:42:06,  6.46s/it]

 75%|███████▌  | 2896/3844 [5:56:59<1:47:18,  6.79s/it]

 75%|███████▌  | 2897/3844 [5:57:12<2:16:38,  8.66s/it]

 75%|███████▌  | 2898/3844 [5:57:19<2:09:34,  8.22s/it]

 75%|███████▌  | 2899/3844 [5:57:24<1:57:09,  7.44s/it]
{'loss': 1.2484, 'grad_norm': 0.3953160106067173, 'learning_rate': 3.0068193795673717e-06, 'epoch': 0.75}


 75%|███████▌  | 2901/3844 [5:57:40<1:58:56,  7.57s/it]

 75%|███████▌  | 2902/3844 [5:57:46<1:50:24,  7.03s/it]

 76%|███████▌  | 2903/3844 [5:57:52<1:46:08,  6.77s/it]

 76%|███████▌  | 2904/3844 [5:57:58<1:45:21,  6.73s/it]

 76%|███████▌  | 2905/3844 [5:58:04<1:39:39,  6.37s/it]

 76%|███████▌  | 2906/3844 [5:58:12<1:46:54,  6.84s/it]

 76%|███████▌  | 2907/3844 [5:58:18<1:41:42,  6.51s/it]

 76%|███████▌  | 2908/3844 [5:58:25<1:47:27,  6.89s/it]

 76%|███████▌  | 2909/3844 [5:58:33<1:49:57,  7.06s/it]

 76%|███████▌  | 2910/3844 [5:58:39<1:45:52,  6.80s/it]

 76%|███████▌  | 2911/3844 [5:58:45<1:39:56,  6.43s/it]

 76%|███████▌  | 2912/3844 [5:58:53<1:46:54,  6.88s/it]

 76%|███████▌  | 2913/3844 [5:58:59<1:44:37,  6.74s/it]

 76%|███████▌  | 2914/3844 [5:59:07<1:48:51,  7.02s/it]

 76%|███████▌  | 2915/3844 [5:59:14<1:49:57,  7.10s/it]

 76%|███████▌  | 2916/3844 [5:59:20<1:44:47,  6.78s/it]

 76%|███████▌  | 2917/3844 [5:59:30<1:58:24,  7.66s/it]

 76%|███████▌  | 2918/3844 [5:59:42<2:18:37,  8.98s/it]

 76%|███████▌  | 2919/3844 [5:59:48<2:06:34,  8.21s/it]

 76%|███████▌  | 2920/3844 [5:59:53<1:53:06,  7.34s/it]
{'loss': 1.1299, 'grad_norm': 0.4148152535003225, 'learning_rate': 2.881422818637227e-06, 'epoch': 0.76}


 76%|███████▌  | 2922/3844 [6:00:08<1:49:32,  7.13s/it]

 76%|███████▌  | 2923/3844 [6:00:17<1:59:15,  7.77s/it]

 76%|███████▌  | 2924/3844 [6:00:23<1:52:23,  7.33s/it]

 76%|███████▌  | 2925/3844 [6:00:29<1:44:09,  6.80s/it]

 76%|███████▌  | 2926/3844 [6:00:35<1:40:31,  6.57s/it]
{'loss': 1.1382, 'grad_norm': 0.3942741726836307, 'learning_rate': 2.846002992943181e-06, 'epoch': 0.76}


 76%|███████▌  | 2928/3844 [6:00:48<1:39:35,  6.52s/it]

 76%|███████▌  | 2929/3844 [6:00:56<1:46:41,  7.00s/it]

 76%|███████▌  | 2930/3844 [6:01:02<1:42:11,  6.71s/it]

 76%|███████▌  | 2931/3844 [6:01:12<1:58:37,  7.80s/it]

 76%|███████▋  | 2932/3844 [6:01:18<1:50:56,  7.30s/it]

 76%|███████▋  | 2933/3844 [6:01:25<1:48:04,  7.12s/it]

 76%|███████▋  | 2934/3844 [6:01:31<1:44:54,  6.92s/it]

 76%|███████▋  | 2935/3844 [6:01:37<1:40:54,  6.66s/it]

 76%|███████▋  | 2936/3844 [6:01:45<1:44:21,  6.90s/it]
{'loss': 1.1688, 'grad_norm': 0.41104594347569234, 'learning_rate': 2.787376825168966e-06, 'epoch': 0.76}


 76%|███████▋  | 2938/3844 [6:02:01<1:50:51,  7.34s/it]

 76%|███████▋  | 2939/3844 [6:02:07<1:44:59,  6.96s/it]

 76%|███████▋  | 2940/3844 [6:02:16<1:54:21,  7.59s/it]

 77%|███████▋  | 2941/3844 [6:02:22<1:46:51,  7.10s/it]
{'loss': 1.1286, 'grad_norm': 0.39873236075204416, 'learning_rate': 2.758255557067483e-06, 'epoch': 0.76}


 77%|███████▋  | 2943/3844 [6:02:37<1:51:27,  7.42s/it]

 77%|███████▋  | 2944/3844 [6:02:45<1:52:10,  7.48s/it]
{'loss': 1.3007, 'grad_norm': 0.4277098902488135, 'learning_rate': 2.7408444755134046e-06, 'epoch': 0.77}


 77%|███████▋  | 2946/3844 [6:02:56<1:38:42,  6.60s/it]

 77%|███████▋  | 2947/3844 [6:03:03<1:38:13,  6.57s/it]

 77%|███████▋  | 2948/3844 [6:03:09<1:36:52,  6.49s/it]

 77%|███████▋  | 2949/3844 [6:03:20<1:55:47,  7.76s/it]

 77%|███████▋  | 2950/3844 [6:03:30<2:05:16,  8.41s/it]

 77%|███████▋  | 2951/3844 [6:03:38<2:03:54,  8.33s/it]
{'loss': 0.9765, 'grad_norm': 0.41952808196562913, 'learning_rate': 2.700399237468505e-06, 'epoch': 0.77}


 77%|███████▋  | 2953/3844 [6:03:52<1:53:00,  7.61s/it]

 77%|███████▋  | 2954/3844 [6:04:00<1:56:19,  7.84s/it]

 77%|███████▋  | 2955/3844 [6:04:08<1:55:02,  7.76s/it]

 77%|███████▋  | 2956/3844 [6:04:18<2:06:28,  8.55s/it]

 77%|███████▋  | 2957/3844 [6:04:24<1:54:31,  7.75s/it]

 77%|███████▋  | 2958/3844 [6:04:34<2:02:54,  8.32s/it]

 77%|███████▋  | 2959/3844 [6:04:43<2:08:51,  8.74s/it]
{'loss': 1.1475, 'grad_norm': 0.4088426946487025, 'learning_rate': 2.654487216177346e-06, 'epoch': 0.77}


 77%|███████▋  | 2961/3844 [6:05:01<2:10:57,  8.90s/it]

 77%|███████▋  | 2962/3844 [6:05:07<1:56:26,  7.92s/it]

 77%|███████▋  | 2963/3844 [6:05:13<1:49:35,  7.46s/it]
{'loss': 1.2071, 'grad_norm': 0.4282074040435356, 'learning_rate': 2.631656268593584e-06, 'epoch': 0.77}


 77%|███████▋  | 2965/3844 [6:05:29<1:52:43,  7.69s/it]
{'loss': 1.1245, 'grad_norm': 0.40017384587158805, 'learning_rate': 2.6202721742159343e-06, 'epoch': 0.77}


 77%|███████▋  | 2967/3844 [6:05:49<2:12:02,  9.03s/it]

 77%|███████▋  | 2968/3844 [6:05:54<1:56:25,  7.97s/it]

 77%|███████▋  | 2969/3844 [6:06:01<1:50:47,  7.60s/it]

 77%|███████▋  | 2970/3844 [6:06:09<1:53:21,  7.78s/it]

 77%|███████▋  | 2971/3844 [6:06:16<1:46:45,  7.34s/it]

 77%|███████▋  | 2972/3844 [6:06:21<1:38:33,  6.78s/it]

 77%|███████▋  | 2973/3844 [6:06:28<1:37:08,  6.69s/it]

 77%|███████▋  | 2974/3844 [6:06:34<1:35:25,  6.58s/it]

 77%|███████▋  | 2975/3844 [6:06:45<1:55:50,  8.00s/it]

 77%|███████▋  | 2976/3844 [6:06:52<1:48:52,  7.53s/it]

 77%|███████▋  | 2977/3844 [6:07:01<1:56:55,  8.09s/it]

 77%|███████▋  | 2978/3844 [6:07:10<2:00:04,  8.32s/it]

 77%|███████▋  | 2979/3844 [6:07:18<1:59:14,  8.27s/it]

 78%|███████▊  | 2980/3844 [6:07:26<1:56:22,  8.08s/it]

 78%|███████▊  | 2981/3844 [6:07:34<1:58:28,  8.24s/it]

 78%|███████▊  | 2982/3844 [6:07:43<2:01:04,  8.43s/it]

 78%|███████▊  | 2983/3844 [6:07:49<1:50:31,  7.70s/it]

 78%|███████▊  | 2984/3844 [6:07:55<1:42:40,  7.16s/it]

 78%|███████▊  | 2985/3844 [6:08:02<1:41:00,  7.06s/it]

 78%|███████▊  | 2986/3844 [6:08:10<1:46:22,  7.44s/it]

 78%|███████▊  | 2987/3844 [6:08:16<1:38:51,  6.92s/it]

 78%|███████▊  | 2988/3844 [6:08:22<1:35:12,  6.67s/it]

 78%|███████▊  | 2989/3844 [6:08:33<1:53:51,  7.99s/it]

 78%|███████▊  | 2990/3844 [6:08:39<1:45:30,  7.41s/it]

 78%|███████▊  | 2991/3844 [6:08:47<1:45:03,  7.39s/it]

 78%|███████▊  | 2992/3844 [6:08:52<1:37:56,  6.90s/it]

 78%|███████▊  | 2993/3844 [6:08:59<1:35:36,  6.74s/it]

 78%|███████▊  | 2994/3844 [6:09:07<1:43:18,  7.29s/it]

 78%|███████▊  | 2995/3844 [6:09:14<1:40:45,  7.12s/it]

 78%|███████▊  | 2996/3844 [6:09:20<1:35:20,  6.75s/it]

 78%|███████▊  | 2997/3844 [6:09:29<1:46:51,  7.57s/it]

 78%|███████▊  | 2998/3844 [6:09:35<1:37:26,  6.91s/it]

 78%|███████▊  | 2999/3844 [6:09:42<1:38:28,  6.99s/it]

 78%|███████▊  | 3000/3844 [6:09:48<1:34:37,  6.73s/it]
 78%|███████▊  | 3000/3844 [6:09:48<1:34:37,  6.73s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1129, 'grad_norm': 0.43954383955359944, 'learning_rate': 2.4189741500881224e-06, 'epoch': 0.78}

 78%|███████▊  | 3002/3844 [6:10:29<2:56:07, 12.55s/it]

 78%|███████▊  | 3003/3844 [6:10:37<2:36:42, 11.18s/it]

 78%|███████▊  | 3004/3844 [6:10:49<2:37:01, 11.22s/it]

 78%|███████▊  | 3005/3844 [6:10:55<2:14:28,  9.62s/it]

 78%|███████▊  | 3006/3844 [6:11:02<2:06:02,  9.02s/it]

 78%|███████▊  | 3007/3844 [6:11:11<2:07:06,  9.11s/it]
{'loss': 1.1258, 'grad_norm': 0.4052580496833412, 'learning_rate': 2.3860978559854096e-06, 'epoch': 0.78}


 78%|███████▊  | 3009/3844 [6:11:32<2:14:01,  9.63s/it]

 78%|███████▊  | 3010/3844 [6:11:38<1:59:57,  8.63s/it]

 78%|███████▊  | 3011/3844 [6:11:46<1:56:12,  8.37s/it]

 78%|███████▊  | 3012/3844 [6:11:56<2:04:51,  9.00s/it]

 78%|███████▊  | 3013/3844 [6:12:05<2:02:35,  8.85s/it]

 78%|███████▊  | 3014/3844 [6:12:14<2:05:55,  9.10s/it]

 78%|███████▊  | 3015/3844 [6:12:21<1:54:03,  8.25s/it]
{'loss': 1.079, 'grad_norm': 0.4264218954789861, 'learning_rate': 2.342565732294888e-06, 'epoch': 0.78}


 78%|███████▊  | 3017/3844 [6:12:36<1:48:08,  7.85s/it]
{'loss': 1.1135, 'grad_norm': 0.41365770253229667, 'learning_rate': 2.3317370032278185e-06, 'epoch': 0.78}

 79%|███████▊  | 3018/3844 [6:12:43<1:46:32,  7.74s/it]
[2024-05-26 01:54:34,710] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▊  | 3019/3844 [6:12:53<1:55:11,  8.38s/it]


 79%|███████▊  | 3021/3844 [6:13:10<1:59:54,  8.74s/it]

 79%|███████▊  | 3022/3844 [6:13:18<1:54:27,  8.36s/it]
{'loss': 1.2651, 'grad_norm': 0.3986637270209526, 'learning_rate': 2.3047605454970456e-06, 'epoch': 0.79}


 79%|███████▊  | 3024/3844 [6:13:35<1:55:57,  8.48s/it]
{'loss': 1.168, 'grad_norm': 0.3972961584090963, 'learning_rate': 2.2940081888398747e-06, 'epoch': 0.79}

 79%|███████▊  | 3025/3844 [6:13:45<2:04:26,  9.12s/it]


 79%|███████▊  | 3027/3844 [6:14:08<2:19:11, 10.22s/it]

 79%|███████▉  | 3028/3844 [6:14:15<2:04:35,  9.16s/it]

 79%|███████▉  | 3029/3844 [6:14:20<1:48:12,  7.97s/it]

 79%|███████▉  | 3030/3844 [6:14:26<1:41:01,  7.45s/it]

 79%|███████▉  | 3031/3844 [6:14:33<1:37:51,  7.22s/it]

 79%|███████▉  | 3032/3844 [6:14:41<1:41:40,  7.51s/it]

 79%|███████▉  | 3033/3844 [6:14:47<1:34:11,  6.97s/it]

 79%|███████▉  | 3034/3844 [6:14:52<1:28:22,  6.55s/it]

 79%|███████▉  | 3035/3844 [6:14:59<1:27:58,  6.52s/it]
{'loss': 1.141, 'grad_norm': 0.3896286497491258, 'learning_rate': 2.235262318318613e-06, 'epoch': 0.79}


 79%|███████▉  | 3037/3844 [6:15:13<1:29:28,  6.65s/it]

 79%|███████▉  | 3038/3844 [6:15:21<1:34:57,  7.07s/it]

 79%|███████▉  | 3039/3844 [6:15:29<1:37:56,  7.30s/it]

 79%|███████▉  | 3040/3844 [6:15:35<1:32:12,  6.88s/it]
{'loss': 1.1285, 'grad_norm': 0.42699293674468664, 'learning_rate': 2.2087799110723184e-06, 'epoch': 0.79}


 79%|███████▉  | 3042/3844 [6:15:48<1:31:03,  6.81s/it]

 79%|███████▉  | 3043/3844 [6:15:55<1:31:21,  6.84s/it]

 79%|███████▉  | 3044/3844 [6:16:02<1:30:37,  6.80s/it]

 79%|███████▉  | 3045/3844 [6:16:11<1:40:16,  7.53s/it]

 79%|███████▉  | 3046/3844 [6:16:19<1:42:21,  7.70s/it]

 79%|███████▉  | 3047/3844 [6:16:25<1:35:36,  7.20s/it]

 79%|███████▉  | 3048/3844 [6:16:36<1:48:32,  8.18s/it]

 79%|███████▉  | 3049/3844 [6:16:45<1:53:31,  8.57s/it]

 79%|███████▉  | 3050/3844 [6:16:51<1:43:19,  7.81s/it]

 79%|███████▉  | 3051/3844 [6:16:57<1:35:29,  7.23s/it]

 79%|███████▉  | 3052/3844 [6:17:04<1:36:29,  7.31s/it]

 79%|███████▉  | 3053/3844 [6:17:12<1:38:22,  7.46s/it]

 79%|███████▉  | 3054/3844 [6:17:18<1:32:56,  7.06s/it]

 79%|███████▉  | 3055/3844 [6:17:28<1:42:09,  7.77s/it]
{'loss': 1.0008, 'grad_norm': 0.3815809866431635, 'learning_rate': 2.1301644925815244e-06, 'epoch': 0.79}


 80%|███████▉  | 3057/3844 [6:17:46<1:49:17,  8.33s/it]

 80%|███████▉  | 3058/3844 [6:17:55<1:48:26,  8.28s/it]
{'loss': 1.285, 'grad_norm': 0.3970743004282138, 'learning_rate': 2.114591904309421e-06, 'epoch': 0.8}


 80%|███████▉  | 3060/3844 [6:18:14<1:57:33,  9.00s/it]

 80%|███████▉  | 3061/3844 [6:18:20<1:44:46,  8.03s/it]

 80%|███████▉  | 3062/3844 [6:18:25<1:33:55,  7.21s/it]

 80%|███████▉  | 3063/3844 [6:18:31<1:27:12,  6.70s/it]

 80%|███████▉  | 3064/3844 [6:18:37<1:25:32,  6.58s/it]

 80%|███████▉  | 3065/3844 [6:18:47<1:38:48,  7.61s/it]

 80%|███████▉  | 3066/3844 [6:18:59<1:54:27,  8.83s/it]

 80%|███████▉  | 3067/3844 [6:19:04<1:40:37,  7.77s/it]

 80%|███████▉  | 3068/3844 [6:19:18<2:04:54,  9.66s/it]
{'loss': 1.0723, 'grad_norm': 0.3989047753448355, 'learning_rate': 2.06304781987412e-06, 'epoch': 0.8}


 80%|███████▉  | 3070/3844 [6:19:33<1:48:17,  8.39s/it]

 80%|███████▉  | 3071/3844 [6:19:39<1:41:28,  7.88s/it]
{'loss': 1.247, 'grad_norm': 0.4519235979746524, 'learning_rate': 2.047694338165005e-06, 'epoch': 0.8}


 80%|███████▉  | 3073/3844 [6:19:57<1:45:52,  8.24s/it]

 80%|███████▉  | 3074/3844 [6:20:02<1:34:04,  7.33s/it]

 80%|███████▉  | 3075/3844 [6:20:09<1:30:13,  7.04s/it]

 80%|████████  | 3076/3844 [6:20:14<1:25:27,  6.68s/it]

 80%|████████  | 3077/3844 [6:20:21<1:26:06,  6.74s/it]

 80%|████████  | 3078/3844 [6:20:28<1:27:20,  6.84s/it]

 80%|████████  | 3079/3844 [6:20:36<1:30:27,  7.09s/it]
{'loss': 1.0183, 'grad_norm': 0.44425928145898463, 'learning_rate': 2.0070004669171726e-06, 'epoch': 0.8}

 80%|████████  | 3080/3844 [6:20:42<1:24:16,  6.62s/it]

 80%|████████  | 3081/3844 [6:20:50<1:29:52,  7.07s/it]


 80%|████████  | 3083/3844 [6:21:01<1:21:20,  6.41s/it]
{'loss': 1.1361, 'grad_norm': 0.414602023102075, 'learning_rate': 1.986789644638639e-06, 'epoch': 0.8}


 80%|████████  | 3085/3844 [6:21:17<1:28:09,  6.97s/it]

 80%|████████  | 3086/3844 [6:21:24<1:25:39,  6.78s/it]

 80%|████████  | 3087/3844 [6:21:34<1:39:17,  7.87s/it]

 80%|████████  | 3088/3844 [6:21:41<1:37:17,  7.72s/it]

 80%|████████  | 3089/3844 [6:21:48<1:33:33,  7.43s/it]
{'loss': 1.1992, 'grad_norm': 0.38992367145488394, 'learning_rate': 1.956644199133092e-06, 'epoch': 0.8}


 80%|████████  | 3091/3844 [6:22:03<1:35:05,  7.58s/it]

 80%|████████  | 3092/3844 [6:22:11<1:37:02,  7.74s/it]

 80%|████████  | 3093/3844 [6:22:17<1:30:31,  7.23s/it]
{'loss': 1.2138, 'grad_norm': 0.4031689211135487, 'learning_rate': 1.936661427078952e-06, 'epoch': 0.8}


 81%|████████  | 3095/3844 [6:22:35<1:41:31,  8.13s/it]

 81%|████████  | 3096/3844 [6:22:41<1:34:30,  7.58s/it]

 81%|████████  | 3097/3844 [6:22:47<1:26:50,  6.98s/it]
{'loss': 1.3129, 'grad_norm': 0.419892230813787, 'learning_rate': 1.9167702733619885e-06, 'epoch': 0.81}


 81%|████████  | 3099/3844 [6:23:04<1:36:41,  7.79s/it]

 81%|████████  | 3100/3844 [6:23:10<1:30:12,  7.27s/it]
{'loss': 1.2246, 'grad_norm': 0.41120472931366964, 'learning_rate': 1.901912168606339e-06, 'epoch': 0.81}


 81%|████████  | 3102/3844 [6:23:23<1:24:57,  6.87s/it]

 81%|████████  | 3103/3844 [6:23:29<1:25:06,  6.89s/it]

 81%|████████  | 3104/3844 [6:23:37<1:25:44,  6.95s/it]

 81%|████████  | 3105/3844 [6:23:46<1:33:05,  7.56s/it]

 81%|████████  | 3106/3844 [6:23:54<1:35:13,  7.74s/it]

 81%|████████  | 3107/3844 [6:24:01<1:34:18,  7.68s/it]

 81%|████████  | 3108/3844 [6:24:07<1:26:23,  7.04s/it]

 81%|████████  | 3109/3844 [6:24:15<1:32:02,  7.51s/it]

 81%|████████  | 3110/3844 [6:24:22<1:27:13,  7.13s/it]
{'loss': 1.108, 'grad_norm': 0.44862915248119944, 'learning_rate': 1.8527594873657918e-06, 'epoch': 0.81}


 81%|████████  | 3112/3844 [6:24:34<1:22:04,  6.73s/it]

 81%|████████  | 3113/3844 [6:24:47<1:43:38,  8.51s/it]

 81%|████████  | 3114/3844 [6:24:54<1:38:58,  8.13s/it]

 81%|████████  | 3115/3844 [6:25:02<1:36:35,  7.95s/it]

 81%|████████  | 3116/3844 [6:25:07<1:26:51,  7.16s/it]
{'loss': 1.3741, 'grad_norm': 0.42192748602781416, 'learning_rate': 1.8235453705295847e-06, 'epoch': 0.81}

 81%|████████  | 3117/3844 [6:25:14<1:25:28,  7.05s/it]


 81%|████████  | 3119/3844 [6:25:26<1:20:36,  6.67s/it]
{'loss': 1.2622, 'grad_norm': 0.4042099455626272, 'learning_rate': 1.8090166529602048e-06, 'epoch': 0.81}


 81%|████████  | 3121/3844 [6:25:44<1:33:13,  7.74s/it]

 81%|████████  | 3122/3844 [6:25:51<1:30:38,  7.53s/it]

 81%|████████  | 3123/3844 [6:25:57<1:27:30,  7.28s/it]

 81%|████████▏ | 3124/3844 [6:26:05<1:29:40,  7.47s/it]

 81%|████████▏ | 3125/3844 [6:26:18<1:47:12,  8.95s/it]

 81%|████████▏ | 3126/3844 [6:26:24<1:36:33,  8.07s/it]

 81%|████████▏ | 3127/3844 [6:26:33<1:39:19,  8.31s/it]

 81%|████████▏ | 3128/3844 [6:26:39<1:31:41,  7.68s/it]

 81%|████████▏ | 3129/3844 [6:26:44<1:23:35,  7.02s/it]
{'loss': 1.2198, 'grad_norm': 0.38437427518430084, 'learning_rate': 1.7609662051634524e-06, 'epoch': 0.81}


 81%|████████▏ | 3131/3844 [6:27:01<1:30:08,  7.59s/it]

 81%|████████▏ | 3132/3844 [6:27:10<1:34:07,  7.93s/it]

 82%|████████▏ | 3133/3844 [6:27:15<1:25:55,  7.25s/it]

 82%|████████▏ | 3134/3844 [6:27:21<1:20:01,  6.76s/it]

 82%|████████▏ | 3135/3844 [6:27:27<1:17:35,  6.57s/it]

 82%|████████▏ | 3136/3844 [6:27:34<1:17:28,  6.57s/it]

 82%|████████▏ | 3137/3844 [6:27:40<1:17:51,  6.61s/it]

 82%|████████▏ | 3138/3844 [6:27:47<1:16:01,  6.46s/it]

 82%|████████▏ | 3139/3844 [6:27:54<1:18:44,  6.70s/it]
{'loss': 1.1077, 'grad_norm': 0.39153469675866825, 'learning_rate': 1.7135008459880775e-06, 'epoch': 0.82}


 82%|████████▏ | 3141/3844 [6:28:10<1:28:08,  7.52s/it]

 82%|████████▏ | 3142/3844 [6:28:19<1:34:23,  8.07s/it]
[2024-05-26 02:10:00,804] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 3143/3844 [6:28:26<1:28:43,  7.59s/it]

 82%|████████▏ | 3144/3844 [6:28:32<1:23:28,  7.16s/it]

 82%|████████▏ | 3145/3844 [6:28:38<1:19:41,  6.84s/it]

 82%|████████▏ | 3146/3844 [6:28:44<1:16:00,  6.53s/it]

 82%|████████▏ | 3147/3844 [6:28:49<1:12:27,  6.24s/it]

 82%|████████▏ | 3148/3844 [6:28:55<1:09:18,  5.97s/it]
{'loss': 1.302, 'grad_norm': 0.4308343768334912, 'learning_rate': 1.6712850602605279e-06, 'epoch': 0.82}


 82%|████████▏ | 3150/3844 [6:29:09<1:14:40,  6.46s/it]

 82%|████████▏ | 3151/3844 [6:29:16<1:16:16,  6.60s/it]
{'loss': 1.1619, 'grad_norm': 0.4163792472797218, 'learning_rate': 1.6573194749864462e-06, 'epoch': 0.82}


 82%|████████▏ | 3153/3844 [6:29:29<1:15:00,  6.51s/it]

 82%|████████▏ | 3154/3844 [6:29:40<1:30:09,  7.84s/it]
{'loss': 1.2646, 'grad_norm': 0.3712853997904331, 'learning_rate': 1.643407210410759e-06, 'epoch': 0.82}


 82%|████████▏ | 3156/3844 [6:29:52<1:20:08,  6.99s/it]

 82%|████████▏ | 3157/3844 [6:29:59<1:19:02,  6.90s/it]

 82%|████████▏ | 3158/3844 [6:30:08<1:26:41,  7.58s/it]
{'loss': 1.1149, 'grad_norm': 0.40287040256677675, 'learning_rate': 1.6249406212541507e-06, 'epoch': 0.82}


 82%|████████▏ | 3160/3844 [6:30:25<1:32:52,  8.15s/it]

 82%|████████▏ | 3161/3844 [6:30:31<1:24:04,  7.39s/it]
{'loss': 1.0525, 'grad_norm': 0.4167442177443571, 'learning_rate': 1.6111531168422179e-06, 'epoch': 0.82}


 82%|████████▏ | 3163/3844 [6:30:44<1:20:42,  7.11s/it]

 82%|████████▏ | 3164/3844 [6:30:53<1:26:40,  7.65s/it]

 82%|████████▏ | 3165/3844 [6:31:00<1:22:36,  7.30s/it]

 82%|████████▏ | 3166/3844 [6:31:06<1:18:38,  6.96s/it]

 82%|████████▏ | 3167/3844 [6:31:17<1:33:53,  8.32s/it]

 82%|████████▏ | 3168/3844 [6:31:24<1:27:23,  7.76s/it]

 82%|████████▏ | 3169/3844 [6:31:30<1:22:13,  7.31s/it]

 82%|████████▏ | 3170/3844 [6:31:39<1:26:56,  7.74s/it]

 82%|████████▏ | 3171/3844 [6:31:45<1:23:46,  7.47s/it]

 83%|████████▎ | 3172/3844 [6:31:52<1:18:54,  7.05s/it]

 83%|████████▎ | 3173/3844 [6:31:58<1:16:53,  6.88s/it]

 83%|████████▎ | 3174/3844 [6:32:06<1:20:16,  7.19s/it]
{'loss': 1.1732, 'grad_norm': 0.38954094838817194, 'learning_rate': 1.552027948757685e-06, 'epoch': 0.83}

 83%|████████▎ | 3175/3844 [6:32:13<1:18:16,  7.02s/it]

 83%|████████▎ | 3176/3844 [6:32:19<1:14:43,  6.71s/it]


 83%|████████▎ | 3178/3844 [6:32:36<1:26:53,  7.83s/it]

 83%|████████▎ | 3179/3844 [6:32:44<1:26:00,  7.76s/it]

 83%|████████▎ | 3180/3844 [6:32:49<1:17:51,  7.04s/it]
{'loss': 1.1597, 'grad_norm': 0.41093651766640915, 'learning_rate': 1.5250809411481283e-06, 'epoch': 0.83}

 83%|████████▎ | 3181/3844 [6:32:56<1:19:14,  7.17s/it]


 83%|████████▎ | 3183/3844 [6:33:10<1:15:52,  6.89s/it]
{'loss': 1.2917, 'grad_norm': 0.4212447785564679, 'learning_rate': 1.5116886431878165e-06, 'epoch': 0.83}

 83%|████████▎ | 3184/3844 [6:33:17<1:16:30,  6.96s/it]

 83%|████████▎ | 3185/3844 [6:33:24<1:19:10,  7.21s/it]

 83%|████████▎ | 3186/3844 [6:33:31<1:16:35,  6.98s/it]


 83%|████████▎ | 3188/3844 [6:33:44<1:13:44,  6.75s/it]

 83%|████████▎ | 3189/3844 [6:33:52<1:18:51,  7.22s/it]
{'loss': 1.0783, 'grad_norm': 0.401419781037968, 'learning_rate': 1.4850668869275497e-06, 'epoch': 0.83}


 83%|████████▎ | 3191/3844 [6:34:06<1:17:55,  7.16s/it]

 83%|████████▎ | 3192/3844 [6:34:12<1:14:01,  6.81s/it]

 83%|████████▎ | 3193/3844 [6:34:20<1:17:18,  7.13s/it]

 83%|████████▎ | 3194/3844 [6:34:28<1:21:00,  7.48s/it]

 83%|████████▎ | 3195/3844 [6:34:34<1:15:25,  6.97s/it]
{'loss': 1.1935, 'grad_norm': 0.4199470588224034, 'learning_rate': 1.4586628167951688e-06, 'epoch': 0.83}


 83%|████████▎ | 3197/3844 [6:34:50<1:18:52,  7.31s/it]

 83%|████████▎ | 3198/3844 [6:34:58<1:23:11,  7.73s/it]
{'loss': 1.0815, 'grad_norm': 0.43377508052492286, 'learning_rate': 1.4455426251903693e-06, 'epoch': 0.83}

 83%|████████▎ | 3199/3844 [6:35:07<1:25:39,  7.97s/it]


 83%|████████▎ | 3201/3844 [6:35:19<1:15:53,  7.08s/it]
{'loss': 1.1254, 'grad_norm': 0.3998952441990127, 'learning_rate': 1.4324771078165011e-06, 'epoch': 0.83}


 83%|████████▎ | 3203/3844 [6:35:36<1:24:46,  7.94s/it]

 83%|████████▎ | 3204/3844 [6:35:42<1:16:49,  7.20s/it]

 83%|████████▎ | 3205/3844 [6:35:50<1:18:53,  7.41s/it]
{'loss': 1.1147, 'grad_norm': 0.3957504845407007, 'learning_rate': 1.415141611069023e-06, 'epoch': 0.83}

 83%|████████▎ | 3206/3844 [6:35:55<1:12:45,  6.84s/it]


 83%|████████▎ | 3208/3844 [6:36:08<1:08:58,  6.51s/it]

 83%|████████▎ | 3209/3844 [6:36:14<1:09:23,  6.56s/it]
{'loss': 1.0877, 'grad_norm': 0.38787430042885546, 'learning_rate': 1.3979036583404892e-06, 'epoch': 0.83}


 84%|████████▎ | 3211/3844 [6:36:28<1:10:23,  6.67s/it]

 84%|████████▎ | 3212/3844 [6:36:34<1:07:18,  6.39s/it]

 84%|████████▎ | 3213/3844 [6:36:42<1:13:50,  7.02s/it]
{'loss': 1.1441, 'grad_norm': 0.39927346690442644, 'learning_rate': 1.3807634454942586e-06, 'epoch': 0.84}


 84%|████████▎ | 3215/3844 [6:36:59<1:19:02,  7.54s/it]
{'loss': 1.2506, 'grad_norm': 0.38198071344848306, 'learning_rate': 1.3722300524306965e-06, 'epoch': 0.84}


 84%|████████▎ | 3217/3844 [6:37:12<1:13:23,  7.02s/it]

 84%|████████▎ | 3218/3844 [6:37:20<1:16:16,  7.31s/it]

 84%|████████▎ | 3219/3844 [6:37:27<1:14:51,  7.19s/it]
{'loss': 1.3126, 'grad_norm': 0.4168753967010201, 'learning_rate': 1.3552368142217743e-06, 'epoch': 0.84}

 84%|████████▍ | 3220/3844 [6:37:37<1:26:09,  8.29s/it]

 84%|████████▍ | 3221/3844 [6:37:45<1:23:37,  8.05s/it]


 84%|████████▍ | 3223/3844 [6:37:59<1:15:53,  7.33s/it]

 84%|████████▍ | 3224/3844 [6:38:04<1:10:14,  6.80s/it]

 84%|████████▍ | 3225/3844 [6:38:10<1:06:31,  6.45s/it]

 84%|████████▍ | 3226/3844 [6:38:19<1:13:43,  7.16s/it]

 84%|████████▍ | 3227/3844 [6:38:28<1:19:30,  7.73s/it]

 84%|████████▍ | 3228/3844 [6:38:36<1:21:34,  7.95s/it]
{'loss': 1.2538, 'grad_norm': 0.4092696746196688, 'learning_rate': 1.3173614545705938e-06, 'epoch': 0.84}


 84%|████████▍ | 3230/3844 [6:38:50<1:14:51,  7.32s/it]

 84%|████████▍ | 3231/3844 [6:38:59<1:19:48,  7.81s/it]
{'loss': 1.1941, 'grad_norm': 0.39835072787120496, 'learning_rate': 1.3048472144069624e-06, 'epoch': 0.84}

 84%|████████▍ | 3232/3844 [6:39:07<1:22:12,  8.06s/it]


 84%|████████▍ | 3234/3844 [6:39:20<1:12:11,  7.10s/it]
{'loss': 1.1886, 'grad_norm': 0.4312271299543556, 'learning_rate': 1.2923885477029342e-06, 'epoch': 0.84}


 84%|████████▍ | 3236/3844 [6:39:33<1:07:54,  6.70s/it]
{'loss': 1.0775, 'grad_norm': 0.4417608177560807, 'learning_rate': 1.284113683381213e-06, 'epoch': 0.84}

 84%|████████▍ | 3237/3844 [6:39:43<1:19:03,  7.81s/it]

 84%|████████▍ | 3238/3844 [6:39:49<1:13:37,  7.29s/it]

 84%|████████▍ | 3239/3844 [6:39:55<1:10:48,  7.02s/it]


 84%|████████▍ | 3241/3844 [6:40:09<1:07:14,  6.69s/it]
{'loss': 1.1783, 'grad_norm': 0.4443096080794692, 'learning_rate': 1.2635348910259826e-06, 'epoch': 0.84}


 84%|████████▍ | 3243/3844 [6:40:25<1:13:21,  7.32s/it]

 84%|████████▍ | 3244/3844 [6:40:31<1:09:31,  6.95s/it]
{'loss': 1.1837, 'grad_norm': 0.4331489023418336, 'learning_rate': 1.2512620422740973e-06, 'epoch': 0.84}


 84%|████████▍ | 3246/3844 [6:40:46<1:11:33,  7.18s/it]

 84%|████████▍ | 3247/3844 [6:40:54<1:14:28,  7.48s/it]

 84%|████████▍ | 3248/3844 [6:41:06<1:26:33,  8.71s/it]
{'loss': 1.0758, 'grad_norm': 0.399222725059011, 'learning_rate': 1.2349852377943038e-06, 'epoch': 0.84}

 85%|████████▍ | 3249/3844 [6:41:11<1:16:13,  7.69s/it]


 85%|████████▍ | 3251/3844 [6:41:25<1:12:37,  7.35s/it]

 85%|████████▍ | 3252/3844 [6:41:34<1:17:30,  7.86s/it]
{'loss': 1.1095, 'grad_norm': 0.4328391895871727, 'learning_rate': 1.2188080243301438e-06, 'epoch': 0.85}


 85%|████████▍ | 3254/3844 [6:41:48<1:14:38,  7.59s/it]

 85%|████████▍ | 3255/3844 [6:41:55<1:10:14,  7.15s/it]

 85%|████████▍ | 3256/3844 [6:42:00<1:04:49,  6.61s/it]

 85%|████████▍ | 3257/3844 [6:42:08<1:09:58,  7.15s/it]
{'loss': 1.1653, 'grad_norm': 0.4075172950383074, 'learning_rate': 1.1987268372821548e-06, 'epoch': 0.85}

 85%|████████▍ | 3258/3844 [6:42:15<1:08:21,  7.00s/it]

 85%|████████▍ | 3259/3844 [6:42:22<1:07:07,  6.88s/it]

 85%|████████▍ | 3260/3844 [6:42:29<1:09:18,  7.12s/it]


 85%|████████▍ | 3262/3844 [6:42:45<1:13:11,  7.54s/it]
{'loss': 1.0976, 'grad_norm': 0.4371698943000053, 'learning_rate': 1.1788019048316457e-06, 'epoch': 0.85}

 85%|████████▍ | 3263/3844 [6:42:51<1:10:41,  7.30s/it]

 85%|████████▍ | 3264/3844 [6:43:00<1:13:39,  7.62s/it]


 85%|████████▍ | 3266/3844 [6:43:14<1:10:52,  7.36s/it]
{'loss': 1.1521, 'grad_norm': 0.41476542437537567, 'learning_rate': 1.1629746999880698e-06, 'epoch': 0.85}


 85%|████████▌ | 3268/3844 [6:43:26<1:04:19,  6.70s/it]

 85%|████████▌ | 3269/3844 [6:43:33<1:03:49,  6.66s/it]

 85%|████████▌ | 3270/3844 [6:43:38<1:01:35,  6.44s/it]

 85%|████████▌ | 3271/3844 [6:43:47<1:07:23,  7.06s/it]
{'loss': 0.8869, 'grad_norm': 0.41175723713486095, 'learning_rate': 1.1433319153696053e-06, 'epoch': 0.85}

 85%|████████▌ | 3272/3844 [6:43:57<1:16:29,  8.02s/it]


 85%|████████▌ | 3274/3844 [6:44:15<1:20:53,  8.51s/it]

 85%|████████▌ | 3275/3844 [6:44:22<1:18:06,  8.24s/it]

 85%|████████▌ | 3276/3844 [6:44:29<1:13:22,  7.75s/it]
{'loss': 1.1371, 'grad_norm': 0.3731717902084745, 'learning_rate': 1.1238463688098844e-06, 'epoch': 0.85}


 85%|████████▌ | 3278/3844 [6:44:48<1:23:08,  8.81s/it]
[2024-05-26 02:26:29,677] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 3279/3844 [6:44:55<1:17:05,  8.19s/it]
{'loss': 1.1544, 'grad_norm': 0.4321325051702822, 'learning_rate': 1.1122306591693078e-06, 'epoch': 0.85}


 85%|████████▌ | 3281/3844 [6:45:11<1:15:10,  8.01s/it]

 85%|████████▌ | 3282/3844 [6:45:18<1:13:24,  7.84s/it]
{'loss': 1.2401, 'grad_norm': 0.4290521690283395, 'learning_rate': 1.1006717540613988e-06, 'epoch': 0.85}


 85%|████████▌ | 3284/3844 [6:45:33<1:10:32,  7.56s/it]

 85%|████████▌ | 3285/3844 [6:45:39<1:05:53,  7.07s/it]

 85%|████████▌ | 3286/3844 [6:45:46<1:07:29,  7.26s/it]

 86%|████████▌ | 3287/3844 [6:45:52<1:03:54,  6.88s/it]

 86%|████████▌ | 3288/3844 [6:45:59<1:02:46,  6.77s/it]
{'loss': 1.1529, 'grad_norm': 0.40932633626179116, 'learning_rate': 1.077724652586466e-06, 'epoch': 0.86}


 86%|████████▌ | 3290/3844 [6:46:12<1:02:28,  6.77s/it]
{'loss': 1.1581, 'grad_norm': 0.4137560375241288, 'learning_rate': 1.0701262788304556e-06, 'epoch': 0.86}

 86%|████████▌ | 3291/3844 [6:46:19<1:02:37,  6.79s/it]


 86%|████████▌ | 3293/3844 [6:46:37<1:12:16,  7.87s/it]
{'loss': 1.2658, 'grad_norm': 0.42856895897868313, 'learning_rate': 1.0587762862982764e-06, 'epoch': 0.86}


 86%|████████▌ | 3295/3844 [6:46:51<1:07:22,  7.36s/it]

 86%|████████▌ | 3296/3844 [6:46:57<1:03:22,  6.94s/it]
{'loss': 1.1558, 'grad_norm': 0.39727257744094796, 'learning_rate': 1.047483439942495e-06, 'epoch': 0.86}


 86%|████████▌ | 3298/3844 [6:47:11<1:03:02,  6.93s/it]

 86%|████████▌ | 3299/3844 [6:47:18<1:04:40,  7.12s/it]
{'loss': 1.2626, 'grad_norm': 0.38455650175789946, 'learning_rate': 1.0362478119392495e-06, 'epoch': 0.86}

 86%|████████▌ | 3300/3844 [6:47:26<1:06:12,  7.30s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[2024-05-26 02:29:49,449] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8928, 'grad_norm': 0.4089073001710356, 'learning_rate': 1.0287892167210544e-06, 'epoch': 0.86}

 86%|████████▌ | 3302/3844 [6:48:14<2:09:21, 14.32s/it]

 86%|████████▌ | 3303/3844 [6:48:21<1:47:54, 11.97s/it]

 86%|████████▌ | 3304/3844 [6:48:31<1:43:07, 11.46s/it]
{'loss': 1.1782, 'grad_norm': 0.37895484654716177, 'learning_rate': 1.0176491120366482e-06, 'epoch': 0.86}

 86%|████████▌ | 3305/3844 [6:48:38<1:30:52, 10.12s/it]

 86%|████████▌ | 3306/3844 [6:48:48<1:30:28, 10.09s/it]


 86%|████████▌ | 3308/3844 [6:49:03<1:20:20,  8.99s/it]

 86%|████████▌ | 3309/3844 [6:49:09<1:10:53,  7.95s/it]

 86%|████████▌ | 3310/3844 [6:49:17<1:10:36,  7.93s/it]
{'loss': 1.1572, 'grad_norm': 0.37609594215634945, 'learning_rate': 9.955412006001885e-07, 'epoch': 0.86}

 86%|████████▌ | 3311/3844 [6:49:24<1:07:37,  7.61s/it]


 86%|████████▌ | 3313/3844 [6:49:41<1:15:07,  8.49s/it]
[2024-05-26 02:31:22,868] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1756, 'grad_norm': 0.36529964256982606, 'learning_rate': 9.84573535146761e-07, 'epoch': 0.86}

 86%|████████▌ | 3314/3844 [6:49:48<1:09:17,  7.84s/it]


 86%|████████▋ | 3316/3844 [6:49:59<59:41,  6.78s/it]

 86%|████████▋ | 3317/3844 [6:50:05<57:55,  6.59s/it]
{'loss': 1.134, 'grad_norm': 0.4345672161848629, 'learning_rate': 9.700396250507905e-07, 'epoch': 0.86}

 86%|████████▋ | 3318/3844 [6:50:12<57:28,  6.56s/it]

 86%|████████▋ | 3319/3844 [6:50:20<1:02:04,  7.10s/it]


 86%|████████▋ | 3321/3844 [6:50:33<58:00,  6.66s/it]
{'loss': 1.2618, 'grad_norm': 0.3983008939144453, 'learning_rate': 9.55608316370703e-07, 'epoch': 0.86}

 86%|████████▋ | 3322/3844 [6:50:42<1:04:28,  7.41s/it]

 86%|████████▋ | 3323/3844 [6:50:50<1:06:18,  7.64s/it]


 86%|████████▋ | 3325/3844 [6:51:05<1:06:32,  7.69s/it]

 87%|████████▋ | 3326/3844 [6:51:13<1:08:13,  7.90s/it]

 87%|████████▋ | 3327/3844 [6:51:19<1:03:24,  7.36s/it]

 87%|████████▋ | 3328/3844 [6:51:25<58:49,  6.84s/it]

 87%|████████▋ | 3329/3844 [6:51:35<1:05:43,  7.66s/it]
{'loss': 1.095, 'grad_norm': 0.41383280417783647, 'learning_rate': 9.270541579838632e-07, 'epoch': 0.87}

 87%|████████▋ | 3330/3844 [6:51:42<1:04:49,  7.57s/it]


 87%|████████▋ | 3332/3844 [6:51:56<1:01:41,  7.23s/it]
{'loss': 1.2081, 'grad_norm': 0.3870481634284716, 'learning_rate': 9.16452590568071e-07, 'epoch': 0.87}

 87%|████████▋ | 3333/3844 [6:52:02<59:42,  7.01s/it]


 87%|████████▋ | 3335/3844 [6:52:18<1:06:28,  7.84s/it]

 87%|████████▋ | 3336/3844 [6:52:25<1:04:12,  7.58s/it]

 87%|████████▋ | 3337/3844 [6:52:36<1:10:22,  8.33s/it]
{'loss': 1.1275, 'grad_norm': 0.38880989958286427, 'learning_rate': 8.989123577499715e-07, 'epoch': 0.87}

 87%|████████▋ | 3338/3844 [6:52:42<1:05:11,  7.73s/it]

 87%|████████▋ | 3339/3844 [6:52:48<1:01:10,  7.27s/it]

 87%|████████▋ | 3340/3844 [6:52:56<1:03:28,  7.56s/it]


 87%|████████▋ | 3342/3844 [6:53:09<58:53,  7.04s/it]

 87%|████████▋ | 3343/3844 [6:53:15<54:43,  6.55s/it]
{'loss': 1.1907, 'grad_norm': 0.4410358426595644, 'learning_rate': 8.780773877306004e-07, 'epoch': 0.87}

 87%|████████▋ | 3344/3844 [6:53:20<52:13,  6.27s/it]

 87%|████████▋ | 3345/3844 [6:53:28<56:31,  6.80s/it]

 87%|████████▋ | 3346/3844 [6:53:36<59:27,  7.16s/it]

 87%|████████▋ | 3347/3844 [6:53:44<1:01:17,  7.40s/it]

 87%|████████▋ | 3348/3844 [6:53:54<1:06:42,  8.07s/it]


 87%|████████▋ | 3350/3844 [6:54:07<59:40,  7.25s/it]
{'loss': 1.1625, 'grad_norm': 0.42860828570993553, 'learning_rate': 8.540647036550764e-07, 'epoch': 0.87}

 87%|████████▋ | 3351/3844 [6:54:12<55:23,  6.74s/it]

 87%|████████▋ | 3352/3844 [6:54:20<58:53,  7.18s/it]


 87%|████████▋ | 3354/3844 [6:54:33<56:29,  6.92s/it]

 87%|████████▋ | 3355/3844 [6:54:43<1:02:47,  7.70s/it]
{'loss': 1.1045, 'grad_norm': 0.39184385155946994, 'learning_rate': 8.371075862221889e-07, 'epoch': 0.87}

 87%|████████▋ | 3356/3844 [6:54:50<1:01:52,  7.61s/it]


 87%|████████▋ | 3358/3844 [6:55:07<1:02:49,  7.76s/it]
{'loss': 1.0281, 'grad_norm': 0.41557686207627503, 'learning_rate': 8.270113804491897e-07, 'epoch': 0.87}


 87%|████████▋ | 3360/3844 [6:55:21<59:23,  7.36s/it]

 87%|████████▋ | 3361/3844 [6:55:29<1:00:23,  7.50s/it]

 87%|████████▋ | 3362/3844 [6:55:35<56:20,  7.01s/it]

 87%|████████▋ | 3363/3844 [6:55:42<55:56,  6.98s/it]
{'loss': 1.1544, 'grad_norm': 0.4461794108804343, 'learning_rate': 8.103146857656019e-07, 'epoch': 0.87}


 88%|████████▊ | 3365/3844 [6:55:55<55:08,  6.91s/it]

 88%|████████▊ | 3366/3844 [6:56:02<53:59,  6.78s/it]
{'loss': 1.1805, 'grad_norm': 0.43130533470316285, 'learning_rate': 8.00374962260706e-07, 'epoch': 0.88}


 88%|████████▊ | 3368/3844 [6:56:15<53:09,  6.70s/it]

 88%|████████▊ | 3369/3844 [6:56:24<57:08,  7.22s/it]

 88%|████████▊ | 3370/3844 [6:56:31<56:37,  7.17s/it]

 88%|████████▊ | 3371/3844 [6:56:38<55:57,  7.10s/it]

 88%|████████▊ | 3372/3844 [6:56:45<56:08,  7.14s/it]
{'loss': 1.243, 'grad_norm': 0.41370129606057143, 'learning_rate': 7.806719715049194e-07, 'epoch': 0.88}


 88%|████████▊ | 3374/3844 [6:56:59<55:50,  7.13s/it]
{'loss': 1.1724, 'grad_norm': 0.36183128231644907, 'learning_rate': 7.741566596935912e-07, 'epoch': 0.88}


 88%|████████▊ | 3376/3844 [6:57:13<54:40,  7.01s/it]

 88%|████████▊ | 3377/3844 [6:57:20<54:03,  6.94s/it]

 88%|████████▊ | 3378/3844 [6:57:27<55:30,  7.15s/it]

 88%|████████▊ | 3379/3844 [6:57:36<58:45,  7.58s/it]
{'loss': 1.1389, 'grad_norm': 0.36227264141711135, 'learning_rate': 7.579830751924822e-07, 'epoch': 0.88}


 88%|████████▊ | 3381/3844 [6:57:51<57:20,  7.43s/it]

 88%|████████▊ | 3382/3844 [6:57:59<58:44,  7.63s/it]

 88%|████████▊ | 3383/3844 [6:58:08<1:01:07,  7.96s/it]
{'loss': 1.1403, 'grad_norm': 0.3894183271560603, 'learning_rate': 7.451623310759182e-07, 'epoch': 0.88}

 88%|████████▊ | 3384/3844 [6:58:15<58:56,  7.69s/it]


 88%|████████▊ | 3386/3844 [6:58:30<58:35,  7.68s/it]

 88%|████████▊ | 3387/3844 [6:58:36<54:38,  7.17s/it]

 88%|████████▊ | 3388/3844 [6:58:41<51:21,  6.76s/it]

 88%|████████▊ | 3389/3844 [6:58:48<50:36,  6.67s/it]

 88%|████████▊ | 3390/3844 [6:58:53<47:45,  6.31s/it]
{'loss': 1.2104, 'grad_norm': 0.41264945307770606, 'learning_rate': 7.229791487700566e-07, 'epoch': 0.88}

 88%|████████▊ | 3391/3844 [6:59:00<49:39,  6.58s/it]


 88%|████████▊ | 3393/3844 [6:59:13<49:10,  6.54s/it]

 88%|████████▊ | 3394/3844 [6:59:22<52:58,  7.06s/it]

 88%|████████▊ | 3395/3844 [6:59:28<50:33,  6.76s/it]

 88%|████████▊ | 3396/3844 [6:59:41<1:05:17,  8.74s/it]
[2024-05-26 02:41:22,553] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 3397/3844 [6:59:47<59:45,  8.02s/it]

 88%|████████▊ | 3398/3844 [6:59:53<54:26,  7.32s/it]
{'loss': 1.2733, 'grad_norm': 0.43324254526255107, 'learning_rate': 6.980222663906932e-07, 'epoch': 0.88}


 88%|████████▊ | 3400/3844 [7:00:16<1:10:48,  9.57s/it]
{'loss': 1.0013, 'grad_norm': 0.34512648065161144, 'learning_rate': 6.918490593273663e-07, 'epoch': 0.88}


 89%|████████▊ | 3402/3844 [7:00:30<1:00:31,  8.22s/it]

 89%|████████▊ | 3403/3844 [7:00:35<54:19,  7.39s/it]
{'loss': 1.1926, 'grad_norm': 0.4991247263056933, 'learning_rate': 6.826388303095721e-07, 'epoch': 0.89}

 89%|████████▊ | 3404/3844 [7:00:40<49:44,  6.78s/it]

 89%|████████▊ | 3405/3844 [7:00:48<52:08,  7.13s/it]

 89%|████████▊ | 3406/3844 [7:00:54<49:50,  6.83s/it]

 89%|████████▊ | 3407/3844 [7:01:05<57:55,  7.95s/it]
{'loss': 1.2183, 'grad_norm': 0.38230907274590215, 'learning_rate': 6.704511686998194e-07, 'epoch': 0.89}


 89%|████████▊ | 3409/3844 [7:01:17<51:34,  7.11s/it]
{'loss': 1.1313, 'grad_norm': 0.45021062828103536, 'learning_rate': 6.643970813268308e-07, 'epoch': 0.89}


 89%|████████▊ | 3411/3844 [7:01:34<53:53,  7.47s/it]
{'loss': 1.2033, 'grad_norm': 0.4032680790056394, 'learning_rate': 6.583695125297274e-07, 'epoch': 0.89}

 89%|████████▉ | 3412/3844 [7:01:43<56:56,  7.91s/it]


 89%|████████▉ | 3414/3844 [7:02:08<1:12:27, 10.11s/it]
{'loss': 1.1079, 'grad_norm': 0.4847439109163483, 'learning_rate': 6.493779190972371e-07, 'epoch': 0.89}

 89%|████████▉ | 3415/3844 [7:02:17<1:09:39,  9.74s/it]


 89%|████████▉ | 3417/3844 [7:02:30<57:30,  8.08s/it]

 89%|████████▉ | 3418/3844 [7:02:36<52:55,  7.45s/it]

 89%|████████▉ | 3419/3844 [7:02:44<53:44,  7.59s/it]

 89%|████████▉ | 3420/3844 [7:02:50<51:01,  7.22s/it]

 89%|████████▉ | 3421/3844 [7:02:56<49:12,  6.98s/it]

 89%|████████▉ | 3422/3844 [7:03:07<57:53,  8.23s/it]
{'loss': 1.0877, 'grad_norm': 0.37597921074289065, 'learning_rate': 6.256926650299344e-07, 'epoch': 0.89}


 89%|████████▉ | 3424/3844 [7:03:22<56:01,  8.00s/it]
{'loss': 1.1636, 'grad_norm': 0.4102814457163967, 'learning_rate': 6.198378809496952e-07, 'epoch': 0.89}


 89%|████████▉ | 3426/3844 [7:03:36<51:29,  7.39s/it]
{'loss': 1.2033, 'grad_norm': 0.40146651728817195, 'learning_rate': 6.140097420195479e-07, 'epoch': 0.89}


 89%|████████▉ | 3428/3844 [7:03:52<52:20,  7.55s/it]

 89%|████████▉ | 3429/3844 [7:03:58<47:42,  6.90s/it]

 89%|████████▉ | 3430/3844 [7:04:04<46:45,  6.78s/it]

 89%|████████▉ | 3431/3844 [7:04:10<44:33,  6.47s/it]

 89%|████████▉ | 3432/3844 [7:04:16<42:48,  6.23s/it]

 89%|████████▉ | 3433/3844 [7:04:23<45:38,  6.66s/it]

 89%|████████▉ | 3434/3844 [7:04:33<51:38,  7.56s/it]

 89%|████████▉ | 3435/3844 [7:04:39<49:11,  7.22s/it]

 89%|████████▉ | 3436/3844 [7:04:48<51:23,  7.56s/it]

 89%|████████▉ | 3437/3844 [7:04:54<48:43,  7.18s/it]

 89%|████████▉ | 3438/3844 [7:05:02<51:09,  7.56s/it]

 89%|████████▉ | 3439/3844 [7:05:08<47:05,  6.98s/it]
{'loss': 1.1949, 'grad_norm': 0.3975500403321656, 'learning_rate': 5.767774506634361e-07, 'epoch': 0.89}

 89%|████████▉ | 3440/3844 [7:05:15<46:02,  6.84s/it]

 90%|████████▉ | 3441/3844 [7:05:21<45:34,  6.78s/it]


 90%|████████▉ | 3443/3844 [7:05:34<44:40,  6.68s/it]
{'loss': 1.2679, 'grad_norm': 0.39894550038395915, 'learning_rate': 5.655486784399089e-07, 'epoch': 0.9}

 90%|████████▉ | 3444/3844 [7:05:45<53:17,  7.99s/it]

 90%|████████▉ | 3445/3844 [7:05:53<53:05,  7.98s/it]


 90%|████████▉ | 3447/3844 [7:06:08<49:37,  7.50s/it]

 90%|████████▉ | 3448/3844 [7:06:13<45:12,  6.85s/it]

 90%|████████▉ | 3449/3844 [7:06:20<44:03,  6.69s/it]

 90%|████████▉ | 3450/3844 [7:06:26<42:13,  6.43s/it]

 90%|████████▉ | 3451/3844 [7:06:34<46:48,  7.15s/it]
{'loss': 1.2489, 'grad_norm': 0.3766535671177818, 'learning_rate': 5.434128524899174e-07, 'epoch': 0.9}


 90%|████████▉ | 3453/3844 [7:06:47<44:23,  6.81s/it]
{'loss': 1.2146, 'grad_norm': 0.4088705339193184, 'learning_rate': 5.379460124985359e-07, 'epoch': 0.9}

 90%|████████▉ | 3454/3844 [7:06:53<42:12,  6.49s/it]

 90%|████████▉ | 3455/3844 [7:06:59<40:56,  6.32s/it]


 90%|████████▉ | 3457/3844 [7:07:12<41:37,  6.45s/it]
{'loss': 1.1746, 'grad_norm': 0.38080439777297875, 'learning_rate': 5.270929812812254e-07, 'epoch': 0.9}


 90%|████████▉ | 3459/3844 [7:07:26<43:27,  6.77s/it]

 90%|█████████ | 3460/3844 [7:07:32<41:30,  6.49s/it]

 90%|█████████ | 3461/3844 [7:07:42<48:29,  7.60s/it]

 90%|█████████ | 3462/3844 [7:07:50<49:53,  7.84s/it]

 90%|█████████ | 3463/3844 [7:07:56<45:00,  7.09s/it]
{'loss': 1.1712, 'grad_norm': 0.44064046293036885, 'learning_rate': 5.110152870128971e-07, 'epoch': 0.9}


 90%|█████████ | 3465/3844 [7:08:10<45:33,  7.21s/it]

 90%|█████████ | 3466/3844 [7:08:18<46:35,  7.40s/it]

 90%|█████████ | 3467/3844 [7:08:24<43:40,  6.95s/it]
{'loss': 1.2693, 'grad_norm': 0.40048759579837634, 'learning_rate': 5.004315701447593e-07, 'epoch': 0.9}


 90%|█████████ | 3469/3844 [7:08:41<47:36,  7.62s/it]

 90%|█████████ | 3470/3844 [7:08:46<43:33,  6.99s/it]

 90%|█████████ | 3471/3844 [7:08:55<46:16,  7.44s/it]

 90%|█████████ | 3472/3844 [7:09:02<45:41,  7.37s/it]

 90%|█████████ | 3473/3844 [7:09:10<46:34,  7.53s/it]

 90%|█████████ | 3474/3844 [7:09:16<43:36,  7.07s/it]

 90%|█████████ | 3475/3844 [7:09:22<41:46,  6.79s/it]

 90%|█████████ | 3476/3844 [7:09:29<41:35,  6.78s/it]
{'loss': 1.1266, 'grad_norm': 0.4092655461571875, 'learning_rate': 4.770130340112566e-07, 'epoch': 0.9}


 90%|█████████ | 3478/3844 [7:09:42<41:50,  6.86s/it]
{'loss': 1.1875, 'grad_norm': 0.44918608956276856, 'learning_rate': 4.7188325757420604e-07, 'epoch': 0.9}

 91%|█████████ | 3479/3844 [7:09:49<42:08,  6.93s/it]


 91%|█████████ | 3481/3844 [7:10:04<43:14,  7.15s/it]
{'loss': 1.0974, 'grad_norm': 0.4068033963005132, 'learning_rate': 4.6423934512837667e-07, 'epoch': 0.91}

 91%|█████████ | 3482/3844 [7:10:13<45:50,  7.60s/it]

 91%|█████████ | 3483/3844 [7:10:19<42:47,  7.11s/it]


 91%|█████████ | 3485/3844 [7:10:34<44:20,  7.41s/it]
{'loss': 1.0007, 'grad_norm': 0.4138823542748575, 'learning_rate': 4.5414227522062594e-07, 'epoch': 0.91}


 91%|█████████ | 3487/3844 [7:10:48<42:27,  7.14s/it]
{'loss': 1.1127, 'grad_norm': 0.41178014656112755, 'learning_rate': 4.491344068494774e-07, 'epoch': 0.91}

 91%|█████████ | 3488/3844 [7:10:53<39:22,  6.64s/it]


 91%|█████████ | 3490/3844 [7:11:05<36:04,  6.11s/it]

 91%|█████████ | 3491/3844 [7:11:15<43:21,  7.37s/it]

 91%|█████████ | 3492/3844 [7:11:22<42:39,  7.27s/it]

 91%|█████████ | 3493/3844 [7:11:30<44:47,  7.66s/it]
{'loss': 1.1721, 'grad_norm': 0.38913580100555684, 'learning_rate': 4.3427363853895235e-07, 'epoch': 0.91}

 91%|█████████ | 3494/3844 [7:11:39<46:58,  8.05s/it]

 91%|█████████ | 3495/3844 [7:11:45<43:00,  7.39s/it]


 91%|█████████ | 3497/3844 [7:12:01<44:26,  7.68s/it]

 91%|█████████ | 3498/3844 [7:12:09<45:02,  7.81s/it]

 91%|█████████ | 3499/3844 [7:12:16<43:31,  7.57s/it]

 91%|█████████ | 3500/3844 [7:12:24<45:05,  7.86s/it]
{'loss': 1.0958, 'grad_norm': 0.3633822347317478, 'learning_rate': 4.1724518572412774e-07, 'epoch': 0.91}


 91%|█████████ | 3502/3844 [7:12:39<43:19,  7.60s/it]

 91%|█████████ | 3503/3844 [7:12:45<40:39,  7.16s/it]

 91%|█████████ | 3504/3844 [7:12:51<37:53,  6.69s/it]

 91%|█████████ | 3505/3844 [7:12:58<39:29,  6.99s/it]

 91%|█████████ | 3506/3844 [7:13:05<38:52,  6.90s/it]
{'loss': 0.9181, 'grad_norm': 0.4000395402573299, 'learning_rate': 4.029147473362849e-07, 'epoch': 0.91}

 91%|█████████ | 3507/3844 [7:13:15<44:25,  7.91s/it]


 91%|█████████▏| 3509/3844 [7:13:27<37:55,  6.79s/it]

 91%|█████████▏| 3510/3844 [7:13:34<38:33,  6.93s/it]
{'loss': 1.2076, 'grad_norm': 0.391132878122223, 'learning_rate': 3.934974058231522e-07, 'epoch': 0.91}


 91%|█████████▏| 3512/3844 [7:13:54<45:54,  8.30s/it]
{'loss': 1.1291, 'grad_norm': 0.39031426886303905, 'learning_rate': 3.888296605325237e-07, 'epoch': 0.91}


 91%|█████████▏| 3514/3844 [7:14:09<43:09,  7.85s/it]
{'loss': 1.1661, 'grad_norm': 0.4470768824632579, 'learning_rate': 3.8418921659053143e-07, 'epoch': 0.91}


 91%|█████████▏| 3516/3844 [7:14:23<40:59,  7.50s/it]

 91%|█████████▏| 3517/3844 [7:14:30<40:48,  7.49s/it]

 92%|█████████▏| 3518/3844 [7:14:39<41:57,  7.72s/it]
{'loss': 1.1756, 'grad_norm': 0.41635471267987395, 'learning_rate': 3.749902854011678e-07, 'epoch': 0.92}


 92%|█████████▏| 3520/3844 [7:14:52<38:35,  7.15s/it]
{'loss': 1.238, 'grad_norm': 0.4188020428648678, 'learning_rate': 3.7043182428414537e-07, 'epoch': 0.92}

 92%|█████████▏| 3521/3844 [7:15:00<39:11,  7.28s/it]

 92%|█████████▏| 3522/3844 [7:15:06<37:00,  6.90s/it]


 92%|█████████▏| 3524/3844 [7:15:20<38:05,  7.14s/it]

 92%|█████████▏| 3525/3844 [7:15:28<39:43,  7.47s/it]
{'loss': 1.0271, 'grad_norm': 0.4175044382871083, 'learning_rate': 3.591553716661422e-07, 'epoch': 0.92}


 92%|█████████▏| 3527/3844 [7:15:40<36:06,  6.84s/it]

 92%|█████████▏| 3528/3844 [7:15:46<34:32,  6.56s/it]

 92%|█████████▏| 3529/3844 [7:15:53<34:33,  6.58s/it]
{'loss': 1.1895, 'grad_norm': 0.3751625839394135, 'learning_rate': 3.502574352955768e-07, 'epoch': 0.92}

 92%|█████████▏| 3530/3844 [7:16:02<37:40,  7.20s/it]


 92%|█████████▏| 3532/3844 [7:16:14<35:16,  6.78s/it]
{'loss': 1.2764, 'grad_norm': 0.41648182472633244, 'learning_rate': 3.43655931139768e-07, 'epoch': 0.92}

 92%|█████████▏| 3533/3844 [7:16:20<33:07,  6.39s/it]


 92%|█████████▏| 3535/3844 [7:16:31<31:09,  6.05s/it]
{'loss': 1.272, 'grad_norm': 0.4140768143728631, 'learning_rate': 3.3711614371974123e-07, 'epoch': 0.92}


 92%|█████████▏| 3537/3844 [7:16:45<33:48,  6.61s/it]
{'loss': 1.218, 'grad_norm': 0.44154198638981296, 'learning_rate': 3.3279059317230856e-07, 'epoch': 0.92}

 92%|█████████▏| 3538/3844 [7:16:53<36:02,  7.07s/it]


 92%|█████████▏| 3540/3844 [7:17:06<34:05,  6.73s/it]

 92%|█████████▏| 3541/3844 [7:17:12<33:07,  6.56s/it]

 92%|█████████▏| 3542/3844 [7:17:22<38:15,  7.60s/it]
{'loss': 1.0482, 'grad_norm': 0.39394140516704196, 'learning_rate': 3.2209688331895304e-07, 'epoch': 0.92}


 92%|█████████▏| 3544/3844 [7:17:35<34:33,  6.91s/it]

 92%|█████████▏| 3545/3844 [7:17:41<33:37,  6.75s/it]

 92%|█████████▏| 3546/3844 [7:17:51<38:37,  7.78s/it]
{'loss': 0.929, 'grad_norm': 0.36968742363566004, 'learning_rate': 3.136656153615847e-07, 'epoch': 0.92}

 92%|█████████▏| 3547/3844 [7:17:58<37:06,  7.49s/it]

 92%|█████████▏| 3548/3844 [7:18:04<34:24,  6.98s/it]

 92%|█████████▏| 3549/3844 [7:18:10<32:50,  6.68s/it]

 92%|█████████▏| 3550/3844 [7:18:16<32:16,  6.59s/it]


 92%|█████████▏| 3552/3844 [7:18:27<29:32,  6.07s/it]

 92%|█████████▏| 3553/3844 [7:18:33<29:05,  6.00s/it]

 92%|█████████▏| 3554/3844 [7:18:39<28:41,  5.94s/it]

 92%|█████████▏| 3555/3844 [7:18:45<28:24,  5.90s/it]

 93%|█████████▎| 3556/3844 [7:18:51<29:07,  6.07s/it]

 93%|█████████▎| 3557/3844 [7:18:57<29:10,  6.10s/it]

 93%|█████████▎| 3558/3844 [7:19:03<28:45,  6.03s/it]

 93%|█████████▎| 3559/3844 [7:19:13<33:30,  7.05s/it]
{'loss': 0.9876, 'grad_norm': 0.39531040233614295, 'learning_rate': 2.870245796316384e-07, 'epoch': 0.93}

 93%|█████████▎| 3560/3844 [7:19:22<36:28,  7.70s/it]


 93%|█████████▎| 3562/3844 [7:19:35<33:55,  7.22s/it]

 93%|█████████▎| 3563/3844 [7:19:44<36:42,  7.84s/it]

 93%|█████████▎| 3564/3844 [7:19:55<41:06,  8.81s/it]
{'loss': 1.1056, 'grad_norm': 0.3844396663993851, 'learning_rate': 2.7708824782764155e-07, 'epoch': 0.93}

 93%|█████████▎| 3565/3844 [7:20:02<38:11,  8.21s/it]


 93%|█████████▎| 3567/3844 [7:20:15<33:19,  7.22s/it]

 93%|█████████▎| 3568/3844 [7:20:23<33:38,  7.31s/it]

 93%|█████████▎| 3569/3844 [7:20:30<34:09,  7.45s/it]

 93%|█████████▎| 3570/3844 [7:20:42<40:12,  8.80s/it]
{'loss': 1.0492, 'grad_norm': 0.39573437596517347, 'learning_rate': 2.653925194547513e-07, 'epoch': 0.93}


 93%|█████████▎| 3572/3844 [7:21:05<45:15,  9.98s/it]

 93%|█████████▎| 3573/3844 [7:21:11<39:12,  8.68s/it]

 93%|█████████▎| 3574/3844 [7:21:17<35:54,  7.98s/it]
{'loss': 1.1202, 'grad_norm': 0.40754403738327455, 'learning_rate': 2.577336086608284e-07, 'epoch': 0.93}

 93%|█████████▎| 3575/3844 [7:21:24<34:02,  7.59s/it]


 93%|█████████▎| 3577/3844 [7:21:39<34:06,  7.66s/it]
{'loss': 1.2058, 'grad_norm': 0.4288287383632152, 'learning_rate': 2.520620643646354e-07, 'epoch': 0.93}


 93%|█████████▎| 3579/3844 [7:21:53<31:34,  7.15s/it]

 93%|█████████▎| 3580/3844 [7:21:59<30:00,  6.82s/it]
{'loss': 1.1433, 'grad_norm': 0.4004731474394726, 'learning_rate': 2.46452822209472e-07, 'epoch': 0.93}

 93%|█████████▎| 3581/3844 [7:22:06<30:46,  7.02s/it]

 93%|█████████▎| 3582/3844 [7:22:14<31:41,  7.26s/it]


 93%|█████████▎| 3584/3844 [7:22:25<27:53,  6.44s/it]

 93%|█████████▎| 3585/3844 [7:22:33<28:45,  6.66s/it]

 93%|█████████▎| 3586/3844 [7:22:39<28:07,  6.54s/it]

 93%|█████████▎| 3587/3844 [7:22:45<27:40,  6.46s/it]

 93%|█████████▎| 3588/3844 [7:22:53<29:25,  6.90s/it]

 93%|█████████▎| 3589/3844 [7:22:59<28:30,  6.71s/it]
{'loss': 1.1175, 'grad_norm': 0.39135943222386865, 'learning_rate': 2.2999926510214654e-07, 'epoch': 0.93}


 93%|█████████▎| 3591/3844 [7:23:15<30:02,  7.13s/it]

 93%|█████████▎| 3592/3844 [7:23:22<29:42,  7.07s/it]

 93%|█████████▎| 3593/3844 [7:23:27<27:27,  6.56s/it]
{'loss': 1.2241, 'grad_norm': 0.42492416481411016, 'learning_rate': 2.2286690852742933e-07, 'epoch': 0.93}


 94%|█████████▎| 3595/3844 [7:23:39<26:20,  6.35s/it]
{'loss': 1.1066, 'grad_norm': 0.4083538054188177, 'learning_rate': 2.1934238436473797e-07, 'epoch': 0.94}


 94%|█████████▎| 3597/3844 [7:23:53<26:59,  6.56s/it]

 94%|█████████▎| 3598/3844 [7:23:59<26:48,  6.54s/it]

 94%|█████████▎| 3599/3844 [7:24:05<26:01,  6.37s/it]

 94%|█████████▎| 3600/3844 [7:24:11<24:47,  6.10s/it]
 94%|█████████▎| 3600/3844 [7:24:11<24:47,  6.10s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 94%|█████████▎| 3601/3844 [7:24:49<1:03:54, 15.78s/it]

 94%|█████████▎| 3602/3844 [7:24:58<54:49, 13.59s/it]

 94%|█████████▎| 3603/3844 [7:25:03<44:41, 11.13s/it]

 94%|█████████▍| 3604/3844 [7:25:09<38:10,  9.54s/it]

 94%|█████████▍| 3605/3844 [7:25:15<34:25,  8.64s/it]
{'loss': 1.0778, 'grad_norm': 0.4115954510256249, 'learning_rate': 2.021367028981025e-07, 'epoch': 0.94}


 94%|█████████▍| 3607/3844 [7:25:32<33:21,  8.44s/it]

 94%|█████████▍| 3608/3844 [7:25:38<30:29,  7.75s/it]

 94%|█████████▍| 3609/3844 [7:25:44<28:34,  7.30s/it]

 94%|█████████▍| 3610/3844 [7:25:53<30:26,  7.80s/it]

 94%|█████████▍| 3611/3844 [7:26:00<29:21,  7.56s/it]

 94%|█████████▍| 3612/3844 [7:26:06<27:08,  7.02s/it]

 94%|█████████▍| 3613/3844 [7:26:12<25:49,  6.71s/it]

 94%|█████████▍| 3614/3844 [7:26:17<24:34,  6.41s/it]

 94%|█████████▍| 3615/3844 [7:26:26<26:40,  6.99s/it]

 94%|█████████▍| 3616/3844 [7:26:33<26:51,  7.07s/it]
{'loss': 1.1507, 'grad_norm': 0.4003205052788642, 'learning_rate': 1.840141336392487e-07, 'epoch': 0.94}

 94%|█████████▍| 3617/3844 [7:26:40<26:47,  7.08s/it]


 94%|█████████▍| 3619/3844 [7:26:52<24:12,  6.46s/it]

 94%|█████████▍| 3620/3844 [7:26:58<23:47,  6.37s/it]
{'loss': 1.2728, 'grad_norm': 0.41077387682638455, 'learning_rate': 1.7763315094439737e-07, 'epoch': 0.94}


 94%|█████████▍| 3622/3844 [7:27:13<25:09,  6.80s/it]
{'loss': 1.1819, 'grad_norm': 0.40941803103556035, 'learning_rate': 1.7448450699083296e-07, 'epoch': 0.94}


 94%|█████████▍| 3624/3844 [7:27:24<22:15,  6.07s/it]

 94%|█████████▍| 3625/3844 [7:27:29<21:31,  5.90s/it]

 94%|█████████▍| 3626/3844 [7:27:39<25:40,  7.07s/it]

 94%|█████████▍| 3627/3844 [7:27:45<24:41,  6.83s/it]

 94%|█████████▍| 3628/3844 [7:27:51<23:45,  6.60s/it]

 94%|█████████▍| 3629/3844 [7:27:57<22:43,  6.34s/it]
{'loss': 0.9912, 'grad_norm': 0.41005832225621963, 'learning_rate': 1.6368410407057834e-07, 'epoch': 0.94}

 94%|█████████▍| 3630/3844 [7:28:03<21:47,  6.11s/it]

 94%|█████████▍| 3631/3844 [7:28:09<21:48,  6.14s/it]

 94%|█████████▍| 3632/3844 [7:28:17<23:38,  6.69s/it]

 95%|█████████▍| 3633/3844 [7:28:23<22:42,  6.46s/it]


 95%|█████████▍| 3635/3844 [7:28:39<26:16,  7.54s/it]
{'loss': 0.9585, 'grad_norm': 0.3814410520795472, 'learning_rate': 1.5469902492045385e-07, 'epoch': 0.95}


 95%|█████████▍| 3637/3844 [7:28:53<24:56,  7.23s/it]

 95%|█████████▍| 3638/3844 [7:28:59<23:26,  6.83s/it]

 95%|█████████▍| 3639/3844 [7:29:05<22:56,  6.71s/it]

 95%|█████████▍| 3640/3844 [7:29:12<22:24,  6.59s/it]

 95%|█████████▍| 3641/3844 [7:29:20<24:05,  7.12s/it]

 95%|█████████▍| 3642/3844 [7:29:25<22:16,  6.61s/it]

 95%|█████████▍| 3643/3844 [7:29:32<21:43,  6.48s/it]

 95%|█████████▍| 3644/3844 [7:29:39<22:46,  6.83s/it]

 95%|█████████▍| 3645/3844 [7:29:48<24:33,  7.40s/it]

 95%|█████████▍| 3646/3844 [7:29:53<22:20,  6.77s/it]
{'loss': 1.2074, 'grad_norm': 0.45290500976972664, 'learning_rate': 1.3888025483588142e-07, 'epoch': 0.95}


 95%|█████████▍| 3648/3844 [7:30:07<22:08,  6.78s/it]
{'loss': 1.2131, 'grad_norm': 0.4059297523377011, 'learning_rate': 1.3609511241659323e-07, 'epoch': 0.95}

 95%|█████████▍| 3649/3844 [7:30:13<21:06,  6.49s/it]


 95%|█████████▍| 3651/3844 [7:30:25<20:23,  6.34s/it]
{'loss': 1.0643, 'grad_norm': 0.3974686377453012, 'learning_rate': 1.3196993735571484e-07, 'epoch': 0.95}


 95%|█████████▌| 3653/3844 [7:30:36<18:42,  5.88s/it]

 95%|█████████▌| 3654/3844 [7:30:42<18:16,  5.77s/it]

 95%|█████████▌| 3655/3844 [7:30:48<18:39,  5.92s/it]
{'loss': 1.0324, 'grad_norm': 0.406728426112115, 'learning_rate': 1.2656781684309837e-07, 'epoch': 0.95}


 95%|█████████▌| 3657/3844 [7:31:04<22:28,  7.21s/it]

 95%|█████████▌| 3658/3844 [7:31:12<22:28,  7.25s/it]

 95%|█████████▌| 3659/3844 [7:31:19<22:28,  7.29s/it]

 95%|█████████▌| 3660/3844 [7:31:25<21:24,  6.98s/it]

 95%|█████████▌| 3661/3844 [7:31:32<20:51,  6.84s/it]

 95%|█████████▌| 3662/3844 [7:31:38<20:01,  6.60s/it]
{'loss': 1.0606, 'grad_norm': 0.43025510710599574, 'learning_rate': 1.1738408784520482e-07, 'epoch': 0.95}


 95%|█████████▌| 3664/3844 [7:31:52<20:34,  6.86s/it]

 95%|█████████▌| 3665/3844 [7:31:58<19:50,  6.65s/it]

 95%|█████████▌| 3666/3844 [7:32:03<18:33,  6.26s/it]
{'loss': 1.2682, 'grad_norm': 0.4657209507009032, 'learning_rate': 1.1229062055973072e-07, 'epoch': 0.95}

 95%|█████████▌| 3667/3844 [7:32:13<21:22,  7.25s/it]

 95%|█████████▌| 3668/3844 [7:32:21<21:59,  7.50s/it]


 95%|█████████▌| 3670/3844 [7:32:34<19:58,  6.89s/it]
{'loss': 1.1966, 'grad_norm': 0.42797798735119325, 'learning_rate': 1.0730950071987656e-07, 'epoch': 0.95}


 96%|█████████▌| 3672/3844 [7:32:52<22:04,  7.70s/it]

 96%|█████████▌| 3673/3844 [7:32:58<20:50,  7.32s/it]

 96%|█████████▌| 3674/3844 [7:33:07<22:35,  7.97s/it]

 96%|█████████▌| 3675/3844 [7:33:16<23:06,  8.21s/it]

 96%|█████████▌| 3676/3844 [7:33:22<21:19,  7.61s/it]

 96%|█████████▌| 3677/3844 [7:33:28<19:27,  6.99s/it]

 96%|█████████▌| 3678/3844 [7:33:34<18:49,  6.80s/it]

 96%|█████████▌| 3679/3844 [7:33:41<18:41,  6.80s/it]

 96%|█████████▌| 3680/3844 [7:33:48<18:52,  6.91s/it]

 96%|█████████▌| 3681/3844 [7:33:56<19:36,  7.22s/it]

 96%|█████████▌| 3682/3844 [7:34:02<18:34,  6.88s/it]
{'loss': 1.3005, 'grad_norm': 0.41346798272863194, 'learning_rate': 9.304078545897766e-08, 'epoch': 0.96}


 96%|█████████▌| 3684/3844 [7:34:18<19:30,  7.32s/it]

 96%|█████████▌| 3685/3844 [7:34:24<18:33,  7.00s/it]
{'loss': 1.2841, 'grad_norm': 0.4053301292141185, 'learning_rate': 8.963184689763316e-08, 'epoch': 0.96}


 96%|█████████▌| 3687/3844 [7:34:37<18:18,  7.00s/it]

 96%|█████████▌| 3688/3844 [7:34:44<18:00,  6.93s/it]
{'loss': 1.1328, 'grad_norm': 0.411588357247109, 'learning_rate': 8.62862486199878e-08, 'epoch': 0.96}

 96%|█████████▌| 3689/3844 [7:34:51<17:50,  6.91s/it]

 96%|█████████▌| 3690/3844 [7:35:01<20:19,  7.92s/it]


 96%|█████████▌| 3692/3844 [7:35:18<20:46,  8.20s/it]
{'loss': 1.1805, 'grad_norm': 0.38340964869606126, 'learning_rate': 8.192401714621101e-08, 'epoch': 0.96}


 96%|█████████▌| 3694/3844 [7:35:32<18:43,  7.49s/it]

 96%|█████████▌| 3695/3844 [7:35:37<17:27,  7.03s/it]

 96%|█████████▌| 3696/3844 [7:35:44<17:10,  6.96s/it]

 96%|█████████▌| 3697/3844 [7:35:52<17:31,  7.15s/it]

 96%|█████████▌| 3698/3844 [7:35:58<16:49,  6.91s/it]

 96%|█████████▌| 3699/3844 [7:36:06<17:06,  7.08s/it]
{'loss': 1.2215, 'grad_norm': 0.40435063904550467, 'learning_rate': 7.456130744410539e-08, 'epoch': 0.96}


 96%|█████████▋| 3701/3844 [7:36:24<19:28,  8.17s/it]

 96%|█████████▋| 3702/3844 [7:36:30<18:20,  7.75s/it]

 96%|█████████▋| 3703/3844 [7:36:36<16:28,  7.01s/it]

 96%|█████████▋| 3704/3844 [7:36:44<17:22,  7.45s/it]

 96%|█████████▋| 3705/3844 [7:36:50<16:17,  7.03s/it]

 96%|█████████▋| 3706/3844 [7:36:56<15:10,  6.60s/it]

 96%|█████████▋| 3707/3844 [7:37:02<14:43,  6.45s/it]
{'loss': 1.1465, 'grad_norm': 0.4455279301771762, 'learning_rate': 6.656970615254943e-08, 'epoch': 0.96}

 96%|█████████▋| 3708/3844 [7:37:09<15:15,  6.74s/it]


 97%|█████████▋| 3710/3844 [7:37:23<15:19,  6.86s/it]

 97%|█████████▋| 3711/3844 [7:37:32<16:10,  7.30s/it]

 97%|█████████▋| 3712/3844 [7:37:38<15:36,  7.10s/it]
{'loss': 0.9736, 'grad_norm': 0.39077363244993485, 'learning_rate': 6.180419346888511e-08, 'epoch': 0.97}


 97%|█████████▋| 3714/3844 [7:38:00<19:36,  9.05s/it]

 97%|█████████▋| 3715/3844 [7:38:07<18:15,  8.50s/it]
{'loss': 1.2459, 'grad_norm': 0.3808081006214687, 'learning_rate': 5.902957134604981e-08, 'epoch': 0.97}


 97%|█████████▋| 3717/3844 [7:38:23<17:46,  8.40s/it]
{'loss': 1.1845, 'grad_norm': 0.39516891614044686, 'learning_rate': 5.7215119893142544e-08, 'epoch': 0.97}


 97%|█████████▋| 3719/3844 [7:38:40<17:34,  8.43s/it]
{'loss': 1.1134, 'grad_norm': 0.4051915622214619, 'learning_rate': 5.542891176824161e-08, 'epoch': 0.97}


 97%|█████████▋| 3721/3844 [7:38:55<15:38,  7.63s/it]

 97%|█████████▋| 3722/3844 [7:39:02<15:40,  7.71s/it]

 97%|█████████▋| 3723/3844 [7:39:08<14:26,  7.16s/it]

 97%|█████████▋| 3724/3844 [7:39:18<15:48,  7.90s/it]
{'loss': 1.0581, 'grad_norm': 0.41758664734672823, 'learning_rate': 5.108698911585386e-08, 'epoch': 0.97}


 97%|█████████▋| 3726/3844 [7:39:37<16:54,  8.59s/it]

 97%|█████████▋| 3727/3844 [7:39:42<14:58,  7.68s/it]

 97%|█████████▋| 3728/3844 [7:39:48<13:58,  7.23s/it]

 97%|█████████▋| 3729/3844 [7:39:56<14:14,  7.43s/it]
{'loss': 1.0592, 'grad_norm': 0.4144027304942774, 'learning_rate': 4.692169584073147e-08, 'epoch': 0.97}

 97%|█████████▋| 3730/3844 [7:40:05<14:54,  7.84s/it]

 97%|█████████▋| 3731/3844 [7:40:11<13:42,  7.28s/it]

 97%|█████████▋| 3732/3844 [7:40:17<13:08,  7.04s/it]


 97%|█████████▋| 3734/3844 [7:40:30<12:27,  6.80s/it]

 97%|█████████▋| 3735/3844 [7:40:40<13:46,  7.58s/it]
{'loss': 1.0945, 'grad_norm': 0.43862467252221676, 'learning_rate': 4.2156598588066754e-08, 'epoch': 0.97}

 97%|█████████▋| 3736/3844 [7:40:49<14:35,  8.10s/it]


 97%|█████████▋| 3738/3844 [7:41:02<12:51,  7.27s/it]

 97%|█████████▋| 3739/3844 [7:41:08<12:17,  7.02s/it]

 97%|█████████▋| 3740/3844 [7:41:15<11:55,  6.88s/it]

 97%|█████████▋| 3741/3844 [7:41:22<11:47,  6.87s/it]

 97%|█████████▋| 3742/3844 [7:41:33<13:50,  8.14s/it]

 97%|█████████▋| 3743/3844 [7:41:40<13:01,  7.74s/it]
{'loss': 1.1603, 'grad_norm': 0.4575182646038009, 'learning_rate': 3.619916039494964e-08, 'epoch': 0.97}

 97%|█████████▋| 3744/3844 [7:41:45<11:42,  7.03s/it]


 97%|█████████▋| 3746/3844 [7:41:57<10:21,  6.34s/it]
{'loss': 1.281, 'grad_norm': 0.41418502299254367, 'learning_rate': 3.4081856752131714e-08, 'epoch': 0.97}


 98%|█████████▊| 3748/3844 [7:42:14<12:08,  7.59s/it]
{'loss': 1.083, 'grad_norm': 0.4177024379134149, 'learning_rate': 3.270570646121907e-08, 'epoch': 0.97}


 98%|█████████▊| 3750/3844 [7:42:30<12:23,  7.91s/it]

 98%|█████████▊| 3751/3844 [7:42:40<13:16,  8.57s/it]

 98%|█████████▊| 3752/3844 [7:42:46<11:57,  7.80s/it]

 98%|█████████▊| 3753/3844 [7:42:53<11:23,  7.51s/it]
{'loss': 1.2417, 'grad_norm': 0.42797079406675664, 'learning_rate': 2.9389208239348766e-08, 'epoch': 0.98}


 98%|█████████▊| 3755/3844 [7:43:06<10:22,  6.99s/it]

 98%|█████████▊| 3756/3844 [7:43:14<10:54,  7.44s/it]

 98%|█████████▊| 3757/3844 [7:43:20<10:04,  6.94s/it]

 98%|█████████▊| 3758/3844 [7:43:26<09:40,  6.75s/it]

 98%|█████████▊| 3759/3844 [7:43:36<10:54,  7.70s/it]

 98%|█████████▊| 3760/3844 [7:43:44<10:54,  7.79s/it]

 98%|█████████▊| 3761/3844 [7:43:50<09:59,  7.23s/it]
{'loss': 1.1827, 'grad_norm': 0.4237242105483691, 'learning_rate': 2.4451024752680795e-08, 'epoch': 0.98}


 98%|█████████▊| 3763/3844 [7:44:06<10:12,  7.56s/it]

 98%|█████████▊| 3764/3844 [7:44:14<10:29,  7.87s/it]
{'loss': 1.1812, 'grad_norm': 0.42492107588899064, 'learning_rate': 2.2716081770980392e-08, 'epoch': 0.98}


 98%|█████████▊| 3766/3844 [7:44:30<10:22,  7.99s/it]

 98%|█████████▊| 3767/3844 [7:44:36<09:33,  7.45s/it]
{'loss': 1.1828, 'grad_norm': 0.4447255469268364, 'learning_rate': 2.1044906752706317e-08, 'epoch': 0.98}


 98%|█████████▊| 3769/3844 [7:44:52<09:40,  7.74s/it]

 98%|█████████▊| 3770/3844 [7:44:59<09:13,  7.47s/it]
{'loss': 1.1266, 'grad_norm': 0.38970128723441166, 'learning_rate': 1.9437510378864653e-08, 'epoch': 0.98}


 98%|█████████▊| 3772/3844 [7:45:13<08:35,  7.16s/it]
{'loss': 1.1698, 'grad_norm': 0.3880223762751618, 'learning_rate': 1.8401350466592526e-08, 'epoch': 0.98}


 98%|█████████▊| 3774/3844 [7:45:30<08:56,  7.67s/it]

 98%|█████████▊| 3775/3844 [7:45:38<09:01,  7.84s/it]

 98%|█████████▊| 3776/3844 [7:45:44<08:09,  7.20s/it]

 98%|█████████▊| 3777/3844 [7:45:53<08:31,  7.64s/it]

 98%|█████████▊| 3778/3844 [7:46:00<08:21,  7.60s/it]

 98%|█████████▊| 3779/3844 [7:46:06<07:42,  7.11s/it]
{'loss': 1.1888, 'grad_norm': 0.45911765134231874, 'learning_rate': 1.499809381915629e-08, 'epoch': 0.98}

 98%|█████████▊| 3780/3844 [7:46:14<07:44,  7.26s/it]

 98%|█████████▊| 3781/3844 [7:46:22<07:50,  7.46s/it]

 98%|█████████▊| 3782/3844 [7:46:27<07:07,  6.89s/it]


 98%|█████████▊| 3784/3844 [7:46:41<06:48,  6.81s/it]

 98%|█████████▊| 3785/3844 [7:46:47<06:21,  6.46s/it]

 98%|█████████▊| 3786/3844 [7:46:54<06:26,  6.66s/it]
{'loss': 1.1626, 'grad_norm': 0.41314074540910073, 'learning_rate': 1.1942286047514906e-08, 'epoch': 0.98}


 99%|█████████▊| 3788/3844 [7:47:09<06:24,  6.86s/it]

 99%|█████████▊| 3789/3844 [7:47:15<06:07,  6.68s/it]

 99%|█████████▊| 3790/3844 [7:47:22<06:08,  6.82s/it]
{'loss': 1.122, 'grad_norm': 0.42676783028353565, 'learning_rate': 1.035214905229287e-08, 'epoch': 0.99}


 99%|█████████▊| 3792/3844 [7:47:34<05:34,  6.44s/it]

 99%|█████████▊| 3793/3844 [7:47:41<05:37,  6.63s/it]

 99%|█████████▊| 3794/3844 [7:47:49<05:46,  6.92s/it]

 99%|█████████▊| 3795/3844 [7:47:55<05:20,  6.53s/it]

 99%|█████████▉| 3796/3844 [7:48:00<05:02,  6.31s/it]

 99%|█████████▉| 3797/3844 [7:48:07<04:56,  6.32s/it]

 99%|█████████▉| 3798/3844 [7:48:15<05:19,  6.95s/it]

 99%|█████████▉| 3799/3844 [7:48:23<05:19,  7.09s/it]

 99%|█████████▉| 3800/3844 [7:48:32<05:47,  7.90s/it]
{'loss': 1.1982, 'grad_norm': 0.3918928615017055, 'learning_rate': 6.873430370423206e-09, 'epoch': 0.99}


 99%|█████████▉| 3802/3844 [7:48:46<05:11,  7.42s/it]

 99%|█████████▉| 3803/3844 [7:48:55<05:13,  7.65s/it]

 99%|█████████▉| 3804/3844 [7:49:01<04:55,  7.40s/it]

 99%|█████████▉| 3805/3844 [7:49:15<06:05,  9.37s/it]
{'loss': 1.0424, 'grad_norm': 0.40354443274214935, 'learning_rate': 5.400177869839818e-09, 'epoch': 0.99}


 99%|█████████▉| 3807/3844 [7:49:33<05:24,  8.77s/it]
{'loss': 1.3803, 'grad_norm': 0.4108980166190001, 'learning_rate': 4.860558846331298e-09, 'epoch': 0.99}


 99%|█████████▉| 3809/3844 [7:49:52<05:31,  9.46s/it]
{'loss': 1.13, 'grad_norm': 0.3935017335276035, 'learning_rate': 4.349331868421791e-09, 'epoch': 0.99}
[2024-05-26 03:31:43,751] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 99%|█████████▉| 3811/3844 [7:50:10<05:00,  9.11s/it]

 99%|█████████▉| 3812/3844 [7:50:19<04:46,  8.95s/it]
{'loss': 1.1438, 'grad_norm': 0.409058460287651, 'learning_rate': 3.635729641654484e-09, 'epoch': 0.99}

 99%|█████████▉| 3813/3844 [7:50:26<04:20,  8.41s/it]

 99%|█████████▉| 3814/3844 [7:50:32<03:49,  7.66s/it]

 99%|█████████▉| 3815/3844 [7:50:38<03:30,  7.25s/it]

 99%|█████████▉| 3816/3844 [7:50:44<03:09,  6.76s/it]


 99%|█████████▉| 3818/3844 [7:50:57<02:52,  6.63s/it]
{'loss': 1.3458, 'grad_norm': 0.4100872406402602, 'learning_rate': 2.4001990765321057e-09, 'epoch': 0.99}


 99%|█████████▉| 3820/3844 [7:51:09<02:33,  6.39s/it]

 99%|█████████▉| 3821/3844 [7:51:17<02:32,  6.65s/it]

 99%|█████████▉| 3822/3844 [7:51:22<02:21,  6.42s/it]

 99%|█████████▉| 3823/3844 [7:51:33<02:38,  7.53s/it]

 99%|█████████▉| 3824/3844 [7:51:39<02:25,  7.29s/it]

100%|█████████▉| 3825/3844 [7:51:46<02:12,  6.99s/it]

100%|█████████▉| 3826/3844 [7:51:55<02:16,  7.60s/it]
{'loss': 0.9935, 'grad_norm': 0.3965469003555866, 'learning_rate': 1.1504152375174659e-09, 'epoch': 1.0}

100%|█████████▉| 3827/3844 [7:52:04<02:18,  8.14s/it]


100%|█████████▉| 3829/3844 [7:52:17<01:49,  7.33s/it]

100%|█████████▉| 3830/3844 [7:52:23<01:36,  6.88s/it]

100%|█████████▉| 3831/3844 [7:52:29<01:25,  6.60s/it]

100%|█████████▉| 3832/3844 [7:52:36<01:19,  6.65s/it]

100%|█████████▉| 3833/3844 [7:52:43<01:16,  6.99s/it]

100%|█████████▉| 3834/3844 [7:52:51<01:11,  7.12s/it]

100%|█████████▉| 3835/3844 [7:52:57<01:01,  6.83s/it]

100%|█████████▉| 3836/3844 [7:53:07<01:03,  7.92s/it]

100%|█████████▉| 3837/3844 [7:53:15<00:55,  7.95s/it]

100%|█████████▉| 3838/3844 [7:53:21<00:44,  7.34s/it]

100%|█████████▉| 3839/3844 [7:53:27<00:34,  6.99s/it]

100%|█████████▉| 3840/3844 [7:53:39<00:33,  8.28s/it]

100%|█████████▉| 3841/3844 [7:53:46<00:23,  7.88s/it]

100%|█████████▉| 3842/3844 [7:53:53<00:15,  7.80s/it]

100%|█████████▉| 3843/3844 [7:54:00<00:07,  7.31s/it]
{'loss': 1.1688, 'grad_norm': 0.42102774723172515, 'learning_rate': 3.550732183832395e-12, 'epoch': 1.0}

100%|██████████| 3844/3844 [7:54:12<00:00,  7.40s/it]
{'train_runtime': 28466.2883, 'train_samples_per_second': 17.287, 'train_steps_per_second': 0.135, 'train_loss': 1.199464779738705, 'epoch': 1.0}
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(