/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/7689 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/7689 [00:12<26:43:20, 12.51s/it]

  0%|          | 2/7689 [00:32<36:28:48, 17.08s/it]
{'loss': 2.2599, 'grad_norm': 1.4833950247952779, 'learning_rate': 1.7316017316017317e-06, 'epoch': 0.0}

  0%|          | 3/7689 [00:44<31:09:56, 14.60s/it]


  0%|          | 5/7689 [01:03<23:49:39, 11.16s/it]

  0%|          | 6/7689 [01:16<25:48:04, 12.09s/it]
{'loss': 2.5106, 'grad_norm': 1.6702027180272145, 'learning_rate': 5.194805194805195e-06, 'epoch': 0.0}

  0%|          | 7/7689 [01:27<24:47:41, 11.62s/it]


  0%|          | 9/7689 [01:47<22:04:54, 10.35s/it]
{'loss': 2.2729, 'grad_norm': 1.611269667710796, 'learning_rate': 7.792207792207792e-06, 'epoch': 0.0}


  0%|          | 11/7689 [02:05<21:29:16, 10.08s/it]
{'loss': 2.1859, 'grad_norm': 1.643272217477823, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.0}

  0%|          | 12/7689 [02:12<19:28:12,  9.13s/it]

  0%|          | 13/7689 [02:28<23:48:41, 11.17s/it]

  0%|          | 14/7689 [02:36<21:43:10, 10.19s/it]


  0%|          | 16/7689 [02:54<20:50:06,  9.78s/it]
{'loss': 2.3001, 'grad_norm': 1.7568419029959952, 'learning_rate': 1.3852813852813853e-05, 'epoch': 0.0}


  0%|          | 18/7689 [03:12<19:32:38,  9.17s/it]

  0%|          | 19/7689 [03:22<20:01:04,  9.40s/it]
{'loss': 1.8739, 'grad_norm': 1.5521630717651, 'learning_rate': 1.645021645021645e-05, 'epoch': 0.0}

  0%|          | 20/7689 [03:28<17:40:46,  8.30s/it]

  0%|          | 21/7689 [03:35<16:57:24,  7.96s/it]


  0%|          | 23/7689 [03:53<17:30:19,  8.22s/it]

  0%|          | 24/7689 [03:59<16:21:19,  7.68s/it]
{'loss': 1.99, 'grad_norm': 1.835470477531858, 'learning_rate': 2.077922077922078e-05, 'epoch': 0.0}

  0%|          | 25/7689 [04:08<16:48:09,  7.89s/it]


  0%|          | 27/7689 [04:29<20:32:42,  9.65s/it]
{'loss': 1.4969, 'grad_norm': 1.092051726606179, 'learning_rate': 2.3376623376623376e-05, 'epoch': 0.0}


  0%|          | 29/7689 [04:45<18:39:25,  8.77s/it]

  0%|          | 30/7689 [04:53<17:59:53,  8.46s/it]
{'loss': 1.6844, 'grad_norm': 0.7870023483831263, 'learning_rate': 2.5974025974025972e-05, 'epoch': 0.0}


  0%|          | 32/7689 [05:08<16:48:19,  7.90s/it]

  0%|          | 33/7689 [05:15<15:46:27,  7.42s/it]
{'loss': 1.6285, 'grad_norm': 0.3987597546910062, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.0}

  0%|          | 34/7689 [05:28<19:21:49,  9.11s/it]


  0%|          | 36/7689 [05:44<18:53:58,  8.89s/it]
{'loss': 1.4693, 'grad_norm': 0.4996613408765686, 'learning_rate': 3.1168831168831166e-05, 'epoch': 0.0}

  0%|          | 37/7689 [05:50<16:51:15,  7.93s/it]


  1%|          | 39/7689 [06:15<21:20:57, 10.05s/it]
{'loss': 1.4676, 'grad_norm': 0.48206563446145884, 'learning_rate': 3.376623376623377e-05, 'epoch': 0.01}

  1%|          | 40/7689 [06:24<21:07:27,  9.94s/it]

  1%|          | 41/7689 [06:31<19:09:42,  9.02s/it]

  1%|          | 42/7689 [06:40<18:57:51,  8.93s/it]


  1%|          | 44/7689 [07:01<20:46:29,  9.78s/it]

  1%|          | 45/7689 [07:17<24:40:00, 11.62s/it]
{'loss': 1.4936, 'grad_norm': 0.30039265954872124, 'learning_rate': 3.8961038961038966e-05, 'epoch': 0.01}

  1%|          | 46/7689 [07:25<22:41:34, 10.69s/it]

  1%|          | 47/7689 [07:34<21:35:18, 10.17s/it]


  1%|          | 49/7689 [07:55<22:24:26, 10.56s/it]
{'loss': 1.3843, 'grad_norm': 0.2638697842997191, 'learning_rate': 4.242424242424243e-05, 'epoch': 0.01}


  1%|          | 51/7689 [08:23<26:36:45, 12.54s/it]
{'loss': 1.2658, 'grad_norm': 0.25731776772000475, 'learning_rate': 4.415584415584416e-05, 'epoch': 0.01}

  1%|          | 52/7689 [08:33<24:46:17, 11.68s/it]


  1%|          | 54/7689 [08:45<19:04:40,  9.00s/it]

  1%|          | 55/7689 [08:55<19:44:43,  9.31s/it]

  1%|          | 56/7689 [09:01<17:43:41,  8.36s/it]
{'loss': 1.4579, 'grad_norm': 0.23417123755131328, 'learning_rate': 4.848484848484849e-05, 'epoch': 0.01}


  1%|          | 58/7689 [09:13<15:18:20,  7.22s/it]
{'loss': 1.3463, 'grad_norm': 0.21922396011592465, 'learning_rate': 5.0216450216450216e-05, 'epoch': 0.01}

  1%|          | 59/7689 [09:24<17:37:22,  8.31s/it]

  1%|          | 60/7689 [09:30<15:54:40,  7.51s/it]


  1%|          | 62/7689 [09:45<16:26:28,  7.76s/it]
{'loss': 1.2531, 'grad_norm': 0.20397808594379827, 'learning_rate': 5.3679653679653686e-05, 'epoch': 0.01}


  1%|          | 64/7689 [10:01<16:27:27,  7.77s/it]

  1%|          | 65/7689 [10:07<15:30:49,  7.33s/it]
{'loss': 1.3858, 'grad_norm': 0.19866276505436437, 'learning_rate': 5.627705627705628e-05, 'epoch': 0.01}


  1%|          | 67/7689 [10:33<21:17:10, 10.05s/it]

  1%|          | 68/7689 [10:41<19:49:53,  9.37s/it]
{'loss': 1.3667, 'grad_norm': 0.2080404872668589, 'learning_rate': 5.887445887445888e-05, 'epoch': 0.01}


  1%|          | 70/7689 [10:59<19:00:04,  8.98s/it]

  1%|          | 71/7689 [11:15<23:12:37, 10.97s/it]

  1%|          | 72/7689 [11:25<22:51:30, 10.80s/it]
[2024-05-24 13:42:35,853] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.3462, 'grad_norm': 0.2179389018179475, 'learning_rate': 6.233766233766233e-05, 'epoch': 0.01}

  1%|          | 73/7689 [11:32<19:51:58,  9.39s/it]

  1%|          | 74/7689 [11:40<19:33:57,  9.25s/it]

  1%|          | 75/7689 [11:53<21:28:01, 10.15s/it]

  1%|          | 76/7689 [12:00<19:51:28,  9.39s/it]


  1%|          | 78/7689 [12:13<16:48:21,  7.95s/it]

  1%|          | 79/7689 [12:21<16:47:36,  7.94s/it]
{'loss': 1.2354, 'grad_norm': 0.2194255862693232, 'learning_rate': 6.83982683982684e-05, 'epoch': 0.01}

  1%|          | 80/7689 [12:29<16:30:43,  7.81s/it]

  1%|          | 81/7689 [12:36<16:16:39,  7.70s/it]


  1%|          | 83/7689 [12:55<18:13:07,  8.62s/it]
{'loss': 1.1942, 'grad_norm': 0.20887349014823603, 'learning_rate': 7.186147186147186e-05, 'epoch': 0.01}


  1%|          | 85/7689 [13:13<18:49:33,  8.91s/it]

  1%|          | 86/7689 [13:25<20:34:28,  9.74s/it]

  1%|          | 87/7689 [13:31<18:27:24,  8.74s/it]

  1%|          | 88/7689 [13:45<21:42:59, 10.29s/it]

  1%|          | 89/7689 [13:53<20:01:51,  9.49s/it]
{'loss': 1.1855, 'grad_norm': 0.23533401756304506, 'learning_rate': 7.705627705627707e-05, 'epoch': 0.01}

  1%|          | 90/7689 [14:02<19:49:09,  9.39s/it]

  1%|          | 91/7689 [14:13<20:32:42,  9.73s/it]

  1%|          | 92/7689 [14:21<19:29:30,  9.24s/it]

  1%|          | 93/7689 [14:29<18:40:53,  8.85s/it]


  1%|          | 95/7689 [14:40<14:58:36,  7.10s/it]
{'loss': 1.3167, 'grad_norm': 0.2225840821964243, 'learning_rate': 8.225108225108226e-05, 'epoch': 0.01}

  1%|          | 96/7689 [14:46<14:42:31,  6.97s/it]


  1%|▏         | 98/7689 [15:01<14:46:45,  7.01s/it]
{'loss': 1.1043, 'grad_norm': 0.21735464666507615, 'learning_rate': 8.484848484848486e-05, 'epoch': 0.01}

  1%|▏         | 99/7689 [15:08<14:58:23,  7.10s/it]

  1%|▏         | 100/7689 [15:17<15:38:35,  7.42s/it]

  1%|▏         | 101/7689 [15:26<17:03:42,  8.09s/it]

  1%|▏         | 102/7689 [15:38<19:23:23,  9.20s/it]

  1%|▏         | 103/7689 [15:47<19:03:25,  9.04s/it]

  1%|▏         | 104/7689 [16:04<24:18:55, 11.54s/it]

  1%|▏         | 105/7689 [16:10<20:46:29,  9.86s/it]

  1%|▏         | 106/7689 [16:16<18:36:47,  8.84s/it]


  1%|▏         | 108/7689 [16:39<22:37:48, 10.75s/it]

  1%|▏         | 109/7689 [16:52<23:26:14, 11.13s/it]

  1%|▏         | 110/7689 [16:58<20:24:24,  9.69s/it]

  1%|▏         | 111/7689 [17:13<24:03:01, 11.43s/it]
{'loss': 1.1324, 'grad_norm': 0.22555375553678467, 'learning_rate': 9.610389610389611e-05, 'epoch': 0.01}


  1%|▏         | 113/7689 [17:38<26:02:21, 12.37s/it]
{'loss': 1.0612, 'grad_norm': 0.21274562267580846, 'learning_rate': 9.783549783549783e-05, 'epoch': 0.01}


  1%|▏         | 115/7689 [17:54<21:15:44, 10.11s/it]
{'loss': 1.089, 'grad_norm': 0.2105570319716786, 'learning_rate': 9.956709956709958e-05, 'epoch': 0.01}


  2%|▏         | 117/7689 [18:08<17:58:29,  8.55s/it]
{'loss': 1.0853, 'grad_norm': 0.22165797198090637, 'learning_rate': 0.0001012987012987013, 'epoch': 0.02}


  2%|▏         | 119/7689 [18:21<16:21:13,  7.78s/it]

  2%|▏         | 120/7689 [18:29<16:26:35,  7.82s/it]

  2%|▏         | 121/7689 [18:34<14:20:05,  6.82s/it]

  2%|▏         | 122/7689 [18:41<14:55:13,  7.10s/it]
{'loss': 1.1059, 'grad_norm': 0.18741797414729544, 'learning_rate': 0.00010562770562770564, 'epoch': 0.02}

  2%|▏         | 123/7689 [18:49<15:13:10,  7.24s/it]

  2%|▏         | 124/7689 [18:57<15:26:31,  7.35s/it]


  2%|▏         | 126/7689 [19:20<19:45:24,  9.40s/it]
{'loss': 1.044, 'grad_norm': 0.24309924023795465, 'learning_rate': 0.00010909090909090909, 'epoch': 0.02}

  2%|▏         | 127/7689 [19:27<18:32:18,  8.83s/it]


  2%|▏         | 129/7689 [19:41<16:42:48,  7.96s/it]

  2%|▏         | 130/7689 [19:51<18:03:38,  8.60s/it]
{'loss': 1.0444, 'grad_norm': 0.22243350690385058, 'learning_rate': 0.00011255411255411256, 'epoch': 0.02}

  2%|▏         | 131/7689 [19:58<16:54:14,  8.05s/it]

  2%|▏         | 132/7689 [20:11<19:41:41,  9.38s/it]

  2%|▏         | 133/7689 [20:19<18:44:41,  8.93s/it]


  2%|▏         | 135/7689 [20:39<19:55:29,  9.50s/it]

  2%|▏         | 136/7689 [20:48<19:25:31,  9.26s/it]
{'loss': 1.1899, 'grad_norm': 0.2055694840881751, 'learning_rate': 0.00011774891774891777, 'epoch': 0.02}


  2%|▏         | 138/7689 [21:04<18:21:00,  8.75s/it]
{'loss': 1.1614, 'grad_norm': 0.2209144340472548, 'learning_rate': 0.00011948051948051949, 'epoch': 0.02}

  2%|▏         | 139/7689 [21:12<17:59:32,  8.58s/it]

  2%|▏         | 140/7689 [21:24<20:07:41,  9.60s/it]

  2%|▏         | 141/7689 [21:33<19:18:46,  9.21s/it]

  2%|▏         | 142/7689 [21:41<18:39:45,  8.90s/it]


  2%|▏         | 144/7689 [21:54<16:21:25,  7.80s/it]
{'loss': 1.1082, 'grad_norm': 0.21720523051803028, 'learning_rate': 0.00012467532467532467, 'epoch': 0.02}


  2%|▏         | 146/7689 [22:10<15:54:25,  7.59s/it]
{'loss': 1.1055, 'grad_norm': 0.22291176420827855, 'learning_rate': 0.00012640692640692643, 'epoch': 0.02}

  2%|▏         | 147/7689 [22:15<14:42:30,  7.02s/it]

  2%|▏         | 148/7689 [22:23<15:06:15,  7.21s/it]

  2%|▏         | 149/7689 [22:33<16:53:47,  8.07s/it]


  2%|▏         | 151/7689 [22:46<15:26:40,  7.38s/it]
{'loss': 1.3095, 'grad_norm': 0.2295127236833076, 'learning_rate': 0.00013073593073593072, 'epoch': 0.02}


  2%|▏         | 153/7689 [23:02<15:32:09,  7.42s/it]

  2%|▏         | 154/7689 [23:08<14:42:33,  7.03s/it]

  2%|▏         | 155/7689 [23:22<19:06:47,  9.13s/it]

  2%|▏         | 156/7689 [23:30<18:17:51,  8.74s/it]
{'loss': 1.2763, 'grad_norm': 0.22165027500415188, 'learning_rate': 0.00013506493506493507, 'epoch': 0.02}

  2%|▏         | 157/7689 [23:41<19:26:01,  9.29s/it]


  2%|▏         | 159/7689 [23:54<16:47:03,  8.02s/it]

  2%|▏         | 160/7689 [24:04<17:46:53,  8.50s/it]

  2%|▏         | 161/7689 [24:12<17:36:35,  8.42s/it]

  2%|▏         | 162/7689 [24:24<19:42:25,  9.43s/it]
{'loss': 1.1727, 'grad_norm': 0.24698787477673068, 'learning_rate': 0.00014025974025974028, 'epoch': 0.02}

  2%|▏         | 163/7689 [24:28<16:44:46,  8.01s/it]


  2%|▏         | 165/7689 [24:54<21:47:11, 10.42s/it]
{'loss': 0.8843, 'grad_norm': 0.19554857411041685, 'learning_rate': 0.00014285714285714287, 'epoch': 0.02}


  2%|▏         | 167/7689 [25:24<26:06:30, 12.50s/it]
{'loss': 1.1195, 'grad_norm': 0.19628886814930252, 'learning_rate': 0.00014458874458874458, 'epoch': 0.02}

  2%|▏         | 168/7689 [25:35<25:01:49, 11.98s/it]


  2%|▏         | 170/7689 [25:54<22:08:16, 10.60s/it]

  2%|▏         | 171/7689 [26:02<20:46:45,  9.95s/it]
{'loss': 1.0941, 'grad_norm': 0.2199614486569172, 'learning_rate': 0.00014805194805194807, 'epoch': 0.02}


  2%|▏         | 173/7689 [26:22<21:25:56, 10.27s/it]
{'loss': 1.0367, 'grad_norm': 0.2355081239007783, 'learning_rate': 0.00014978354978354978, 'epoch': 0.02}

  2%|▏         | 174/7689 [26:29<19:23:33,  9.29s/it]

  2%|▏         | 175/7689 [26:39<19:48:05,  9.49s/it]

  2%|▏         | 176/7689 [26:45<17:50:32,  8.55s/it]


  2%|▏         | 178/7689 [26:58<16:05:08,  7.71s/it]

  2%|▏         | 179/7689 [27:06<16:02:16,  7.69s/it]

  2%|▏         | 180/7689 [27:18<18:44:34,  8.99s/it]
{'loss': 1.1689, 'grad_norm': 0.25037722978671606, 'learning_rate': 0.00015584415584415587, 'epoch': 0.02}

  2%|▏         | 181/7689 [27:25<17:37:56,  8.45s/it]


  2%|▏         | 183/7689 [27:38<15:27:51,  7.42s/it]
{'loss': 1.0391, 'grad_norm': 0.1859493906195887, 'learning_rate': 0.00015844155844155845, 'epoch': 0.02}


  2%|▏         | 185/7689 [28:04<21:01:31, 10.09s/it]
{'loss': 1.1778, 'grad_norm': 0.23486065350015994, 'learning_rate': 0.00016017316017316016, 'epoch': 0.02}


  2%|▏         | 187/7689 [28:20<18:56:35,  9.09s/it]
{'loss': 1.1349, 'grad_norm': 0.21913189320102225, 'learning_rate': 0.00016190476190476192, 'epoch': 0.02}

  2%|▏         | 188/7689 [28:27<17:49:24,  8.55s/it]


  2%|▏         | 190/7689 [28:46<19:05:58,  9.17s/it]
{'loss': 1.0852, 'grad_norm': 0.20844013137085354, 'learning_rate': 0.0001645021645021645, 'epoch': 0.02}

  2%|▏         | 191/7689 [29:01<22:26:13, 10.77s/it]


  3%|▎         | 193/7689 [29:17<19:20:05,  9.29s/it]
{'loss': 1.2447, 'grad_norm': 0.19737197899147588, 'learning_rate': 0.0001670995670995671, 'epoch': 0.03}


  3%|▎         | 195/7689 [29:32<18:20:04,  8.81s/it]
{'loss': 1.1479, 'grad_norm': 0.22467849307643392, 'learning_rate': 0.00016883116883116884, 'epoch': 0.03}

  3%|▎         | 196/7689 [29:38<16:15:33,  7.81s/it]


  3%|▎         | 198/7689 [29:56<17:34:57,  8.45s/it]
{'loss': 1.1728, 'grad_norm': 0.2160127610707628, 'learning_rate': 0.00017142857142857143, 'epoch': 0.03}


  3%|▎         | 200/7689 [30:12<16:25:27,  7.90s/it]
{'loss': 1.1559, 'grad_norm': 0.24056261670952278, 'learning_rate': 0.00017316017316017316, 'epoch': 0.03}

  3%|▎         | 201/7689 [30:20<16:14:17,  7.81s/it]

  3%|▎         | 202/7689 [30:25<14:38:38,  7.04s/it]

  3%|▎         | 203/7689 [30:37<18:03:06,  8.68s/it]


  3%|▎         | 205/7689 [30:52<16:59:52,  8.18s/it]

  3%|▎         | 206/7689 [30:59<15:43:29,  7.57s/it]

  3%|▎         | 207/7689 [31:09<17:22:41,  8.36s/it]
{'loss': 1.0724, 'grad_norm': 0.2198631239776588, 'learning_rate': 0.00017922077922077922, 'epoch': 0.03}

  3%|▎         | 208/7689 [31:14<15:17:55,  7.36s/it]


  3%|▎         | 210/7689 [31:28<15:26:58,  7.44s/it]
{'loss': 1.2453, 'grad_norm': 0.200360833368196, 'learning_rate': 0.00018181818181818183, 'epoch': 0.03}


  3%|▎         | 212/7689 [31:46<17:08:40,  8.25s/it]

  3%|▎         | 213/7689 [31:54<16:57:35,  8.17s/it]

  3%|▎         | 214/7689 [32:02<16:41:12,  8.04s/it]

  3%|▎         | 215/7689 [32:09<15:41:15,  7.56s/it]
{'loss': 1.36, 'grad_norm': 0.2166807230027262, 'learning_rate': 0.00018614718614718616, 'epoch': 0.03}

  3%|▎         | 216/7689 [32:17<16:16:32,  7.84s/it]

  3%|▎         | 217/7689 [32:28<18:13:46,  8.78s/it]


  3%|▎         | 219/7689 [32:47<18:49:45,  9.07s/it]

  3%|▎         | 220/7689 [32:58<20:24:25,  9.84s/it]

  3%|▎         | 221/7689 [33:06<19:19:55,  9.32s/it]

  3%|▎         | 222/7689 [33:18<20:59:28, 10.12s/it]
{'loss': 1.102, 'grad_norm': 0.2241704141616389, 'learning_rate': 0.00019220779220779222, 'epoch': 0.03}


  3%|▎         | 224/7689 [33:36<19:09:54,  9.24s/it]
{'loss': 1.1729, 'grad_norm': 0.20633673073765904, 'learning_rate': 0.00019393939393939395, 'epoch': 0.03}


  3%|▎         | 226/7689 [34:03<24:27:10, 11.80s/it]
{'loss': 0.9204, 'grad_norm': 0.2203873771899074, 'learning_rate': 0.00019567099567099566, 'epoch': 0.03}


  3%|▎         | 228/7689 [34:20<20:59:40, 10.13s/it]

  3%|▎         | 229/7689 [34:29<20:06:44,  9.71s/it]
{'loss': 0.9918, 'grad_norm': 0.17228912879100472, 'learning_rate': 0.00019826839826839827, 'epoch': 0.03}


  3%|▎         | 231/7689 [34:43<17:18:01,  8.35s/it]
{'loss': 1.1543, 'grad_norm': 0.22649233956207132, 'learning_rate': 0.0002, 'epoch': 0.03}

  3%|▎         | 232/7689 [34:53<18:29:45,  8.93s/it]

  3%|▎         | 233/7689 [35:00<17:03:17,  8.23s/it]

  3%|▎         | 234/7689 [35:08<17:09:05,  8.28s/it]


  3%|▎         | 236/7689 [35:21<14:42:02,  7.10s/it]

  3%|▎         | 237/7689 [35:31<16:46:08,  8.10s/it]

  3%|▎         | 238/7689 [35:37<15:29:27,  7.48s/it]

  3%|▎         | 239/7689 [35:48<17:51:39,  8.63s/it]
{'loss': 1.0422, 'grad_norm': 0.222334557082851, 'learning_rate': 0.00019999943218801557, 'epoch': 0.03}


  3%|▎         | 241/7689 [36:09<19:01:50,  9.20s/it]
{'loss': 1.1891, 'grad_norm': 0.20075181464971054, 'learning_rate': 0.00019999911279424662, 'epoch': 0.03}

  3%|▎         | 242/7689 [36:14<16:43:27,  8.08s/it]

  3%|▎         | 243/7689 [36:20<15:04:18,  7.29s/it]

  3%|▎         | 244/7689 [36:28<15:52:14,  7.67s/it]


  3%|▎         | 246/7689 [36:45<16:37:50,  8.04s/it]

  3%|▎         | 247/7689 [36:51<15:14:30,  7.37s/it]

  3%|▎         | 248/7689 [37:03<18:09:04,  8.78s/it]
{'loss': 1.0363, 'grad_norm': 0.18619582450262745, 'learning_rate': 0.00019999743598253834, 'epoch': 0.03}


  3%|▎         | 250/7689 [37:17<16:25:59,  7.95s/it]

  3%|▎         | 251/7689 [37:27<17:37:43,  8.53s/it]

  3%|▎         | 252/7689 [37:39<19:47:42,  9.58s/it]
{'loss': 1.0435, 'grad_norm': 0.18671879546186415, 'learning_rate': 0.0001999960874423558, 'epoch': 0.03}

  3%|▎         | 253/7689 [37:50<20:53:02, 10.11s/it]

  3%|▎         | 254/7689 [37:57<19:01:35,  9.21s/it]


  3%|▎         | 256/7689 [38:21<22:41:48, 10.99s/it]

  3%|▎         | 257/7689 [38:29<21:01:13, 10.18s/it]
{'loss': 1.2894, 'grad_norm': 0.2281861028519615, 'learning_rate': 0.00019999400254018873, 'epoch': 0.03}

  3%|▎         | 258/7689 [38:38<20:14:22,  9.81s/it]


  3%|▎         | 260/7689 [38:57<19:47:06,  9.59s/it]
{'loss': 1.2467, 'grad_norm': 0.18885633927583043, 'learning_rate': 0.0001999925386813681, 'epoch': 0.03}


  3%|▎         | 262/7689 [39:21<23:02:04, 11.17s/it]

  3%|▎         | 263/7689 [39:29<21:03:45, 10.21s/it]
{'loss': 1.0134, 'grad_norm': 0.17938874994815135, 'learning_rate': 0.00019999091513721252, 'epoch': 0.03}

  3%|▎         | 264/7689 [39:35<18:33:23,  9.00s/it]

  3%|▎         | 265/7689 [39:52<23:05:26, 11.20s/it]

  3%|▎         | 266/7689 [40:02<22:20:07, 10.83s/it]


  3%|▎         | 268/7689 [40:17<19:06:24,  9.27s/it]
{'loss': 1.1161, 'grad_norm': 0.21085971750811666, 'learning_rate': 0.00019998785438114406, 'epoch': 0.03}


  4%|▎         | 270/7689 [40:31<16:57:13,  8.23s/it]
{'loss': 1.1391, 'grad_norm': 0.1880158577680607, 'learning_rate': 0.00019998650588403294, 'epoch': 0.04}

  4%|▎         | 271/7689 [40:48<22:27:50, 10.90s/it]

  4%|▎         | 272/7689 [40:55<19:32:45,  9.49s/it]

  4%|▎         | 273/7689 [41:02<18:17:18,  8.88s/it]

  4%|▎         | 274/7689 [41:16<21:12:43, 10.30s/it]


  4%|▎         | 276/7689 [41:39<22:24:24, 10.88s/it]

  4%|▎         | 277/7689 [41:45<19:26:42,  9.44s/it]

  4%|▎         | 278/7689 [41:51<17:36:52,  8.56s/it]

  4%|▎         | 279/7689 [41:57<16:09:58,  7.85s/it]
{'loss': 1.044, 'grad_norm': 0.1733631232388141, 'learning_rate': 0.0001999795594456138, 'epoch': 0.04}

  4%|▎         | 280/7689 [42:04<15:14:33,  7.41s/it]

  4%|▎         | 281/7689 [42:14<16:52:53,  8.20s/it]


  4%|▎         | 283/7689 [42:27<14:51:41,  7.22s/it]

  4%|▎         | 284/7689 [42:39<18:02:22,  8.77s/it]
{'loss': 1.1089, 'grad_norm': 0.1689903908956186, 'learning_rate': 0.00019997507938866588, 'epoch': 0.04}


  4%|▎         | 286/7689 [43:01<19:44:43,  9.60s/it]
{'loss': 1.0499, 'grad_norm': 0.16827211797927372, 'learning_rate': 0.00019997316318671806, 'epoch': 0.04}

  4%|▎         | 287/7689 [43:10<19:07:47,  9.30s/it]

  4%|▎         | 288/7689 [43:22<21:04:46, 10.25s/it]

  4%|▍         | 289/7689 [43:28<18:33:16,  9.03s/it]

  4%|▍         | 290/7689 [43:40<20:27:07,  9.95s/it]

  4%|▍         | 291/7689 [43:48<18:55:39,  9.21s/it]


  4%|▍         | 293/7689 [44:07<19:39:36,  9.57s/it]
{'loss': 1.1406, 'grad_norm': 0.18730837271378636, 'learning_rate': 0.00019996589769886427, 'epoch': 0.04}


  4%|▍         | 295/7689 [44:21<16:31:52,  8.05s/it]

  4%|▍         | 296/7689 [44:25<14:11:31,  6.91s/it]

  4%|▍         | 297/7689 [44:35<15:53:10,  7.74s/it]

  4%|▍         | 298/7689 [44:41<15:00:32,  7.31s/it]
{'loss': 0.9897, 'grad_norm': 0.18918745023748865, 'learning_rate': 0.00019996017591838175, 'epoch': 0.04}


  4%|▍         | 300/7689 [44:59<17:05:19,  8.33s/it]
  4%|▍         | 300/7689 [44:59<17:05:19,  8.33s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  4%|▍         | 301/7689 [45:40<36:55:07, 17.99s/it]

  4%|▍         | 302/7689 [45:47<30:34:14, 14.90s/it]
{'loss': 1.2067, 'grad_norm': 0.17562087794995382, 'learning_rate': 0.00019995527922548174, 'epoch': 0.04}

  4%|▍         | 303/7689 [45:57<27:20:17, 13.32s/it]


  4%|▍         | 305/7689 [46:14<21:55:53, 10.69s/it]

  4%|▍         | 306/7689 [46:22<20:12:59,  9.86s/it]

  4%|▍         | 307/7689 [46:29<18:46:29,  9.16s/it]

  4%|▍         | 308/7689 [46:36<17:12:15,  8.39s/it]
{'loss': 1.3055, 'grad_norm': 0.1926502438093408, 'learning_rate': 0.00019994740210464268, 'epoch': 0.04}


  4%|▍         | 310/7689 [46:49<15:16:55,  7.46s/it]
{'loss': 1.0297, 'grad_norm': 0.20738406107889804, 'learning_rate': 0.0001999446345167034, 'epoch': 0.04}


  4%|▍         | 312/7689 [47:03<14:59:51,  7.32s/it]
{'loss': 1.1376, 'grad_norm': 0.2017369890657072, 'learning_rate': 0.00019994179599149963, 'epoch': 0.04}

  4%|▍         | 313/7689 [47:15<17:37:55,  8.61s/it]

  4%|▍         | 314/7689 [47:20<15:42:50,  7.67s/it]

  4%|▍         | 315/7689 [47:30<17:17:29,  8.44s/it]

  4%|▍         | 316/7689 [47:43<19:34:01,  9.55s/it]


  4%|▍         | 318/7689 [47:57<17:26:58,  8.52s/it]
{'loss': 1.1186, 'grad_norm': 0.17271716372125598, 'learning_rate': 0.00019993285481269987, 'epoch': 0.04}

  4%|▍         | 319/7689 [48:06<17:47:22,  8.69s/it]


  4%|▍         | 321/7689 [48:18<14:37:09,  7.14s/it]
{'loss': 0.9349, 'grad_norm': 0.1973585649821334, 'learning_rate': 0.00019992814483463434, 'epoch': 0.04}


  4%|▍         | 323/7689 [48:32<14:12:18,  6.94s/it]

  4%|▍         | 324/7689 [48:44<17:18:06,  8.46s/it]

  4%|▍         | 325/7689 [48:52<17:00:07,  8.31s/it]

  4%|▍         | 326/7689 [48:58<15:20:58,  7.50s/it]
{'loss': 1.1535, 'grad_norm': 0.19936925008873158, 'learning_rate': 0.0001999199402472443, 'epoch': 0.04}

  4%|▍         | 327/7689 [49:05<15:21:59,  7.51s/it]


  4%|▍         | 329/7689 [49:21<16:17:25,  7.97s/it]

  4%|▍         | 330/7689 [49:32<17:51:23,  8.74s/it]
{'loss': 1.1694, 'grad_norm': 0.18509854728982772, 'learning_rate': 0.00019991305743680013, 'epoch': 0.04}

  4%|▍         | 331/7689 [49:47<21:48:48, 10.67s/it]

  4%|▍         | 332/7689 [49:57<21:13:58, 10.39s/it]


  4%|▍         | 334/7689 [50:13<18:44:41,  9.17s/it]

  4%|▍         | 335/7689 [50:20<17:01:32,  8.33s/it]
{'loss': 1.0804, 'grad_norm': 0.19028573138451643, 'learning_rate': 0.0001999040550301388, 'epoch': 0.04}

  4%|▍         | 336/7689 [50:27<16:29:22,  8.07s/it]


  4%|▍         | 338/7689 [50:39<14:14:56,  6.98s/it]
{'loss': 1.3611, 'grad_norm': 0.2188679523841283, 'learning_rate': 0.00019989844085818264, 'epoch': 0.04}


  4%|▍         | 340/7689 [50:50<12:16:14,  6.01s/it]
{'loss': 1.0827, 'grad_norm': 0.17729539035474168, 'learning_rate': 0.00019989460944572392, 'epoch': 0.04}

  4%|▍         | 341/7689 [51:00<14:59:16,  7.34s/it]

  4%|▍         | 342/7689 [51:07<14:37:03,  7.16s/it]


  4%|▍         | 344/7689 [51:20<13:57:08,  6.84s/it]
{'loss': 1.2096, 'grad_norm': 0.19300847638569463, 'learning_rate': 0.0001998867339183008, 'epoch': 0.04}

  4%|▍         | 345/7689 [51:25<12:43:58,  6.24s/it]

  4%|▍         | 346/7689 [51:29<11:32:45,  5.66s/it]


  5%|▍         | 348/7689 [51:56<19:12:14,  9.42s/it]

  5%|▍         | 349/7689 [52:02<17:08:51,  8.41s/it]

  5%|▍         | 350/7689 [52:20<23:09:52, 11.36s/it]
{'loss': 1.0988, 'grad_norm': 0.173768810822574, 'learning_rate': 0.00019987438891320297, 'epoch': 0.05}

  5%|▍         | 351/7689 [52:32<23:48:19, 11.68s/it]


  5%|▍         | 353/7689 [52:50<20:54:58, 10.26s/it]
{'loss': 1.047, 'grad_norm': 0.17154207516792688, 'learning_rate': 0.00019986797716068212, 'epoch': 0.05}

  5%|▍         | 354/7689 [53:01<21:20:44, 10.48s/it]


  5%|▍         | 356/7689 [53:14<17:23:04,  8.53s/it]

  5%|▍         | 357/7689 [53:22<16:52:55,  8.29s/it]
{'loss': 1.2149, 'grad_norm': 0.20184942871013756, 'learning_rate': 0.00019985918006915068, 'epoch': 0.05}

  5%|▍         | 358/7689 [53:31<17:10:44,  8.44s/it]


  5%|▍         | 360/7689 [53:46<15:54:16,  7.81s/it]
{'loss': 0.9968, 'grad_norm': 0.19929385299091398, 'learning_rate': 0.0001998523961980328, 'epoch': 0.05}


  5%|▍         | 362/7689 [54:06<18:49:57,  9.25s/it]
{'loss': 0.8831, 'grad_norm': 0.17259625328995037, 'learning_rate': 0.0001998477850268687, 'epoch': 0.05}

  5%|▍         | 363/7689 [54:11<16:00:18,  7.86s/it]


  5%|▍         | 365/7689 [54:22<13:42:00,  6.73s/it]
{'loss': 1.1759, 'grad_norm': 0.19717737155237025, 'learning_rate': 0.00019984073539267648, 'epoch': 0.05}


  5%|▍         | 367/7689 [54:40<16:35:20,  8.16s/it]

  5%|▍         | 368/7689 [54:54<20:26:14, 10.05s/it]
{'loss': 1.1662, 'grad_norm': 0.1823071349490351, 'learning_rate': 0.00019983352631557493, 'epoch': 0.05}


  5%|▍         | 370/7689 [55:18<22:39:19, 11.14s/it]

  5%|▍         | 371/7689 [55:26<21:00:21, 10.33s/it]

  5%|▍         | 372/7689 [55:36<20:32:24, 10.11s/it]
{'loss': 0.9868, 'grad_norm': 0.16936321046543215, 'learning_rate': 0.00019982366621040467, 'epoch': 0.05}


  5%|▍         | 374/7689 [55:52<18:31:26,  9.12s/it]
{'loss': 0.9862, 'grad_norm': 0.17290192396625445, 'learning_rate': 0.00019981862987894934, 'epoch': 0.05}


  5%|▍         | 376/7689 [56:10<18:15:15,  8.99s/it]

  5%|▍         | 377/7689 [56:20<18:43:42,  9.22s/it]
{'loss': 1.0939, 'grad_norm': 0.18790106922068395, 'learning_rate': 0.00019981094254321446, 'epoch': 0.05}

  5%|▍         | 378/7689 [56:39<24:32:43, 12.09s/it]

  5%|▍         | 379/7689 [56:51<24:28:28, 12.05s/it]


  5%|▍         | 381/7689 [57:02<18:11:51,  8.96s/it]
{'loss': 1.0362, 'grad_norm': 0.17898810494512563, 'learning_rate': 0.0001998004448161006, 'epoch': 0.05}


  5%|▍         | 383/7689 [57:18<16:53:20,  8.32s/it]
{'loss': 1.0492, 'grad_norm': 0.20936538955613174, 'learning_rate': 0.00019979508969828293, 'epoch': 0.05}


  5%|▌         | 385/7689 [57:28<13:36:40,  6.71s/it]

  5%|▌         | 386/7689 [57:34<12:49:26,  6.32s/it]
{'loss': 1.1396, 'grad_norm': 0.18736457283319097, 'learning_rate': 0.00019978692421440282, 'epoch': 0.05}


  5%|▌         | 388/7689 [57:56<17:04:48,  8.42s/it]

  5%|▌         | 389/7689 [58:04<17:02:07,  8.40s/it]
{'loss': 1.1465, 'grad_norm': 0.17960362260410345, 'learning_rate': 0.0001997785993735484, 'epoch': 0.05}

  5%|▌         | 390/7689 [58:16<18:47:01,  9.26s/it]


  5%|▌         | 392/7689 [58:42<22:20:10, 11.02s/it]

  5%|▌         | 393/7689 [58:54<22:53:32, 11.30s/it]
{'loss': 1.1431, 'grad_norm': 0.19493375023277237, 'learning_rate': 0.0001997672517201306, 'epoch': 0.05}

  5%|▌         | 394/7689 [59:05<22:50:17, 11.27s/it]

  5%|▌         | 395/7689 [59:15<21:54:53, 10.82s/it]

  5%|▌         | 396/7689 [59:21<18:58:35,  9.37s/it]

  5%|▌         | 397/7689 [59:29<18:16:18,  9.02s/it]


  5%|▌         | 399/7689 [59:42<15:49:37,  7.82s/it]
{'loss': 1.1191, 'grad_norm': 0.17057239225073786, 'learning_rate': 0.00019974969916515352, 'epoch': 0.05}


  5%|▌         | 401/7689 [59:55<14:13:26,  7.03s/it]
{'loss': 0.9815, 'grad_norm': 0.18282336631021162, 'learning_rate': 0.00019974370671009504, 'epoch': 0.05}

  5%|▌         | 402/7689 [1:00:00<13:10:22,  6.51s/it]


  5%|▌         | 404/7689 [1:00:13<13:08:34,  6.49s/it]
{'loss': 1.1553, 'grad_norm': 0.19168663712923734, 'learning_rate': 0.0001997345852888761, 'epoch': 0.05}


  5%|▌         | 406/7689 [1:00:25<12:27:01,  6.15s/it]

  5%|▌         | 407/7689 [1:00:38<17:10:06,  8.49s/it]
{'loss': 0.9757, 'grad_norm': 0.17652563245171216, 'learning_rate': 0.00019972530459426663, 'epoch': 0.05}

  5%|▌         | 408/7689 [1:00:45<16:06:29,  7.96s/it]


  5%|▌         | 410/7689 [1:00:57<13:44:43,  6.80s/it]

  5%|▌         | 411/7689 [1:01:08<16:34:26,  8.20s/it]

  5%|▌         | 412/7689 [1:01:20<19:04:27,  9.44s/it]
{'loss': 0.9434, 'grad_norm': 0.189895392879494, 'learning_rate': 0.00019970948286941953, 'epoch': 0.05}

  5%|▌         | 413/7689 [1:01:30<19:01:22,  9.41s/it]

  5%|▌         | 414/7689 [1:01:37<17:45:19,  8.79s/it]

  5%|▌         | 415/7689 [1:01:51<21:02:39, 10.42s/it]

  5%|▌         | 416/7689 [1:01:59<19:36:35,  9.71s/it]

  5%|▌         | 417/7689 [1:02:07<18:23:51,  9.11s/it]


  5%|▌         | 419/7689 [1:02:18<14:25:19,  7.14s/it]
{'loss': 1.2953, 'grad_norm': 0.20434984605613404, 'learning_rate': 0.0001996865893821589, 'epoch': 0.05}


  5%|▌         | 421/7689 [1:02:33<15:04:16,  7.47s/it]

  5%|▌         | 422/7689 [1:02:40<15:00:41,  7.44s/it]

  6%|▌         | 423/7689 [1:02:52<17:44:46,  8.79s/it]
{'loss': 0.9602, 'grad_norm': 0.16545458235223168, 'learning_rate': 0.00019967311822899846, 'epoch': 0.06}

  6%|▌         | 424/7689 [1:03:01<18:04:27,  8.96s/it]


  6%|▌         | 426/7689 [1:03:24<19:59:05,  9.91s/it]

  6%|▌         | 427/7689 [1:03:32<18:56:25,  9.39s/it]
{'loss': 1.028, 'grad_norm': 0.21162254335630512, 'learning_rate': 0.00019965936409768192, 'epoch': 0.06}

  6%|▌         | 428/7689 [1:03:37<16:09:15,  8.01s/it]

  6%|▌         | 429/7689 [1:03:53<21:02:34, 10.43s/it]


  6%|▌         | 431/7689 [1:04:06<17:03:59,  8.47s/it]
{'loss': 1.1327, 'grad_norm': 0.18970058931727454, 'learning_rate': 0.00019964532702725803, 'epoch': 0.06}

  6%|▌         | 432/7689 [1:04:18<18:56:51,  9.40s/it]

  6%|▌         | 433/7689 [1:04:26<18:12:44,  9.04s/it]


  6%|▌         | 435/7689 [1:04:47<19:45:59,  9.81s/it]
{'loss': 1.0137, 'grad_norm': 0.17108693180227982, 'learning_rate': 0.00019963100705757897, 'epoch': 0.06}


  6%|▌         | 437/7689 [1:05:13<22:20:22, 11.09s/it]
{'loss': 1.1914, 'grad_norm': 0.18254733023770714, 'learning_rate': 0.00019962374099819212, 'epoch': 0.06}

  6%|▌         | 438/7689 [1:05:19<19:37:07,  9.74s/it]


  6%|▌         | 440/7689 [1:05:32<16:22:25,  8.13s/it]
{'loss': 1.2028, 'grad_norm': 0.1928410003237435, 'learning_rate': 0.00019961270933041477, 'epoch': 0.06}


  6%|▌         | 442/7689 [1:05:43<13:31:11,  6.72s/it]

  6%|▌         | 443/7689 [1:05:51<14:12:44,  7.06s/it]
{'loss': 0.9499, 'grad_norm': 0.16821631514480653, 'learning_rate': 0.0001996015185838794, 'epoch': 0.06}

  6%|▌         | 444/7689 [1:06:03<17:26:47,  8.67s/it]


  6%|▌         | 446/7689 [1:06:17<15:25:59,  7.67s/it]
{'loss': 1.0718, 'grad_norm': 0.17013606192722458, 'learning_rate': 0.00019959016877645747, 'epoch': 0.06}


  6%|▌         | 448/7689 [1:06:33<15:49:50,  7.87s/it]

  6%|▌         | 449/7689 [1:06:43<17:04:08,  8.49s/it]

  6%|▌         | 450/7689 [1:06:49<15:17:45,  7.61s/it]
{'loss': 1.093, 'grad_norm': 0.20402392886332638, 'learning_rate': 0.00019957478830321326, 'epoch': 0.06}

  6%|▌         | 451/7689 [1:07:00<17:45:00,  8.83s/it]


  6%|▌         | 453/7689 [1:07:11<13:56:24,  6.94s/it]
{'loss': 0.9886, 'grad_norm': 0.17393345678646244, 'learning_rate': 0.00019956306742464889, 'epoch': 0.06}


  6%|▌         | 455/7689 [1:07:26<15:16:17,  7.60s/it]
{'loss': 0.9967, 'grad_norm': 0.1819729376969028, 'learning_rate': 0.00019955516517139495, 'epoch': 0.06}

  6%|▌         | 456/7689 [1:07:32<14:15:36,  7.10s/it]


  6%|▌         | 458/7689 [1:07:47<14:34:25,  7.26s/it]
{'loss': 1.1254, 'grad_norm': 0.16175497618710646, 'learning_rate': 0.00019954317930421947, 'epoch': 0.06}

  6%|▌         | 459/7689 [1:07:53<14:20:39,  7.14s/it]

  6%|▌         | 460/7689 [1:07:58<12:53:17,  6.42s/it]


  6%|▌         | 462/7689 [1:08:15<14:35:10,  7.27s/it]
{'loss': 1.1854, 'grad_norm': 0.17564617863863136, 'learning_rate': 0.00019952695086820975, 'epoch': 0.06}

  6%|▌         | 463/7689 [1:08:28<18:06:46,  9.02s/it]


  6%|▌         | 465/7689 [1:08:43<16:46:02,  8.36s/it]

  6%|▌         | 466/7689 [1:08:55<18:46:48,  9.36s/it]
{'loss': 1.0264, 'grad_norm': 0.16775840077207232, 'learning_rate': 0.00019951043986902209, 'epoch': 0.06}


  6%|▌         | 468/7689 [1:09:27<26:20:34, 13.13s/it]
{'loss': 0.9937, 'grad_norm': 0.15746036744368794, 'learning_rate': 0.00019950207842285388, 'epoch': 0.06}

  6%|▌         | 469/7689 [1:09:38<24:40:52, 12.31s/it]

  6%|▌         | 470/7689 [1:09:44<21:09:20, 10.55s/it]

  6%|▌         | 471/7689 [1:09:52<19:21:55,  9.66s/it]

  6%|▌         | 472/7689 [1:10:06<21:57:50, 10.96s/it]

  6%|▌         | 473/7689 [1:10:18<23:05:20, 11.52s/it]


  6%|▌         | 475/7689 [1:10:33<18:38:11,  9.30s/it]
{'loss': 0.9619, 'grad_norm': 0.18934932845475183, 'learning_rate': 0.00019947225724333052, 'epoch': 0.06}


  6%|▌         | 477/7689 [1:10:45<15:32:13,  7.76s/it]
{'loss': 1.1392, 'grad_norm': 0.18398501429726336, 'learning_rate': 0.00019946357804043107, 'epoch': 0.06}

  6%|▌         | 478/7689 [1:10:52<15:09:35,  7.57s/it]


  6%|▌         | 480/7689 [1:11:13<18:47:24,  9.38s/it]
{'loss': 1.0981, 'grad_norm': 0.1829829242388836, 'learning_rate': 0.00019945042687084477, 'epoch': 0.06}

  6%|▋         | 481/7689 [1:11:20<17:24:16,  8.69s/it]


  6%|▋         | 483/7689 [1:11:41<19:16:24,  9.63s/it]

  6%|▋         | 484/7689 [1:11:53<20:44:47, 10.37s/it]
{'loss': 0.9027, 'grad_norm': 0.17411840851240207, 'learning_rate': 0.00019943264492902258, 'epoch': 0.06}

  6%|▋         | 485/7689 [1:12:00<18:43:07,  9.35s/it]


  6%|▋         | 487/7689 [1:12:17<17:08:14,  8.57s/it]

  6%|▋         | 488/7689 [1:12:33<21:33:21, 10.78s/it]

  6%|▋         | 489/7689 [1:12:45<22:11:59, 11.10s/it]

  6%|▋         | 490/7689 [1:12:53<20:41:55, 10.35s/it]
{'loss': 1.0043, 'grad_norm': 0.16373950952435137, 'learning_rate': 0.00019940544272833933, 'epoch': 0.06}


  6%|▋         | 492/7689 [1:13:13<20:40:48, 10.34s/it]

  6%|▋         | 493/7689 [1:13:21<19:11:16,  9.60s/it]

  6%|▋         | 494/7689 [1:13:31<19:36:51,  9.81s/it]
{'loss': 1.1205, 'grad_norm': 0.18634588653080797, 'learning_rate': 0.00019938695514433398, 'epoch': 0.06}

  6%|▋         | 495/7689 [1:13:38<17:52:37,  8.95s/it]


  6%|▋         | 497/7689 [1:13:55<16:50:27,  8.43s/it]
{'loss': 1.1985, 'grad_norm': 0.1867176458947581, 'learning_rate': 0.0001993729042821748, 'epoch': 0.06}

  6%|▋         | 498/7689 [1:14:04<17:15:37,  8.64s/it]

  6%|▋         | 499/7689 [1:14:13<17:05:34,  8.56s/it]


  7%|▋         | 501/7689 [1:14:27<15:50:15,  7.93s/it]

  7%|▋         | 502/7689 [1:14:32<13:55:28,  6.97s/it]

  7%|▋         | 503/7689 [1:14:39<14:09:37,  7.09s/it]
{'loss': 1.1667, 'grad_norm': 0.18630176976132787, 'learning_rate': 0.00019934432649316102, 'epoch': 0.07}


  7%|▋         | 505/7689 [1:14:56<15:32:02,  7.78s/it]

  7%|▋         | 506/7689 [1:15:06<16:56:48,  8.49s/it]

  7%|▋         | 507/7689 [1:15:15<17:29:02,  8.76s/it]

  7%|▋         | 508/7689 [1:15:21<15:43:14,  7.88s/it]

  7%|▋         | 509/7689 [1:15:31<17:16:22,  8.66s/it]
{'loss': 1.0515, 'grad_norm': 0.18009998817894654, 'learning_rate': 0.00019931511410376884, 'epoch': 0.07}

  7%|▋         | 510/7689 [1:15:37<15:20:34,  7.69s/it]


  7%|▋         | 512/7689 [1:16:06<21:32:57, 10.81s/it]
{'loss': 1.065, 'grad_norm': 0.17817222485834858, 'learning_rate': 0.00019930026999208673, 'epoch': 0.07}

  7%|▋         | 513/7689 [1:16:11<18:03:22,  9.06s/it]


  7%|▋         | 515/7689 [1:16:26<16:20:10,  8.20s/it]
{'loss': 1.1602, 'grad_norm': 0.1867555205836769, 'learning_rate': 0.00019928526730060372, 'epoch': 0.07}

  7%|▋         | 516/7689 [1:16:32<15:22:16,  7.71s/it]


  7%|▋         | 518/7689 [1:16:53<17:46:43,  8.93s/it]

  7%|▋         | 519/7689 [1:16:58<15:15:15,  7.66s/it]

  7%|▋         | 520/7689 [1:17:09<17:25:30,  8.75s/it]

  7%|▋         | 521/7689 [1:17:19<18:12:13,  9.14s/it]
{'loss': 1.1249, 'grad_norm': 0.17300947828677193, 'learning_rate': 0.00019925478627432364, 'epoch': 0.07}


  7%|▋         | 523/7689 [1:17:32<15:04:06,  7.57s/it]
{'loss': 1.0113, 'grad_norm': 0.17518495784071694, 'learning_rate': 0.0001992444850272971, 'epoch': 0.07}

  7%|▋         | 524/7689 [1:17:46<19:28:23,  9.78s/it]


  7%|▋         | 526/7689 [1:17:58<15:10:39,  7.63s/it]

  7%|▋         | 527/7689 [1:18:03<13:56:14,  7.01s/it]
{'loss': 1.2293, 'grad_norm': 0.18130681230542803, 'learning_rate': 0.00019922367121963808, 'epoch': 0.07}


  7%|▋         | 529/7689 [1:18:23<17:11:39,  8.65s/it]

  7%|▋         | 530/7689 [1:18:35<18:59:54,  9.55s/it]
{'loss': 0.9305, 'grad_norm': 0.1600578832760704, 'learning_rate': 0.00019920787599359737, 'epoch': 0.07}

  7%|▋         | 531/7689 [1:18:42<17:40:11,  8.89s/it]


  7%|▋         | 533/7689 [1:18:57<16:14:56,  8.17s/it]
{'loss': 1.1715, 'grad_norm': 0.17199776710177891, 'learning_rate': 0.0001991919223353064, 'epoch': 0.07}


  7%|▋         | 535/7689 [1:19:13<15:49:14,  7.96s/it]
{'loss': 1.128, 'grad_norm': 0.1815672581224942, 'learning_rate': 0.0001991811985577667, 'epoch': 0.07}

  7%|▋         | 536/7689 [1:19:20<15:08:48,  7.62s/it]

  7%|▋         | 537/7689 [1:19:31<16:56:20,  8.53s/it]


  7%|▋         | 539/7689 [1:19:42<13:51:30,  6.98s/it]

  7%|▋         | 540/7689 [1:19:48<13:20:21,  6.72s/it]

  7%|▋         | 541/7689 [1:19:53<12:33:04,  6.32s/it]
{'loss': 1.1712, 'grad_norm': 0.20891042714499036, 'learning_rate': 0.00019914860488341943, 'epoch': 0.07}


  7%|▋         | 543/7689 [1:20:15<18:02:05,  9.09s/it]

  7%|▋         | 544/7689 [1:20:24<17:30:14,  8.82s/it]
{'loss': 1.1633, 'grad_norm': 0.1795705426517634, 'learning_rate': 0.00019913207052677588, 'epoch': 0.07}


  7%|▋         | 546/7689 [1:20:35<14:25:33,  7.27s/it]

  7%|▋         | 547/7689 [1:20:44<15:04:41,  7.60s/it]

  7%|▋         | 548/7689 [1:20:50<14:10:37,  7.15s/it]
{'loss': 1.109, 'grad_norm': 0.18668551392121513, 'learning_rate': 0.00019910977846066544, 'epoch': 0.07}

  7%|▋         | 549/7689 [1:20:56<13:41:35,  6.90s/it]

  7%|▋         | 550/7689 [1:21:05<14:52:55,  7.50s/it]

  7%|▋         | 551/7689 [1:21:11<13:46:02,  6.94s/it]


  7%|▋         | 553/7689 [1:21:32<16:40:37,  8.41s/it]

  7%|▋         | 554/7689 [1:21:40<16:23:22,  8.27s/it]

  7%|▋         | 555/7689 [1:21:48<16:13:46,  8.19s/it]

  7%|▋         | 556/7689 [1:21:54<15:07:59,  7.64s/it]
{'loss': 1.0949, 'grad_norm': 0.1814382504408574, 'learning_rate': 0.00019906435025613298, 'epoch': 0.07}

  7%|▋         | 557/7689 [1:22:01<14:53:24,  7.52s/it]

  7%|▋         | 558/7689 [1:22:10<15:53:47,  8.03s/it]

  7%|▋         | 559/7689 [1:22:18<15:49:59,  7.99s/it]


  7%|▋         | 561/7689 [1:22:43<20:27:42, 10.33s/it]

  7%|▋         | 562/7689 [1:22:50<17:54:28,  9.05s/it]
{'loss': 1.068, 'grad_norm': 0.17813374022016765, 'learning_rate': 0.00019902954079377064, 'epoch': 0.07}

  7%|▋         | 563/7689 [1:23:01<19:13:55,  9.72s/it]


  7%|▋         | 565/7689 [1:23:22<20:16:54, 10.25s/it]

  7%|▋         | 566/7689 [1:23:38<23:15:41, 11.76s/it]
{'loss': 0.9982, 'grad_norm': 0.16822925617047557, 'learning_rate': 0.00019900598303299098, 'epoch': 0.07}
[2024-05-24 14:55:00,928] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 568/7689 [1:24:02<23:40:55, 11.97s/it]
{'loss': 1.1812, 'grad_norm': 0.18758134434151963, 'learning_rate': 0.0001989940987418456, 'epoch': 0.07}


  7%|▋         | 570/7689 [1:24:18<19:48:14, 10.01s/it]

  7%|▋         | 571/7689 [1:24:28<20:03:23, 10.14s/it]
{'loss': 1.0082, 'grad_norm': 0.1897125335504801, 'learning_rate': 0.00019897614056538912, 'epoch': 0.07}


  7%|▋         | 573/7689 [1:24:50<21:25:44, 10.84s/it]

  7%|▋         | 574/7689 [1:24:56<18:48:23,  9.52s/it]

  7%|▋         | 575/7689 [1:25:06<18:58:28,  9.60s/it]
{'loss': 1.0285, 'grad_norm': 0.18556836879984864, 'learning_rate': 0.00019895195046061242, 'epoch': 0.07}

  7%|▋         | 576/7689 [1:25:13<17:28:37,  8.85s/it]

  8%|▊         | 577/7689 [1:25:19<15:47:05,  7.99s/it]

  8%|▊         | 578/7689 [1:25:24<14:10:42,  7.18s/it]

  8%|▊         | 579/7689 [1:25:31<13:37:15,  6.90s/it]

  8%|▊         | 580/7689 [1:25:43<16:41:21,  8.45s/it]

  8%|▊         | 581/7689 [1:25:57<19:58:24, 10.12s/it]

  8%|▊         | 582/7689 [1:26:13<23:36:58, 11.96s/it]

  8%|▊         | 583/7689 [1:26:21<21:11:57, 10.74s/it]


  8%|▊         | 585/7689 [1:26:40<19:36:58,  9.94s/it]
{'loss': 1.1099, 'grad_norm': 0.18130137134906374, 'learning_rate': 0.00019889024627898144, 'epoch': 0.08}


  8%|▊         | 587/7689 [1:26:50<14:41:50,  7.45s/it]

  8%|▊         | 588/7689 [1:27:00<16:12:49,  8.22s/it]

  8%|▊         | 589/7689 [1:27:08<16:01:27,  8.13s/it]

  8%|▊         | 590/7689 [1:27:14<14:34:17,  7.39s/it]
{'loss': 1.1069, 'grad_norm': 0.1960142373890839, 'learning_rate': 0.00019885873609855318, 'epoch': 0.08}


  8%|▊         | 592/7689 [1:27:32<15:40:02,  7.95s/it]

  8%|▊         | 593/7689 [1:27:38<14:48:05,  7.51s/it]

  8%|▊         | 594/7689 [1:27:48<16:13:31,  8.23s/it]
{'loss': 0.9542, 'grad_norm': 0.1619733816978525, 'learning_rate': 0.0001988332121981436, 'epoch': 0.08}


  8%|▊         | 596/7689 [1:28:06<16:20:25,  8.29s/it]

  8%|▊         | 597/7689 [1:28:13<15:24:45,  7.82s/it]

  8%|▊         | 598/7689 [1:28:20<15:11:58,  7.72s/it]

  8%|▊         | 599/7689 [1:28:30<16:37:59,  8.45s/it]

  8%|▊         | 600/7689 [1:28:45<20:07:43, 10.22s/it]
  8%|▊         | 600/7689 [1:28:45<20:07:43, 10.22s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  8%|▊         | 601/7689 [1:29:38<45:57:53, 23.35s/it]

  8%|▊         | 602/7689 [1:29:51<39:20:05, 19.98s/it]
{'loss': 1.1101, 'grad_norm': 0.1922920130510241, 'learning_rate': 0.0001987813226897523, 'epoch': 0.08}


  8%|▊         | 604/7689 [1:30:03<25:24:33, 12.91s/it]
{'loss': 1.3068, 'grad_norm': 0.1999091541619773, 'learning_rate': 0.00019876817501054722, 'epoch': 0.08}


  8%|▊         | 606/7689 [1:30:24<23:32:43, 11.97s/it]

  8%|▊         | 607/7689 [1:30:34<22:27:45, 11.42s/it]
{'loss': 0.8884, 'grad_norm': 0.19057267625785135, 'learning_rate': 0.00019874832205294313, 'epoch': 0.08}

  8%|▊         | 608/7689 [1:30:45<22:01:09, 11.19s/it]

  8%|▊         | 609/7689 [1:30:55<21:33:04, 10.96s/it]


  8%|▊         | 611/7689 [1:31:15<20:16:20, 10.31s/it]

  8%|▊         | 612/7689 [1:31:20<17:30:30,  8.91s/it]

  8%|▊         | 613/7689 [1:31:28<17:02:18,  8.67s/it]
{'loss': 1.0564, 'grad_norm': 0.17835689345513445, 'learning_rate': 0.00019870814307462566, 'epoch': 0.08}

  8%|▊         | 614/7689 [1:31:38<17:28:27,  8.89s/it]

  8%|▊         | 615/7689 [1:31:45<16:38:58,  8.47s/it]

  8%|▊         | 616/7689 [1:31:55<17:15:08,  8.78s/it]


  8%|▊         | 618/7689 [1:32:11<16:36:01,  8.45s/it]
{'loss': 1.0693, 'grad_norm': 0.1906932770287177, 'learning_rate': 0.00019867417892184693, 'epoch': 0.08}


  8%|▊         | 620/7689 [1:32:25<15:21:08,  7.82s/it]

  8%|▊         | 621/7689 [1:32:34<16:14:19,  8.27s/it]

  8%|▊         | 622/7689 [1:32:45<17:39:40,  9.00s/it]
{'loss': 0.9434, 'grad_norm': 0.16560092197556045, 'learning_rate': 0.00019864669243249992, 'epoch': 0.08}

  8%|▊         | 623/7689 [1:32:55<18:26:20,  9.39s/it]

  8%|▊         | 624/7689 [1:33:05<18:52:43,  9.62s/it]

  8%|▊         | 625/7689 [1:33:11<16:33:41,  8.44s/it]


  8%|▊         | 627/7689 [1:33:31<18:34:18,  9.47s/it]
{'loss': 0.996, 'grad_norm': 0.15553553674990575, 'learning_rate': 0.00019861194048993863, 'epoch': 0.08}
[2024-05-24 15:04:58,486] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 628/7689 [1:33:48<23:13:13, 11.84s/it]

  8%|▊         | 629/7689 [1:33:55<20:32:44, 10.48s/it]


  8%|▊         | 631/7689 [1:34:11<17:33:06,  8.95s/it]
{'loss': 1.2462, 'grad_norm': 0.18783670167554503, 'learning_rate': 0.00019858382396738394, 'epoch': 0.08}


  8%|▊         | 633/7689 [1:34:31<19:10:25,  9.78s/it]

  8%|▊         | 634/7689 [1:34:45<21:39:20, 11.05s/it]

  8%|▊         | 635/7689 [1:34:50<18:31:14,  9.45s/it]

  8%|▊         | 636/7689 [1:35:03<20:20:11, 10.38s/it]
{'loss': 1.1286, 'grad_norm': 0.1870061056220169, 'learning_rate': 0.0001985482847345206, 'epoch': 0.08}


  8%|▊         | 638/7689 [1:35:23<19:48:24, 10.11s/it]
{'loss': 1.0011, 'grad_norm': 0.1690483376003874, 'learning_rate': 0.00019853394662669847, 'epoch': 0.08}

  8%|▊         | 639/7689 [1:35:35<21:08:24, 10.79s/it]

  8%|▊         | 640/7689 [1:35:42<18:27:44,  9.43s/it]


  8%|▊         | 642/7689 [1:36:09<21:51:35, 11.17s/it]

  8%|▊         | 643/7689 [1:36:14<18:28:33,  9.44s/it]

  8%|▊         | 644/7689 [1:36:25<18:55:51,  9.67s/it]
{'loss': 0.9925, 'grad_norm': 0.1814072140957692, 'learning_rate': 0.00019849051272814657, 'epoch': 0.08}

  8%|▊         | 645/7689 [1:36:32<17:26:49,  8.92s/it]

  8%|▊         | 646/7689 [1:36:38<15:49:13,  8.09s/it]

  8%|▊         | 647/7689 [1:36:44<14:50:07,  7.58s/it]

  8%|▊         | 648/7689 [1:36:52<14:49:13,  7.58s/it]

  8%|▊         | 649/7689 [1:37:04<17:14:15,  8.81s/it]

  8%|▊         | 650/7689 [1:37:10<15:48:00,  8.08s/it]

  8%|▊         | 651/7689 [1:37:17<15:30:33,  7.93s/it]


  8%|▊         | 653/7689 [1:37:35<16:12:35,  8.29s/it]

  9%|▊         | 654/7689 [1:37:39<13:58:11,  7.15s/it]

  9%|▊         | 655/7689 [1:37:46<14:05:16,  7.21s/it]
{'loss': 1.0562, 'grad_norm': 0.174711709230163, 'learning_rate': 0.0001984092500842975, 'epoch': 0.09}

  9%|▊         | 656/7689 [1:37:52<12:48:43,  6.56s/it]


  9%|▊         | 658/7689 [1:38:11<15:11:03,  7.77s/it]
{'loss': 1.1655, 'grad_norm': 0.16908397058751912, 'learning_rate': 0.00019838672077182552, 'epoch': 0.09}

  9%|▊         | 659/7689 [1:38:22<17:27:23,  8.94s/it]

  9%|▊         | 660/7689 [1:38:32<18:07:14,  9.28s/it]

  9%|▊         | 661/7689 [1:38:42<18:24:07,  9.43s/it]

  9%|▊         | 662/7689 [1:38:51<18:21:21,  9.40s/it]


  9%|▊         | 664/7689 [1:39:09<17:41:39,  9.07s/it]
{'loss': 0.9366, 'grad_norm': 0.18822516832650515, 'learning_rate': 0.00019834119082044738, 'epoch': 0.09}

  9%|▊         | 665/7689 [1:39:22<20:17:48, 10.40s/it]

  9%|▊         | 666/7689 [1:39:34<21:02:26, 10.79s/it]

  9%|▊         | 667/7689 [1:39:42<19:36:42, 10.05s/it]


  9%|▊         | 669/7689 [1:40:03<20:18:16, 10.41s/it]
{'loss': 1.0749, 'grad_norm': 0.1678505349805782, 'learning_rate': 0.00019830276931259475, 'epoch': 0.09}

  9%|▊         | 670/7689 [1:40:10<18:12:20,  9.34s/it]


  9%|▊         | 672/7689 [1:40:31<19:55:49, 10.23s/it]

  9%|▉         | 673/7689 [1:40:39<18:37:47,  9.56s/it]

  9%|▉         | 674/7689 [1:40:51<20:22:55, 10.46s/it]
{'loss': 0.8837, 'grad_norm': 0.1784264611846994, 'learning_rate': 0.000198263911730346, 'epoch': 0.09}

  9%|▉         | 675/7689 [1:40:58<18:16:55,  9.38s/it]


  9%|▉         | 677/7689 [1:41:19<19:04:06,  9.79s/it]

  9%|▉         | 678/7689 [1:41:25<17:09:33,  8.81s/it]
{'loss': 1.1295, 'grad_norm': 0.19613833967321148, 'learning_rate': 0.00019823251180675175, 'epoch': 0.09}


  9%|▉         | 680/7689 [1:41:39<15:23:31,  7.91s/it]
{'loss': 1.2352, 'grad_norm': 0.18564930462709425, 'learning_rate': 0.00019821670725628262, 'epoch': 0.09}

  9%|▉         | 681/7689 [1:41:46<14:58:26,  7.69s/it]


  9%|▉         | 683/7689 [1:42:05<16:28:57,  8.47s/it]

  9%|▉         | 684/7689 [1:42:13<16:10:03,  8.31s/it]

  9%|▉         | 685/7689 [1:42:21<15:51:05,  8.15s/it]
{'loss': 1.1974, 'grad_norm': 0.19164724064912977, 'learning_rate': 0.0001981768909198397, 'epoch': 0.09}


  9%|▉         | 687/7689 [1:42:39<16:55:39,  8.70s/it]
{'loss': 0.9925, 'grad_norm': 0.17113835490235135, 'learning_rate': 0.00019816084243082786, 'epoch': 0.09}

  9%|▉         | 688/7689 [1:42:52<19:07:30,  9.83s/it]


  9%|▉         | 690/7689 [1:43:09<17:35:21,  9.05s/it]
{'loss': 0.9628, 'grad_norm': 0.1772025825409805, 'learning_rate': 0.00019813663906740144, 'epoch': 0.09}


  9%|▉         | 692/7689 [1:43:32<20:54:35, 10.76s/it]

  9%|▉         | 693/7689 [1:43:37<17:59:59,  9.26s/it]

  9%|▉         | 694/7689 [1:43:45<17:03:40,  8.78s/it]

  9%|▉         | 695/7689 [1:43:51<15:23:23,  7.92s/it]
{'loss': 1.1422, 'grad_norm': 0.2001333537434065, 'learning_rate': 0.00019809595187752637, 'epoch': 0.09}


  9%|▉         | 697/7689 [1:44:10<17:13:33,  8.87s/it]

  9%|▉         | 698/7689 [1:44:15<15:26:28,  7.95s/it]
{'loss': 1.0431, 'grad_norm': 0.1722159841451745, 'learning_rate': 0.00019807133067663508, 'epoch': 0.09}

  9%|▉         | 699/7689 [1:44:24<15:44:52,  8.11s/it]


  9%|▉         | 701/7689 [1:44:41<16:25:44,  8.46s/it]
{'loss': 0.9354, 'grad_norm': 0.1692407324174839, 'learning_rate': 0.0001980465528585252, 'epoch': 0.09}


  9%|▉         | 703/7689 [1:44:59<17:49:39,  9.19s/it]
{'loss': 1.1951, 'grad_norm': 0.20172579707595623, 'learning_rate': 0.0001980299473230829, 'epoch': 0.09}

  9%|▉         | 704/7689 [1:45:08<17:42:20,  9.13s/it]

  9%|▉         | 705/7689 [1:45:16<17:05:30,  8.81s/it]

  9%|▉         | 706/7689 [1:45:22<15:15:20,  7.86s/it]


  9%|▉         | 708/7689 [1:45:33<12:55:03,  6.66s/it]
{'loss': 1.261, 'grad_norm': 0.1801390881378784, 'learning_rate': 0.00019798812910538205, 'epoch': 0.09}

  9%|▉         | 709/7689 [1:45:46<16:31:47,  8.53s/it]


  9%|▉         | 711/7689 [1:46:01<15:08:45,  7.81s/it]
{'loss': 1.1435, 'grad_norm': 0.18108882883801555, 'learning_rate': 0.00019796282951706093, 'epoch': 0.09}

  9%|▉         | 712/7689 [1:46:10<15:41:51,  8.10s/it]


  9%|▉         | 714/7689 [1:46:28<16:40:59,  8.61s/it]

  9%|▉         | 715/7689 [1:46:35<16:11:28,  8.36s/it]
{'loss': 1.2371, 'grad_norm': 0.18129377384842413, 'learning_rate': 0.00019792885338240374, 'epoch': 0.09}

  9%|▉         | 716/7689 [1:46:44<16:24:50,  8.47s/it]

  9%|▉         | 717/7689 [1:46:53<16:47:47,  8.67s/it]

  9%|▉         | 718/7689 [1:47:01<16:29:29,  8.52s/it]

  9%|▉         | 719/7689 [1:47:07<14:49:00,  7.65s/it]


  9%|▉         | 721/7689 [1:47:25<16:07:31,  8.33s/it]
{'loss': 1.0785, 'grad_norm': 0.17635481816795476, 'learning_rate': 0.00019787736791187726, 'epoch': 0.09}

  9%|▉         | 722/7689 [1:47:38<18:34:08,  9.60s/it]

  9%|▉         | 723/7689 [1:47:45<17:13:57,  8.91s/it]

  9%|▉         | 724/7689 [1:47:54<17:18:32,  8.95s/it]

  9%|▉         | 725/7689 [1:48:00<15:36:08,  8.07s/it]


  9%|▉         | 727/7689 [1:48:23<20:02:21, 10.36s/it]

  9%|▉         | 728/7689 [1:48:31<18:52:19,  9.76s/it]
{'loss': 1.0384, 'grad_norm': 0.1714166414801667, 'learning_rate': 0.0001978165113321517, 'epoch': 0.09}


  9%|▉         | 730/7689 [1:48:48<16:52:47,  8.73s/it]

 10%|▉         | 731/7689 [1:48:59<18:25:52,  9.54s/it]
{'loss': 1.1483, 'grad_norm': 0.17520581060035326, 'learning_rate': 0.00019779016955941486, 'epoch': 0.1}

 10%|▉         | 732/7689 [1:49:05<16:10:56,  8.37s/it]


 10%|▉         | 734/7689 [1:49:22<16:38:04,  8.61s/it]
{'loss': 0.9665, 'grad_norm': 0.1683580516456312, 'learning_rate': 0.000197763671618466, 'epoch': 0.1}

 10%|▉         | 735/7689 [1:49:31<16:51:04,  8.72s/it]

 10%|▉         | 736/7689 [1:49:42<18:23:57,  9.53s/it]


 10%|▉         | 738/7689 [1:50:00<17:50:17,  9.24s/it]
{'loss': 1.1426, 'grad_norm': 0.17590185846714362, 'learning_rate': 0.00019772809817537981, 'epoch': 0.1}


 10%|▉         | 740/7689 [1:50:18<17:38:29,  9.14s/it]

 10%|▉         | 741/7689 [1:50:32<20:18:54, 10.53s/it]

 10%|▉         | 742/7689 [1:50:40<19:01:26,  9.86s/it]
{'loss': 1.0958, 'grad_norm': 0.1770577577429136, 'learning_rate': 0.00019769224727616996, 'epoch': 0.1}


 10%|▉         | 744/7689 [1:50:53<15:55:01,  8.25s/it]
{'loss': 1.0106, 'grad_norm': 0.16533540738002186, 'learning_rate': 0.000197674217812295, 'epoch': 0.1}

 10%|▉         | 745/7689 [1:50:59<14:31:33,  7.53s/it]


 10%|▉         | 747/7689 [1:51:17<16:53:38,  8.76s/it]

 10%|▉         | 748/7689 [1:51:24<15:22:55,  7.98s/it]
{'loss': 1.0285, 'grad_norm': 0.17044730053374657, 'learning_rate': 0.00019763795091998858, 'epoch': 0.1}


 10%|▉         | 750/7689 [1:51:35<13:06:30,  6.80s/it]
{'loss': 1.0968, 'grad_norm': 0.1813149798480686, 'learning_rate': 0.0001976197135172981, 'epoch': 0.1}


 10%|▉         | 752/7689 [1:51:48<12:19:23,  6.40s/it]

 10%|▉         | 753/7689 [1:51:56<13:19:22,  6.92s/it]
{'loss': 1.109, 'grad_norm': 0.19934921836640299, 'learning_rate': 0.00019759222750397927, 'epoch': 0.1}


 10%|▉         | 755/7689 [1:52:16<15:49:37,  8.22s/it]

 10%|▉         | 756/7689 [1:52:24<15:52:59,  8.25s/it]

 10%|▉         | 757/7689 [1:52:30<14:31:04,  7.54s/it]
{'loss': 1.0655, 'grad_norm': 0.16310451880089755, 'learning_rate': 0.00019755533705723708, 'epoch': 0.1}

 10%|▉         | 758/7689 [1:52:39<15:18:42,  7.95s/it]

 10%|▉         | 759/7689 [1:52:44<13:50:51,  7.19s/it]

 10%|▉         | 760/7689 [1:53:01<19:18:17, 10.03s/it]

 10%|▉         | 761/7689 [1:53:08<17:46:33,  9.24s/it]


 10%|▉         | 763/7689 [1:53:22<15:38:39,  8.13s/it]
{'loss': 1.18, 'grad_norm': 0.1915866519710643, 'learning_rate': 0.0001974994821094854, 'epoch': 0.1}


 10%|▉         | 765/7689 [1:53:36<14:12:31,  7.39s/it]
{'loss': 1.0193, 'grad_norm': 0.1822720193357259, 'learning_rate': 0.00019748072537234068, 'epoch': 0.1}
[2024-05-24 15:25:02,957] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|▉         | 766/7689 [1:53:53<19:37:44, 10.21s/it]

 10%|▉         | 767/7689 [1:54:05<20:49:32, 10.83s/it]

 10%|▉         | 768/7689 [1:54:14<19:36:37, 10.20s/it]


 10%|█         | 770/7689 [1:54:34<19:39:08, 10.23s/it]
{'loss': 1.1358, 'grad_norm': 0.1768033911626667, 'learning_rate': 0.0001974335308591806, 'epoch': 0.1}

 10%|█         | 771/7689 [1:54:45<20:08:27, 10.48s/it]


 10%|█         | 773/7689 [1:55:02<17:28:58,  9.10s/it]
{'loss': 1.0766, 'grad_norm': 0.20782203230010002, 'learning_rate': 0.00019740500667296188, 'epoch': 0.1}

 10%|█         | 774/7689 [1:55:19<21:58:21, 11.44s/it]

 10%|█         | 775/7689 [1:55:33<23:22:12, 12.17s/it]

 10%|█         | 776/7689 [1:55:41<20:59:03, 10.93s/it]

 10%|█         | 777/7689 [1:55:49<19:12:17, 10.00s/it]

 10%|█         | 778/7689 [1:55:59<19:24:20, 10.11s/it]


 10%|█         | 780/7689 [1:56:18<18:39:12,  9.72s/it]
{'loss': 1.1537, 'grad_norm': 0.1674041910941677, 'learning_rate': 0.0001973378453888589, 'epoch': 0.1}

 10%|█         | 781/7689 [1:56:30<19:55:54, 10.39s/it]

 10%|█         | 782/7689 [1:56:38<18:30:50,  9.65s/it]


 10%|█         | 784/7689 [1:56:56<18:26:08,  9.61s/it]
{'loss': 0.9604, 'grad_norm': 0.17939650853256722, 'learning_rate': 0.0001972990874959202, 'epoch': 0.1}

 10%|█         | 785/7689 [1:57:02<16:06:14,  8.40s/it]

 10%|█         | 786/7689 [1:57:10<15:39:00,  8.16s/it]

 10%|█         | 787/7689 [1:57:18<15:37:12,  8.15s/it]

 10%|█         | 788/7689 [1:57:27<16:25:40,  8.57s/it]

 10%|█         | 789/7689 [1:57:34<15:23:49,  8.03s/it]

 10%|█         | 790/7689 [1:57:43<15:42:08,  8.19s/it]


 10%|█         | 792/7689 [1:58:01<16:41:08,  8.71s/it]
{'loss': 0.9618, 'grad_norm': 0.17152563179083297, 'learning_rate': 0.00019722074310645553, 'epoch': 0.1}

 10%|█         | 793/7689 [1:58:08<15:42:57,  8.20s/it]

 10%|█         | 794/7689 [1:58:14<14:26:09,  7.54s/it]


 10%|█         | 796/7689 [1:58:37<18:23:14,  9.60s/it]
{'loss': 1.0804, 'grad_norm': 0.17226431627508165, 'learning_rate': 0.00019718115683235417, 'epoch': 0.1}

 10%|█         | 797/7689 [1:58:45<17:57:11,  9.38s/it]

 10%|█         | 798/7689 [1:58:55<17:50:03,  9.32s/it]


 10%|█         | 800/7689 [1:59:17<19:30:45, 10.20s/it]
{'loss': 0.8373, 'grad_norm': 0.1573790226980299, 'learning_rate': 0.00019714129465492946, 'epoch': 0.1}

 10%|█         | 801/7689 [1:59:28<20:10:52, 10.55s/it]

 10%|█         | 802/7689 [1:59:37<19:13:11, 10.05s/it]

 10%|█         | 803/7689 [1:59:43<17:03:31,  8.92s/it]


 10%|█         | 805/7689 [2:00:04<19:22:06, 10.13s/it]
{'loss': 0.9091, 'grad_norm': 0.16465614959232114, 'learning_rate': 0.00019709107911658382, 'epoch': 0.1}

 10%|█         | 806/7689 [2:00:20<22:17:10, 11.66s/it]

 10%|█         | 807/7689 [2:00:25<18:49:10,  9.84s/it]

 11%|█         | 808/7689 [2:00:40<21:24:33, 11.20s/it]

 11%|█         | 809/7689 [2:00:47<19:13:42, 10.06s/it]

 11%|█         | 810/7689 [2:01:02<22:07:38, 11.58s/it]

 11%|█         | 811/7689 [2:01:11<20:27:41, 10.71s/it]

 11%|█         | 812/7689 [2:01:15<16:52:18,  8.83s/it]

 11%|█         | 813/7689 [2:01:21<15:17:08,  8.00s/it]

 11%|█         | 814/7689 [2:01:28<14:36:54,  7.65s/it]

 11%|█         | 815/7689 [2:01:38<15:43:03,  8.23s/it]

 11%|█         | 816/7689 [2:01:45<15:18:34,  8.02s/it]

 11%|█         | 817/7689 [2:01:51<14:06:03,  7.39s/it]


 11%|█         | 819/7689 [2:02:05<13:33:22,  7.10s/it]

 11%|█         | 820/7689 [2:02:15<15:14:16,  7.99s/it]

 11%|█         | 821/7689 [2:02:25<16:22:24,  8.58s/it]
{'loss': 1.1063, 'grad_norm': 0.1759436680172623, 'learning_rate': 0.0001969274962045867, 'epoch': 0.11}

 11%|█         | 822/7689 [2:02:30<14:27:58,  7.58s/it]

 11%|█         | 823/7689 [2:02:48<20:33:47, 10.78s/it]

 11%|█         | 824/7689 [2:02:55<18:17:37,  9.59s/it]

 11%|█         | 825/7689 [2:03:02<16:48:08,  8.81s/it]

 11%|█         | 826/7689 [2:03:11<16:47:16,  8.81s/it]

 11%|█         | 827/7689 [2:03:16<14:46:04,  7.75s/it]

 11%|█         | 828/7689 [2:03:21<13:07:20,  6.89s/it]

 11%|█         | 829/7689 [2:03:32<15:39:33,  8.22s/it]

 11%|█         | 830/7689 [2:03:42<16:23:38,  8.60s/it]

 11%|█         | 831/7689 [2:03:55<18:57:23,  9.95s/it]

 11%|█         | 832/7689 [2:04:02<17:14:02,  9.05s/it]


 11%|█         | 834/7689 [2:04:21<17:42:45,  9.30s/it]

 11%|█         | 835/7689 [2:04:29<17:03:08,  8.96s/it]
{'loss': 0.9775, 'grad_norm': 0.203933351710086, 'learning_rate': 0.00019678074913146907, 'epoch': 0.11}

 11%|█         | 836/7689 [2:04:38<17:01:50,  8.95s/it]

 11%|█         | 837/7689 [2:04:44<15:31:27,  8.16s/it]


 11%|█         | 839/7689 [2:04:59<14:39:49,  7.71s/it]
{'loss': 1.0403, 'grad_norm': 0.20077277685665987, 'learning_rate': 0.00019673820294710287, 'epoch': 0.11}

 11%|█         | 840/7689 [2:05:14<18:46:26,  9.87s/it]

 11%|█         | 841/7689 [2:05:22<17:44:16,  9.32s/it]

 11%|█         | 842/7689 [2:05:34<19:21:56, 10.18s/it]

 11%|█         | 843/7689 [2:05:44<19:12:07, 10.10s/it]

 11%|█         | 844/7689 [2:05:54<19:28:39, 10.24s/it]

 11%|█         | 845/7689 [2:06:00<17:01:47,  8.96s/it]


 11%|█         | 847/7689 [2:06:19<17:27:21,  9.18s/it]
{'loss': 0.9728, 'grad_norm': 0.16958762160500412, 'learning_rate': 0.0001966522867626919, 'epoch': 0.11}

 11%|█         | 848/7689 [2:06:27<17:01:06,  8.96s/it]

 11%|█         | 849/7689 [2:06:38<17:53:02,  9.41s/it]

 11%|█         | 850/7689 [2:06:46<16:59:39,  8.95s/it]

 11%|█         | 851/7689 [2:06:53<16:22:00,  8.62s/it]

 11%|█         | 852/7689 [2:07:07<19:01:31, 10.02s/it]


 11%|█         | 854/7689 [2:07:21<16:14:08,  8.55s/it]
{'loss': 0.8901, 'grad_norm': 0.18448262417883643, 'learning_rate': 0.00019657620968731673, 'epoch': 0.11}

 11%|█         | 855/7689 [2:07:26<14:11:17,  7.47s/it]


 11%|█         | 857/7689 [2:07:45<15:56:28,  8.40s/it]

 11%|█         | 858/7689 [2:07:51<14:21:33,  7.57s/it]
{'loss': 1.2525, 'grad_norm': 0.17574810261973434, 'learning_rate': 0.000196532360024742, 'epoch': 0.11}


 11%|█         | 860/7689 [2:08:13<18:26:29,  9.72s/it]
{'loss': 0.8277, 'grad_norm': 0.15189828377467687, 'learning_rate': 0.00019651033241263765, 'epoch': 0.11}

 11%|█         | 861/7689 [2:08:28<21:13:52, 11.19s/it]

 11%|█         | 862/7689 [2:08:35<18:57:44, 10.00s/it]

 11%|█         | 863/7689 [2:08:44<18:32:54,  9.78s/it]

 11%|█         | 864/7689 [2:08:51<17:08:39,  9.04s/it]

 11%|█         | 865/7689 [2:09:02<18:10:46,  9.59s/it]

 11%|█▏        | 866/7689 [2:09:11<17:59:29,  9.49s/it]

 11%|█▏        | 867/7689 [2:09:22<18:35:08,  9.81s/it]

 11%|█▏        | 868/7689 [2:09:34<19:49:45, 10.47s/it]

 11%|█▏        | 869/7689 [2:09:48<22:04:13, 11.65s/it]

 11%|█▏        | 870/7689 [2:09:54<18:46:26,  9.91s/it]

 11%|█▏        | 871/7689 [2:10:03<17:57:19,  9.48s/it]

 11%|█▏        | 872/7689 [2:10:17<20:27:09, 10.80s/it]


 11%|█▏        | 874/7689 [2:10:33<18:08:30,  9.58s/it]
{'loss': 1.1511, 'grad_norm': 0.18024111006387994, 'learning_rate': 0.00019635422201754076, 'epoch': 0.11}

 11%|█▏        | 875/7689 [2:10:40<16:28:52,  8.71s/it]


 11%|█▏        | 877/7689 [2:10:59<16:57:48,  8.96s/it]
{'loss': 1.0944, 'grad_norm': 0.17001493257725792, 'learning_rate': 0.00019632033362473766, 'epoch': 0.11}

 11%|█▏        | 878/7689 [2:11:06<16:05:06,  8.50s/it]

 11%|█▏        | 879/7689 [2:11:15<16:04:01,  8.49s/it]


 11%|█▏        | 881/7689 [2:11:35<18:21:42,  9.71s/it]
{'loss': 0.971, 'grad_norm': 0.1658363528143562, 'learning_rate': 0.00019627490983339585, 'epoch': 0.11}

 11%|█▏        | 882/7689 [2:11:42<16:45:32,  8.86s/it]


 11%|█▏        | 884/7689 [2:11:59<16:06:34,  8.52s/it]
{'loss': 0.9861, 'grad_norm': 0.17973341302666768, 'learning_rate': 0.00019624066260971173, 'epoch': 0.11}

 12%|█▏        | 885/7689 [2:12:06<15:23:49,  8.15s/it]


 12%|█▏        | 887/7689 [2:12:23<15:31:50,  8.22s/it]
{'loss': 1.1621, 'grad_norm': 0.21220319508834437, 'learning_rate': 0.0001962062616923356, 'epoch': 0.12}

 12%|█▏        | 888/7689 [2:12:30<14:35:42,  7.73s/it]

 12%|█▏        | 889/7689 [2:12:40<15:55:50,  8.43s/it]

 12%|█▏        | 890/7689 [2:12:47<15:04:57,  7.99s/it]

 12%|█▏        | 891/7689 [2:12:53<13:56:40,  7.38s/it]


 12%|█▏        | 893/7689 [2:13:16<16:59:35,  9.00s/it]
{'loss': 1.1311, 'grad_norm': 0.17812584278276253, 'learning_rate': 0.00019613699899650175, 'epoch': 0.12}


 12%|█▏        | 895/7689 [2:13:29<15:16:34,  8.09s/it]
{'loss': 0.9928, 'grad_norm': 0.16559806762619764, 'learning_rate': 0.0001961137749398548, 'epoch': 0.12}

 12%|█▏        | 896/7689 [2:13:35<13:54:21,  7.37s/it]


 12%|█▏        | 898/7689 [2:13:49<13:41:27,  7.26s/it]

 12%|█▏        | 899/7689 [2:14:04<17:37:13,  9.34s/it]
{'loss': 1.0845, 'grad_norm': 0.16567856645687143, 'learning_rate': 0.00019606712218833662, 'epoch': 0.12}

 12%|█▏        | 900/7689 [2:14:09<15:24:41,  8.17s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 12%|█▏        | 901/7689 [2:14:42<29:08:57, 15.46s/it]
{'loss': 1.1467, 'grad_norm': 0.1660168131885522, 'learning_rate': 0.00019604369352657787, 'epoch': 0.12}

 12%|█▏        | 902/7689 [2:14:46<23:10:28, 12.29s/it]

 12%|█▏        | 903/7689 [2:14:58<22:47:53, 12.09s/it]


 12%|█▏        | 905/7689 [2:15:13<18:44:48,  9.95s/it]
{'loss': 0.8899, 'grad_norm': 0.17709562110304825, 'learning_rate': 0.00019599663171420543, 'epoch': 0.12}

 12%|█▏        | 906/7689 [2:15:19<16:14:01,  8.62s/it]

 12%|█▏        | 907/7689 [2:15:24<14:26:42,  7.67s/it]

 12%|█▏        | 908/7689 [2:15:37<17:17:08,  9.18s/it]


 12%|█▏        | 910/7689 [2:15:56<17:48:34,  9.46s/it]
{'loss': 1.0431, 'grad_norm': 0.18497983862558534, 'learning_rate': 0.0001959374212045572, 'epoch': 0.12}

 12%|█▏        | 911/7689 [2:16:02<16:02:32,  8.52s/it]

 12%|█▏        | 912/7689 [2:16:09<15:15:11,  8.10s/it]

 12%|█▏        | 913/7689 [2:16:18<15:38:11,  8.31s/it]

 12%|█▏        | 914/7689 [2:16:31<18:10:59,  9.66s/it]


 12%|█▏        | 916/7689 [2:16:48<16:51:37,  8.96s/it]
{'loss': 1.04, 'grad_norm': 0.17545965613352613, 'learning_rate': 0.00019586580684847175, 'epoch': 0.12}

 12%|█▏        | 917/7689 [2:16:53<14:47:01,  7.86s/it]

 12%|█▏        | 918/7689 [2:17:00<14:29:58,  7.71s/it]

 12%|█▏        | 919/7689 [2:17:12<16:41:40,  8.88s/it]

 12%|█▏        | 920/7689 [2:17:19<15:31:01,  8.25s/it]

 12%|█▏        | 921/7689 [2:17:27<15:18:25,  8.14s/it]


 12%|█▏        | 923/7689 [2:17:46<17:01:20,  9.06s/it]
{'loss': 1.0043, 'grad_norm': 0.1664172411254054, 'learning_rate': 0.00019578148281845036, 'epoch': 0.12}

 12%|█▏        | 924/7689 [2:17:55<17:15:02,  9.18s/it]


 12%|█▏        | 926/7689 [2:18:13<17:45:05,  9.45s/it]
{'loss': 1.0929, 'grad_norm': 0.1624352804337773, 'learning_rate': 0.00019574508897159446, 'epoch': 0.12}


 12%|█▏        | 928/7689 [2:18:30<16:17:06,  8.67s/it]
{'loss': 1.0575, 'grad_norm': 0.1922672704062094, 'learning_rate': 0.00019572074145771806, 'epoch': 0.12}


 12%|█▏        | 930/7689 [2:18:44<14:47:52,  7.88s/it]

 12%|█▏        | 931/7689 [2:18:52<14:52:36,  7.92s/it]
{'loss': 1.2793, 'grad_norm': 0.18146404339393704, 'learning_rate': 0.00019568409280614732, 'epoch': 0.12}

 12%|█▏        | 932/7689 [2:18:59<14:23:16,  7.67s/it]

 12%|█▏        | 933/7689 [2:19:06<14:17:55,  7.62s/it]


 12%|█▏        | 935/7689 [2:19:30<18:41:54,  9.97s/it]
{'loss': 0.8038, 'grad_norm': 0.1505505605537151, 'learning_rate': 0.00019563499025107998, 'epoch': 0.12}

 12%|█▏        | 936/7689 [2:19:39<18:16:25,  9.74s/it]


 12%|█▏        | 938/7689 [2:19:58<17:24:07,  9.28s/it]
{'loss': 1.0364, 'grad_norm': 0.1806736862828651, 'learning_rate': 0.00019559798514628697, 'epoch': 0.12}

 12%|█▏        | 939/7689 [2:20:09<18:33:49,  9.90s/it]

 12%|█▏        | 940/7689 [2:20:15<16:00:05,  8.54s/it]

 12%|█▏        | 941/7689 [2:20:22<15:30:38,  8.27s/it]

 12%|█▏        | 942/7689 [2:20:29<14:49:53,  7.91s/it]

 12%|█▏        | 943/7689 [2:20:41<16:53:04,  9.01s/it]


 12%|█▏        | 945/7689 [2:20:52<13:22:42,  7.14s/it]
{'loss': 0.8416, 'grad_norm': 0.1803907836146198, 'learning_rate': 0.00019551104629799194, 'epoch': 0.12}

 12%|█▏        | 946/7689 [2:20:57<12:15:58,  6.55s/it]

 12%|█▏        | 947/7689 [2:21:07<14:21:33,  7.67s/it]

 12%|█▏        | 948/7689 [2:21:21<18:02:03,  9.63s/it]

 12%|█▏        | 949/7689 [2:21:28<16:07:36,  8.61s/it]


 12%|█▏        | 951/7689 [2:21:40<13:46:58,  7.36s/it]

 12%|█▏        | 952/7689 [2:21:51<15:31:21,  8.29s/it]

 12%|█▏        | 953/7689 [2:22:02<17:08:57,  9.17s/it]
{'loss': 0.8974, 'grad_norm': 0.1704019686740167, 'learning_rate': 0.0001954106708024685, 'epoch': 0.12}


 12%|█▏        | 955/7689 [2:22:18<16:13:22,  8.67s/it]
{'loss': 1.1134, 'grad_norm': 0.1942155018070902, 'learning_rate': 0.00019538540758590805, 'epoch': 0.12}

 12%|█▏        | 956/7689 [2:22:28<16:50:42,  9.01s/it]

 12%|█▏        | 957/7689 [2:22:40<18:57:17, 10.14s/it]

 12%|█▏        | 958/7689 [2:22:51<19:01:52, 10.18s/it]

 12%|█▏        | 959/7689 [2:22:56<16:08:43,  8.64s/it]

 12%|█▏        | 960/7689 [2:23:03<15:32:58,  8.32s/it]

 12%|█▏        | 961/7689 [2:23:08<13:48:06,  7.39s/it]

 13%|█▎        | 962/7689 [2:23:21<16:33:51,  8.86s/it]

 13%|█▎        | 963/7689 [2:23:32<17:42:00,  9.47s/it]

 13%|█▎        | 964/7689 [2:23:38<15:44:53,  8.43s/it]


 13%|█▎        | 966/7689 [2:24:00<18:40:01, 10.00s/it]
{'loss': 1.2077, 'grad_norm': 0.16002612504227556, 'learning_rate': 0.0001952452502175985, 'epoch': 0.13}

 13%|█▎        | 967/7689 [2:24:05<16:07:49,  8.64s/it]

 13%|█▎        | 968/7689 [2:24:16<17:03:04,  9.13s/it]

 13%|█▎        | 969/7689 [2:24:24<16:53:06,  9.05s/it]

 13%|█▎        | 970/7689 [2:24:29<14:22:41,  7.70s/it]


 13%|█▎        | 972/7689 [2:24:52<17:53:34,  9.59s/it]
{'loss': 1.0742, 'grad_norm': 0.21211343628712276, 'learning_rate': 0.0001951679386285163, 'epoch': 0.13}

 13%|█▎        | 973/7689 [2:24:59<16:16:09,  8.72s/it]

 13%|█▎        | 974/7689 [2:25:04<14:07:23,  7.57s/it]


 13%|█▎        | 976/7689 [2:25:22<15:21:00,  8.23s/it]
{'loss': 0.9924, 'grad_norm': 0.18812855777496085, 'learning_rate': 0.00019511605980399296, 'epoch': 0.13}

 13%|█▎        | 977/7689 [2:25:30<15:09:07,  8.13s/it]


 13%|█▎        | 979/7689 [2:25:46<15:12:08,  8.16s/it]
{'loss': 1.1017, 'grad_norm': 0.18414032300927505, 'learning_rate': 0.00019507697346351414, 'epoch': 0.13}

 13%|█▎        | 980/7689 [2:25:54<15:16:41,  8.20s/it]

 13%|█▎        | 981/7689 [2:26:07<17:34:49,  9.43s/it]

 13%|█▎        | 982/7689 [2:26:15<17:05:48,  9.18s/it]

 13%|█▎        | 983/7689 [2:26:24<16:38:24,  8.93s/it]

 13%|█▎        | 984/7689 [2:26:39<20:05:01, 10.78s/it]

 13%|█▎        | 985/7689 [2:26:49<19:56:08, 10.71s/it]

 13%|█▎        | 986/7689 [2:27:02<20:57:41, 11.26s/it]

 13%|█▎        | 987/7689 [2:27:10<19:08:47, 10.28s/it]


 13%|█▎        | 989/7689 [2:27:22<14:59:10,  8.05s/it]
{'loss': 1.2894, 'grad_norm': 0.1962706047097668, 'learning_rate': 0.0001949455894259959, 'epoch': 0.13}

 13%|█▎        | 990/7689 [2:27:30<14:43:48,  7.92s/it]


 13%|█▎        | 992/7689 [2:27:56<20:46:32, 11.17s/it]
{'loss': 1.0754, 'grad_norm': 0.1792338807620994, 'learning_rate': 0.0001949058455864186, 'epoch': 0.13}

 13%|█▎        | 993/7689 [2:28:06<19:51:58, 10.68s/it]

 13%|█▎        | 994/7689 [2:28:15<19:00:13, 10.22s/it]

 13%|█▎        | 995/7689 [2:28:26<19:27:58, 10.47s/it]

 13%|█▎        | 996/7689 [2:28:34<17:53:00,  9.62s/it]

 13%|█▎        | 997/7689 [2:28:39<15:30:11,  8.34s/it]


 13%|█▎        | 999/7689 [2:28:58<16:26:36,  8.85s/it]
{'loss': 1.0637, 'grad_norm': 0.18123135221006423, 'learning_rate': 0.00019481252066301364, 'epoch': 0.13}

 13%|█▎        | 1000/7689 [2:29:07<16:16:16,  8.76s/it]


 13%|█▎        | 1002/7689 [2:29:28<18:10:40,  9.79s/it]
{'loss': 1.0397, 'grad_norm': 0.17700611060165747, 'learning_rate': 0.00019477227186486963, 'epoch': 0.13}

 13%|█▎        | 1003/7689 [2:29:35<16:21:42,  8.81s/it]

 13%|█▎        | 1004/7689 [2:29:43<16:06:30,  8.67s/it]

 13%|█▎        | 1005/7689 [2:29:50<14:59:15,  8.07s/it]

 13%|█▎        | 1006/7689 [2:29:58<14:58:23,  8.07s/it]

 13%|█▎        | 1007/7689 [2:30:09<16:37:40,  8.96s/it]

 13%|█▎        | 1008/7689 [2:30:16<15:22:23,  8.28s/it]

 13%|█▎        | 1009/7689 [2:30:27<17:02:04,  9.18s/it]

 13%|█▎        | 1010/7689 [2:30:33<15:27:58,  8.34s/it]


 13%|█▎        | 1012/7689 [2:30:45<12:40:14,  6.83s/it]
{'loss': 1.0827, 'grad_norm': 0.2107690413795222, 'learning_rate': 0.00019463701649322343, 'epoch': 0.13}

 13%|█▎        | 1013/7689 [2:30:53<13:36:27,  7.34s/it]


 13%|█▎        | 1015/7689 [2:31:09<13:53:13,  7.49s/it]
{'loss': 1.111, 'grad_norm': 0.19538775263353766, 'learning_rate': 0.0001945961123179787, 'epoch': 0.13}


 13%|█▎        | 1017/7689 [2:31:28<16:12:25,  8.74s/it]
{'loss': 1.1682, 'grad_norm': 0.16954952624356398, 'learning_rate': 0.00019456875893744533, 'epoch': 0.13}

 13%|█▎        | 1018/7689 [2:31:34<14:22:57,  7.76s/it]

 13%|█▎        | 1019/7689 [2:31:39<13:09:59,  7.11s/it]

 13%|█▎        | 1020/7689 [2:31:55<17:56:44,  9.69s/it]

 13%|█▎        | 1021/7689 [2:32:03<17:07:16,  9.24s/it]


 13%|█▎        | 1023/7689 [2:32:21<16:17:48,  8.80s/it]
{'loss': 1.1378, 'grad_norm': 0.19629633306409427, 'learning_rate': 0.0001944862961438239, 'epoch': 0.13}

 13%|█▎        | 1024/7689 [2:32:35<19:24:46, 10.49s/it]


 13%|█▎        | 1026/7689 [2:33:07<23:56:16, 12.93s/it]

 13%|█▎        | 1027/7689 [2:33:15<21:17:53, 11.51s/it]
{'loss': 1.1371, 'grad_norm': 0.168968500964743, 'learning_rate': 0.00019443098559994182, 'epoch': 0.13}


 13%|█▎        | 1029/7689 [2:33:35<20:25:27, 11.04s/it]
{'loss': 0.9234, 'grad_norm': 0.1822871448949524, 'learning_rate': 0.0001944032297823753, 'epoch': 0.13}

 13%|█▎        | 1030/7689 [2:33:43<18:39:40, 10.09s/it]

 13%|█▎        | 1031/7689 [2:33:53<18:49:08, 10.18s/it]

 13%|█▎        | 1032/7689 [2:34:02<17:50:50,  9.65s/it]


 13%|█▎        | 1034/7689 [2:34:27<21:08:26, 11.44s/it]
{'loss': 0.8844, 'grad_norm': 0.19242167724172535, 'learning_rate': 0.00019433354713844386, 'epoch': 0.13}

 13%|█▎        | 1035/7689 [2:34:37<20:29:23, 11.09s/it]

 13%|█▎        | 1036/7689 [2:34:42<17:11:16,  9.30s/it]

 13%|█▎        | 1037/7689 [2:34:47<14:42:49,  7.96s/it]

 13%|█▎        | 1038/7689 [2:35:01<18:17:04,  9.90s/it]

 14%|█▎        | 1039/7689 [2:35:09<17:09:09,  9.29s/it]


 14%|█▎        | 1041/7689 [2:35:21<13:40:25,  7.40s/it]
{'loss': 1.2273, 'grad_norm': 0.19493389428473304, 'learning_rate': 0.0001942352884823229, 'epoch': 0.14}

 14%|█▎        | 1042/7689 [2:35:29<14:26:00,  7.82s/it]

 14%|█▎        | 1043/7689 [2:35:34<12:50:36,  6.96s/it]

 14%|█▎        | 1044/7689 [2:35:41<12:47:41,  6.93s/it]

 14%|█▎        | 1045/7689 [2:35:50<13:36:49,  7.38s/it]


 14%|█▎        | 1047/7689 [2:36:11<16:32:03,  8.96s/it]
{'loss': 1.2025, 'grad_norm': 0.1806554153520308, 'learning_rate': 0.00019415041461650923, 'epoch': 0.14}

 14%|█▎        | 1048/7689 [2:36:16<14:30:08,  7.86s/it]

 14%|█▎        | 1049/7689 [2:36:24<14:18:50,  7.76s/it]

 14%|█▎        | 1050/7689 [2:36:31<14:05:59,  7.65s/it]


 14%|█▎        | 1052/7689 [2:36:49<15:13:33,  8.26s/it]
{'loss': 1.1769, 'grad_norm': 0.1807472542180842, 'learning_rate': 0.0001940792269520966, 'epoch': 0.14}

 14%|█▎        | 1053/7689 [2:36:56<14:26:16,  7.83s/it]

 14%|█▎        | 1054/7689 [2:37:04<14:48:02,  8.03s/it]


 14%|█▎        | 1056/7689 [2:37:23<15:47:12,  8.57s/it]

 14%|█▎        | 1057/7689 [2:37:29<14:16:01,  7.74s/it]
{'loss': 0.9923, 'grad_norm': 0.18724173617225404, 'learning_rate': 0.00019400762194906397, 'epoch': 0.14}

 14%|█▍        | 1058/7689 [2:37:36<13:54:05,  7.55s/it]

 14%|█▍        | 1059/7689 [2:37:43<13:29:08,  7.32s/it]

 14%|█▍        | 1060/7689 [2:37:53<15:20:51,  8.33s/it]

 14%|█▍        | 1061/7689 [2:38:03<16:18:36,  8.86s/it]


 14%|█▍        | 1063/7689 [2:38:27<19:23:19, 10.53s/it]
[2024-05-24 16:09:37,471] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9291, 'grad_norm': 0.17121951802271526, 'learning_rate': 0.00019392114550581692, 'epoch': 0.14}

 14%|█▍        | 1064/7689 [2:38:34<17:09:58,  9.33s/it]

 14%|█▍        | 1065/7689 [2:38:40<15:19:22,  8.33s/it]

 14%|█▍        | 1066/7689 [2:38:58<20:53:59, 11.36s/it]


 14%|█▍        | 1068/7689 [2:39:09<15:14:05,  8.28s/it]
{'loss': 1.1856, 'grad_norm': 0.1923836065693483, 'learning_rate': 0.00019384862347852553, 'epoch': 0.14}

 14%|█▍        | 1069/7689 [2:39:19<15:55:09,  8.66s/it]
[2024-05-24 16:10:44,671] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 14%|█▍        | 1071/7689 [2:39:45<19:44:56, 10.74s/it]
{'loss': 1.0737, 'grad_norm': 0.17711333548524494, 'learning_rate': 0.0001938049104099699, 'epoch': 0.14}

 14%|█▍        | 1072/7689 [2:39:53<18:04:53,  9.84s/it]

 14%|█▍        | 1073/7689 [2:39:58<15:46:26,  8.58s/it]

 14%|█▍        | 1074/7689 [2:40:07<15:35:20,  8.48s/it]

 14%|█▍        | 1075/7689 [2:40:11<13:33:06,  7.38s/it]

 14%|█▍        | 1076/7689 [2:40:17<12:18:18,  6.70s/it]

 14%|█▍        | 1077/7689 [2:40:24<12:58:34,  7.07s/it]

 14%|█▍        | 1078/7689 [2:40:30<11:58:45,  6.52s/it]

 14%|█▍        | 1079/7689 [2:40:38<12:49:48,  6.99s/it]

 14%|█▍        | 1080/7689 [2:40:48<14:35:12,  7.95s/it]

 14%|█▍        | 1081/7689 [2:41:04<18:47:37, 10.24s/it]


 14%|█▍        | 1083/7689 [2:41:23<17:51:40,  9.73s/it]
{'loss': 0.8361, 'grad_norm': 0.16492287753185242, 'learning_rate': 0.00019362856079879325, 'epoch': 0.14}

 14%|█▍        | 1084/7689 [2:41:30<16:23:41,  8.94s/it]

 14%|█▍        | 1085/7689 [2:41:36<14:40:49,  8.00s/it]

 14%|█▍        | 1086/7689 [2:41:44<14:54:03,  8.12s/it]

 14%|█▍        | 1087/7689 [2:41:50<13:18:11,  7.25s/it]

 14%|█▍        | 1088/7689 [2:42:03<16:38:50,  9.08s/it]


 14%|█▍        | 1090/7689 [2:42:23<18:25:59, 10.06s/it]
{'loss': 1.049, 'grad_norm': 0.18374263510588054, 'learning_rate': 0.00019352458509973457, 'epoch': 0.14}


 14%|█▍        | 1092/7689 [2:42:43<18:29:07, 10.09s/it]
{'loss': 1.1794, 'grad_norm': 0.17257468623051483, 'learning_rate': 0.0001934947283613322, 'epoch': 0.14}

 14%|█▍        | 1093/7689 [2:42:51<17:03:09,  9.31s/it]

 14%|█▍        | 1094/7689 [2:43:00<17:12:27,  9.39s/it]


 14%|█▍        | 1096/7689 [2:43:15<15:43:48,  8.59s/it]

 14%|█▍        | 1097/7689 [2:43:23<15:31:25,  8.48s/it]
{'loss': 0.9652, 'grad_norm': 0.18241816367447797, 'learning_rate': 0.0001934197962396728, 'epoch': 0.14}


 14%|█▍        | 1099/7689 [2:43:37<14:09:49,  7.74s/it]

 14%|█▍        | 1100/7689 [2:43:45<14:14:04,  7.78s/it]
{'loss': 0.9612, 'grad_norm': 0.16671574112963178, 'learning_rate': 0.00019337463802691264, 'epoch': 0.14}


 14%|█▍        | 1102/7689 [2:44:00<13:37:28,  7.45s/it]
{'loss': 0.9678, 'grad_norm': 0.17612835646017075, 'learning_rate': 0.00019334444970465025, 'epoch': 0.14}

 14%|█▍        | 1103/7689 [2:44:09<14:35:40,  7.98s/it]

 14%|█▍        | 1104/7689 [2:44:21<16:56:10,  9.26s/it]

 14%|█▍        | 1105/7689 [2:44:31<17:14:56,  9.43s/it]


 14%|█▍        | 1107/7689 [2:44:49<16:19:11,  8.93s/it]
{'loss': 1.1565, 'grad_norm': 0.18625342178245116, 'learning_rate': 0.00019326868909050435, 'epoch': 0.14}


 14%|█▍        | 1109/7689 [2:45:03<14:50:03,  8.12s/it]
{'loss': 1.1709, 'grad_norm': 0.1908064625712734, 'learning_rate': 0.00019323826897791094, 'epoch': 0.14}

 14%|█▍        | 1110/7689 [2:45:15<16:45:12,  9.17s/it]

 14%|█▍        | 1111/7689 [2:45:23<16:02:06,  8.78s/it]

 14%|█▍        | 1112/7689 [2:45:29<14:37:46,  8.01s/it]

 14%|█▍        | 1113/7689 [2:45:44<18:41:22, 10.23s/it]

 14%|█▍        | 1114/7689 [2:45:54<18:23:41, 10.07s/it]

 15%|█▍        | 1115/7689 [2:46:02<17:15:10,  9.45s/it]

 15%|█▍        | 1116/7689 [2:46:12<17:21:22,  9.51s/it]


 15%|█▍        | 1118/7689 [2:46:29<16:48:57,  9.21s/it]
{'loss': 1.0151, 'grad_norm': 0.17275496865488035, 'learning_rate': 0.00019310055983976822, 'epoch': 0.15}

 15%|█▍        | 1119/7689 [2:46:36<15:39:04,  8.58s/it]

 15%|█▍        | 1120/7689 [2:46:44<15:00:47,  8.23s/it]

 15%|█▍        | 1121/7689 [2:46:53<15:14:18,  8.35s/it]

 15%|█▍        | 1122/7689 [2:47:01<15:02:48,  8.25s/it]

 15%|█▍        | 1123/7689 [2:47:07<14:12:52,  7.79s/it]

 15%|█▍        | 1124/7689 [2:47:14<13:50:45,  7.59s/it]


 15%|█▍        | 1126/7689 [2:47:28<13:10:26,  7.23s/it]

 15%|█▍        | 1127/7689 [2:47:36<13:38:24,  7.48s/it]

 15%|█▍        | 1128/7689 [2:47:49<16:58:42,  9.32s/it]
{'loss': 0.867, 'grad_norm': 0.14914856465111198, 'learning_rate': 0.0001929459803847669, 'epoch': 0.15}

 15%|█▍        | 1129/7689 [2:47:56<15:27:43,  8.49s/it]


 15%|█▍        | 1131/7689 [2:48:07<12:58:10,  7.12s/it]

 15%|█▍        | 1132/7689 [2:48:18<14:42:16,  8.07s/it]

 15%|█▍        | 1133/7689 [2:48:24<13:30:48,  7.42s/it]

 15%|█▍        | 1134/7689 [2:48:31<13:41:13,  7.52s/it]

 15%|█▍        | 1135/7689 [2:48:40<14:09:27,  7.78s/it]
{'loss': 1.1172, 'grad_norm': 0.1798574645792267, 'learning_rate': 0.00019283679330160726, 'epoch': 0.15}

 15%|█▍        | 1136/7689 [2:48:46<13:18:35,  7.31s/it]


 15%|█▍        | 1138/7689 [2:49:14<19:32:43, 10.74s/it]
{'loss': 0.9961, 'grad_norm': 0.16964163259855897, 'learning_rate': 0.00019278975168559453, 'epoch': 0.15}

 15%|█▍        | 1139/7689 [2:49:26<20:30:45, 11.27s/it]

 15%|█▍        | 1140/7689 [2:49:35<18:55:45, 10.41s/it]

 15%|█▍        | 1141/7689 [2:49:47<20:07:11, 11.06s/it]

 15%|█▍        | 1142/7689 [2:49:55<18:07:50,  9.97s/it]

 15%|█▍        | 1143/7689 [2:50:00<15:37:45,  8.60s/it]

 15%|█▍        | 1144/7689 [2:50:05<13:22:17,  7.35s/it]

 15%|█▍        | 1145/7689 [2:50:11<12:45:49,  7.02s/it]

 15%|█▍        | 1146/7689 [2:50:18<12:51:07,  7.07s/it]


 15%|█▍        | 1148/7689 [2:50:36<14:39:55,  8.07s/it]
{'loss': 1.2234, 'grad_norm': 0.19883242388632408, 'learning_rate': 0.0001926318765143911, 'epoch': 0.15}

 15%|█▍        | 1149/7689 [2:50:43<14:18:51,  7.88s/it]


 15%|█▍        | 1151/7689 [2:51:08<18:08:37,  9.99s/it]

 15%|█▍        | 1152/7689 [2:51:16<17:06:04,  9.42s/it]

 15%|█▍        | 1153/7689 [2:51:24<16:18:30,  8.98s/it]
{'loss': 0.9837, 'grad_norm': 0.18900344789308454, 'learning_rate': 0.0001925523223760614, 'epoch': 0.15}

 15%|█▌        | 1154/7689 [2:51:35<17:29:57,  9.64s/it]

 15%|█▌        | 1155/7689 [2:51:41<15:48:45,  8.71s/it]


 15%|█▌        | 1157/7689 [2:51:56<14:25:32,  7.95s/it]
{'loss': 1.2795, 'grad_norm': 0.18381140294377932, 'learning_rate': 0.00019248838344143867, 'epoch': 0.15}

 15%|█▌        | 1158/7689 [2:52:05<14:57:30,  8.25s/it]

 15%|█▌        | 1159/7689 [2:52:11<13:43:44,  7.57s/it]


 15%|█▌        | 1161/7689 [2:52:26<13:29:47,  7.44s/it]
{'loss': 0.9727, 'grad_norm': 0.1887656595965531, 'learning_rate': 0.00019242418192656687, 'epoch': 0.15}

 15%|█▌        | 1162/7689 [2:52:35<14:45:28,  8.14s/it]

 15%|█▌        | 1163/7689 [2:52:46<15:52:16,  8.76s/it]

 15%|█▌        | 1164/7689 [2:52:51<14:13:55,  7.85s/it]

 15%|█▌        | 1165/7689 [2:53:01<15:25:28,  8.51s/it]

 15%|█▌        | 1166/7689 [2:53:11<16:00:02,  8.83s/it]

 15%|█▌        | 1167/7689 [2:53:20<16:13:24,  8.95s/it]


 15%|█▌        | 1169/7689 [2:53:38<15:51:19,  8.75s/it]

 15%|█▌        | 1170/7689 [2:53:44<14:17:56,  7.90s/it]
{'loss': 1.1349, 'grad_norm': 0.1850592849577747, 'learning_rate': 0.00019227876940438793, 'epoch': 0.15}

 15%|█▌        | 1171/7689 [2:53:59<18:17:35, 10.10s/it]

 15%|█▌        | 1172/7689 [2:54:07<17:05:37,  9.44s/it]

 15%|█▌        | 1173/7689 [2:54:15<16:11:28,  8.95s/it]

 15%|█▌        | 1174/7689 [2:54:29<18:39:03, 10.31s/it]


 15%|█▌        | 1176/7689 [2:54:42<15:15:18,  8.43s/it]
{'loss': 1.128, 'grad_norm': 0.20387539091868448, 'learning_rate': 0.00019218109076063643, 'epoch': 0.15}


 15%|█▌        | 1178/7689 [2:55:00<16:13:07,  8.97s/it]
{'loss': 0.948, 'grad_norm': 0.17917473769933542, 'learning_rate': 0.00019214840032792365, 'epoch': 0.15}

 15%|█▌        | 1179/7689 [2:55:10<16:59:51,  9.40s/it]

 15%|█▌        | 1180/7689 [2:55:21<17:54:42,  9.91s/it]

 15%|█▌        | 1181/7689 [2:55:37<21:00:50, 11.62s/it]

 15%|█▌        | 1182/7689 [2:55:44<18:10:28, 10.06s/it]

 15%|█▌        | 1183/7689 [2:55:56<19:22:26, 10.72s/it]

 15%|█▌        | 1184/7689 [2:56:07<19:32:21, 10.81s/it]


 15%|█▌        | 1186/7689 [2:56:18<14:44:36,  8.16s/it]

 15%|█▌        | 1187/7689 [2:56:26<14:37:19,  8.10s/it]
{'loss': 1.0613, 'grad_norm': 0.17719954853359898, 'learning_rate': 0.00019200048434519257, 'epoch': 0.15}

 15%|█▌        | 1188/7689 [2:56:35<14:56:08,  8.27s/it]

 15%|█▌        | 1189/7689 [2:56:45<16:06:06,  8.92s/it]
[2024-05-24 16:28:12,269] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 15%|█▌        | 1191/7689 [2:57:10<18:37:16, 10.32s/it]
{'loss': 1.0945, 'grad_norm': 0.18813737553503843, 'learning_rate': 0.0001919343193397748, 'epoch': 0.15}

 16%|█▌        | 1192/7689 [2:57:19<17:51:23,  9.89s/it]


 16%|█▌        | 1194/7689 [2:57:38<18:00:46,  9.98s/it]
{'loss': 1.1493, 'grad_norm': 0.19401446725437255, 'learning_rate': 0.0001918845242894182, 'epoch': 0.16}


 16%|█▌        | 1196/7689 [2:57:54<15:40:24,  8.69s/it]

 16%|█▌        | 1197/7689 [2:58:00<14:12:42,  7.88s/it]
{'loss': 1.106, 'grad_norm': 0.1918648569390119, 'learning_rate': 0.00019183458250200265, 'epoch': 0.16}

 16%|█▌        | 1198/7689 [2:58:06<13:03:39,  7.24s/it]


 16%|█▌        | 1200/7689 [2:58:26<15:13:09,  8.44s/it]
 16%|█▌        | 1200/7689 [2:58:26<15:13:09,  8.44s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1802, 'grad_norm': 0.1708788699156407, 'learning_rate': 0.0001917677653323511, 'epoch': 0.16}
 16%|█▌        | 1201/7689 [2:59:05<31:34:25, 17.52s/it]

 16%|█▌        | 1202/7689 [2:59:15<27:43:02, 15.38s/it]

 16%|█▌        | 1203/7689 [2:59:30<27:36:21, 15.32s/it]

 16%|█▌        | 1204/7689 [2:59:37<22:46:59, 12.65s/it]

 16%|█▌        | 1205/7689 [2:59:41<18:15:00, 10.13s/it]

 16%|█▌        | 1206/7689 [2:59:47<15:54:18,  8.83s/it]

 16%|█▌        | 1207/7689 [2:59:56<16:04:39,  8.93s/it]

 16%|█▌        | 1208/7689 [3:00:04<15:25:11,  8.57s/it]


 16%|█▌        | 1210/7689 [3:00:14<12:22:07,  6.87s/it]
{'loss': 1.0578, 'grad_norm': 0.1757596147163307, 'learning_rate': 0.00019161647441203646, 'epoch': 0.16}

 16%|█▌        | 1211/7689 [3:00:21<12:13:56,  6.80s/it]

 16%|█▌        | 1212/7689 [3:00:29<13:00:55,  7.23s/it]

 16%|█▌        | 1213/7689 [3:00:41<15:36:02,  8.67s/it]

 16%|█▌        | 1214/7689 [3:00:47<14:07:19,  7.85s/it]


 16%|█▌        | 1216/7689 [3:01:04<15:11:49,  8.45s/it]
{'loss': 0.8973, 'grad_norm': 0.18316536056911553, 'learning_rate': 0.0001915148821193136, 'epoch': 0.16}

 16%|█▌        | 1217/7689 [3:01:12<14:41:25,  8.17s/it]

 16%|█▌        | 1218/7689 [3:01:17<13:07:02,  7.30s/it]

 16%|█▌        | 1219/7689 [3:01:30<15:55:33,  8.86s/it]

 16%|█▌        | 1220/7689 [3:01:38<15:34:26,  8.67s/it]

 16%|█▌        | 1221/7689 [3:01:48<16:13:43,  9.03s/it]


 16%|█▌        | 1223/7689 [3:02:15<20:11:18, 11.24s/it]

 16%|█▌        | 1224/7689 [3:02:25<19:30:42, 10.87s/it]
{'loss': 1.1044, 'grad_norm': 0.17567117772674085, 'learning_rate': 0.0001913785164845245, 'epoch': 0.16}

 16%|█▌        | 1225/7689 [3:02:30<16:21:02,  9.11s/it]

 16%|█▌        | 1226/7689 [3:02:35<14:26:35,  8.05s/it]

 16%|█▌        | 1227/7689 [3:02:44<14:56:33,  8.32s/it]

 16%|█▌        | 1228/7689 [3:02:53<15:15:01,  8.50s/it]

 16%|█▌        | 1229/7689 [3:03:00<14:11:12,  7.91s/it]

 16%|█▌        | 1230/7689 [3:03:15<18:24:11, 10.26s/it]

 16%|█▌        | 1231/7689 [3:03:21<15:54:00,  8.86s/it]

 16%|█▌        | 1232/7689 [3:03:37<19:49:21, 11.05s/it]

 16%|█▌        | 1233/7689 [3:03:46<18:35:52, 10.37s/it]

 16%|█▌        | 1234/7689 [3:03:54<17:16:35,  9.64s/it]

 16%|█▌        | 1235/7689 [3:04:03<17:03:33,  9.52s/it]

 16%|█▌        | 1236/7689 [3:04:11<16:28:10,  9.19s/it]

 16%|█▌        | 1237/7689 [3:04:17<14:27:56,  8.07s/it]

 16%|█▌        | 1238/7689 [3:04:26<14:48:27,  8.26s/it]


 16%|█▌        | 1240/7689 [3:04:47<16:15:34,  9.08s/it]
{'loss': 0.9396, 'grad_norm': 0.18283713786805844, 'learning_rate': 0.0001911026736263248, 'epoch': 0.16}


 16%|█▌        | 1242/7689 [3:05:01<14:10:33,  7.92s/it]
{'loss': 0.9617, 'grad_norm': 0.19271960404640565, 'learning_rate': 0.00019106790203416715, 'epoch': 0.16}

 16%|█▌        | 1243/7689 [3:05:14<16:56:23,  9.46s/it]

 16%|█▌        | 1244/7689 [3:05:22<16:23:15,  9.15s/it]

 16%|█▌        | 1245/7689 [3:05:36<18:41:15, 10.44s/it]


 16%|█▌        | 1247/7689 [3:05:57<17:56:52, 10.03s/it]
{'loss': 1.0824, 'grad_norm': 0.18620706313049074, 'learning_rate': 0.000190980690321588, 'epoch': 0.16}

 16%|█▌        | 1248/7689 [3:06:04<16:19:51,  9.13s/it]

 16%|█▌        | 1249/7689 [3:06:11<15:32:29,  8.69s/it]

 16%|█▋        | 1250/7689 [3:06:20<15:23:08,  8.60s/it]

 16%|█▋        | 1251/7689 [3:06:28<15:25:47,  8.63s/it]

 16%|█▋        | 1252/7689 [3:06:42<17:56:27, 10.03s/it]

 16%|█▋        | 1253/7689 [3:06:50<17:13:15,  9.63s/it]

 16%|█▋        | 1254/7689 [3:06:55<14:44:44,  8.25s/it]

 16%|█▋        | 1255/7689 [3:07:08<17:08:20,  9.59s/it]

 16%|█▋        | 1256/7689 [3:07:14<14:50:54,  8.31s/it]

 16%|█▋        | 1257/7689 [3:07:21<14:37:01,  8.18s/it]

 16%|█▋        | 1258/7689 [3:07:32<16:02:12,  8.98s/it]


 16%|█▋        | 1260/7689 [3:07:53<16:35:22,  9.29s/it]
{'loss': 1.0101, 'grad_norm': 0.16292212762153585, 'learning_rate': 0.00019075205202251326, 'epoch': 0.16}

 16%|█▋        | 1261/7689 [3:08:04<17:29:28,  9.80s/it]

 16%|█▋        | 1262/7689 [3:08:12<16:42:30,  9.36s/it]

 16%|█▋        | 1263/7689 [3:08:17<14:30:50,  8.13s/it]

 16%|█▋        | 1264/7689 [3:08:22<12:44:39,  7.14s/it]

 16%|█▋        | 1265/7689 [3:08:28<11:51:41,  6.65s/it]

 16%|█▋        | 1266/7689 [3:08:34<11:54:46,  6.68s/it]

 16%|█▋        | 1267/7689 [3:08:42<12:31:56,  7.03s/it]

 16%|█▋        | 1268/7689 [3:08:52<14:10:03,  7.94s/it]

 17%|█▋        | 1269/7689 [3:08:57<12:42:07,  7.12s/it]

 17%|█▋        | 1270/7689 [3:09:08<14:26:55,  8.10s/it]


 17%|█▋        | 1272/7689 [3:09:23<13:46:11,  7.73s/it]
{'loss': 1.167, 'grad_norm': 0.1936633181557963, 'learning_rate': 0.00019053858565769394, 'epoch': 0.17}

 17%|█▋        | 1273/7689 [3:09:29<13:05:39,  7.35s/it]

 17%|█▋        | 1274/7689 [3:09:40<14:55:01,  8.37s/it]

 17%|█▋        | 1275/7689 [3:09:54<17:56:10, 10.07s/it]

 17%|█▋        | 1276/7689 [3:10:10<20:56:28, 11.76s/it]


 17%|█▋        | 1278/7689 [3:10:23<16:34:43,  9.31s/it]
{'loss': 0.8615, 'grad_norm': 0.158833496533037, 'learning_rate': 0.0001904309846080145, 'epoch': 0.17}


 17%|█▋        | 1280/7689 [3:10:39<15:24:31,  8.66s/it]
{'loss': 0.9149, 'grad_norm': 0.1667838452165456, 'learning_rate': 0.00019039498918786024, 'epoch': 0.17}


 17%|█▋        | 1282/7689 [3:10:51<13:01:16,  7.32s/it]
{'loss': 0.9977, 'grad_norm': 0.17457117744219716, 'learning_rate': 0.00019035892960845137, 'epoch': 0.17}

 17%|█▋        | 1283/7689 [3:11:00<14:02:09,  7.89s/it]

 17%|█▋        | 1284/7689 [3:11:07<13:33:57,  7.62s/it]

 17%|█▋        | 1285/7689 [3:11:12<12:07:38,  6.82s/it]

 17%|█▋        | 1286/7689 [3:11:28<16:33:29,  9.31s/it]

 17%|█▋        | 1287/7689 [3:11:33<14:20:45,  8.07s/it]

 17%|█▋        | 1288/7689 [3:11:42<15:02:30,  8.46s/it]


 17%|█▋        | 1290/7689 [3:11:59<14:48:28,  8.33s/it]
{'loss': 0.9612, 'grad_norm': 0.1938687850571408, 'learning_rate': 0.00019021405021082873, 'epoch': 0.17}

 17%|█▋        | 1291/7689 [3:12:04<13:07:34,  7.39s/it]

 17%|█▋        | 1292/7689 [3:12:13<13:38:45,  7.68s/it]

 17%|█▋        | 1293/7689 [3:12:18<12:31:12,  7.05s/it]

 17%|█▋        | 1294/7689 [3:12:29<14:12:58,  8.00s/it]


 17%|█▋        | 1296/7689 [3:12:43<13:35:21,  7.65s/it]
{'loss': 1.1303, 'grad_norm': 0.18208039723297167, 'learning_rate': 0.00019010471824907493, 'epoch': 0.17}

 17%|█▋        | 1297/7689 [3:12:54<15:24:54,  8.68s/it]

 17%|█▋        | 1298/7689 [3:13:05<16:22:05,  9.22s/it]

 17%|█▋        | 1299/7689 [3:13:10<14:02:51,  7.91s/it]

 17%|█▋        | 1300/7689 [3:13:19<14:50:26,  8.36s/it]

 17%|█▋        | 1301/7689 [3:13:32<17:21:26,  9.78s/it]

 17%|█▋        | 1302/7689 [3:13:40<16:12:52,  9.14s/it]

 17%|█▋        | 1303/7689 [3:13:59<21:35:20, 12.17s/it]

 17%|█▋        | 1304/7689 [3:14:06<18:57:13, 10.69s/it]

 17%|█▋        | 1305/7689 [3:14:13<16:48:10,  9.48s/it]

 17%|█▋        | 1306/7689 [3:14:19<14:55:45,  8.42s/it]

 17%|█▋        | 1307/7689 [3:14:30<16:16:22,  9.18s/it]


 17%|█▋        | 1309/7689 [3:14:59<21:58:50, 12.40s/it]
{'loss': 1.0549, 'grad_norm': 0.20305780455714906, 'learning_rate': 0.0001898658587137379, 'epoch': 0.17}

 17%|█▋        | 1310/7689 [3:15:09<20:23:30, 11.51s/it]

 17%|█▋        | 1311/7689 [3:15:17<18:35:51, 10.50s/it]


 17%|█▋        | 1313/7689 [3:15:36<18:15:44, 10.31s/it]
{'loss': 1.0186, 'grad_norm': 0.17635995700525683, 'learning_rate': 0.00018979182097728838, 'epoch': 0.17}


 17%|█▋        | 1315/7689 [3:15:52<16:07:50,  9.11s/it]
{'loss': 0.999, 'grad_norm': 0.16039601767939, 'learning_rate': 0.00018975470649919502, 'epoch': 0.17}

 17%|█▋        | 1316/7689 [3:15:59<15:04:29,  8.52s/it]


 17%|█▋        | 1318/7689 [3:16:17<16:06:55,  9.11s/it]
{'loss': 1.138, 'grad_norm': 0.16900496959407468, 'learning_rate': 0.00018969891534379184, 'epoch': 0.17}

 17%|█▋        | 1319/7689 [3:16:26<15:51:33,  8.96s/it]

 17%|█▋        | 1320/7689 [3:16:32<14:21:22,  8.11s/it]

 17%|█▋        | 1321/7689 [3:16:40<14:11:09,  8.02s/it]

 17%|█▋        | 1322/7689 [3:16:46<13:15:05,  7.49s/it]

 17%|█▋        | 1323/7689 [3:16:54<13:15:55,  7.50s/it]

 17%|█▋        | 1324/7689 [3:17:03<14:00:40,  7.92s/it]

 17%|█▋        | 1325/7689 [3:17:09<13:04:53,  7.40s/it]

 17%|█▋        | 1326/7689 [3:17:21<15:21:06,  8.69s/it]


 17%|█▋        | 1328/7689 [3:17:49<20:35:13, 11.65s/it]
{'loss': 0.8777, 'grad_norm': 0.1768737836836261, 'learning_rate': 0.0001895119107684661, 'epoch': 0.17}

 17%|█▋        | 1329/7689 [3:18:03<21:48:18, 12.34s/it]


 17%|█▋        | 1331/7689 [3:18:19<18:07:47, 10.27s/it]
{'loss': 0.9771, 'grad_norm': 0.16906769451181972, 'learning_rate': 0.0001894554995237703, 'epoch': 0.17}


 17%|█▋        | 1333/7689 [3:18:33<15:21:14,  8.70s/it]

 17%|█▋        | 1334/7689 [3:18:42<15:04:46,  8.54s/it]
{'loss': 1.1075, 'grad_norm': 0.1783304273484308, 'learning_rate': 0.00018939894542110132, 'epoch': 0.17}

 17%|█▋        | 1335/7689 [3:19:01<20:49:28, 11.80s/it]


 17%|█▋        | 1337/7689 [3:19:20<18:32:57, 10.51s/it]

 17%|█▋        | 1338/7689 [3:19:30<18:19:58, 10.39s/it]
{'loss': 0.7958, 'grad_norm': 0.1532282577156286, 'learning_rate': 0.00018932331788349568, 'epoch': 0.17}

 17%|█▋        | 1339/7689 [3:19:44<20:19:06, 11.52s/it]

 17%|█▋        | 1340/7689 [3:19:55<19:51:20, 11.26s/it]


 17%|█▋        | 1342/7689 [3:20:10<16:46:40,  9.52s/it]
{'loss': 1.0263, 'grad_norm': 0.20079985642933995, 'learning_rate': 0.00018924743675145813, 'epoch': 0.17}

 17%|█▋        | 1343/7689 [3:20:18<16:18:02,  9.25s/it]


 17%|█▋        | 1345/7689 [3:20:46<19:29:07, 11.06s/it]
{'loss': 1.1545, 'grad_norm': 0.18119388977554196, 'learning_rate': 0.00018919035961065231, 'epoch': 0.17}

 18%|█▊        | 1346/7689 [3:21:03<22:38:49, 12.85s/it]


 18%|█▊        | 1348/7689 [3:21:26<22:13:26, 12.62s/it]
{'loss': 1.1582, 'grad_norm': 0.18121451696710505, 'learning_rate': 0.00018913314003529447, 'epoch': 0.18}


 18%|█▊        | 1350/7689 [3:21:40<17:11:57,  9.77s/it]
{'loss': 1.1926, 'grad_norm': 0.1916182549253318, 'learning_rate': 0.00018909491456653153, 'epoch': 0.18}

 18%|█▊        | 1351/7689 [3:21:47<16:00:13,  9.09s/it]

 18%|█▊        | 1352/7689 [3:21:55<15:19:57,  8.71s/it]


 18%|█▊        | 1354/7689 [3:22:06<12:09:59,  6.91s/it]

 18%|█▊        | 1355/7689 [3:22:12<11:42:18,  6.65s/it]
{'loss': 1.1413, 'grad_norm': 0.19344727082181648, 'learning_rate': 0.00018899907429436845, 'epoch': 0.18}

 18%|█▊        | 1356/7689 [3:22:20<12:49:23,  7.29s/it]


 18%|█▊        | 1358/7689 [3:22:40<15:25:34,  8.77s/it]

 18%|█▊        | 1359/7689 [3:22:50<16:02:52,  9.13s/it]
{'loss': 1.0836, 'grad_norm': 0.1883766833429707, 'learning_rate': 0.0001889221177980899, 'epoch': 0.18}

 18%|█▊        | 1360/7689 [3:23:00<16:37:25,  9.46s/it]

 18%|█▊        | 1361/7689 [3:23:05<14:22:53,  8.18s/it]

 18%|█▊        | 1362/7689 [3:23:17<16:01:30,  9.12s/it]

 18%|█▊        | 1363/7689 [3:23:25<15:28:26,  8.81s/it]

 18%|█▊        | 1364/7689 [3:23:30<13:41:04,  7.79s/it]

 18%|█▊        | 1365/7689 [3:23:41<15:12:47,  8.66s/it]

 18%|█▊        | 1366/7689 [3:23:53<17:13:49,  9.81s/it]

 18%|█▊        | 1367/7689 [3:24:01<15:48:32,  9.00s/it]

 18%|█▊        | 1368/7689 [3:24:15<18:29:47, 10.53s/it]


 18%|█▊        | 1370/7689 [3:24:28<14:47:40,  8.43s/it]
{'loss': 1.1176, 'grad_norm': 0.19811134034515543, 'learning_rate': 0.0001887091863698869, 'epoch': 0.18}


 18%|█▊        | 1372/7689 [3:24:48<16:57:43,  9.67s/it]
{'loss': 1.1126, 'grad_norm': 0.1865402839319374, 'learning_rate': 0.00018867026680162996, 'epoch': 0.18}


 18%|█▊        | 1374/7689 [3:25:12<19:31:18, 11.13s/it]
{'loss': 0.9266, 'grad_norm': 0.1812602488990629, 'learning_rate': 0.00018863128429826698, 'epoch': 0.18}

 18%|█▊        | 1375/7689 [3:25:21<18:12:28, 10.38s/it]

 18%|█▊        | 1376/7689 [3:25:32<18:49:59, 10.74s/it]

 18%|█▊        | 1377/7689 [3:25:39<16:40:19,  9.51s/it]

 18%|█▊        | 1378/7689 [3:25:45<14:40:09,  8.37s/it]


 18%|█▊        | 1380/7689 [3:26:04<15:32:55,  8.87s/it]
{'loss': 0.8866, 'grad_norm': 0.18588382862701514, 'learning_rate': 0.00018851395945444953, 'epoch': 0.18}

 18%|█▊        | 1381/7689 [3:26:11<14:28:59,  8.27s/it]

 18%|█▊        | 1382/7689 [3:26:22<16:13:49,  9.26s/it]

 18%|█▊        | 1383/7689 [3:26:29<14:55:25,  8.52s/it]

 18%|█▊        | 1384/7689 [3:26:37<14:30:17,  8.28s/it]

 18%|█▊        | 1385/7689 [3:26:43<13:31:44,  7.73s/it]

 18%|█▊        | 1386/7689 [3:26:51<13:14:43,  7.57s/it]

 18%|█▊        | 1387/7689 [3:26:57<12:34:47,  7.19s/it]

 18%|█▊        | 1388/7689 [3:27:03<12:10:43,  6.96s/it]

 18%|█▊        | 1389/7689 [3:27:14<13:56:32,  7.97s/it]


 18%|█▊        | 1391/7689 [3:27:32<14:40:25,  8.39s/it]
{'loss': 1.1892, 'grad_norm': 0.22113826118369506, 'learning_rate': 0.0001882973959364959, 'epoch': 0.18}


 18%|█▊        | 1393/7689 [3:27:56<18:34:38, 10.62s/it]
{'loss': 0.8917, 'grad_norm': 0.16817180784258007, 'learning_rate': 0.00018825781693589341, 'epoch': 0.18}

 18%|█▊        | 1394/7689 [3:28:15<22:36:15, 12.93s/it]

 18%|█▊        | 1395/7689 [3:28:22<19:28:23, 11.14s/it]


 18%|█▊        | 1397/7689 [3:28:36<16:20:33,  9.35s/it]
{'loss': 1.0992, 'grad_norm': 0.18460303342746343, 'learning_rate': 0.00018817847103573486, 'epoch': 0.18}

 18%|█▊        | 1398/7689 [3:28:42<14:25:36,  8.26s/it]

 18%|█▊        | 1399/7689 [3:28:50<14:14:02,  8.15s/it]


 18%|█▊        | 1401/7689 [3:29:04<13:34:16,  7.77s/it]
{'loss': 0.9443, 'grad_norm': 0.1747579316579505, 'learning_rate': 0.00018809887479143548, 'epoch': 0.18}


 18%|█▊        | 1403/7689 [3:29:25<15:50:25,  9.07s/it]
{'loss': 0.8918, 'grad_norm': 0.16754160555404135, 'learning_rate': 0.0001880589828608236, 'epoch': 0.18}

 18%|█▊        | 1404/7689 [3:29:31<14:42:04,  8.42s/it]

 18%|█▊        | 1405/7689 [3:29:47<18:26:10, 10.56s/it]

 18%|█▊        | 1406/7689 [3:29:54<16:27:04,  9.43s/it]

 18%|█▊        | 1407/7689 [3:30:01<15:29:33,  8.88s/it]

 18%|█▊        | 1408/7689 [3:30:17<19:05:34, 10.94s/it]

 18%|█▊        | 1409/7689 [3:30:23<16:15:52,  9.32s/it]


 18%|█▊        | 1411/7689 [3:30:46<17:35:37, 10.09s/it]
{'loss': 0.9537, 'grad_norm': 0.19903223185232524, 'learning_rate': 0.00018789879040980343, 'epoch': 0.18}

 18%|█▊        | 1412/7689 [3:30:52<15:22:32,  8.82s/it]

 18%|█▊        | 1413/7689 [3:31:07<18:38:58, 10.70s/it]

 18%|█▊        | 1414/7689 [3:31:15<17:20:58,  9.95s/it]

 18%|█▊        | 1415/7689 [3:31:22<15:38:49,  8.98s/it]

 18%|█▊        | 1416/7689 [3:31:32<16:08:14,  9.26s/it]


 18%|█▊        | 1418/7689 [3:31:50<16:42:31,  9.59s/it]

 18%|█▊        | 1419/7689 [3:32:05<19:15:02, 11.05s/it]
{'loss': 0.864, 'grad_norm': 0.1573635573713757, 'learning_rate': 0.00018773759975905098, 'epoch': 0.18}

 18%|█▊        | 1420/7689 [3:32:14<18:03:23, 10.37s/it]


 18%|█▊        | 1422/7689 [3:32:29<15:32:28,  8.93s/it]
{'loss': 1.2398, 'grad_norm': 0.1958214315070753, 'learning_rate': 0.00018767689628990122, 'epoch': 0.18}

 19%|█▊        | 1423/7689 [3:32:36<14:33:03,  8.36s/it]

 19%|█▊        | 1424/7689 [3:32:42<13:19:43,  7.66s/it]


 19%|█▊        | 1426/7689 [3:33:02<15:49:03,  9.09s/it]
{'loss': 1.2205, 'grad_norm': 0.18645986279685375, 'learning_rate': 0.00018759574054267824, 'epoch': 0.19}

 19%|█▊        | 1427/7689 [3:33:14<17:00:26,  9.78s/it]

 19%|█▊        | 1428/7689 [3:33:20<14:58:11,  8.61s/it]

 19%|█▊        | 1429/7689 [3:33:28<14:54:44,  8.58s/it]

 19%|█▊        | 1430/7689 [3:33:36<14:43:59,  8.47s/it]

 19%|█▊        | 1431/7689 [3:33:43<13:50:26,  7.96s/it]

 19%|█▊        | 1432/7689 [3:33:49<12:49:53,  7.38s/it]

 19%|█▊        | 1433/7689 [3:33:58<13:30:23,  7.77s/it]

 19%|█▊        | 1434/7689 [3:34:04<12:41:49,  7.31s/it]

 19%|█▊        | 1435/7689 [3:34:10<12:05:30,  6.96s/it]

 19%|█▊        | 1436/7689 [3:34:17<12:15:38,  7.06s/it]


 19%|█▊        | 1438/7689 [3:34:33<12:41:17,  7.31s/it]
{'loss': 0.9784, 'grad_norm': 0.195709937617778, 'learning_rate': 0.00018735078208776657, 'epoch': 0.19}

 19%|█▊        | 1439/7689 [3:34:41<13:22:23,  7.70s/it]


 19%|█▊        | 1441/7689 [3:35:05<16:25:44,  9.47s/it]
{'loss': 0.9644, 'grad_norm': 0.17686782192050265, 'learning_rate': 0.00018728919348699283, 'epoch': 0.19}

 19%|█▉        | 1442/7689 [3:35:17<18:08:11, 10.45s/it]

 19%|█▉        | 1443/7689 [3:35:28<18:19:31, 10.56s/it]

 19%|█▉        | 1444/7689 [3:35:36<17:00:46,  9.81s/it]

 19%|█▉        | 1445/7689 [3:35:45<16:28:01,  9.49s/it]

 19%|█▉        | 1446/7689 [3:35:52<15:09:29,  8.74s/it]

 19%|█▉        | 1447/7689 [3:36:02<15:52:11,  9.15s/it]

 19%|█▉        | 1448/7689 [3:36:07<13:48:54,  7.97s/it]

 19%|█▉        | 1449/7689 [3:36:19<15:54:05,  9.17s/it]

 19%|█▉        | 1450/7689 [3:36:28<15:43:22,  9.07s/it]


 19%|█▉        | 1452/7689 [3:36:49<17:16:42,  9.97s/it]
{'loss': 0.9805, 'grad_norm': 0.1776453263178588, 'learning_rate': 0.00018706217673675811, 'epoch': 0.19}


 19%|█▉        | 1454/7689 [3:37:05<15:25:35,  8.91s/it]
{'loss': 1.2004, 'grad_norm': 0.1859934631804342, 'learning_rate': 0.00018702069999122344, 'epoch': 0.19}


 19%|█▉        | 1456/7689 [3:37:27<18:16:36, 10.56s/it]
{'loss': 0.9606, 'grad_norm': 0.186799970404765, 'learning_rate': 0.00018697916148138858, 'epoch': 0.19}


 19%|█▉        | 1458/7689 [3:37:43<15:54:45,  9.19s/it]
{'loss': 1.093, 'grad_norm': 0.17985047952449293, 'learning_rate': 0.00018693756123673602, 'epoch': 0.19}


 19%|█▉        | 1460/7689 [3:37:55<13:24:08,  7.75s/it]

 19%|█▉        | 1461/7689 [3:38:07<15:29:47,  8.96s/it]
{'loss': 1.0805, 'grad_norm': 0.1965009938175435, 'learning_rate': 0.00018687504518157523, 'epoch': 0.19}

 19%|█▉        | 1462/7689 [3:38:16<15:25:31,  8.92s/it]


 19%|█▉        | 1464/7689 [3:38:35<15:58:45,  9.24s/it]
{'loss': 1.2125, 'grad_norm': 0.1878902661957505, 'learning_rate': 0.00018681239038935587, 'epoch': 0.19}

 19%|█▉        | 1465/7689 [3:38:44<15:48:53,  9.15s/it]

 19%|█▉        | 1466/7689 [3:38:52<15:33:10,  9.00s/it]

 19%|█▉        | 1467/7689 [3:39:06<17:44:34, 10.27s/it]

 19%|█▉        | 1468/7689 [3:39:13<16:07:25,  9.33s/it]


 19%|█▉        | 1470/7689 [3:39:29<14:31:33,  8.41s/it]

 19%|█▉        | 1471/7689 [3:39:35<13:27:02,  7.79s/it]

 19%|█▉        | 1472/7689 [3:39:43<13:30:20,  7.82s/it]
{'loss': 1.1086, 'grad_norm': 0.17333280415240646, 'learning_rate': 0.00018664463343500995, 'epoch': 0.19}

 19%|█▉        | 1473/7689 [3:39:48<12:17:50,  7.12s/it]


 19%|█▉        | 1475/7689 [3:40:07<13:58:59,  8.10s/it]
{'loss': 0.9071, 'grad_norm': 0.190506034793322, 'learning_rate': 0.00018658147079797752, 'epoch': 0.19}

 19%|█▉        | 1476/7689 [3:40:12<12:36:13,  7.30s/it]

 19%|█▉        | 1477/7689 [3:40:18<12:02:13,  6.98s/it]

 19%|█▉        | 1478/7689 [3:40:25<12:03:08,  6.99s/it]

 19%|█▉        | 1479/7689 [3:40:32<11:40:59,  6.77s/it]

 19%|█▉        | 1480/7689 [3:40:42<13:40:43,  7.93s/it]

 19%|█▉        | 1481/7689 [3:40:48<12:30:33,  7.25s/it]


 19%|█▉        | 1483/7689 [3:41:03<12:38:02,  7.33s/it]
{'loss': 1.1705, 'grad_norm': 0.17404224381087607, 'learning_rate': 0.00018641236139588406, 'epoch': 0.19}

 19%|█▉        | 1484/7689 [3:41:18<16:24:40,  9.52s/it]

 19%|█▉        | 1485/7689 [3:41:24<14:36:55,  8.48s/it]


 19%|█▉        | 1487/7689 [3:41:41<15:15:14,  8.85s/it]
{'loss': 1.0593, 'grad_norm': 0.1956613961158021, 'learning_rate': 0.00018632743857956578, 'epoch': 0.19}

 19%|█▉        | 1488/7689 [3:41:49<14:28:05,  8.40s/it]

 19%|█▉        | 1489/7689 [3:42:04<18:10:04, 10.55s/it]

 19%|█▉        | 1490/7689 [3:42:13<17:08:26,  9.95s/it]

 19%|█▉        | 1491/7689 [3:42:23<17:08:58,  9.96s/it]

 19%|█▉        | 1492/7689 [3:42:34<17:40:08, 10.26s/it]

 19%|█▉        | 1493/7689 [3:42:40<15:37:02,  9.07s/it]

 19%|█▉        | 1494/7689 [3:42:50<15:53:43,  9.24s/it]

 19%|█▉        | 1495/7689 [3:42:56<14:24:27,  8.37s/it]

 19%|█▉        | 1496/7689 [3:43:10<17:20:21, 10.08s/it]

 19%|█▉        | 1497/7689 [3:43:17<15:43:13,  9.14s/it]


 19%|█▉        | 1499/7689 [3:43:27<12:15:41,  7.13s/it]
{'loss': 1.0234, 'grad_norm': 0.1778502481942165, 'learning_rate': 0.00018607120056482386, 'epoch': 0.19}

 20%|█▉        | 1500/7689 [3:43:33<11:17:46,  6.57s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1266, 'grad_norm': 0.2016984473125581, 'learning_rate': 0.00018602828023551294, 'epoch': 0.2}
 20%|█▉        | 1501/7689 [3:44:06<25:24:49, 14.78s/it]


 20%|█▉        | 1503/7689 [3:44:21<19:01:34, 11.07s/it]

 20%|█▉        | 1504/7689 [3:44:35<20:21:43, 11.85s/it]
{'loss': 0.9795, 'grad_norm': 0.18192667672708782, 'learning_rate': 0.00018596378526373786, 'epoch': 0.2}


 20%|█▉        | 1506/7689 [3:44:47<15:22:45,  8.95s/it]

 20%|█▉        | 1507/7689 [3:44:57<15:54:47,  9.27s/it]

 20%|█▉        | 1508/7689 [3:45:13<19:20:04, 11.26s/it]
{'loss': 1.1975, 'grad_norm': 0.17245777202353907, 'learning_rate': 0.00018587757843640227, 'epoch': 0.2}

 20%|█▉        | 1509/7689 [3:45:23<18:21:14, 10.69s/it]

 20%|█▉        | 1510/7689 [3:45:32<17:49:04, 10.38s/it]

 20%|█▉        | 1511/7689 [3:45:40<16:40:06,  9.71s/it]

 20%|█▉        | 1512/7689 [3:45:46<14:49:54,  8.64s/it]


 20%|█▉        | 1514/7689 [3:46:00<13:11:08,  7.69s/it]
{'loss': 1.0741, 'grad_norm': 0.18422520295057623, 'learning_rate': 0.0001857478111250136, 'epoch': 0.2}

 20%|█▉        | 1515/7689 [3:46:08<13:29:34,  7.87s/it]

 20%|█▉        | 1516/7689 [3:46:19<15:07:55,  8.82s/it]

 20%|█▉        | 1517/7689 [3:46:27<14:38:06,  8.54s/it]

 20%|█▉        | 1518/7689 [3:46:40<17:14:16, 10.06s/it]


 20%|█▉        | 1520/7689 [3:46:55<14:53:03,  8.69s/it]
{'loss': 1.1489, 'grad_norm': 0.17943099868137394, 'learning_rate': 0.00018561749606625663, 'epoch': 0.2}

 20%|█▉        | 1521/7689 [3:47:07<16:24:10,  9.57s/it]


 20%|█▉        | 1523/7689 [3:47:33<18:48:54, 10.99s/it]

 20%|█▉        | 1524/7689 [3:47:45<19:20:10, 11.29s/it]
{'loss': 0.7473, 'grad_norm': 0.1441659845835732, 'learning_rate': 0.00018553031546726185, 'epoch': 0.2}

 20%|█▉        | 1525/7689 [3:48:01<21:21:27, 12.47s/it]

 20%|█▉        | 1526/7689 [3:48:10<19:55:20, 11.64s/it]

 20%|█▉        | 1527/7689 [3:48:21<19:26:35, 11.36s/it]


 20%|█▉        | 1529/7689 [3:48:36<15:52:06,  9.27s/it]
{'loss': 1.1955, 'grad_norm': 0.19144899988673827, 'learning_rate': 0.0001854209982737192, 'epoch': 0.2}
[2024-05-24 17:20:02,684] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 20%|█▉        | 1531/7689 [3:49:02<18:33:40, 10.85s/it]
{'loss': 1.1615, 'grad_norm': 0.19544866175154227, 'learning_rate': 0.00018537716526857004, 'epoch': 0.2}

 20%|█▉        | 1532/7689 [3:49:09<16:35:58,  9.71s/it]

 20%|█▉        | 1533/7689 [3:49:15<14:44:27,  8.62s/it]

 20%|█▉        | 1534/7689 [3:49:29<17:38:26, 10.32s/it]


 20%|█▉        | 1536/7689 [3:49:42<14:14:30,  8.33s/it]
{'loss': 0.98, 'grad_norm': 0.1847100800982103, 'learning_rate': 0.00018526731770858947, 'epoch': 0.2}


 20%|██        | 1538/7689 [3:50:00<15:05:46,  8.84s/it]
{'loss': 0.7883, 'grad_norm': 0.19033569869716616, 'learning_rate': 0.0001852232727476185, 'epoch': 0.2}

 20%|██        | 1539/7689 [3:50:07<14:33:15,  8.52s/it]

 20%|██        | 1540/7689 [3:50:20<16:48:12,  9.84s/it]


 20%|██        | 1542/7689 [3:50:42<18:02:34, 10.57s/it]

 20%|██        | 1543/7689 [3:50:54<18:39:17, 10.93s/it]
{'loss': 0.8446, 'grad_norm': 0.17090084020681257, 'learning_rate': 0.00018511289577628315, 'epoch': 0.2}

 20%|██        | 1544/7689 [3:51:00<16:24:28,  9.61s/it]

 20%|██        | 1545/7689 [3:51:11<16:58:17,  9.94s/it]

 20%|██        | 1546/7689 [3:51:20<16:46:53,  9.83s/it]

 20%|██        | 1547/7689 [3:51:29<15:52:38,  9.31s/it]


 20%|██        | 1549/7689 [3:51:48<16:18:16,  9.56s/it]
{'loss': 1.3706, 'grad_norm': 0.2132168054402048, 'learning_rate': 0.00018497994506983846, 'epoch': 0.2}

 20%|██        | 1550/7689 [3:51:56<15:48:28,  9.27s/it]

 20%|██        | 1551/7689 [3:52:05<15:25:15,  9.04s/it]

 20%|██        | 1552/7689 [3:52:15<15:57:39,  9.36s/it]

 20%|██        | 1553/7689 [3:52:29<18:22:53, 10.78s/it]


 20%|██        | 1555/7689 [3:52:40<13:41:48,  8.04s/it]
{'loss': 1.1533, 'grad_norm': 0.18669505731514233, 'learning_rate': 0.0001848464515210675, 'epoch': 0.2}

 20%|██        | 1556/7689 [3:52:47<13:27:45,  7.90s/it]

 20%|██        | 1557/7689 [3:52:56<14:04:54,  8.27s/it]

 20%|██        | 1558/7689 [3:53:03<13:02:32,  7.66s/it]

 20%|██        | 1559/7689 [3:53:10<12:36:45,  7.41s/it]

 20%|██        | 1560/7689 [3:53:17<12:41:14,  7.45s/it]

 20%|██        | 1561/7689 [3:53:24<12:10:14,  7.15s/it]

 20%|██        | 1562/7689 [3:53:32<13:03:37,  7.67s/it]

 20%|██        | 1563/7689 [3:53:40<13:06:07,  7.70s/it]

 20%|██        | 1564/7689 [3:53:50<14:09:34,  8.32s/it]

 20%|██        | 1565/7689 [3:53:59<14:35:15,  8.58s/it]

 20%|██        | 1566/7689 [3:54:09<15:07:34,  8.89s/it]

 20%|██        | 1567/7689 [3:54:21<17:03:34, 10.03s/it]

 20%|██        | 1568/7689 [3:54:31<16:47:47,  9.88s/it]

 20%|██        | 1569/7689 [3:54:36<14:18:58,  8.42s/it]

 20%|██        | 1570/7689 [3:54:44<14:10:26,  8.34s/it]

 20%|██        | 1571/7689 [3:54:51<13:28:02,  7.92s/it]


 20%|██        | 1573/7689 [3:55:12<15:01:44,  8.85s/it]
{'loss': 1.0594, 'grad_norm': 0.2070370581515452, 'learning_rate': 0.0001844427223655199, 'epoch': 0.2}

 20%|██        | 1574/7689 [3:55:17<12:45:00,  7.51s/it]

 20%|██        | 1575/7689 [3:55:26<13:54:13,  8.19s/it]

 20%|██        | 1576/7689 [3:55:44<18:31:38, 10.91s/it]

 21%|██        | 1577/7689 [3:55:56<19:13:07, 11.32s/it]

 21%|██        | 1578/7689 [3:56:01<16:16:46,  9.59s/it]

 21%|██        | 1579/7689 [3:56:10<15:37:58,  9.21s/it]

 21%|██        | 1580/7689 [3:56:17<14:41:34,  8.66s/it]

 21%|██        | 1581/7689 [3:56:28<15:41:56,  9.25s/it]

 21%|██        | 1582/7689 [3:56:33<13:23:49,  7.90s/it]

 21%|██        | 1583/7689 [3:56:38<11:58:10,  7.06s/it]

 21%|██        | 1584/7689 [3:56:43<11:12:41,  6.61s/it]

 21%|██        | 1585/7689 [3:56:49<10:40:19,  6.29s/it]

 21%|██        | 1586/7689 [3:56:57<11:53:18,  7.01s/it]

 21%|██        | 1587/7689 [3:57:12<15:28:38,  9.13s/it]

 21%|██        | 1588/7689 [3:57:17<13:38:40,  8.05s/it]

 21%|██        | 1589/7689 [3:57:32<17:03:14, 10.06s/it]

 21%|██        | 1590/7689 [3:57:48<20:03:53, 11.84s/it]
[2024-05-24 17:29:15,288] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 1591/7689 [3:58:05<22:42:27, 13.41s/it]

 21%|██        | 1592/7689 [3:58:12<19:44:39, 11.66s/it]

 21%|██        | 1593/7689 [3:58:23<19:23:06, 11.45s/it]

 21%|██        | 1594/7689 [3:58:31<17:18:24, 10.22s/it]

 21%|██        | 1595/7689 [3:58:40<16:49:10,  9.94s/it]

 21%|██        | 1596/7689 [3:58:50<16:47:05,  9.92s/it]

 21%|██        | 1597/7689 [3:58:56<14:41:23,  8.68s/it]

 21%|██        | 1598/7689 [3:59:01<13:00:49,  7.69s/it]

 21%|██        | 1599/7689 [3:59:14<15:47:06,  9.33s/it]

 21%|██        | 1600/7689 [3:59:21<14:37:31,  8.65s/it]

 21%|██        | 1601/7689 [3:59:32<15:53:29,  9.40s/it]

 21%|██        | 1602/7689 [3:59:37<13:32:18,  8.01s/it]

 21%|██        | 1603/7689 [3:59:49<15:30:53,  9.18s/it]

 21%|██        | 1604/7689 [3:59:57<14:38:56,  8.67s/it]

 21%|██        | 1605/7689 [4:00:02<12:57:10,  7.66s/it]

 21%|██        | 1606/7689 [4:00:11<13:42:13,  8.11s/it]
[2024-05-24 17:31:40,309] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 1607/7689 [4:00:30<19:08:04, 11.33s/it]

 21%|██        | 1608/7689 [4:00:45<21:05:39, 12.49s/it]

 21%|██        | 1609/7689 [4:00:53<18:38:22, 11.04s/it]

 21%|██        | 1610/7689 [4:01:01<17:24:36, 10.31s/it]

 21%|██        | 1611/7689 [4:01:07<14:57:23,  8.86s/it]

 21%|██        | 1612/7689 [4:01:14<14:03:22,  8.33s/it]

 21%|██        | 1613/7689 [4:01:22<13:59:35,  8.29s/it]

 21%|██        | 1614/7689 [4:01:41<19:35:40, 11.61s/it]

 21%|██        | 1615/7689 [4:01:50<18:03:57, 10.71s/it]

 21%|██        | 1616/7689 [4:01:59<17:03:18, 10.11s/it]

 21%|██        | 1617/7689 [4:02:16<20:30:40, 12.16s/it]

 21%|██        | 1618/7689 [4:02:21<16:55:13, 10.03s/it]

 21%|██        | 1619/7689 [4:02:27<15:07:50,  8.97s/it]

 21%|██        | 1620/7689 [4:02:36<15:01:46,  8.92s/it]

 21%|██        | 1621/7689 [4:02:47<15:59:16,  9.49s/it]

 21%|██        | 1622/7689 [4:02:56<15:33:03,  9.23s/it]

 21%|██        | 1623/7689 [4:03:10<17:58:55, 10.67s/it]

 21%|██        | 1624/7689 [4:03:22<18:54:31, 11.22s/it]

 21%|██        | 1625/7689 [4:03:33<18:43:22, 11.12s/it]

 21%|██        | 1626/7689 [4:03:41<17:11:10, 10.20s/it]


 21%|██        | 1628/7689 [4:03:57<14:46:44,  8.78s/it]
{'loss': 1.0805, 'grad_norm': 0.20509297442310595, 'learning_rate': 0.0001831791260308958, 'epoch': 0.21}

 21%|██        | 1629/7689 [4:04:12<17:56:39, 10.66s/it]

 21%|██        | 1630/7689 [4:04:27<20:10:06, 11.98s/it]

 21%|██        | 1631/7689 [4:04:35<18:10:26, 10.80s/it]

 21%|██        | 1632/7689 [4:04:44<17:03:38, 10.14s/it]

 21%|██        | 1633/7689 [4:04:50<15:22:09,  9.14s/it]

 21%|██▏       | 1634/7689 [4:04:59<14:56:40,  8.89s/it]

 21%|██▏       | 1635/7689 [4:05:06<14:23:04,  8.55s/it]

 21%|██▏       | 1636/7689 [4:05:13<13:13:53,  7.87s/it]

 21%|██▏       | 1637/7689 [4:05:20<12:54:57,  7.68s/it]

 21%|██▏       | 1638/7689 [4:05:26<12:01:27,  7.15s/it]

 21%|██▏       | 1639/7689 [4:05:32<11:30:57,  6.85s/it]

 21%|██▏       | 1640/7689 [4:05:40<12:06:57,  7.21s/it]

 21%|██▏       | 1641/7689 [4:05:49<12:46:03,  7.60s/it]

 21%|██▏       | 1642/7689 [4:05:55<11:54:54,  7.09s/it]

 21%|██▏       | 1643/7689 [4:06:00<11:08:25,  6.63s/it]

 21%|██▏       | 1644/7689 [4:06:07<11:25:41,  6.81s/it]

 21%|██▏       | 1645/7689 [4:06:26<17:14:53, 10.27s/it]

 21%|██▏       | 1646/7689 [4:06:30<14:25:31,  8.59s/it]

 21%|██▏       | 1647/7689 [4:06:40<15:08:04,  9.02s/it]

 21%|██▏       | 1648/7689 [4:06:47<13:45:55,  8.20s/it]

 21%|██▏       | 1649/7689 [4:06:53<12:52:56,  7.68s/it]

 21%|██▏       | 1650/7689 [4:07:02<13:21:18,  7.96s/it]

 21%|██▏       | 1651/7689 [4:07:09<13:07:54,  7.83s/it]

 21%|██▏       | 1652/7689 [4:07:16<12:33:04,  7.48s/it]

 21%|██▏       | 1653/7689 [4:07:26<13:38:28,  8.14s/it]

 22%|██▏       | 1654/7689 [4:07:32<12:52:24,  7.68s/it]

 22%|██▏       | 1655/7689 [4:07:37<11:40:05,  6.96s/it]

 22%|██▏       | 1656/7689 [4:07:53<15:55:19,  9.50s/it]

 22%|██▏       | 1657/7689 [4:07:59<14:02:11,  8.38s/it]

 22%|██▏       | 1658/7689 [4:08:05<12:58:27,  7.74s/it]

 22%|██▏       | 1659/7689 [4:08:10<11:28:55,  6.86s/it]

 22%|██▏       | 1660/7689 [4:08:17<11:48:06,  7.05s/it]

 22%|██▏       | 1661/7689 [4:08:24<11:42:04,  6.99s/it]

 22%|██▏       | 1662/7689 [4:08:35<13:40:03,  8.16s/it]

 22%|██▏       | 1663/7689 [4:08:45<14:25:28,  8.62s/it]

 22%|██▏       | 1664/7689 [4:08:54<14:43:58,  8.80s/it]

 22%|██▏       | 1665/7689 [4:09:01<13:52:23,  8.29s/it]

 22%|██▏       | 1666/7689 [4:09:07<12:46:45,  7.64s/it]

 22%|██▏       | 1667/7689 [4:09:16<13:22:31,  8.00s/it]

 22%|██▏       | 1668/7689 [4:09:24<13:34:34,  8.12s/it]

 22%|██▏       | 1669/7689 [4:09:30<12:11:14,  7.29s/it]

 22%|██▏       | 1670/7689 [4:09:36<11:56:44,  7.14s/it]

 22%|██▏       | 1671/7689 [4:09:42<11:02:34,  6.61s/it]

 22%|██▏       | 1672/7689 [4:09:49<11:16:35,  6.75s/it]

 22%|██▏       | 1673/7689 [4:09:56<11:31:08,  6.89s/it]

 22%|██▏       | 1674/7689 [4:10:03<11:19:13,  6.78s/it]

 22%|██▏       | 1675/7689 [4:10:16<14:26:18,  8.64s/it]

 22%|██▏       | 1676/7689 [4:10:27<15:35:03,  9.33s/it]

 22%|██▏       | 1677/7689 [4:10:34<14:50:58,  8.89s/it]

 22%|██▏       | 1678/7689 [4:10:42<14:12:27,  8.51s/it]

 22%|██▏       | 1679/7689 [4:10:55<16:13:12,  9.72s/it]

 22%|██▏       | 1680/7689 [4:11:08<18:15:54, 10.94s/it]

 22%|██▏       | 1681/7689 [4:11:14<15:47:51,  9.47s/it]

 22%|██▏       | 1682/7689 [4:11:22<14:50:28,  8.89s/it]

 22%|██▏       | 1683/7689 [4:11:29<13:59:21,  8.39s/it]

 22%|██▏       | 1684/7689 [4:11:36<13:20:43,  8.00s/it]

 22%|██▏       | 1685/7689 [4:11:41<11:44:45,  7.04s/it]


 22%|██▏       | 1687/7689 [4:11:59<13:41:22,  8.21s/it]
{'loss': 1.0443, 'grad_norm': 0.1881347280960795, 'learning_rate': 0.00018177401684246568, 'epoch': 0.22}

 22%|██▏       | 1688/7689 [4:12:09<14:15:40,  8.56s/it]

 22%|██▏       | 1689/7689 [4:12:14<12:51:55,  7.72s/it]

 22%|██▏       | 1690/7689 [4:12:23<13:15:44,  7.96s/it]

 22%|██▏       | 1691/7689 [4:12:34<14:35:13,  8.76s/it]

 22%|██▏       | 1692/7689 [4:12:40<13:36:56,  8.17s/it]

 22%|██▏       | 1693/7689 [4:12:55<16:51:30, 10.12s/it]

 22%|██▏       | 1694/7689 [4:13:07<17:59:47, 10.81s/it]

 22%|██▏       | 1695/7689 [4:13:19<18:25:49, 11.07s/it]

 22%|██▏       | 1696/7689 [4:13:30<18:15:25, 10.97s/it]

 22%|██▏       | 1697/7689 [4:13:39<17:26:51, 10.48s/it]

 22%|██▏       | 1698/7689 [4:13:45<15:02:35,  9.04s/it]

 22%|██▏       | 1699/7689 [4:13:57<16:34:24,  9.96s/it]

 22%|██▏       | 1700/7689 [4:14:05<15:23:34,  9.25s/it]

 22%|██▏       | 1701/7689 [4:14:12<14:30:52,  8.73s/it]

 22%|██▏       | 1702/7689 [4:14:17<12:45:16,  7.67s/it]
[2024-05-24 17:45:46,407] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 1703/7689 [4:14:36<18:14:16, 10.97s/it]
[2024-05-24 17:46:00,562] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 1704/7689 [4:14:50<19:49:17, 11.92s/it]


 22%|██▏       | 1706/7689 [4:15:08<17:20:44, 10.44s/it]
{'loss': 1.0875, 'grad_norm': 0.18611778862565154, 'learning_rate': 0.0001813107299990085, 'epoch': 0.22}

 22%|██▏       | 1707/7689 [4:15:16<16:19:01,  9.82s/it]

 22%|██▏       | 1708/7689 [4:15:28<17:15:49, 10.39s/it]

 22%|██▏       | 1709/7689 [4:15:38<17:12:22, 10.36s/it]

 22%|██▏       | 1710/7689 [4:15:47<16:47:39, 10.11s/it]

 22%|██▏       | 1711/7689 [4:15:57<16:42:53, 10.07s/it]

 22%|██▏       | 1712/7689 [4:16:12<18:52:31, 11.37s/it]

 22%|██▏       | 1713/7689 [4:16:19<16:52:46, 10.17s/it]

 22%|██▏       | 1714/7689 [4:16:30<17:12:19, 10.37s/it]

 22%|██▏       | 1715/7689 [4:16:49<21:26:15, 12.92s/it]

 22%|██▏       | 1716/7689 [4:16:56<18:34:37, 11.20s/it]

 22%|██▏       | 1717/7689 [4:17:02<16:05:38,  9.70s/it]

 22%|██▏       | 1718/7689 [4:17:11<15:48:44,  9.53s/it]

 22%|██▏       | 1719/7689 [4:17:24<17:10:19, 10.36s/it]

 22%|██▏       | 1720/7689 [4:17:33<16:50:14, 10.15s/it]

 22%|██▏       | 1721/7689 [4:17:44<17:00:12, 10.26s/it]

 22%|██▏       | 1722/7689 [4:17:51<15:38:53,  9.44s/it]

 22%|██▏       | 1723/7689 [4:18:01<15:56:48,  9.62s/it]

 22%|██▏       | 1724/7689 [4:18:07<13:58:18,  8.43s/it]

 22%|██▏       | 1725/7689 [4:18:14<13:18:05,  8.03s/it]

 22%|██▏       | 1726/7689 [4:18:23<13:29:18,  8.14s/it]

 22%|██▏       | 1727/7689 [4:18:35<15:37:14,  9.43s/it]

 22%|██▏       | 1728/7689 [4:18:41<13:40:59,  8.26s/it]

 22%|██▏       | 1729/7689 [4:18:50<14:03:00,  8.49s/it]

 22%|██▏       | 1730/7689 [4:18:55<12:22:11,  7.47s/it]

 23%|██▎       | 1731/7689 [4:19:01<12:00:06,  7.25s/it]

 23%|██▎       | 1732/7689 [4:19:10<12:35:00,  7.60s/it]

 23%|██▎       | 1733/7689 [4:19:18<12:37:30,  7.63s/it]

 23%|██▎       | 1734/7689 [4:19:25<12:46:08,  7.72s/it]

 23%|██▎       | 1735/7689 [4:19:34<12:56:06,  7.82s/it]

 23%|██▎       | 1736/7689 [4:19:41<12:35:30,  7.61s/it]

 23%|██▎       | 1737/7689 [4:19:46<11:29:39,  6.95s/it]

 23%|██▎       | 1738/7689 [4:19:51<10:21:06,  6.26s/it]

 23%|██▎       | 1739/7689 [4:19:58<10:52:08,  6.58s/it]

 23%|██▎       | 1740/7689 [4:20:07<12:01:13,  7.27s/it]

 23%|██▎       | 1741/7689 [4:20:13<11:33:49,  7.00s/it]

 23%|██▎       | 1742/7689 [4:20:21<12:02:28,  7.29s/it]

 23%|██▎       | 1743/7689 [4:20:29<12:17:00,  7.44s/it]

 23%|██▎       | 1744/7689 [4:20:44<16:08:13,  9.77s/it]

 23%|██▎       | 1745/7689 [4:20:54<16:01:10,  9.70s/it]

 23%|██▎       | 1746/7689 [4:20:59<13:49:30,  8.37s/it]

 23%|██▎       | 1747/7689 [4:21:09<14:36:59,  8.86s/it]

 23%|██▎       | 1748/7689 [4:21:17<14:12:04,  8.61s/it]

 23%|██▎       | 1749/7689 [4:21:22<12:31:19,  7.59s/it]

 23%|██▎       | 1750/7689 [4:21:31<13:11:51,  8.00s/it]

 23%|██▎       | 1751/7689 [4:21:42<14:28:14,  8.77s/it]

 23%|██▎       | 1752/7689 [4:21:50<14:03:49,  8.53s/it]

 23%|██▎       | 1753/7689 [4:21:55<12:09:22,  7.37s/it]

 23%|██▎       | 1754/7689 [4:22:03<12:39:54,  7.68s/it]

 23%|██▎       | 1755/7689 [4:22:10<12:19:33,  7.48s/it]

 23%|██▎       | 1756/7689 [4:22:23<15:05:33,  9.16s/it]

 23%|██▎       | 1757/7689 [4:22:30<14:06:24,  8.56s/it]

 23%|██▎       | 1758/7689 [4:22:37<13:18:44,  8.08s/it]

 23%|██▎       | 1759/7689 [4:22:47<13:58:11,  8.48s/it]

 23%|██▎       | 1760/7689 [4:22:54<13:34:10,  8.24s/it]

 23%|██▎       | 1761/7689 [4:23:01<12:50:15,  7.80s/it]

 23%|██▎       | 1762/7689 [4:23:15<16:00:07,  9.72s/it]

 23%|██▎       | 1763/7689 [4:23:21<14:13:20,  8.64s/it]

 23%|██▎       | 1764/7689 [4:23:33<15:42:54,  9.55s/it]

 23%|██▎       | 1765/7689 [4:23:42<15:15:06,  9.27s/it]

 23%|██▎       | 1766/7689 [4:23:57<18:12:20, 11.07s/it]

 23%|██▎       | 1767/7689 [4:24:05<16:37:52, 10.11s/it]

 23%|██▎       | 1768/7689 [4:24:13<15:40:24,  9.53s/it]

 23%|██▎       | 1769/7689 [4:24:25<16:54:59, 10.29s/it]

 23%|██▎       | 1770/7689 [4:24:38<18:16:49, 11.12s/it]

 23%|██▎       | 1771/7689 [4:24:45<16:11:31,  9.85s/it]

 23%|██▎       | 1772/7689 [4:24:53<15:16:14,  9.29s/it]

 23%|██▎       | 1773/7689 [4:25:02<15:26:12,  9.39s/it]

 23%|██▎       | 1774/7689 [4:25:14<16:41:55, 10.16s/it]

 23%|██▎       | 1775/7689 [4:25:31<20:04:18, 12.22s/it]

 23%|██▎       | 1776/7689 [4:25:40<18:15:51, 11.12s/it]

 23%|██▎       | 1777/7689 [4:25:45<15:12:30,  9.26s/it]

 23%|██▎       | 1778/7689 [4:25:58<17:03:00, 10.38s/it]

 23%|██▎       | 1779/7689 [4:26:05<15:10:27,  9.24s/it]

 23%|██▎       | 1780/7689 [4:26:09<12:59:03,  7.91s/it]

 23%|██▎       | 1781/7689 [4:26:16<12:08:02,  7.39s/it]

 23%|██▎       | 1782/7689 [4:26:22<11:43:55,  7.15s/it]


 23%|██▎       | 1784/7689 [4:26:38<12:28:05,  7.60s/it]
{'loss': 0.9049, 'grad_norm': 0.19387841948503326, 'learning_rate': 0.00017935456257590953, 'epoch': 0.23}

 23%|██▎       | 1785/7689 [4:26:49<13:47:35,  8.41s/it]

 23%|██▎       | 1786/7689 [4:26:56<13:18:35,  8.12s/it]

 23%|██▎       | 1787/7689 [4:27:01<11:55:36,  7.27s/it]

 23%|██▎       | 1788/7689 [4:27:11<13:12:48,  8.06s/it]

 23%|██▎       | 1789/7689 [4:27:22<14:25:44,  8.80s/it]

 23%|██▎       | 1790/7689 [4:27:27<12:34:50,  7.68s/it]

 23%|██▎       | 1791/7689 [4:27:36<13:12:49,  8.07s/it]

 23%|██▎       | 1792/7689 [4:27:42<12:20:00,  7.53s/it]

 23%|██▎       | 1793/7689 [4:27:58<16:33:46, 10.11s/it]

 23%|██▎       | 1794/7689 [4:28:08<16:24:20, 10.02s/it]

 23%|██▎       | 1795/7689 [4:28:16<15:17:41,  9.34s/it]

 23%|██▎       | 1796/7689 [4:28:25<15:05:41,  9.22s/it]

 23%|██▎       | 1797/7689 [4:28:43<19:22:09, 11.83s/it]

 23%|██▎       | 1798/7689 [4:28:50<17:00:51, 10.40s/it]

 23%|██▎       | 1799/7689 [4:28:59<16:25:01, 10.03s/it]

 23%|██▎       | 1800/7689 [4:29:07<15:31:09,  9.49s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0693, 'grad_norm': 0.19733215801599235, 'learning_rate': 0.0001789167743326491, 'epoch': 0.23}
 23%|██▎       | 1801/7689 [4:29:45<29:34:56, 18.09s/it]

 23%|██▎       | 1802/7689 [4:29:53<24:45:32, 15.14s/it]

 23%|██▎       | 1803/7689 [4:30:00<20:37:07, 12.61s/it]

 23%|██▎       | 1804/7689 [4:30:19<23:28:18, 14.36s/it]

 23%|██▎       | 1805/7689 [4:30:24<19:11:27, 11.74s/it]

 23%|██▎       | 1806/7689 [4:30:35<18:54:02, 11.57s/it]

 24%|██▎       | 1807/7689 [4:30:47<18:52:35, 11.55s/it]

 24%|██▎       | 1808/7689 [4:30:56<17:35:25, 10.77s/it]

 24%|██▎       | 1809/7689 [4:31:02<15:19:35,  9.38s/it]

 24%|██▎       | 1810/7689 [4:31:07<13:19:21,  8.16s/it]

 24%|██▎       | 1811/7689 [4:31:12<11:40:56,  7.15s/it]

 24%|██▎       | 1812/7689 [4:31:19<11:19:36,  6.94s/it]

 24%|██▎       | 1813/7689 [4:31:26<11:29:10,  7.04s/it]

 24%|██▎       | 1814/7689 [4:31:32<11:03:28,  6.78s/it]

 24%|██▎       | 1815/7689 [4:31:48<15:20:23,  9.40s/it]

 24%|██▎       | 1816/7689 [4:32:04<18:33:24, 11.37s/it]

 24%|██▎       | 1817/7689 [4:32:11<16:37:32, 10.19s/it]

 24%|██▎       | 1818/7689 [4:32:20<16:12:09,  9.94s/it]

 24%|██▎       | 1819/7689 [4:32:26<13:53:32,  8.52s/it]

 24%|██▎       | 1820/7689 [4:32:31<12:26:54,  7.64s/it]

 24%|██▎       | 1821/7689 [4:32:39<12:37:56,  7.75s/it]

 24%|██▎       | 1822/7689 [4:32:49<13:44:38,  8.43s/it]

 24%|██▎       | 1823/7689 [4:32:55<12:16:24,  7.53s/it]

 24%|██▎       | 1824/7689 [4:33:02<12:15:52,  7.53s/it]

 24%|██▎       | 1825/7689 [4:33:07<10:58:58,  6.74s/it]

 24%|██▎       | 1826/7689 [4:33:14<10:54:40,  6.70s/it]

 24%|██▍       | 1827/7689 [4:33:20<10:39:28,  6.55s/it]

 24%|██▍       | 1828/7689 [4:33:27<11:08:44,  6.85s/it]

 24%|██▍       | 1829/7689 [4:33:37<12:23:11,  7.61s/it]

 24%|██▍       | 1830/7689 [4:33:42<11:26:01,  7.03s/it]

 24%|██▍       | 1831/7689 [4:33:49<11:09:32,  6.86s/it]

 24%|██▍       | 1832/7689 [4:34:00<13:12:07,  8.11s/it]

 24%|██▍       | 1833/7689 [4:34:10<13:57:04,  8.58s/it]

 24%|██▍       | 1834/7689 [4:34:23<16:27:56, 10.12s/it]

 24%|██▍       | 1835/7689 [4:34:33<16:13:40,  9.98s/it]

 24%|██▍       | 1836/7689 [4:34:43<16:05:09,  9.89s/it]

 24%|██▍       | 1837/7689 [4:34:51<15:10:17,  9.33s/it]

 24%|██▍       | 1838/7689 [4:35:06<18:10:34, 11.18s/it]

 24%|██▍       | 1839/7689 [4:35:18<18:38:57, 11.48s/it]

 24%|██▍       | 1840/7689 [4:35:24<16:02:32,  9.87s/it]

 24%|██▍       | 1841/7689 [4:35:33<15:18:41,  9.43s/it]

 24%|██▍       | 1842/7689 [4:35:40<14:21:16,  8.84s/it]

 24%|██▍       | 1843/7689 [4:35:47<13:12:36,  8.13s/it]

 24%|██▍       | 1844/7689 [4:35:54<12:36:51,  7.77s/it]

 24%|██▍       | 1845/7689 [4:36:06<14:56:40,  9.21s/it]

 24%|██▍       | 1846/7689 [4:36:18<16:19:23, 10.06s/it]

 24%|██▍       | 1847/7689 [4:36:27<15:25:49,  9.51s/it]

 24%|██▍       | 1848/7689 [4:36:39<17:01:52, 10.50s/it]

 24%|██▍       | 1849/7689 [4:36:46<15:13:10,  9.38s/it]

 24%|██▍       | 1850/7689 [4:36:54<14:15:48,  8.79s/it]

 24%|██▍       | 1851/7689 [4:36:59<12:48:54,  7.90s/it]

 24%|██▍       | 1852/7689 [4:37:10<14:03:22,  8.67s/it]

 24%|██▍       | 1853/7689 [4:37:16<12:58:55,  8.01s/it]

 24%|██▍       | 1854/7689 [4:37:31<16:12:23, 10.00s/it]

 24%|██▍       | 1855/7689 [4:37:41<16:12:38, 10.00s/it]


 24%|██▍       | 1857/7689 [4:37:55<13:34:58,  8.38s/it]
{'loss': 1.0079, 'grad_norm': 0.19645518027178305, 'learning_rate': 0.00017744614903671466, 'epoch': 0.24}

 24%|██▍       | 1858/7689 [4:38:01<12:38:42,  7.81s/it]

 24%|██▍       | 1859/7689 [4:38:12<13:47:59,  8.52s/it]

 24%|██▍       | 1860/7689 [4:38:23<15:16:26,  9.43s/it]

 24%|██▍       | 1861/7689 [4:38:29<13:37:55,  8.42s/it]

 24%|██▍       | 1862/7689 [4:38:40<14:47:20,  9.14s/it]

 24%|██▍       | 1863/7689 [4:38:49<14:36:18,  9.02s/it]

 24%|██▍       | 1864/7689 [4:38:58<14:29:42,  8.96s/it]


 24%|██▍       | 1866/7689 [4:39:17<15:35:08,  9.64s/it]
{'loss': 1.0728, 'grad_norm': 0.18957421360477833, 'learning_rate': 0.0001772057574560244, 'epoch': 0.24}

 24%|██▍       | 1867/7689 [4:39:31<17:45:01, 10.98s/it]

 24%|██▍       | 1868/7689 [4:39:37<15:08:50,  9.37s/it]

 24%|██▍       | 1869/7689 [4:39:42<13:07:37,  8.12s/it]

 24%|██▍       | 1870/7689 [4:39:49<12:22:07,  7.65s/it]

 24%|██▍       | 1871/7689 [4:39:54<11:29:20,  7.11s/it]

 24%|██▍       | 1872/7689 [4:40:13<16:53:11, 10.45s/it]

 24%|██▍       | 1873/7689 [4:40:20<15:26:45,  9.56s/it]

 24%|██▍       | 1874/7689 [4:40:30<15:37:02,  9.67s/it]

 24%|██▍       | 1875/7689 [4:40:36<13:34:14,  8.40s/it]

 24%|██▍       | 1876/7689 [4:40:43<13:14:01,  8.20s/it]

 24%|██▍       | 1877/7689 [4:40:51<13:06:48,  8.12s/it]

 24%|██▍       | 1878/7689 [4:41:00<13:24:39,  8.31s/it]

 24%|██▍       | 1879/7689 [4:41:07<13:02:02,  8.08s/it]

 24%|██▍       | 1880/7689 [4:41:15<12:57:24,  8.03s/it]

 24%|██▍       | 1881/7689 [4:41:21<11:53:26,  7.37s/it]

 24%|██▍       | 1882/7689 [4:41:30<12:30:44,  7.76s/it]

 24%|██▍       | 1883/7689 [4:41:37<11:58:39,  7.43s/it]

 25%|██▍       | 1884/7689 [4:41:44<12:03:11,  7.47s/it]

 25%|██▍       | 1885/7689 [4:41:50<11:29:12,  7.12s/it]

 25%|██▍       | 1886/7689 [4:42:04<14:41:03,  9.11s/it]

 25%|██▍       | 1887/7689 [4:42:10<13:10:23,  8.17s/it]

 25%|██▍       | 1888/7689 [4:42:17<12:30:30,  7.76s/it]

 25%|██▍       | 1889/7689 [4:42:24<12:04:21,  7.49s/it]

 25%|██▍       | 1890/7689 [4:42:29<11:10:37,  6.94s/it]

 25%|██▍       | 1891/7689 [4:42:37<11:23:47,  7.08s/it]

 25%|██▍       | 1892/7689 [4:42:52<15:27:18,  9.60s/it]

 25%|██▍       | 1893/7689 [4:43:00<14:21:44,  8.92s/it]

 25%|██▍       | 1894/7689 [4:43:13<16:16:44, 10.11s/it]

 25%|██▍       | 1895/7689 [4:43:18<14:13:39,  8.84s/it]

 25%|██▍       | 1896/7689 [4:43:26<13:30:42,  8.40s/it]

 25%|██▍       | 1897/7689 [4:43:38<15:06:50,  9.39s/it]

 25%|██▍       | 1898/7689 [4:43:51<17:01:55, 10.59s/it]

 25%|██▍       | 1899/7689 [4:44:00<16:15:53, 10.11s/it]

 25%|██▍       | 1900/7689 [4:44:06<14:14:23,  8.86s/it]

 25%|██▍       | 1901/7689 [4:44:11<12:28:50,  7.76s/it]

 25%|██▍       | 1902/7689 [4:44:24<15:08:50,  9.42s/it]

 25%|██▍       | 1903/7689 [4:44:36<16:15:10, 10.11s/it]

 25%|██▍       | 1904/7689 [4:44:46<15:57:55,  9.94s/it]

 25%|██▍       | 1905/7689 [4:44:53<14:41:28,  9.14s/it]

 25%|██▍       | 1906/7689 [4:45:03<14:56:53,  9.31s/it]

 25%|██▍       | 1907/7689 [4:45:13<15:21:10,  9.56s/it]

 25%|██▍       | 1908/7689 [4:45:21<14:43:40,  9.17s/it]

 25%|██▍       | 1909/7689 [4:45:29<14:14:23,  8.87s/it]

 25%|██▍       | 1910/7689 [4:45:42<16:07:48, 10.05s/it]

 25%|██▍       | 1911/7689 [4:45:50<15:09:41,  9.45s/it]

 25%|██▍       | 1912/7689 [4:45:55<13:00:27,  8.11s/it]

 25%|██▍       | 1913/7689 [4:46:01<11:47:07,  7.35s/it]

 25%|██▍       | 1914/7689 [4:46:07<11:28:03,  7.15s/it]

 25%|██▍       | 1915/7689 [4:46:14<11:08:26,  6.95s/it]

 25%|██▍       | 1916/7689 [4:46:21<11:21:11,  7.08s/it]

 25%|██▍       | 1917/7689 [4:46:26<10:29:44,  6.55s/it]

 25%|██▍       | 1918/7689 [4:46:32<10:12:11,  6.36s/it]

 25%|██▍       | 1919/7689 [4:46:43<12:29:21,  7.79s/it]

 25%|██▍       | 1920/7689 [4:46:55<14:02:37,  8.76s/it]

 25%|██▍       | 1921/7689 [4:47:04<14:35:09,  9.10s/it]

 25%|██▍       | 1922/7689 [4:47:12<13:42:40,  8.56s/it]

 25%|██▌       | 1923/7689 [4:47:20<13:24:15,  8.37s/it]

 25%|██▌       | 1924/7689 [4:47:29<13:39:29,  8.53s/it]

 25%|██▌       | 1925/7689 [4:47:43<16:24:19, 10.25s/it]

 25%|██▌       | 1926/7689 [4:48:01<20:00:23, 12.50s/it]

 25%|██▌       | 1927/7689 [4:48:12<19:38:34, 12.27s/it]

 25%|██▌       | 1928/7689 [4:48:18<16:31:18, 10.32s/it]

 25%|██▌       | 1929/7689 [4:48:31<17:33:07, 10.97s/it]

 25%|██▌       | 1930/7689 [4:48:36<14:52:24,  9.30s/it]

 25%|██▌       | 1931/7689 [4:48:41<13:01:12,  8.14s/it]

 25%|██▌       | 1932/7689 [4:48:47<11:47:59,  7.38s/it]


 25%|██▌       | 1934/7689 [4:49:00<10:59:51,  6.88s/it]
{'loss': 1.1782, 'grad_norm': 0.19816769607951845, 'learning_rate': 0.00017535384697338439, 'epoch': 0.25}

 25%|██▌       | 1935/7689 [4:49:06<10:34:48,  6.62s/it]

 25%|██▌       | 1936/7689 [4:49:18<13:25:02,  8.40s/it]

 25%|██▌       | 1937/7689 [4:49:26<12:59:17,  8.13s/it]

 25%|██▌       | 1938/7689 [4:49:32<11:58:33,  7.50s/it]

 25%|██▌       | 1939/7689 [4:49:47<15:37:02,  9.78s/it]

 25%|██▌       | 1940/7689 [4:50:00<17:03:28, 10.68s/it]

 25%|██▌       | 1941/7689 [4:50:08<15:41:46,  9.83s/it]

 25%|██▌       | 1942/7689 [4:50:13<13:33:00,  8.49s/it]

 25%|██▌       | 1943/7689 [4:50:23<14:21:07,  8.99s/it]

 25%|██▌       | 1944/7689 [4:50:34<15:28:14,  9.69s/it]

 25%|██▌       | 1945/7689 [4:50:39<13:14:19,  8.30s/it]

 25%|██▌       | 1946/7689 [4:50:48<13:10:21,  8.26s/it]

 25%|██▌       | 1947/7689 [4:50:54<12:08:57,  7.62s/it]

 25%|██▌       | 1948/7689 [4:51:06<14:20:38,  8.99s/it]

 25%|██▌       | 1949/7689 [4:51:14<13:51:57,  8.70s/it]

 25%|██▌       | 1950/7689 [4:51:23<13:50:22,  8.68s/it]

 25%|██▌       | 1951/7689 [4:51:29<12:52:40,  8.08s/it]

 25%|██▌       | 1952/7689 [4:51:39<13:31:38,  8.49s/it]

 25%|██▌       | 1953/7689 [4:51:52<15:36:01,  9.79s/it]

 25%|██▌       | 1954/7689 [4:51:59<14:35:37,  9.16s/it]

 25%|██▌       | 1955/7689 [4:52:09<14:50:44,  9.32s/it]

 25%|██▌       | 1956/7689 [4:52:16<13:35:30,  8.53s/it]

 25%|██▌       | 1957/7689 [4:52:24<13:38:24,  8.57s/it]

 25%|██▌       | 1958/7689 [4:52:38<16:09:33, 10.15s/it]

 25%|██▌       | 1959/7689 [4:52:45<14:24:27,  9.05s/it]

 25%|██▌       | 1960/7689 [4:52:55<15:14:49,  9.58s/it]

 26%|██▌       | 1961/7689 [4:53:02<13:50:45,  8.70s/it]

 26%|██▌       | 1962/7689 [4:53:08<12:34:58,  7.91s/it]

 26%|██▌       | 1963/7689 [4:53:15<12:02:59,  7.58s/it]

 26%|██▌       | 1964/7689 [4:53:29<15:00:07,  9.43s/it]

 26%|██▌       | 1965/7689 [4:53:35<13:27:24,  8.46s/it]

 26%|██▌       | 1966/7689 [4:53:43<13:13:20,  8.32s/it]

 26%|██▌       | 1967/7689 [4:53:54<14:42:18,  9.25s/it]

 26%|██▌       | 1968/7689 [4:54:05<15:13:17,  9.58s/it]

 26%|██▌       | 1969/7689 [4:54:13<14:35:47,  9.19s/it]

 26%|██▌       | 1970/7689 [4:54:20<13:32:18,  8.52s/it]

 26%|██▌       | 1971/7689 [4:54:30<14:12:47,  8.95s/it]

 26%|██▌       | 1972/7689 [4:54:38<13:47:23,  8.68s/it]

 26%|██▌       | 1973/7689 [4:54:45<13:04:27,  8.23s/it]


 26%|██▌       | 1975/7689 [4:55:06<14:27:48,  9.11s/it]

 26%|██▌       | 1976/7689 [4:55:20<16:51:58, 10.63s/it]

 26%|██▌       | 1977/7689 [4:55:29<16:06:33, 10.15s/it]

 26%|██▌       | 1978/7689 [4:55:38<15:14:39,  9.61s/it]

 26%|██▌       | 1979/7689 [4:55:46<14:36:00,  9.21s/it]

 26%|██▌       | 1980/7689 [4:55:55<14:20:39,  9.05s/it]

 26%|██▌       | 1981/7689 [4:56:01<12:55:22,  8.15s/it]

 26%|██▌       | 1982/7689 [4:56:11<13:52:27,  8.75s/it]

 26%|██▌       | 1983/7689 [4:56:20<13:51:11,  8.74s/it]

 26%|██▌       | 1984/7689 [4:56:27<13:14:30,  8.36s/it]

 26%|██▌       | 1985/7689 [4:56:35<13:00:50,  8.21s/it]

 26%|██▌       | 1986/7689 [4:56:48<15:19:53,  9.68s/it]

 26%|██▌       | 1987/7689 [4:56:58<15:33:11,  9.82s/it]

 26%|██▌       | 1988/7689 [4:57:05<14:06:20,  8.91s/it]

 26%|██▌       | 1989/7689 [4:57:13<13:29:43,  8.52s/it]

 26%|██▌       | 1990/7689 [4:57:19<12:36:55,  7.97s/it]

 26%|██▌       | 1991/7689 [4:57:25<11:45:43,  7.43s/it]

 26%|██▌       | 1992/7689 [4:57:33<11:41:21,  7.39s/it]

 26%|██▌       | 1993/7689 [4:57:44<13:38:29,  8.62s/it]

 26%|██▌       | 1994/7689 [4:57:50<12:22:23,  7.82s/it]

 26%|██▌       | 1995/7689 [4:57:57<11:56:09,  7.55s/it]

 26%|██▌       | 1996/7689 [4:58:03<11:22:36,  7.19s/it]

 26%|██▌       | 1997/7689 [4:58:16<14:01:33,  8.87s/it]

 26%|██▌       | 1998/7689 [4:58:26<14:26:07,  9.13s/it]

 26%|██▌       | 1999/7689 [4:58:34<14:01:44,  8.88s/it]

 26%|██▌       | 2000/7689 [4:58:40<12:42:33,  8.04s/it]

 26%|██▌       | 2001/7689 [4:58:49<12:53:59,  8.16s/it]

 26%|██▌       | 2002/7689 [4:58:54<11:42:21,  7.41s/it]

 26%|██▌       | 2003/7689 [4:59:00<10:46:02,  6.82s/it]

 26%|██▌       | 2004/7689 [4:59:07<11:04:32,  7.01s/it]

 26%|██▌       | 2005/7689 [4:59:20<13:40:22,  8.66s/it]

 26%|██▌       | 2006/7689 [4:59:30<14:33:51,  9.23s/it]

 26%|██▌       | 2007/7689 [4:59:38<13:42:52,  8.69s/it]

 26%|██▌       | 2008/7689 [4:59:45<13:02:00,  8.26s/it]

 26%|██▌       | 2009/7689 [4:59:51<11:45:48,  7.46s/it]

 26%|██▌       | 2010/7689 [5:00:01<13:02:41,  8.27s/it]

 26%|██▌       | 2011/7689 [5:00:11<13:54:03,  8.81s/it]

 26%|██▌       | 2012/7689 [5:00:17<12:29:08,  7.92s/it]

 26%|██▌       | 2013/7689 [5:00:26<13:12:39,  8.38s/it]

 26%|██▌       | 2014/7689 [5:00:32<12:04:30,  7.66s/it]

 26%|██▌       | 2015/7689 [5:00:39<11:54:50,  7.56s/it]

 26%|██▌       | 2016/7689 [5:00:46<11:28:11,  7.28s/it]

 26%|██▌       | 2017/7689 [5:00:52<10:42:29,  6.80s/it]

 26%|██▌       | 2018/7689 [5:00:59<10:46:40,  6.84s/it]

 26%|██▋       | 2019/7689 [5:01:12<13:48:01,  8.76s/it]

 26%|██▋       | 2020/7689 [5:01:21<14:03:24,  8.93s/it]

 26%|██▋       | 2021/7689 [5:01:28<13:12:23,  8.39s/it]

 26%|██▋       | 2022/7689 [5:01:40<14:48:51,  9.41s/it]

 26%|██▋       | 2023/7689 [5:01:48<13:57:43,  8.87s/it]

 26%|██▋       | 2024/7689 [5:01:54<12:42:56,  8.08s/it]

 26%|██▋       | 2025/7689 [5:02:00<11:48:44,  7.51s/it]

 26%|██▋       | 2026/7689 [5:02:14<14:56:18,  9.50s/it]

 26%|██▋       | 2027/7689 [5:02:22<14:12:25,  9.03s/it]

 26%|██▋       | 2028/7689 [5:02:30<13:33:40,  8.62s/it]

 26%|██▋       | 2029/7689 [5:02:38<13:22:47,  8.51s/it]

 26%|██▋       | 2030/7689 [5:02:43<11:37:26,  7.39s/it]

 26%|██▋       | 2031/7689 [5:02:50<11:27:14,  7.29s/it]

 26%|██▋       | 2032/7689 [5:02:58<11:56:39,  7.60s/it]

 26%|██▋       | 2033/7689 [5:03:06<12:09:49,  7.74s/it]

 26%|██▋       | 2034/7689 [5:03:17<13:34:59,  8.65s/it]

 26%|██▋       | 2035/7689 [5:03:28<14:45:56,  9.40s/it]

 26%|██▋       | 2036/7689 [5:03:38<14:52:15,  9.47s/it]

 26%|██▋       | 2037/7689 [5:03:47<14:32:08,  9.26s/it]

 27%|██▋       | 2038/7689 [5:03:58<15:24:06,  9.81s/it]

 27%|██▋       | 2039/7689 [5:04:07<14:53:20,  9.49s/it]

 27%|██▋       | 2040/7689 [5:04:14<13:50:53,  8.83s/it]

 27%|██▋       | 2041/7689 [5:04:26<15:13:48,  9.71s/it]

 27%|██▋       | 2042/7689 [5:04:34<14:28:17,  9.23s/it]

 27%|██▋       | 2043/7689 [5:04:45<15:17:17,  9.75s/it]

 27%|██▋       | 2044/7689 [5:04:58<17:09:32, 10.94s/it]
[2024-05-24 18:36:08,851] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2045/7689 [5:05:17<20:36:57, 13.15s/it]
[2024-05-24 18:36:27,151] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2046/7689 [5:05:25<18:24:18, 11.74s/it]

 27%|██▋       | 2047/7689 [5:05:34<16:57:50, 10.82s/it]

 27%|██▋       | 2048/7689 [5:05:41<15:06:25,  9.64s/it]

 27%|██▋       | 2049/7689 [5:05:50<14:46:57,  9.44s/it]

 27%|██▋       | 2050/7689 [5:05:57<13:45:58,  8.79s/it]

 27%|██▋       | 2051/7689 [5:06:04<13:06:55,  8.37s/it]

 27%|██▋       | 2052/7689 [5:06:15<14:07:54,  9.03s/it]

 27%|██▋       | 2053/7689 [5:06:20<12:29:02,  7.97s/it]

 27%|██▋       | 2054/7689 [5:06:27<12:01:16,  7.68s/it]

 27%|██▋       | 2055/7689 [5:06:32<10:41:17,  6.83s/it]

 27%|██▋       | 2056/7689 [5:06:46<13:50:58,  8.85s/it]

 27%|██▋       | 2057/7689 [5:06:57<14:58:19,  9.57s/it]

 27%|██▋       | 2058/7689 [5:07:03<13:21:05,  8.54s/it]

 27%|██▋       | 2059/7689 [5:07:09<12:00:25,  7.68s/it]

 27%|██▋       | 2060/7689 [5:07:25<15:46:03, 10.08s/it]

 27%|██▋       | 2061/7689 [5:07:32<14:16:15,  9.13s/it]

 27%|██▋       | 2062/7689 [5:07:36<12:12:44,  7.81s/it]

 27%|██▋       | 2063/7689 [5:07:43<11:45:07,  7.52s/it]

 27%|██▋       | 2064/7689 [5:07:48<10:44:08,  6.87s/it]

 27%|██▋       | 2065/7689 [5:07:54<10:19:12,  6.61s/it]

 27%|██▋       | 2066/7689 [5:08:01<10:23:42,  6.66s/it]

 27%|██▋       | 2067/7689 [5:08:09<11:08:03,  7.13s/it]

 27%|██▋       | 2068/7689 [5:08:19<12:13:10,  7.83s/it]

 27%|██▋       | 2069/7689 [5:08:27<12:11:53,  7.81s/it]

 27%|██▋       | 2070/7689 [5:08:34<11:47:12,  7.55s/it]

 27%|██▋       | 2071/7689 [5:08:39<10:58:11,  7.03s/it]

 27%|██▋       | 2072/7689 [5:08:48<11:27:54,  7.35s/it]

 27%|██▋       | 2073/7689 [5:08:57<12:32:09,  8.04s/it]

 27%|██▋       | 2074/7689 [5:09:03<11:28:32,  7.36s/it]

 27%|██▋       | 2075/7689 [5:09:13<12:54:14,  8.27s/it]

 27%|██▋       | 2076/7689 [5:09:19<11:38:18,  7.46s/it]

 27%|██▋       | 2077/7689 [5:09:30<13:24:29,  8.60s/it]

 27%|██▋       | 2078/7689 [5:09:39<13:29:56,  8.66s/it]

 27%|██▋       | 2079/7689 [5:09:45<12:17:11,  7.88s/it]

 27%|██▋       | 2080/7689 [5:09:57<14:04:29,  9.03s/it]

 27%|██▋       | 2081/7689 [5:10:04<13:08:00,  8.43s/it]

 27%|██▋       | 2082/7689 [5:10:17<15:15:37,  9.80s/it]

 27%|██▋       | 2083/7689 [5:10:24<13:58:29,  8.97s/it]

 27%|██▋       | 2084/7689 [5:10:32<13:32:40,  8.70s/it]

 27%|██▋       | 2085/7689 [5:10:39<12:53:59,  8.29s/it]

 27%|██▋       | 2086/7689 [5:10:56<16:58:29, 10.91s/it]
[2024-05-24 18:42:06,648] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2087/7689 [5:11:04<15:32:07,  9.98s/it]

 27%|██▋       | 2088/7689 [5:11:11<14:00:48,  9.01s/it]

 27%|██▋       | 2089/7689 [5:11:24<15:46:03, 10.14s/it]

 27%|██▋       | 2090/7689 [5:11:34<15:52:10, 10.20s/it]

 27%|██▋       | 2091/7689 [5:11:43<15:09:32,  9.75s/it]

 27%|██▋       | 2092/7689 [5:11:48<13:10:50,  8.48s/it]

 27%|██▋       | 2093/7689 [5:11:56<12:41:19,  8.16s/it]

 27%|██▋       | 2094/7689 [5:12:00<10:49:50,  6.97s/it]

 27%|██▋       | 2095/7689 [5:12:09<11:44:11,  7.55s/it]

 27%|██▋       | 2096/7689 [5:12:20<13:29:15,  8.68s/it]

 27%|██▋       | 2097/7689 [5:12:31<14:22:29,  9.25s/it]

 27%|██▋       | 2098/7689 [5:12:36<12:36:13,  8.12s/it]

 27%|██▋       | 2099/7689 [5:12:50<15:32:29, 10.01s/it]

 27%|██▋       | 2100/7689 [5:12:58<14:25:58,  9.30s/it]
 27%|██▋       | 2100/7689 [5:12:58<14:25:58,  9.30s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 27%|██▋       | 2101/7689 [5:13:37<28:25:32, 18.31s/it]

 27%|██▋       | 2102/7689 [5:13:46<23:42:17, 15.27s/it]

 27%|██▋       | 2103/7689 [5:13:51<19:13:47, 12.39s/it]

 27%|██▋       | 2104/7689 [5:13:57<16:03:19, 10.35s/it]

 27%|██▋       | 2105/7689 [5:14:10<17:08:52, 11.06s/it]

 27%|██▋       | 2106/7689 [5:14:19<16:28:33, 10.62s/it]

 27%|██▋       | 2107/7689 [5:14:29<16:15:42, 10.49s/it]

 27%|██▋       | 2108/7689 [5:14:35<14:05:40,  9.09s/it]

 27%|██▋       | 2109/7689 [5:14:42<12:47:59,  8.26s/it]

 27%|██▋       | 2110/7689 [5:14:50<12:45:03,  8.23s/it]

 27%|██▋       | 2111/7689 [5:14:55<11:36:32,  7.49s/it]

 27%|██▋       | 2112/7689 [5:15:02<11:16:46,  7.28s/it]

 27%|██▋       | 2113/7689 [5:15:15<13:59:25,  9.03s/it]

 27%|██▋       | 2114/7689 [5:15:23<13:12:21,  8.53s/it]

 28%|██▊       | 2115/7689 [5:15:31<13:09:48,  8.50s/it]

 28%|██▊       | 2116/7689 [5:15:38<12:35:42,  8.14s/it]

 28%|██▊       | 2117/7689 [5:15:46<12:33:03,  8.11s/it]

 28%|██▊       | 2118/7689 [5:15:59<14:29:50,  9.37s/it]

 28%|██▊       | 2119/7689 [5:16:06<13:40:33,  8.84s/it]

 28%|██▊       | 2120/7689 [5:16:20<16:07:26, 10.42s/it]

 28%|██▊       | 2121/7689 [5:16:28<14:36:12,  9.44s/it]

 28%|██▊       | 2122/7689 [5:16:40<15:45:38, 10.19s/it]

 28%|██▊       | 2123/7689 [5:16:48<15:03:35,  9.74s/it]

 28%|██▊       | 2124/7689 [5:16:57<14:26:40,  9.34s/it]

 28%|██▊       | 2125/7689 [5:17:01<12:13:44,  7.91s/it]

 28%|██▊       | 2126/7689 [5:17:16<15:13:43,  9.86s/it]

 28%|██▊       | 2127/7689 [5:17:25<14:50:51,  9.61s/it]

 28%|██▊       | 2128/7689 [5:17:36<15:30:39, 10.04s/it]

 28%|██▊       | 2129/7689 [5:17:41<13:17:30,  8.61s/it]

 28%|██▊       | 2130/7689 [5:17:51<14:00:52,  9.08s/it]

 28%|██▊       | 2131/7689 [5:18:00<13:49:25,  8.95s/it]

 28%|██▊       | 2132/7689 [5:18:14<16:12:04, 10.50s/it]

 28%|██▊       | 2133/7689 [5:18:20<14:22:32,  9.31s/it]

 28%|██▊       | 2134/7689 [5:18:30<14:23:00,  9.32s/it]

 28%|██▊       | 2135/7689 [5:18:48<18:37:19, 12.07s/it]

 28%|██▊       | 2136/7689 [5:18:59<17:50:01, 11.56s/it]

 28%|██▊       | 2137/7689 [5:19:07<16:05:38, 10.44s/it]

 28%|██▊       | 2138/7689 [5:19:15<15:01:40,  9.75s/it]

 28%|██▊       | 2139/7689 [5:19:23<14:15:45,  9.25s/it]

 28%|██▊       | 2140/7689 [5:19:30<13:10:52,  8.55s/it]

 28%|██▊       | 2141/7689 [5:19:40<14:09:36,  9.19s/it]

 28%|██▊       | 2142/7689 [5:19:47<13:00:55,  8.45s/it]

 28%|██▊       | 2143/7689 [5:19:55<12:58:18,  8.42s/it]

 28%|██▊       | 2144/7689 [5:20:06<13:46:45,  8.95s/it]

 28%|██▊       | 2145/7689 [5:20:10<11:53:56,  7.73s/it]

 28%|██▊       | 2146/7689 [5:20:20<12:43:23,  8.26s/it]

 28%|██▊       | 2147/7689 [5:20:27<12:09:46,  7.90s/it]

 28%|██▊       | 2148/7689 [5:20:35<12:03:34,  7.84s/it]

 28%|██▊       | 2149/7689 [5:20:54<17:32:17, 11.40s/it]
[2024-05-24 18:52:04,819] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2150/7689 [5:21:03<16:11:49, 10.53s/it]

 28%|██▊       | 2151/7689 [5:21:08<13:49:16,  8.98s/it]

 28%|██▊       | 2152/7689 [5:21:28<18:43:27, 12.17s/it]

 28%|██▊       | 2153/7689 [5:21:34<15:59:45, 10.40s/it]

 28%|██▊       | 2154/7689 [5:21:41<14:25:19,  9.38s/it]

 28%|██▊       | 2155/7689 [5:21:52<15:17:16,  9.95s/it]

 28%|██▊       | 2156/7689 [5:21:59<13:45:26,  8.95s/it]

 28%|██▊       | 2157/7689 [5:22:08<13:39:28,  8.89s/it]

 28%|██▊       | 2158/7689 [5:22:21<15:39:32, 10.19s/it]

 28%|██▊       | 2159/7689 [5:22:35<17:09:53, 11.17s/it]

 28%|██▊       | 2160/7689 [5:22:43<15:45:49, 10.26s/it]

 28%|██▊       | 2161/7689 [5:22:57<17:30:24, 11.40s/it]

 28%|██▊       | 2162/7689 [5:23:08<17:30:00, 11.40s/it]

 28%|██▊       | 2163/7689 [5:23:16<16:03:02, 10.46s/it]

 28%|██▊       | 2164/7689 [5:23:31<18:07:32, 11.81s/it]

 28%|██▊       | 2165/7689 [5:23:39<16:09:04, 10.53s/it]

 28%|██▊       | 2166/7689 [5:23:46<14:22:01,  9.36s/it]

 28%|██▊       | 2167/7689 [5:23:54<13:54:10,  9.06s/it]

 28%|██▊       | 2168/7689 [5:24:06<15:05:59,  9.85s/it]

 28%|██▊       | 2169/7689 [5:24:15<14:57:28,  9.76s/it]

 28%|██▊       | 2170/7689 [5:24:24<14:23:48,  9.39s/it]

 28%|██▊       | 2171/7689 [5:24:29<12:43:02,  8.30s/it]

 28%|██▊       | 2172/7689 [5:24:43<15:07:06,  9.87s/it]

 28%|██▊       | 2173/7689 [5:24:48<12:55:01,  8.43s/it]

 28%|██▊       | 2174/7689 [5:24:55<12:13:04,  7.98s/it]

 28%|██▊       | 2175/7689 [5:25:02<11:59:04,  7.82s/it]

 28%|██▊       | 2176/7689 [5:25:14<13:33:15,  8.85s/it]

 28%|██▊       | 2177/7689 [5:25:22<13:29:13,  8.81s/it]

 28%|██▊       | 2178/7689 [5:25:29<12:23:50,  8.10s/it]

 28%|██▊       | 2179/7689 [5:25:36<12:04:33,  7.89s/it]

 28%|██▊       | 2180/7689 [5:25:44<12:01:43,  7.86s/it]

 28%|██▊       | 2181/7689 [5:25:58<14:45:10,  9.64s/it]

 28%|██▊       | 2182/7689 [5:26:06<13:59:31,  9.15s/it]

 28%|██▊       | 2183/7689 [5:26:19<15:50:31, 10.36s/it]

 28%|██▊       | 2184/7689 [5:26:26<14:11:38,  9.28s/it]

 28%|██▊       | 2185/7689 [5:26:34<13:32:39,  8.86s/it]

 28%|██▊       | 2186/7689 [5:26:50<16:51:34, 11.03s/it]

 28%|██▊       | 2187/7689 [5:26:56<14:51:26,  9.72s/it]

 28%|██▊       | 2188/7689 [5:27:11<16:55:54, 11.08s/it]

 28%|██▊       | 2189/7689 [5:27:15<14:03:09,  9.20s/it]

 28%|██▊       | 2190/7689 [5:27:20<12:03:22,  7.89s/it]

 28%|██▊       | 2191/7689 [5:27:27<11:30:50,  7.54s/it]

 29%|██▊       | 2192/7689 [5:27:35<11:40:12,  7.64s/it]

 29%|██▊       | 2193/7689 [5:27:44<12:14:08,  8.01s/it]

 29%|██▊       | 2194/7689 [5:27:51<11:51:15,  7.77s/it]

 29%|██▊       | 2195/7689 [5:27:57<11:07:07,  7.29s/it]

 29%|██▊       | 2196/7689 [5:28:04<11:05:23,  7.27s/it]

 29%|██▊       | 2197/7689 [5:28:12<11:14:39,  7.37s/it]

 29%|██▊       | 2198/7689 [5:28:17<10:07:21,  6.64s/it]

 29%|██▊       | 2199/7689 [5:28:27<11:53:24,  7.80s/it]

 29%|██▊       | 2200/7689 [5:28:33<10:41:31,  7.01s/it]

 29%|██▊       | 2201/7689 [5:28:44<12:55:56,  8.48s/it]

 29%|██▊       | 2202/7689 [5:28:51<12:09:01,  7.97s/it]

 29%|██▊       | 2203/7689 [5:28:57<11:13:23,  7.36s/it]

 29%|██▊       | 2204/7689 [5:29:06<12:02:45,  7.91s/it]

 29%|██▊       | 2205/7689 [5:29:13<11:17:37,  7.41s/it]

 29%|██▊       | 2206/7689 [5:29:28<14:44:34,  9.68s/it]

 29%|██▊       | 2207/7689 [5:29:38<15:00:11,  9.85s/it]

 29%|██▊       | 2208/7689 [5:29:44<13:30:50,  8.88s/it]

 29%|██▊       | 2209/7689 [5:29:52<12:58:09,  8.52s/it]

 29%|██▊       | 2210/7689 [5:30:10<17:28:01, 11.48s/it]

 29%|██▉       | 2211/7689 [5:30:25<18:39:30, 12.26s/it]

 29%|██▉       | 2212/7689 [5:30:31<16:11:42, 10.64s/it]

 29%|██▉       | 2213/7689 [5:30:40<15:18:06, 10.06s/it]

 29%|██▉       | 2214/7689 [5:30:50<15:09:55,  9.97s/it]

 29%|██▉       | 2215/7689 [5:30:57<13:42:54,  9.02s/it]

 29%|██▉       | 2216/7689 [5:31:08<14:37:54,  9.62s/it]

 29%|██▉       | 2217/7689 [5:31:17<14:31:22,  9.55s/it]

 29%|██▉       | 2218/7689 [5:31:27<14:32:21,  9.57s/it]

 29%|██▉       | 2219/7689 [5:31:32<12:33:59,  8.27s/it]

 29%|██▉       | 2220/7689 [5:31:43<13:39:21,  8.99s/it]

 29%|██▉       | 2221/7689 [5:31:50<12:44:09,  8.38s/it]

 29%|██▉       | 2222/7689 [5:31:54<11:01:55,  7.26s/it]

 29%|██▉       | 2223/7689 [5:32:00<10:17:30,  6.78s/it]

 29%|██▉       | 2224/7689 [5:32:06<9:54:01,  6.52s/it]

 29%|██▉       | 2225/7689 [5:32:14<10:39:00,  7.02s/it]

 29%|██▉       | 2226/7689 [5:32:23<11:28:09,  7.56s/it]

 29%|██▉       | 2227/7689 [5:32:33<12:47:37,  8.43s/it]

 29%|██▉       | 2228/7689 [5:32:46<14:42:43,  9.70s/it]

 29%|██▉       | 2229/7689 [5:32:54<13:47:13,  9.09s/it]

 29%|██▉       | 2230/7689 [5:33:00<12:25:42,  8.20s/it]

 29%|██▉       | 2231/7689 [5:33:13<14:39:52,  9.67s/it]

 29%|██▉       | 2232/7689 [5:33:32<18:51:03, 12.44s/it]

 29%|██▉       | 2233/7689 [5:33:40<17:00:41, 11.22s/it]

 29%|██▉       | 2234/7689 [5:33:46<14:26:42,  9.53s/it]

 29%|██▉       | 2235/7689 [5:34:03<17:56:03, 11.84s/it]

 29%|██▉       | 2236/7689 [5:34:10<15:39:01, 10.33s/it]

 29%|██▉       | 2237/7689 [5:34:18<14:45:08,  9.74s/it]

 29%|██▉       | 2238/7689 [5:34:25<13:23:13,  8.84s/it]

 29%|██▉       | 2239/7689 [5:34:34<13:32:05,  8.94s/it]

 29%|██▉       | 2240/7689 [5:34:41<12:43:46,  8.41s/it]

 29%|██▉       | 2241/7689 [5:34:51<13:29:04,  8.91s/it]

 29%|██▉       | 2242/7689 [5:34:57<12:05:54,  8.00s/it]

 29%|██▉       | 2243/7689 [5:35:07<12:59:15,  8.59s/it]

 29%|██▉       | 2244/7689 [5:35:18<14:12:54,  9.40s/it]

 29%|██▉       | 2245/7689 [5:35:25<12:44:51,  8.43s/it]

 29%|██▉       | 2246/7689 [5:35:33<12:47:46,  8.46s/it]

 29%|██▉       | 2247/7689 [5:35:41<12:39:06,  8.37s/it]

 29%|██▉       | 2248/7689 [5:35:47<11:20:41,  7.51s/it]

 29%|██▉       | 2249/7689 [5:35:56<11:58:11,  7.92s/it]

 29%|██▉       | 2250/7689 [5:36:09<14:33:17,  9.63s/it]

 29%|██▉       | 2251/7689 [5:36:14<12:23:14,  8.20s/it]

 29%|██▉       | 2252/7689 [5:36:23<12:52:23,  8.52s/it]

 29%|██▉       | 2253/7689 [5:36:36<14:37:12,  9.68s/it]

 29%|██▉       | 2254/7689 [5:36:43<13:19:57,  8.83s/it]

 29%|██▉       | 2255/7689 [5:36:47<11:30:33,  7.62s/it]

 29%|██▉       | 2256/7689 [5:36:54<10:48:40,  7.16s/it]

 29%|██▉       | 2257/7689 [5:37:05<12:42:30,  8.42s/it]

 29%|██▉       | 2258/7689 [5:37:12<12:03:32,  7.99s/it]

 29%|██▉       | 2259/7689 [5:37:20<12:03:32,  7.99s/it]

 29%|██▉       | 2260/7689 [5:37:28<11:54:02,  7.89s/it]

 29%|██▉       | 2261/7689 [5:37:34<11:06:35,  7.37s/it]

 29%|██▉       | 2262/7689 [5:37:41<11:12:06,  7.43s/it]

 29%|██▉       | 2263/7689 [5:37:47<10:29:23,  6.96s/it]

 29%|██▉       | 2264/7689 [5:37:55<10:42:07,  7.10s/it]

 29%|██▉       | 2265/7689 [5:38:02<10:51:46,  7.21s/it]

 29%|██▉       | 2266/7689 [5:38:07<9:52:10,  6.55s/it]

 29%|██▉       | 2267/7689 [5:38:20<12:41:30,  8.43s/it]

 29%|██▉       | 2268/7689 [5:38:30<13:41:57,  9.10s/it]

 30%|██▉       | 2269/7689 [5:38:45<16:21:36, 10.87s/it]

 30%|██▉       | 2270/7689 [5:38:59<17:39:52, 11.74s/it]

 30%|██▉       | 2271/7689 [5:39:06<15:21:24, 10.20s/it]

 30%|██▉       | 2272/7689 [5:39:25<19:28:40, 12.94s/it]

 30%|██▉       | 2273/7689 [5:39:35<18:04:20, 12.01s/it]

 30%|██▉       | 2274/7689 [5:39:42<15:37:07, 10.38s/it]

 30%|██▉       | 2275/7689 [5:39:53<16:14:23, 10.80s/it]

 30%|██▉       | 2276/7689 [5:40:03<15:34:00, 10.35s/it]

 30%|██▉       | 2277/7689 [5:40:19<18:23:30, 12.23s/it]

 30%|██▉       | 2278/7689 [5:40:33<19:07:41, 12.73s/it]

 30%|██▉       | 2279/7689 [5:40:40<16:14:18, 10.81s/it]

 30%|██▉       | 2280/7689 [5:40:48<15:05:32, 10.04s/it]

 30%|██▉       | 2281/7689 [5:40:54<13:28:38,  8.97s/it]

 30%|██▉       | 2282/7689 [5:41:00<12:08:12,  8.08s/it]

 30%|██▉       | 2283/7689 [5:41:14<14:40:44,  9.78s/it]

 30%|██▉       | 2284/7689 [5:41:27<16:08:26, 10.75s/it]

 30%|██▉       | 2285/7689 [5:41:34<14:30:51,  9.67s/it]

 30%|██▉       | 2286/7689 [5:41:46<15:28:28, 10.31s/it]

 30%|██▉       | 2287/7689 [5:41:58<16:03:00, 10.70s/it]

 30%|██▉       | 2288/7689 [5:42:06<15:04:47, 10.05s/it]

 30%|██▉       | 2289/7689 [5:42:14<13:53:47,  9.26s/it]

 30%|██▉       | 2290/7689 [5:42:21<13:16:29,  8.85s/it]

 30%|██▉       | 2291/7689 [5:42:31<13:31:22,  9.02s/it]

 30%|██▉       | 2292/7689 [5:42:39<13:12:19,  8.81s/it]

 30%|██▉       | 2293/7689 [5:42:48<13:16:35,  8.86s/it]

 30%|██▉       | 2294/7689 [5:43:00<14:48:29,  9.88s/it]

 30%|██▉       | 2295/7689 [5:43:06<13:05:15,  8.73s/it]

 30%|██▉       | 2296/7689 [5:43:16<13:32:42,  9.04s/it]

 30%|██▉       | 2297/7689 [5:43:22<12:04:09,  8.06s/it]

 30%|██▉       | 2298/7689 [5:43:28<11:18:43,  7.55s/it]

 30%|██▉       | 2299/7689 [5:43:35<10:48:31,  7.22s/it]

 30%|██▉       | 2300/7689 [5:43:44<11:28:30,  7.67s/it]

 30%|██▉       | 2301/7689 [5:43:50<11:04:52,  7.40s/it]

 30%|██▉       | 2302/7689 [5:43:56<10:20:11,  6.91s/it]

 30%|██▉       | 2303/7689 [5:44:01<9:30:27,  6.35s/it]

 30%|██▉       | 2304/7689 [5:44:09<10:01:12,  6.70s/it]

 30%|██▉       | 2305/7689 [5:44:16<10:30:35,  7.03s/it]

 30%|██▉       | 2306/7689 [5:44:34<15:19:37, 10.25s/it]

 30%|███       | 2307/7689 [5:44:47<16:25:06, 10.98s/it]

 30%|███       | 2308/7689 [5:44:55<14:58:24, 10.02s/it]

 30%|███       | 2309/7689 [5:45:00<12:55:41,  8.65s/it]

 30%|███       | 2310/7689 [5:45:05<11:16:36,  7.55s/it]

 30%|███       | 2311/7689 [5:45:12<10:57:14,  7.33s/it]

 30%|███       | 2312/7689 [5:45:17<10:06:39,  6.77s/it]

 30%|███       | 2313/7689 [5:45:30<12:45:16,  8.54s/it]

 30%|███       | 2314/7689 [5:45:41<13:55:08,  9.32s/it]

 30%|███       | 2315/7689 [5:45:50<13:38:52,  9.14s/it]

 30%|███       | 2316/7689 [5:45:57<12:37:35,  8.46s/it]

 30%|███       | 2317/7689 [5:46:09<14:09:52,  9.49s/it]

 30%|███       | 2318/7689 [5:46:21<15:34:28, 10.44s/it]

 30%|███       | 2319/7689 [5:46:34<16:34:39, 11.11s/it]

 30%|███       | 2320/7689 [5:46:43<15:26:39, 10.36s/it]

 30%|███       | 2321/7689 [5:46:51<14:29:24,  9.72s/it]

 30%|███       | 2322/7689 [5:46:57<12:43:10,  8.53s/it]

 30%|███       | 2323/7689 [5:47:03<11:55:41,  8.00s/it]

 30%|███       | 2324/7689 [5:47:09<10:48:17,  7.25s/it]

 30%|███       | 2325/7689 [5:47:20<12:36:20,  8.46s/it]

 30%|███       | 2326/7689 [5:47:29<12:52:22,  8.64s/it]

 30%|███       | 2327/7689 [5:47:39<13:27:29,  9.04s/it]

 30%|███       | 2328/7689 [5:47:49<13:38:17,  9.16s/it]

 30%|███       | 2329/7689 [5:47:57<13:21:03,  8.97s/it]

 30%|███       | 2330/7689 [5:48:05<12:48:14,  8.60s/it]

 30%|███       | 2331/7689 [5:48:11<11:39:08,  7.83s/it]

 30%|███       | 2332/7689 [5:48:23<13:35:56,  9.14s/it]

 30%|███       | 2333/7689 [5:48:30<12:38:42,  8.50s/it]

 30%|███       | 2334/7689 [5:48:36<11:20:23,  7.62s/it]

 30%|███       | 2335/7689 [5:48:46<12:31:21,  8.42s/it]

 30%|███       | 2336/7689 [5:48:56<13:01:20,  8.76s/it]

 30%|███       | 2337/7689 [5:49:01<11:31:30,  7.75s/it]

 30%|███       | 2338/7689 [5:49:10<12:15:12,  8.24s/it]

 30%|███       | 2339/7689 [5:49:17<11:44:58,  7.91s/it]

 30%|███       | 2340/7689 [5:49:22<10:18:05,  6.93s/it]

 30%|███       | 2341/7689 [5:49:30<10:40:58,  7.19s/it]

 30%|███       | 2342/7689 [5:49:38<10:57:54,  7.38s/it]

 30%|███       | 2343/7689 [5:49:46<11:10:16,  7.52s/it]

 30%|███       | 2344/7689 [5:49:52<10:31:49,  7.09s/it]

 30%|███       | 2345/7689 [5:50:02<12:05:51,  8.15s/it]

 31%|███       | 2346/7689 [5:50:10<11:45:33,  7.92s/it]

 31%|███       | 2347/7689 [5:50:22<13:53:39,  9.36s/it]

 31%|███       | 2348/7689 [5:50:28<12:16:37,  8.28s/it]

 31%|███       | 2349/7689 [5:50:39<13:14:07,  8.92s/it]

 31%|███       | 2350/7689 [5:50:43<11:15:42,  7.59s/it]

 31%|███       | 2351/7689 [5:50:52<11:48:42,  7.97s/it]

 31%|███       | 2352/7689 [5:51:04<13:26:53,  9.07s/it]

 31%|███       | 2353/7689 [5:51:13<13:29:01,  9.10s/it]

 31%|███       | 2354/7689 [5:51:21<13:17:01,  8.96s/it]

 31%|███       | 2355/7689 [5:51:26<11:34:50,  7.82s/it]

 31%|███       | 2356/7689 [5:51:39<13:31:50,  9.13s/it]

 31%|███       | 2357/7689 [5:51:47<13:12:03,  8.91s/it]

 31%|███       | 2358/7689 [5:51:52<11:37:24,  7.85s/it]

 31%|███       | 2359/7689 [5:52:00<11:21:05,  7.67s/it]

 31%|███       | 2360/7689 [5:52:06<10:33:40,  7.13s/it]

 31%|███       | 2361/7689 [5:52:16<11:58:41,  8.09s/it]

 31%|███       | 2362/7689 [5:52:26<12:59:30,  8.78s/it]

 31%|███       | 2363/7689 [5:52:36<13:24:03,  9.06s/it]

 31%|███       | 2364/7689 [5:52:41<11:43:31,  7.93s/it]

 31%|███       | 2365/7689 [5:52:47<10:39:07,  7.20s/it]

 31%|███       | 2366/7689 [5:52:55<11:14:26,  7.60s/it]

 31%|███       | 2367/7689 [5:53:03<11:27:10,  7.75s/it]

 31%|███       | 2368/7689 [5:53:11<11:15:31,  7.62s/it]

 31%|███       | 2369/7689 [5:53:19<11:25:12,  7.73s/it]

 31%|███       | 2370/7689 [5:53:29<12:38:03,  8.55s/it]

 31%|███       | 2371/7689 [5:53:40<13:39:44,  9.25s/it]

 31%|███       | 2372/7689 [5:53:49<13:31:50,  9.16s/it]

 31%|███       | 2373/7689 [5:53:56<12:31:35,  8.48s/it]

 31%|███       | 2374/7689 [5:54:03<11:45:01,  7.96s/it]

 31%|███       | 2375/7689 [5:54:15<13:27:58,  9.12s/it]

 31%|███       | 2376/7689 [5:54:32<17:14:51, 11.69s/it]

 31%|███       | 2377/7689 [5:54:42<16:21:36, 11.09s/it]

 31%|███       | 2378/7689 [5:54:50<15:12:47, 10.31s/it]

 31%|███       | 2379/7689 [5:54:59<14:32:30,  9.86s/it]

 31%|███       | 2380/7689 [5:55:05<12:51:06,  8.71s/it]

 31%|███       | 2381/7689 [5:55:20<15:36:15, 10.58s/it]

 31%|███       | 2382/7689 [5:55:26<13:25:28,  9.11s/it]

 31%|███       | 2383/7689 [5:55:43<16:49:07, 11.41s/it]

 31%|███       | 2384/7689 [5:55:55<17:22:29, 11.79s/it]

 31%|███       | 2385/7689 [5:56:01<14:32:14,  9.87s/it]

 31%|███       | 2386/7689 [5:56:08<13:33:02,  9.20s/it]

 31%|███       | 2387/7689 [5:56:19<14:13:06,  9.65s/it]

 31%|███       | 2388/7689 [5:56:31<15:05:19, 10.25s/it]

 31%|███       | 2389/7689 [5:56:36<12:47:27,  8.69s/it]

 31%|███       | 2390/7689 [5:56:45<13:10:05,  8.95s/it]

 31%|███       | 2391/7689 [5:56:57<14:22:49,  9.77s/it]

 31%|███       | 2392/7689 [5:57:06<14:09:35,  9.62s/it]

 31%|███       | 2393/7689 [5:57:15<13:35:48,  9.24s/it]

 31%|███       | 2394/7689 [5:57:21<12:32:03,  8.52s/it]

 31%|███       | 2395/7689 [5:57:28<11:42:32,  7.96s/it]

 31%|███       | 2396/7689 [5:57:39<12:51:27,  8.75s/it]

 31%|███       | 2397/7689 [5:57:43<11:07:37,  7.57s/it]

 31%|███       | 2398/7689 [5:57:52<11:20:28,  7.72s/it]

 31%|███       | 2399/7689 [5:57:57<10:31:06,  7.16s/it]

 31%|███       | 2400/7689 [5:58:07<11:23:21,  7.75s/it]
 31%|███       | 2400/7689 [5:58:07<11:23:21,  7.75s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 31%|███       | 2401/7689 [5:58:50<27:05:30, 18.44s/it]

 31%|███       | 2402/7689 [5:59:02<24:22:05, 16.59s/it]

 31%|███▏      | 2403/7689 [5:59:09<20:13:00, 13.77s/it]

 31%|███▏      | 2404/7689 [5:59:19<18:32:34, 12.63s/it]

 31%|███▏      | 2405/7689 [5:59:27<16:21:48, 11.15s/it]

 31%|███▏      | 2406/7689 [5:59:32<13:32:35,  9.23s/it]

 31%|███▏      | 2407/7689 [5:59:37<11:38:27,  7.93s/it]

 31%|███▏      | 2408/7689 [5:59:41<10:08:31,  6.91s/it]

 31%|███▏      | 2409/7689 [5:59:51<11:25:31,  7.79s/it]

 31%|███▏      | 2410/7689 [6:00:03<13:09:03,  8.97s/it]

 31%|███▏      | 2411/7689 [6:00:09<11:48:23,  8.05s/it]

 31%|███▏      | 2412/7689 [6:00:17<11:43:39,  8.00s/it]

 31%|███▏      | 2413/7689 [6:00:23<10:53:00,  7.43s/it]

 31%|███▏      | 2414/7689 [6:00:42<16:01:30, 10.94s/it]

 31%|███▏      | 2415/7689 [6:00:49<14:32:41,  9.93s/it]

 31%|███▏      | 2416/7689 [6:00:58<13:50:19,  9.45s/it]

 31%|███▏      | 2417/7689 [6:01:10<14:52:50, 10.16s/it]

 31%|███▏      | 2418/7689 [6:01:19<14:31:22,  9.92s/it]

 31%|███▏      | 2419/7689 [6:01:29<14:29:19,  9.90s/it]

 31%|███▏      | 2420/7689 [6:01:38<14:01:18,  9.58s/it]

 31%|███▏      | 2421/7689 [6:01:44<12:49:48,  8.77s/it]

 31%|███▏      | 2422/7689 [6:01:50<11:18:56,  7.73s/it]

 32%|███▏      | 2423/7689 [6:02:01<12:54:19,  8.82s/it]

 32%|███▏      | 2424/7689 [6:02:10<12:56:07,  8.84s/it]

 32%|███▏      | 2425/7689 [6:02:19<12:57:00,  8.86s/it]

 32%|███▏      | 2426/7689 [6:02:25<11:52:49,  8.13s/it]

 32%|███▏      | 2427/7689 [6:02:31<11:00:28,  7.53s/it]

 32%|███▏      | 2428/7689 [6:02:41<11:42:05,  8.01s/it]

 32%|███▏      | 2429/7689 [6:02:47<11:11:09,  7.66s/it]

 32%|███▏      | 2430/7689 [6:02:56<11:34:34,  7.92s/it]

 32%|███▏      | 2431/7689 [6:03:05<12:10:02,  8.33s/it]

 32%|███▏      | 2432/7689 [6:03:14<12:19:02,  8.43s/it]

 32%|███▏      | 2433/7689 [6:03:19<10:55:52,  7.49s/it]

 32%|███▏      | 2434/7689 [6:03:31<12:49:39,  8.79s/it]

 32%|███▏      | 2435/7689 [6:03:37<11:42:34,  8.02s/it]

 32%|███▏      | 2436/7689 [6:03:44<11:17:20,  7.74s/it]

 32%|███▏      | 2437/7689 [6:03:50<10:19:18,  7.08s/it]

 32%|███▏      | 2438/7689 [6:03:59<11:07:20,  7.63s/it]

 32%|███▏      | 2439/7689 [6:04:07<11:31:11,  7.90s/it]

 32%|███▏      | 2440/7689 [6:04:13<10:31:22,  7.22s/it]

 32%|███▏      | 2441/7689 [6:04:21<11:05:23,  7.61s/it]

 32%|███▏      | 2442/7689 [6:04:33<12:57:20,  8.89s/it]

 32%|███▏      | 2443/7689 [6:04:39<11:38:39,  7.99s/it]

 32%|███▏      | 2444/7689 [6:04:47<11:37:02,  7.97s/it]

 32%|███▏      | 2445/7689 [6:04:57<12:20:40,  8.47s/it]

 32%|███▏      | 2446/7689 [6:05:04<11:46:00,  8.08s/it]

 32%|███▏      | 2447/7689 [6:05:13<12:21:45,  8.49s/it]

 32%|███▏      | 2448/7689 [6:05:22<12:20:35,  8.48s/it]

 32%|███▏      | 2449/7689 [6:05:38<15:37:49, 10.74s/it]

 32%|███▏      | 2450/7689 [6:05:49<15:42:54, 10.80s/it]

 32%|███▏      | 2451/7689 [6:05:56<13:58:53,  9.61s/it]

 32%|███▏      | 2452/7689 [6:06:03<12:59:44,  8.93s/it]

 32%|███▏      | 2453/7689 [6:06:16<14:49:43, 10.20s/it]

 32%|███▏      | 2454/7689 [6:06:25<14:15:04,  9.80s/it]

 32%|███▏      | 2455/7689 [6:06:41<16:46:37, 11.54s/it]

 32%|███▏      | 2456/7689 [6:06:50<15:44:54, 10.83s/it]

 32%|███▏      | 2457/7689 [6:06:59<14:50:18, 10.21s/it]

 32%|███▏      | 2458/7689 [6:07:04<12:39:52,  8.72s/it]

 32%|███▏      | 2459/7689 [6:07:10<11:34:04,  7.96s/it]

 32%|███▏      | 2460/7689 [6:07:18<11:30:06,  7.92s/it]

 32%|███▏      | 2461/7689 [6:07:26<11:32:06,  7.94s/it]

 32%|███▏      | 2462/7689 [6:07:33<11:09:55,  7.69s/it]

 32%|███▏      | 2463/7689 [6:07:40<10:41:36,  7.37s/it]

 32%|███▏      | 2464/7689 [6:07:47<10:44:45,  7.40s/it]

 32%|███▏      | 2465/7689 [6:07:53<10:00:45,  6.90s/it]

 32%|███▏      | 2466/7689 [6:08:08<13:44:48,  9.48s/it]

 32%|███▏      | 2467/7689 [6:08:18<13:50:34,  9.54s/it]

 32%|███▏      | 2468/7689 [6:08:24<12:29:49,  8.62s/it]

 32%|███▏      | 2469/7689 [6:08:33<12:33:35,  8.66s/it]

 32%|███▏      | 2470/7689 [6:08:40<11:46:27,  8.12s/it]

 32%|███▏      | 2471/7689 [6:08:53<13:40:35,  9.44s/it]

 32%|███▏      | 2472/7689 [6:09:02<13:30:08,  9.32s/it]

 32%|███▏      | 2473/7689 [6:09:10<13:04:05,  9.02s/it]

 32%|███▏      | 2474/7689 [6:09:22<14:16:59,  9.86s/it]

 32%|███▏      | 2475/7689 [6:09:28<12:45:29,  8.81s/it]

 32%|███▏      | 2476/7689 [6:09:34<11:41:51,  8.08s/it]

 32%|███▏      | 2477/7689 [6:09:44<12:24:15,  8.57s/it]

 32%|███▏      | 2478/7689 [6:09:51<11:48:55,  8.16s/it]

 32%|███▏      | 2479/7689 [6:10:04<13:46:13,  9.52s/it]

 32%|███▏      | 2480/7689 [6:10:15<14:26:06,  9.98s/it]

 32%|███▏      | 2481/7689 [6:10:23<13:35:50,  9.40s/it]

 32%|███▏      | 2482/7689 [6:10:30<12:41:35,  8.78s/it]

 32%|███▏      | 2483/7689 [6:10:45<15:06:15, 10.44s/it]

 32%|███▏      | 2484/7689 [6:10:59<16:38:51, 11.51s/it]

 32%|███▏      | 2485/7689 [6:11:08<15:28:39, 10.71s/it]

 32%|███▏      | 2486/7689 [6:11:15<13:57:58,  9.66s/it]

 32%|███▏      | 2487/7689 [6:11:21<12:29:17,  8.64s/it]

 32%|███▏      | 2488/7689 [6:11:33<13:58:46,  9.68s/it]

 32%|███▏      | 2489/7689 [6:11:42<13:34:08,  9.39s/it]

 32%|███▏      | 2490/7689 [6:11:48<11:57:55,  8.29s/it]

 32%|███▏      | 2491/7689 [6:11:56<11:57:03,  8.28s/it]

 32%|███▏      | 2492/7689 [6:12:08<13:27:06,  9.32s/it]

 32%|███▏      | 2493/7689 [6:12:16<13:02:03,  9.03s/it]

 32%|███▏      | 2494/7689 [6:12:24<12:46:19,  8.85s/it]

 32%|███▏      | 2495/7689 [6:12:32<12:10:59,  8.44s/it]

 32%|███▏      | 2496/7689 [6:12:46<14:37:14, 10.14s/it]

 32%|███▏      | 2497/7689 [6:12:51<12:33:02,  8.70s/it]

 32%|███▏      | 2498/7689 [6:12:58<11:42:47,  8.12s/it]

 33%|███▎      | 2499/7689 [6:13:07<12:07:05,  8.41s/it]

 33%|███▎      | 2500/7689 [6:13:14<11:27:32,  7.95s/it]

 33%|███▎      | 2501/7689 [6:13:20<10:33:26,  7.33s/it]

 33%|███▎      | 2502/7689 [6:13:26<10:00:09,  6.94s/it]

 33%|███▎      | 2503/7689 [6:13:33<10:04:22,  6.99s/it]

 33%|███▎      | 2504/7689 [6:13:48<13:25:08,  9.32s/it]

 33%|███▎      | 2505/7689 [6:13:54<11:52:45,  8.25s/it]

 33%|███▎      | 2506/7689 [6:14:02<12:03:57,  8.38s/it]

 33%|███▎      | 2507/7689 [6:14:08<10:51:27,  7.54s/it]

 33%|███▎      | 2508/7689 [6:14:14<10:20:49,  7.19s/it]

 33%|███▎      | 2509/7689 [6:14:22<10:38:22,  7.39s/it]

 33%|███▎      | 2510/7689 [6:14:33<11:59:16,  8.33s/it]

 33%|███▎      | 2511/7689 [6:14:42<12:24:27,  8.63s/it]

 33%|███▎      | 2512/7689 [6:14:49<11:38:03,  8.09s/it]

 33%|███▎      | 2513/7689 [6:14:59<12:23:55,  8.62s/it]

 33%|███▎      | 2514/7689 [6:15:07<12:21:38,  8.60s/it]

 33%|███▎      | 2515/7689 [6:15:14<11:43:00,  8.15s/it]

 33%|███▎      | 2516/7689 [6:15:21<11:03:14,  7.69s/it]

 33%|███▎      | 2517/7689 [6:15:26<10:02:42,  6.99s/it]

 33%|███▎      | 2518/7689 [6:15:34<10:32:45,  7.34s/it]

 33%|███▎      | 2519/7689 [6:15:40<9:50:51,  6.86s/it]

 33%|███▎      | 2520/7689 [6:15:51<11:40:58,  8.14s/it]

 33%|███▎      | 2521/7689 [6:15:59<11:28:45,  8.00s/it]

 33%|███▎      | 2522/7689 [6:16:11<13:06:20,  9.13s/it]

 33%|███▎      | 2523/7689 [6:16:22<13:48:24,  9.62s/it]

 33%|███▎      | 2524/7689 [6:16:31<13:32:45,  9.44s/it]

 33%|███▎      | 2525/7689 [6:16:39<13:12:42,  9.21s/it]

 33%|███▎      | 2526/7689 [6:16:47<12:26:39,  8.68s/it]

 33%|███▎      | 2527/7689 [6:16:58<13:23:11,  9.34s/it]

 33%|███▎      | 2528/7689 [6:17:07<13:20:06,  9.30s/it]

 33%|███▎      | 2529/7689 [6:17:18<14:07:21,  9.85s/it]

 33%|███▎      | 2530/7689 [6:17:24<12:36:12,  8.79s/it]

 33%|███▎      | 2531/7689 [6:17:31<11:47:15,  8.23s/it]

 33%|███▎      | 2532/7689 [6:17:37<10:41:21,  7.46s/it]

 33%|███▎      | 2533/7689 [6:17:44<10:22:17,  7.24s/it]

 33%|███▎      | 2534/7689 [6:17:50<10:06:26,  7.06s/it]

 33%|███▎      | 2535/7689 [6:18:02<12:17:55,  8.59s/it]

 33%|███▎      | 2536/7689 [6:18:16<14:20:42, 10.02s/it]

 33%|███▎      | 2537/7689 [6:18:22<12:46:12,  8.92s/it]

 33%|███▎      | 2538/7689 [6:18:34<14:00:30,  9.79s/it]

 33%|███▎      | 2539/7689 [6:18:40<12:36:11,  8.81s/it]

 33%|███▎      | 2540/7689 [6:18:49<12:29:37,  8.74s/it]

 33%|███▎      | 2541/7689 [6:18:56<11:56:32,  8.35s/it]

 33%|███▎      | 2542/7689 [6:19:02<10:56:43,  7.66s/it]

 33%|███▎      | 2543/7689 [6:19:11<11:25:45,  8.00s/it]

 33%|███▎      | 2544/7689 [6:19:23<13:00:59,  9.11s/it]

 33%|███▎      | 2545/7689 [6:19:30<12:04:00,  8.44s/it]

 33%|███▎      | 2546/7689 [6:19:34<10:22:12,  7.26s/it]

 33%|███▎      | 2547/7689 [6:19:43<11:11:45,  7.84s/it]

 33%|███▎      | 2548/7689 [6:19:48<9:57:54,  6.98s/it]

 33%|███▎      | 2549/7689 [6:19:54<9:26:11,  6.61s/it]

 33%|███▎      | 2550/7689 [6:20:05<11:17:10,  7.91s/it]

 33%|███▎      | 2551/7689 [6:20:12<10:40:35,  7.48s/it]

 33%|███▎      | 2552/7689 [6:20:19<10:33:01,  7.39s/it]

 33%|███▎      | 2553/7689 [6:20:28<11:23:54,  7.99s/it]

 33%|███▎      | 2554/7689 [6:20:40<13:03:36,  9.16s/it]

 33%|███▎      | 2555/7689 [6:20:50<13:12:31,  9.26s/it]

 33%|███▎      | 2556/7689 [6:21:00<13:36:15,  9.54s/it]

 33%|███▎      | 2557/7689 [6:21:07<12:35:42,  8.84s/it]

 33%|███▎      | 2558/7689 [6:21:18<13:33:50,  9.52s/it]

 33%|███▎      | 2559/7689 [6:21:27<13:17:05,  9.32s/it]

 33%|███▎      | 2560/7689 [6:21:36<13:13:57,  9.29s/it]

 33%|███▎      | 2561/7689 [6:21:43<12:05:24,  8.49s/it]

 33%|███▎      | 2562/7689 [6:21:50<11:42:53,  8.23s/it]

 33%|███▎      | 2563/7689 [6:21:56<10:35:23,  7.44s/it]

 33%|███▎      | 2564/7689 [6:22:04<10:47:33,  7.58s/it]

 33%|███▎      | 2565/7689 [6:22:10<10:10:25,  7.15s/it]

 33%|███▎      | 2566/7689 [6:22:19<10:57:17,  7.70s/it]

 33%|███▎      | 2567/7689 [6:22:31<12:47:38,  8.99s/it]

 33%|███▎      | 2568/7689 [6:22:37<11:25:53,  8.04s/it]

 33%|███▎      | 2569/7689 [6:22:46<11:55:43,  8.39s/it]

 33%|███▎      | 2570/7689 [6:22:53<11:26:10,  8.04s/it]

 33%|███▎      | 2571/7689 [6:23:05<12:48:44,  9.01s/it]

 33%|███▎      | 2572/7689 [6:23:12<12:08:47,  8.55s/it]

 33%|███▎      | 2573/7689 [6:23:18<11:10:20,  7.86s/it]

 33%|███▎      | 2574/7689 [6:23:27<11:38:18,  8.19s/it]

 33%|███▎      | 2575/7689 [6:23:35<11:30:04,  8.10s/it]

 34%|███▎      | 2576/7689 [6:23:43<11:15:17,  7.92s/it]

 34%|███▎      | 2577/7689 [6:23:50<10:59:03,  7.74s/it]

 34%|███▎      | 2578/7689 [6:24:01<12:13:59,  8.62s/it]

 34%|███▎      | 2579/7689 [6:24:09<12:13:12,  8.61s/it]

 34%|███▎      | 2580/7689 [6:24:15<11:05:28,  7.82s/it]

 34%|███▎      | 2581/7689 [6:24:27<12:40:17,  8.93s/it]

 34%|███▎      | 2582/7689 [6:24:39<13:56:31,  9.83s/it]

 34%|███▎      | 2583/7689 [6:24:44<12:07:04,  8.54s/it]

 34%|███▎      | 2584/7689 [6:24:50<10:56:07,  7.71s/it]

 34%|███▎      | 2585/7689 [6:25:01<12:31:38,  8.84s/it]

 34%|███▎      | 2586/7689 [6:25:06<10:51:29,  7.66s/it]

 34%|███▎      | 2587/7689 [6:25:13<10:17:08,  7.26s/it]

 34%|███▎      | 2588/7689 [6:25:25<12:38:32,  8.92s/it]

 34%|███▎      | 2589/7689 [6:25:32<11:39:22,  8.23s/it]

 34%|███▎      | 2590/7689 [6:25:37<10:21:55,  7.32s/it]

 34%|███▎      | 2591/7689 [6:25:44<10:03:05,  7.10s/it]

 34%|███▎      | 2592/7689 [6:25:50<9:34:50,  6.77s/it]

 34%|███▎      | 2593/7689 [6:26:01<11:25:47,  8.07s/it]

 34%|███▎      | 2594/7689 [6:26:13<13:05:30,  9.25s/it]

 34%|███▎      | 2595/7689 [6:26:19<11:37:54,  8.22s/it]

 34%|███▍      | 2596/7689 [6:26:25<10:43:57,  7.59s/it]

 34%|███▍      | 2597/7689 [6:26:31<10:18:04,  7.28s/it]

 34%|███▍      | 2598/7689 [6:26:40<10:45:56,  7.61s/it]

 34%|███▍      | 2599/7689 [6:26:46<10:18:10,  7.29s/it]

 34%|███▍      | 2600/7689 [6:26:56<11:19:40,  8.01s/it]

 34%|███▍      | 2601/7689 [6:27:04<11:15:22,  7.96s/it]

 34%|███▍      | 2602/7689 [6:27:16<12:59:01,  9.19s/it]

 34%|███▍      | 2603/7689 [6:27:29<14:41:43, 10.40s/it]

 34%|███▍      | 2604/7689 [6:27:37<13:33:08,  9.59s/it]

 34%|███▍      | 2605/7689 [6:27:45<13:01:30,  9.22s/it]

 34%|███▍      | 2606/7689 [6:27:51<11:25:41,  8.09s/it]

 34%|███▍      | 2607/7689 [6:28:00<11:50:55,  8.39s/it]

 34%|███▍      | 2608/7689 [6:28:09<12:23:16,  8.78s/it]

 34%|███▍      | 2609/7689 [6:28:16<11:34:05,  8.20s/it]

 34%|███▍      | 2610/7689 [6:28:24<11:15:36,  7.98s/it]

 34%|███▍      | 2611/7689 [6:28:30<10:38:25,  7.54s/it]

 34%|███▍      | 2612/7689 [6:28:38<10:40:08,  7.57s/it]

 34%|███▍      | 2613/7689 [6:28:53<13:40:12,  9.70s/it]

 34%|███▍      | 2614/7689 [6:29:01<12:55:46,  9.17s/it]

 34%|███▍      | 2615/7689 [6:29:11<13:27:52,  9.55s/it]

 34%|███▍      | 2616/7689 [6:29:20<13:03:32,  9.27s/it]

 34%|███▍      | 2617/7689 [6:29:24<11:11:31,  7.94s/it]

 34%|███▍      | 2618/7689 [6:29:32<11:07:30,  7.90s/it]

 34%|███▍      | 2619/7689 [6:29:44<12:50:15,  9.12s/it]

 34%|███▍      | 2620/7689 [6:29:53<12:40:44,  9.00s/it]

 34%|███▍      | 2621/7689 [6:30:01<12:16:12,  8.72s/it]

 34%|███▍      | 2622/7689 [6:30:08<11:44:43,  8.34s/it]

 34%|███▍      | 2623/7689 [6:30:23<14:10:10, 10.07s/it]

 34%|███▍      | 2624/7689 [6:30:29<12:42:58,  9.04s/it]

 34%|███▍      | 2625/7689 [6:30:36<11:55:38,  8.48s/it]

 34%|███▍      | 2626/7689 [6:30:42<10:36:27,  7.54s/it]

 34%|███▍      | 2627/7689 [6:30:47<9:46:07,  6.95s/it]

 34%|███▍      | 2628/7689 [6:30:52<8:55:08,  6.34s/it]

 34%|███▍      | 2629/7689 [6:31:00<9:38:49,  6.86s/it]

 34%|███▍      | 2630/7689 [6:31:11<11:10:38,  7.95s/it]

 34%|███▍      | 2631/7689 [6:31:16<9:57:06,  7.08s/it]

 34%|███▍      | 2632/7689 [6:31:26<11:04:59,  7.89s/it]

 34%|███▍      | 2633/7689 [6:31:35<11:44:09,  8.36s/it]

 34%|███▍      | 2634/7689 [6:31:43<11:27:45,  8.16s/it]

 34%|███▍      | 2635/7689 [6:31:55<13:04:37,  9.31s/it]

 34%|███▍      | 2636/7689 [6:32:03<12:37:21,  8.99s/it]

 34%|███▍      | 2637/7689 [6:32:10<11:55:04,  8.49s/it]

 34%|███▍      | 2638/7689 [6:32:15<10:21:38,  7.38s/it]

 34%|███▍      | 2639/7689 [6:32:23<10:45:27,  7.67s/it]


 34%|███▍      | 2640/7689 [6:32:31<10:29:20,  7.48s/it]

 34%|███▍      | 2641/7689 [6:32:36<9:31:26,  6.79s/it]

 34%|███▍      | 2642/7689 [6:32:51<12:57:57,  9.25s/it]

 34%|███▍      | 2643/7689 [6:32:56<11:09:14,  7.96s/it]

 34%|███▍      | 2644/7689 [6:33:02<10:38:22,  7.59s/it]

 34%|███▍      | 2645/7689 [6:33:13<11:57:12,  8.53s/it]

 34%|███▍      | 2646/7689 [6:33:31<16:05:12, 11.48s/it]

 34%|███▍      | 2647/7689 [6:33:40<14:51:00, 10.60s/it]

 34%|███▍      | 2648/7689 [6:33:46<12:52:25,  9.19s/it]

 34%|███▍      | 2649/7689 [6:33:54<12:30:06,  8.93s/it]

 34%|███▍      | 2650/7689 [6:34:09<14:53:39, 10.64s/it]

 34%|███▍      | 2651/7689 [6:34:18<14:08:22, 10.10s/it]

 34%|███▍      | 2652/7689 [6:34:28<14:20:16, 10.25s/it]

 35%|███▍      | 2653/7689 [6:34:34<12:33:19,  8.98s/it]

 35%|███▍      | 2654/7689 [6:34:42<12:07:14,  8.67s/it]

 35%|███▍      | 2655/7689 [6:34:50<11:45:24,  8.41s/it]

 35%|███▍      | 2656/7689 [6:34:57<11:03:52,  7.91s/it]

 35%|███▍      | 2657/7689 [6:35:12<13:58:02,  9.99s/it]

 35%|███▍      | 2658/7689 [6:35:19<13:03:15,  9.34s/it]

 35%|███▍      | 2659/7689 [6:35:31<14:06:53, 10.10s/it]

 35%|███▍      | 2660/7689 [6:35:39<13:09:39,  9.42s/it]

 35%|███▍      | 2661/7689 [6:35:44<11:15:27,  8.06s/it]

 35%|███▍      | 2662/7689 [6:35:51<10:51:06,  7.77s/it]

 35%|███▍      | 2663/7689 [6:36:00<11:22:21,  8.15s/it]

 35%|███▍      | 2664/7689 [6:36:12<12:52:33,  9.22s/it]

 35%|███▍      | 2665/7689 [6:36:19<11:47:37,  8.45s/it]

 35%|███▍      | 2666/7689 [6:36:26<11:19:43,  8.12s/it]

 35%|███▍      | 2667/7689 [6:36:32<10:18:54,  7.39s/it]

 35%|███▍      | 2668/7689 [6:36:44<12:11:28,  8.74s/it]

 35%|███▍      | 2669/7689 [6:36:55<13:08:37,  9.43s/it]

 35%|███▍      | 2670/7689 [6:37:01<11:59:30,  8.60s/it]

 35%|███▍      | 2671/7689 [6:37:07<10:57:57,  7.87s/it]

 35%|███▍      | 2672/7689 [6:37:15<10:47:21,  7.74s/it]

 35%|███▍      | 2673/7689 [6:37:22<10:36:14,  7.61s/it]

 35%|███▍      | 2674/7689 [6:37:30<10:48:28,  7.76s/it]

 35%|███▍      | 2675/7689 [6:37:45<13:42:29,  9.84s/it]

 35%|███▍      | 2676/7689 [6:37:54<13:28:46,  9.68s/it]

 35%|███▍      | 2677/7689 [6:38:04<13:38:31,  9.80s/it]

 35%|███▍      | 2678/7689 [6:38:20<16:02:06, 11.52s/it]

 35%|███▍      | 2679/7689 [6:38:27<14:06:19, 10.14s/it]

 35%|███▍      | 2680/7689 [6:38:33<12:33:15,  9.02s/it]

 35%|███▍      | 2681/7689 [6:38:41<12:14:09,  8.80s/it]

 35%|███▍      | 2682/7689 [6:38:47<11:04:07,  7.96s/it]

 35%|███▍      | 2683/7689 [6:38:55<10:57:51,  7.88s/it]

 35%|███▍      | 2684/7689 [6:39:01<10:12:01,  7.34s/it]

 35%|███▍      | 2685/7689 [6:39:10<10:36:08,  7.63s/it]

 35%|███▍      | 2686/7689 [6:39:15<9:44:35,  7.01s/it]

 35%|███▍      | 2687/7689 [6:39:22<9:43:56,  7.00s/it]

 35%|███▍      | 2688/7689 [6:39:29<9:39:44,  6.96s/it]

 35%|███▍      | 2689/7689 [6:39:42<12:03:43,  8.68s/it]

 35%|███▍      | 2690/7689 [6:39:55<14:00:05, 10.08s/it]

 35%|███▍      | 2691/7689 [6:40:01<12:24:34,  8.94s/it]

 35%|███▌      | 2692/7689 [6:40:06<10:35:09,  7.63s/it]

 35%|███▌      | 2693/7689 [6:40:12<9:56:31,  7.16s/it]

 35%|███▌      | 2694/7689 [6:40:19<9:55:52,  7.16s/it]

 35%|███▌      | 2695/7689 [6:40:27<10:05:31,  7.27s/it]

 35%|███▌      | 2696/7689 [6:40:33<9:52:34,  7.12s/it]

 35%|███▌      | 2697/7689 [6:40:40<9:47:55,  7.07s/it]

 35%|███▌      | 2698/7689 [6:40:46<9:09:18,  6.60s/it]

 35%|███▌      | 2699/7689 [6:40:55<10:18:50,  7.44s/it]

 35%|███▌      | 2700/7689 [6:41:07<12:09:21,  8.77s/it]
 35%|███▌      | 2700/7689 [6:41:07<12:09:21,  8.77s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 35%|███▌      | 2701/7689 [6:41:48<25:18:48, 18.27s/it]

 35%|███▌      | 2702/7689 [6:41:53<20:08:21, 14.54s/it]

 35%|███▌      | 2703/7689 [6:42:01<17:20:47, 12.52s/it]

 35%|███▌      | 2704/7689 [6:42:07<14:39:42, 10.59s/it]

 35%|███▌      | 2705/7689 [6:42:13<12:43:37,  9.19s/it]

 35%|███▌      | 2706/7689 [6:42:21<12:17:38,  8.88s/it]

 35%|███▌      | 2707/7689 [6:42:28<11:12:50,  8.10s/it]

 35%|███▌      | 2708/7689 [6:42:34<10:27:46,  7.56s/it]

 35%|███▌      | 2709/7689 [6:42:42<10:48:41,  7.82s/it]

 35%|███▌      | 2710/7689 [6:42:49<10:20:48,  7.48s/it]

 35%|███▌      | 2711/7689 [6:42:58<11:02:45,  7.99s/it]

 35%|███▌      | 2712/7689 [6:43:09<12:11:13,  8.82s/it]

 35%|███▌      | 2713/7689 [6:43:18<12:24:45,  8.98s/it]

 35%|███▌      | 2714/7689 [6:43:26<11:59:17,  8.67s/it]

 35%|███▌      | 2715/7689 [6:43:43<15:29:42, 11.21s/it]

 35%|███▌      | 2716/7689 [6:43:53<14:52:59, 10.77s/it]

 35%|███▌      | 2717/7689 [6:44:01<13:49:01, 10.00s/it]

 35%|███▌      | 2718/7689 [6:44:06<11:32:30,  8.36s/it]

 35%|███▌      | 2719/7689 [6:44:13<11:13:16,  8.13s/it]

 35%|███▌      | 2720/7689 [6:44:23<11:44:37,  8.51s/it]

 35%|███▌      | 2721/7689 [6:44:32<12:00:46,  8.71s/it]

 35%|███▌      | 2722/7689 [6:44:46<13:58:59, 10.13s/it]

 35%|███▌      | 2723/7689 [6:44:54<13:10:54,  9.56s/it]

 35%|███▌      | 2724/7689 [6:45:02<12:27:09,  9.03s/it]

 35%|███▌      | 2725/7689 [6:45:13<13:39:18,  9.90s/it]

 35%|███▌      | 2726/7689 [6:45:18<11:36:55,  8.43s/it]

 35%|███▌      | 2727/7689 [6:45:28<12:10:31,  8.83s/it]

 35%|███▌      | 2728/7689 [6:45:36<11:47:59,  8.56s/it]

 35%|███▌      | 2729/7689 [6:45:49<13:24:34,  9.73s/it]

 36%|███▌      | 2730/7689 [6:45:54<11:25:29,  8.29s/it]

 36%|███▌      | 2731/7689 [6:46:01<10:57:12,  7.95s/it]

 36%|███▌      | 2732/7689 [6:46:09<11:11:37,  8.13s/it]

 36%|███▌      | 2733/7689 [6:46:16<10:39:52,  7.75s/it]

 36%|███▌      | 2734/7689 [6:46:22<9:51:04,  7.16s/it]

 36%|███▌      | 2735/7689 [6:46:30<10:06:29,  7.35s/it]

 36%|███▌      | 2736/7689 [6:46:36<9:30:05,  6.91s/it]

 36%|███▌      | 2737/7689 [6:46:48<11:39:33,  8.48s/it]

 36%|███▌      | 2738/7689 [6:46:54<10:49:58,  7.88s/it]

 36%|███▌      | 2739/7689 [6:47:00<10:00:25,  7.28s/it]

 36%|███▌      | 2740/7689 [6:47:09<10:40:40,  7.77s/it]

 36%|███▌      | 2741/7689 [6:47:20<11:52:35,  8.64s/it]

 36%|███▌      | 2742/7689 [6:47:26<10:47:29,  7.85s/it]

 36%|███▌      | 2743/7689 [6:47:35<11:17:21,  8.22s/it]

 36%|███▌      | 2744/7689 [6:47:41<10:34:22,  7.70s/it]

 36%|███▌      | 2745/7689 [6:47:51<11:20:45,  8.26s/it]

 36%|███▌      | 2746/7689 [6:47:56<10:13:18,  7.44s/it]

 36%|███▌      | 2747/7689 [6:48:03<10:00:25,  7.29s/it]

 36%|███▌      | 2748/7689 [6:48:13<11:06:11,  8.09s/it]

 36%|███▌      | 2749/7689 [6:48:22<11:15:26,  8.20s/it]

 36%|███▌      | 2750/7689 [6:48:29<10:54:54,  7.96s/it]

 36%|███▌      | 2751/7689 [6:48:35<10:05:16,  7.35s/it]

 36%|███▌      | 2752/7689 [6:48:45<11:05:34,  8.09s/it]

 36%|███▌      | 2753/7689 [6:48:55<11:54:01,  8.68s/it]

 36%|███▌      | 2754/7689 [6:49:06<12:47:25,  9.33s/it]

 36%|███▌      | 2755/7689 [6:49:17<13:23:53,  9.78s/it]

 36%|███▌      | 2756/7689 [6:49:22<11:45:00,  8.57s/it]

 36%|███▌      | 2757/7689 [6:49:30<11:32:53,  8.43s/it]

 36%|███▌      | 2758/7689 [6:49:41<12:37:34,  9.22s/it]

 36%|███▌      | 2759/7689 [6:49:48<11:32:32,  8.43s/it]

 36%|███▌      | 2760/7689 [6:49:56<11:12:39,  8.19s/it]

 36%|███▌      | 2761/7689 [6:50:05<11:35:17,  8.47s/it]

 36%|███▌      | 2762/7689 [6:50:15<12:15:17,  8.95s/it]

 36%|███▌      | 2763/7689 [6:50:21<11:07:17,  8.13s/it]

 36%|███▌      | 2764/7689 [6:50:33<12:36:36,  9.22s/it]

 36%|███▌      | 2765/7689 [6:50:46<14:12:30, 10.39s/it]

 36%|███▌      | 2766/7689 [6:50:51<11:54:28,  8.71s/it]

 36%|███▌      | 2767/7689 [6:50:56<10:30:37,  7.69s/it]

 36%|███▌      | 2768/7689 [6:51:10<13:00:32,  9.52s/it]

 36%|███▌      | 2769/7689 [6:51:15<11:09:03,  8.16s/it]

 36%|███▌      | 2770/7689 [6:51:30<13:56:38, 10.21s/it]

 36%|███▌      | 2771/7689 [6:51:37<12:39:41,  9.27s/it]

 36%|███▌      | 2772/7689 [6:51:44<11:44:07,  8.59s/it]

 36%|███▌      | 2773/7689 [6:51:51<11:12:18,  8.21s/it]

 36%|███▌      | 2774/7689 [6:52:01<11:53:51,  8.71s/it]

 36%|███▌      | 2775/7689 [6:52:12<12:39:10,  9.27s/it]

 36%|███▌      | 2776/7689 [6:52:22<13:01:24,  9.54s/it]

 36%|███▌      | 2777/7689 [6:52:37<15:22:36, 11.27s/it]

 36%|███▌      | 2778/7689 [6:52:44<13:26:35,  9.85s/it]

 36%|███▌      | 2779/7689 [6:52:54<13:40:16, 10.02s/it]

 36%|███▌      | 2780/7689 [6:53:00<12:04:03,  8.85s/it]

 36%|███▌      | 2781/7689 [6:53:07<11:03:06,  8.11s/it]

 36%|███▌      | 2782/7689 [6:53:18<12:28:54,  9.16s/it]

 36%|███▌      | 2783/7689 [6:53:35<15:46:48, 11.58s/it]

 36%|███▌      | 2784/7689 [6:53:48<16:12:59, 11.90s/it]

 36%|███▌      | 2785/7689 [6:53:53<13:32:40,  9.94s/it]

 36%|███▌      | 2786/7689 [6:53:59<11:46:00,  8.64s/it]

 36%|███▌      | 2787/7689 [6:54:06<10:53:12,  8.00s/it]

 36%|███▋      | 2788/7689 [6:54:19<12:56:19,  9.50s/it]

 36%|███▋      | 2789/7689 [6:54:27<12:31:47,  9.21s/it]

 36%|███▋      | 2790/7689 [6:54:34<11:44:54,  8.63s/it]

 36%|███▋      | 2791/7689 [6:54:44<12:04:36,  8.88s/it]

 36%|███▋      | 2792/7689 [6:54:51<11:33:37,  8.50s/it]

 36%|███▋      | 2793/7689 [6:54:59<11:12:44,  8.24s/it]

 36%|███▋      | 2794/7689 [6:55:13<13:32:48,  9.96s/it]

 36%|███▋      | 2795/7689 [6:55:20<12:24:01,  9.12s/it]

 36%|███▋      | 2796/7689 [6:55:31<13:00:42,  9.57s/it]

 36%|███▋      | 2797/7689 [6:55:40<12:51:11,  9.46s/it]

 36%|███▋      | 2798/7689 [6:55:48<12:23:02,  9.12s/it]

 36%|███▋      | 2799/7689 [6:55:55<11:22:52,  8.38s/it]

 36%|███▋      | 2800/7689 [6:56:04<11:28:33,  8.45s/it]

 36%|███▋      | 2801/7689 [6:56:15<12:49:05,  9.44s/it]

 36%|███▋      | 2802/7689 [6:56:24<12:33:33,  9.25s/it]

 36%|███▋      | 2803/7689 [6:56:32<11:50:02,  8.72s/it]

 36%|███▋      | 2804/7689 [6:56:39<11:04:10,  8.16s/it]

 36%|███▋      | 2805/7689 [6:56:47<11:07:42,  8.20s/it]

 36%|███▋      | 2806/7689 [6:56:52<9:57:51,  7.35s/it]

 37%|███▋      | 2807/7689 [6:57:01<10:26:22,  7.70s/it]

 37%|███▋      | 2808/7689 [6:57:10<11:15:28,  8.30s/it]

 37%|███▋      | 2809/7689 [6:57:17<10:22:38,  7.66s/it]

 37%|███▋      | 2810/7689 [6:57:25<10:32:03,  7.77s/it]

 37%|███▋      | 2811/7689 [6:57:32<10:16:48,  7.59s/it]

 37%|███▋      | 2812/7689 [6:57:44<12:14:15,  9.03s/it]

 37%|███▋      | 2813/7689 [6:57:50<10:45:54,  7.95s/it]

 37%|███▋      | 2814/7689 [6:57:56<10:09:42,  7.50s/it]

 37%|███▋      | 2815/7689 [6:58:02<9:23:51,  6.94s/it]

 37%|███▋      | 2816/7689 [6:58:19<13:32:43, 10.01s/it]

 37%|███▋      | 2817/7689 [6:58:27<12:54:56,  9.54s/it]

 37%|███▋      | 2818/7689 [6:58:35<11:58:58,  8.86s/it]

 37%|███▋      | 2819/7689 [6:58:41<11:08:50,  8.24s/it]

 37%|███▋      | 2820/7689 [6:58:54<12:54:28,  9.54s/it]

 37%|███▋      | 2821/7689 [6:59:02<12:07:49,  8.97s/it]

 37%|███▋      | 2822/7689 [6:59:08<11:05:46,  8.21s/it]

 37%|███▋      | 2823/7689 [6:59:16<11:02:45,  8.17s/it]

 37%|███▋      | 2824/7689 [6:59:21<9:47:13,  7.24s/it]

 37%|███▋      | 2825/7689 [6:59:34<11:55:56,  8.83s/it]

 37%|███▋      | 2826/7689 [6:59:47<13:36:24, 10.07s/it]

 37%|███▋      | 2827/7689 [6:59:57<13:30:55, 10.01s/it]

 37%|███▋      | 2828/7689 [7:00:02<11:51:14,  8.78s/it]

 37%|███▋      | 2829/7689 [7:00:07<10:08:39,  7.51s/it]

 37%|███▋      | 2830/7689 [7:00:13<9:35:28,  7.11s/it]

 37%|███▋      | 2831/7689 [7:00:23<10:52:12,  8.06s/it]

 37%|███▋      | 2832/7689 [7:00:31<10:38:00,  7.88s/it]

 37%|███▋      | 2833/7689 [7:00:39<10:47:36,  8.00s/it]

 37%|███▋      | 2834/7689 [7:00:45<9:54:23,  7.35s/it]

 37%|███▋      | 2835/7689 [7:00:51<9:11:06,  6.81s/it]

 37%|███▋      | 2836/7689 [7:01:01<10:31:41,  7.81s/it]

 37%|███▋      | 2837/7689 [7:01:12<11:54:46,  8.84s/it]

 37%|███▋      | 2838/7689 [7:01:19<11:22:29,  8.44s/it]

 37%|███▋      | 2839/7689 [7:01:26<10:28:38,  7.78s/it]

 37%|███▋      | 2840/7689 [7:01:35<10:59:15,  8.16s/it]

 37%|███▋      | 2841/7689 [7:01:53<14:57:47, 11.11s/it]

 37%|███▋      | 2842/7689 [7:02:00<13:21:05,  9.92s/it]

 37%|███▋      | 2843/7689 [7:02:07<12:09:04,  9.03s/it]

 37%|███▋      | 2844/7689 [7:02:13<11:07:54,  8.27s/it]

 37%|███▋      | 2845/7689 [7:02:22<11:22:18,  8.45s/it]

 37%|███▋      | 2846/7689 [7:02:30<11:17:16,  8.39s/it]

 37%|███▋      | 2847/7689 [7:02:40<11:34:40,  8.61s/it]

 37%|███▋      | 2848/7689 [7:02:45<10:15:12,  7.63s/it]

 37%|███▋      | 2849/7689 [7:02:52<10:02:33,  7.47s/it]

 37%|███▋      | 2850/7689 [7:02:59<9:43:31,  7.24s/it]

 37%|███▋      | 2851/7689 [7:03:09<10:45:55,  8.01s/it]

 37%|███▋      | 2852/7689 [7:03:18<11:12:11,  8.34s/it]

 37%|███▋      | 2853/7689 [7:03:23<10:12:16,  7.60s/it]

 37%|███▋      | 2854/7689 [7:03:43<14:53:14, 11.08s/it]

 37%|███▋      | 2855/7689 [7:03:51<13:43:50, 10.23s/it]

 37%|███▋      | 2856/7689 [7:03:57<11:51:52,  8.84s/it]

 37%|███▋      | 2857/7689 [7:04:06<12:02:18,  8.97s/it]

 37%|███▋      | 2858/7689 [7:04:11<10:41:42,  7.97s/it]

 37%|███▋      | 2859/7689 [7:04:18<10:11:41,  7.60s/it]

 37%|███▋      | 2860/7689 [7:04:37<14:33:14, 10.85s/it]

 37%|███▋      | 2861/7689 [7:04:44<13:02:07,  9.72s/it]

 37%|███▋      | 2862/7689 [7:04:50<11:39:17,  8.69s/it]

 37%|███▋      | 2863/7689 [7:04:56<10:34:34,  7.89s/it]

 37%|███▋      | 2864/7689 [7:05:09<12:47:41,  9.55s/it]

 37%|███▋      | 2865/7689 [7:05:24<14:38:57, 10.93s/it]

 37%|███▋      | 2866/7689 [7:05:37<15:35:11, 11.63s/it]

 37%|███▋      | 2867/7689 [7:05:45<14:14:56, 10.64s/it]

 37%|███▋      | 2868/7689 [7:05:54<13:31:04, 10.09s/it]

 37%|███▋      | 2869/7689 [7:06:01<12:27:58,  9.31s/it]

 37%|███▋      | 2870/7689 [7:06:12<13:07:40,  9.81s/it]

 37%|███▋      | 2871/7689 [7:06:20<12:04:02,  9.02s/it]

 37%|███▋      | 2872/7689 [7:06:28<11:59:30,  8.96s/it]

 37%|███▋      | 2873/7689 [7:06:36<11:28:38,  8.58s/it]

 37%|███▋      | 2874/7689 [7:06:45<11:43:55,  8.77s/it]

 37%|███▋      | 2875/7689 [7:06:54<11:51:05,  8.86s/it]

 37%|███▋      | 2876/7689 [7:07:01<11:05:27,  8.30s/it]

 37%|███▋      | 2877/7689 [7:07:09<11:00:01,  8.23s/it]

 37%|███▋      | 2878/7689 [7:07:26<14:30:38, 10.86s/it]

 37%|███▋      | 2879/7689 [7:07:37<14:11:33, 10.62s/it]

 37%|███▋      | 2880/7689 [7:07:45<13:27:06, 10.07s/it]

 37%|███▋      | 2881/7689 [7:07:52<11:58:22,  8.96s/it]

 37%|███▋      | 2882/7689 [7:07:58<10:55:52,  8.19s/it]

 37%|███▋      | 2883/7689 [7:08:12<13:16:20,  9.94s/it]

 38%|███▊      | 2884/7689 [7:08:18<11:46:51,  8.83s/it]

 38%|███▊      | 2885/7689 [7:08:27<11:39:58,  8.74s/it]

 38%|███▊      | 2886/7689 [7:08:35<11:27:37,  8.59s/it]

 38%|███▊      | 2887/7689 [7:08:42<10:43:22,  8.04s/it]

 38%|███▊      | 2888/7689 [7:08:59<14:33:19, 10.91s/it]

 38%|███▊      | 2889/7689 [7:09:09<14:08:48, 10.61s/it]

 38%|███▊      | 2890/7689 [7:09:22<14:46:43, 11.09s/it]

 38%|███▊      | 2891/7689 [7:09:27<12:38:33,  9.49s/it]

 38%|███▊      | 2892/7689 [7:09:37<12:52:00,  9.66s/it]

 38%|███▊      | 2893/7689 [7:09:46<12:16:20,  9.21s/it]

 38%|███▊      | 2894/7689 [7:09:54<11:47:54,  8.86s/it]

 38%|███▊      | 2895/7689 [7:10:06<13:01:37,  9.78s/it]

 38%|███▊      | 2896/7689 [7:10:12<11:32:25,  8.67s/it]

 38%|███▊      | 2897/7689 [7:10:19<11:05:40,  8.33s/it]

 38%|███▊      | 2898/7689 [7:10:26<10:38:38,  8.00s/it]

 38%|███▊      | 2899/7689 [7:10:39<12:18:42,  9.25s/it]

 38%|███▊      | 2900/7689 [7:10:46<11:29:00,  8.63s/it]

 38%|███▊      | 2901/7689 [7:10:55<11:41:37,  8.79s/it]

 38%|███▊      | 2902/7689 [7:11:05<12:19:12,  9.27s/it]

 38%|███▊      | 2903/7689 [7:11:15<12:34:41,  9.46s/it]

 38%|███▊      | 2904/7689 [7:11:22<11:31:25,  8.67s/it]

 38%|███▊      | 2905/7689 [7:11:31<11:48:57,  8.89s/it]

 38%|███▊      | 2906/7689 [7:11:39<11:26:38,  8.61s/it]

 38%|███▊      | 2907/7689 [7:11:45<10:19:21,  7.77s/it]

 38%|███▊      | 2908/7689 [7:11:52<9:53:04,  7.44s/it]

 38%|███▊      | 2909/7689 [7:11:58<9:24:11,  7.08s/it]

 38%|███▊      | 2910/7689 [7:12:07<9:59:35,  7.53s/it]

 38%|███▊      | 2911/7689 [7:12:14<9:59:57,  7.53s/it]

 38%|███▊      | 2912/7689 [7:12:20<9:18:14,  7.01s/it]

 38%|███▊      | 2913/7689 [7:12:26<8:45:41,  6.60s/it]

 38%|███▊      | 2914/7689 [7:12:35<9:46:10,  7.37s/it]

 38%|███▊      | 2915/7689 [7:12:42<9:39:42,  7.29s/it]

 38%|███▊      | 2916/7689 [7:12:57<12:52:13,  9.71s/it]

 38%|███▊      | 2917/7689 [7:13:07<12:41:56,  9.58s/it]

 38%|███▊      | 2918/7689 [7:13:16<12:41:14,  9.57s/it]

 38%|███▊      | 2919/7689 [7:13:23<11:30:14,  8.68s/it]

 38%|███▊      | 2920/7689 [7:13:30<10:48:53,  8.16s/it]

 38%|███▊      | 2921/7689 [7:13:41<12:03:14,  9.10s/it]

 38%|███▊      | 2922/7689 [7:13:49<11:33:46,  8.73s/it]

 38%|███▊      | 2923/7689 [7:13:54<10:08:03,  7.65s/it]

 38%|███▊      | 2924/7689 [7:14:10<13:20:27, 10.08s/it]

 38%|███▊      | 2925/7689 [7:14:19<13:11:46,  9.97s/it]

 38%|███▊      | 2926/7689 [7:14:35<15:23:30, 11.63s/it]

 38%|███▊      | 2927/7689 [7:14:47<15:26:15, 11.67s/it]

 38%|███▊      | 2928/7689 [7:14:56<14:39:17, 11.08s/it]

 38%|███▊      | 2929/7689 [7:15:03<12:53:59,  9.76s/it]

 38%|███▊      | 2930/7689 [7:15:11<12:12:43,  9.24s/it]

 38%|███▊      | 2931/7689 [7:15:19<11:46:02,  8.90s/it]

 38%|███▊      | 2932/7689 [7:15:27<11:20:16,  8.58s/it]

 38%|███▊      | 2933/7689 [7:15:40<13:14:20, 10.02s/it]

 38%|███▊      | 2934/7689 [7:15:50<12:52:15,  9.74s/it]

 38%|███▊      | 2935/7689 [7:15:56<11:33:05,  8.75s/it]

 38%|███▊      | 2936/7689 [7:16:09<13:27:01, 10.19s/it]

 38%|███▊      | 2937/7689 [7:16:21<14:09:32, 10.73s/it]

 38%|███▊      | 2938/7689 [7:16:28<12:32:57,  9.51s/it]

 38%|███▊      | 2939/7689 [7:16:38<12:38:37,  9.58s/it]

 38%|███▊      | 2940/7689 [7:16:45<11:41:27,  8.86s/it]

 38%|███▊      | 2941/7689 [7:16:53<11:23:14,  8.63s/it]

 38%|███▊      | 2942/7689 [7:17:00<10:50:34,  8.22s/it]

 38%|███▊      | 2943/7689 [7:17:09<10:56:48,  8.30s/it]

 38%|███▊      | 2944/7689 [7:17:15<10:03:17,  7.63s/it]

 38%|███▊      | 2945/7689 [7:17:27<11:40:20,  8.86s/it]

 38%|███▊      | 2946/7689 [7:17:33<10:32:15,  8.00s/it]

 38%|███▊      | 2947/7689 [7:17:39<10:02:56,  7.63s/it]

 38%|███▊      | 2948/7689 [7:17:54<12:50:51,  9.76s/it]

 38%|███▊      | 2949/7689 [7:18:02<12:13:22,  9.28s/it]

 38%|███▊      | 2950/7689 [7:18:12<12:29:02,  9.48s/it]

 38%|███▊      | 2951/7689 [7:18:18<10:53:11,  8.27s/it]

 38%|███▊      | 2952/7689 [7:18:28<11:41:37,  8.89s/it]

 38%|███▊      | 2953/7689 [7:18:34<10:34:39,  8.04s/it]

 38%|███▊      | 2954/7689 [7:18:43<11:02:36,  8.40s/it]

 38%|███▊      | 2955/7689 [7:18:56<12:47:38,  9.73s/it]

 38%|███▊      | 2956/7689 [7:19:03<11:42:36,  8.91s/it]

 38%|███▊      | 2957/7689 [7:19:08<10:13:31,  7.78s/it]

 38%|███▊      | 2958/7689 [7:19:14<9:29:03,  7.22s/it]

 38%|███▊      | 2959/7689 [7:19:29<12:30:02,  9.51s/it]

 38%|███▊      | 2960/7689 [7:19:38<12:15:14,  9.33s/it]

 39%|███▊      | 2961/7689 [7:19:48<12:20:44,  9.40s/it]

 39%|███▊      | 2962/7689 [7:20:00<13:23:35, 10.20s/it]

 39%|███▊      | 2963/7689 [7:20:08<12:40:24,  9.65s/it]

 39%|███▊      | 2964/7689 [7:20:18<12:45:55,  9.73s/it]

 39%|███▊      | 2965/7689 [7:20:24<11:09:18,  8.50s/it]

 39%|███▊      | 2966/7689 [7:20:38<13:31:48, 10.31s/it]

 39%|███▊      | 2967/7689 [7:20:47<13:07:14, 10.00s/it]

 39%|███▊      | 2968/7689 [7:20:54<11:46:12,  8.98s/it]

 39%|███▊      | 2969/7689 [7:21:06<12:55:09,  9.85s/it]

 39%|███▊      | 2970/7689 [7:21:17<13:19:58, 10.17s/it]

 39%|███▊      | 2971/7689 [7:21:22<11:23:36,  8.69s/it]

 39%|███▊      | 2972/7689 [7:21:36<13:30:30, 10.31s/it]

 39%|███▊      | 2973/7689 [7:21:43<11:57:24,  9.13s/it]

 39%|███▊      | 2974/7689 [7:21:55<13:09:42, 10.05s/it]

 39%|███▊      | 2975/7689 [7:22:02<11:52:49,  9.07s/it]

 39%|███▊      | 2976/7689 [7:22:09<11:05:11,  8.47s/it]

 39%|███▊      | 2977/7689 [7:22:14<10:04:31,  7.70s/it]

 39%|███▊      | 2978/7689 [7:22:19<8:50:37,  6.76s/it]

 39%|███▊      | 2979/7689 [7:22:33<11:44:12,  8.97s/it]

 39%|███▉      | 2980/7689 [7:22:42<11:40:09,  8.92s/it]

 39%|███▉      | 2981/7689 [7:22:50<11:16:55,  8.63s/it]

 39%|███▉      | 2982/7689 [7:23:00<11:43:27,  8.97s/it]

 39%|███▉      | 2983/7689 [7:23:13<13:36:40, 10.41s/it]

 39%|███▉      | 2984/7689 [7:23:20<12:07:44,  9.28s/it]

 39%|███▉      | 2985/7689 [7:23:32<13:12:01, 10.10s/it]

 39%|███▉      | 2986/7689 [7:23:45<14:21:32, 10.99s/it]

 39%|███▉      | 2987/7689 [7:23:57<14:44:07, 11.28s/it]

 39%|███▉      | 2988/7689 [7:24:04<13:01:30,  9.97s/it]

 39%|███▉      | 2989/7689 [7:24:12<12:23:29,  9.49s/it]

 39%|███▉      | 2990/7689 [7:24:20<11:30:04,  8.81s/it]

 39%|███▉      | 2991/7689 [7:24:31<12:39:43,  9.70s/it]

 39%|███▉      | 2992/7689 [7:24:43<13:12:52, 10.13s/it]

 39%|███▉      | 2993/7689 [7:24:54<13:35:29, 10.42s/it]

 39%|███▉      | 2994/7689 [7:25:04<13:41:01, 10.49s/it]

 39%|███▉      | 2995/7689 [7:25:14<13:20:36, 10.23s/it]
{'loss': 1.0695, 'grad_norm': 0.19452210683983376, 'learning_rate': 0.0001395392032867438, 'epoch': 0.39}


 39%|███▉      | 2997/7689 [7:25:34<13:34:45, 10.42s/it]

 39%|███▉      | 2998/7689 [7:25:41<12:04:43,  9.27s/it]
{'loss': 0.8712, 'grad_norm': 0.18137552715161925, 'learning_rate': 0.0001394230980535066, 'epoch': 0.39}


 39%|███▉      | 3000/7689 [7:26:10<15:23:13, 11.81s/it]
 39%|███▉      | 3000/7689 [7:26:10<15:23:13, 11.81s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 39%|███▉      | 3001/7689 [7:26:45<24:41:49, 18.97s/it]

 39%|███▉      | 3002/7689 [7:26:53<20:25:24, 15.69s/it]

 39%|███▉      | 3003/7689 [7:27:05<18:37:36, 14.31s/it]

 39%|███▉      | 3004/7689 [7:27:11<15:25:37, 11.85s/it]

 39%|███▉      | 3005/7689 [7:27:17<13:13:17, 10.16s/it]

 39%|███▉      | 3006/7689 [7:27:21<11:01:29,  8.48s/it]
{'loss': 1.0204, 'grad_norm': 0.2087914188727617, 'learning_rate': 0.00013911317680939824, 'epoch': 0.39}

 39%|███▉      | 3007/7689 [7:27:28<10:25:11,  8.01s/it]


 39%|███▉      | 3009/7689 [7:27:42<9:38:15,  7.41s/it]

 39%|███▉      | 3010/7689 [7:27:49<9:25:47,  7.26s/it]

 39%|███▉      | 3011/7689 [7:27:55<9:03:40,  6.97s/it]

 39%|███▉      | 3012/7689 [7:28:05<10:14:32,  7.88s/it]

 39%|███▉      | 3013/7689 [7:28:15<10:58:15,  8.45s/it]

 39%|███▉      | 3014/7689 [7:28:29<13:10:54, 10.15s/it]

 39%|███▉      | 3015/7689 [7:28:35<11:41:15,  9.00s/it]

 39%|███▉      | 3016/7689 [7:28:45<11:51:09,  9.13s/it]
{'loss': 0.9435, 'grad_norm': 0.20527623124956076, 'learning_rate': 0.00013872515104101823, 'epoch': 0.39}


 39%|███▉      | 3018/7689 [7:29:01<11:36:40,  8.95s/it]
{'loss': 0.9824, 'grad_norm': 0.198887705897611, 'learning_rate': 0.00013864746320957697, 'epoch': 0.39}


 39%|███▉      | 3020/7689 [7:29:22<12:35:18,  9.71s/it]

 39%|███▉      | 3021/7689 [7:29:35<13:54:12, 10.72s/it]

 39%|███▉      | 3022/7689 [7:29:49<15:14:16, 11.75s/it]

 39%|███▉      | 3023/7689 [7:29:55<13:04:18, 10.09s/it]

 39%|███▉      | 3024/7689 [7:30:05<13:04:18, 10.09s/it]

 39%|███▉      | 3025/7689 [7:30:11<11:29:17,  8.87s/it]

 39%|███▉      | 3026/7689 [7:30:22<12:14:02,  9.45s/it]

 39%|███▉      | 3027/7689 [7:30:30<11:39:21,  9.00s/it]

 39%|███▉      | 3028/7689 [7:30:36<10:34:42,  8.17s/it]

 39%|███▉      | 3029/7689 [7:30:43<9:53:51,  7.65s/it]

 39%|███▉      | 3030/7689 [7:30:50<9:51:23,  7.62s/it]

 39%|███▉      | 3031/7689 [7:30:58<9:52:37,  7.63s/it]

 39%|███▉      | 3032/7689 [7:31:07<10:23:01,  8.03s/it]

 39%|███▉      | 3033/7689 [7:31:16<10:41:05,  8.26s/it]

 39%|███▉      | 3034/7689 [7:31:26<11:36:14,  8.97s/it]
{'loss': 0.9424, 'grad_norm': 0.1774305144352227, 'learning_rate': 0.00013802497769084706, 'epoch': 0.39}


 39%|███▉      | 3036/7689 [7:31:44<11:33:07,  8.94s/it]

 39%|███▉      | 3037/7689 [7:31:54<11:54:37,  9.22s/it]
{'loss': 0.8058, 'grad_norm': 0.18304497538822787, 'learning_rate': 0.0001379080685075781, 'epoch': 0.39}


 40%|███▉      | 3039/7689 [7:32:14<12:51:58,  9.96s/it]

 40%|███▉      | 3040/7689 [7:32:28<14:25:19, 11.17s/it]

 40%|███▉      | 3041/7689 [7:32:38<14:09:09, 10.96s/it]

 40%|███▉      | 3042/7689 [7:32:47<13:14:21, 10.26s/it]

 40%|███▉      | 3043/7689 [7:32:54<11:56:43,  9.26s/it]

 40%|███▉      | 3044/7689 [7:33:03<11:50:19,  9.18s/it]
{'loss': 1.1922, 'grad_norm': 0.19554436103239467, 'learning_rate': 0.00013763504531004529, 'epoch': 0.4}


 40%|███▉      | 3046/7689 [7:33:26<13:10:30, 10.22s/it]

 40%|███▉      | 3047/7689 [7:33:34<12:32:32,  9.73s/it]
{'loss': 1.055, 'grad_norm': 0.17617596819889159, 'learning_rate': 0.0001375179350595669, 'epoch': 0.4}


 40%|███▉      | 3049/7689 [7:33:56<13:41:05, 10.62s/it]

 40%|███▉      | 3050/7689 [7:34:04<12:47:18,  9.92s/it]

 40%|███▉      | 3051/7689 [7:34:10<11:10:22,  8.67s/it]

 40%|███▉      | 3052/7689 [7:34:19<11:18:07,  8.77s/it]

 40%|███▉      | 3053/7689 [7:34:25<10:12:28,  7.93s/it]

 40%|███▉      | 3054/7689 [7:34:38<12:03:34,  9.37s/it]

 40%|███▉      | 3055/7689 [7:34:46<11:39:20,  9.05s/it]

 40%|███▉      | 3056/7689 [7:34:52<10:29:27,  8.15s/it]

 40%|███▉      | 3057/7689 [7:35:03<11:29:55,  8.94s/it]

 40%|███▉      | 3058/7689 [7:35:12<11:19:25,  8.80s/it]

 40%|███▉      | 3059/7689 [7:35:19<10:50:10,  8.43s/it]

 40%|███▉      | 3060/7689 [7:35:29<11:29:13,  8.93s/it]

 40%|███▉      | 3061/7689 [7:35:34<9:54:57,  7.71s/it]
{'loss': 1.1582, 'grad_norm': 0.2040042978070986, 'learning_rate': 0.000136970631371364, 'epoch': 0.4}

 40%|███▉      | 3062/7689 [7:35:49<12:40:51,  9.87s/it]


 40%|███▉      | 3064/7689 [7:36:01<10:02:47,  7.82s/it]

 40%|███▉      | 3065/7689 [7:36:10<10:30:24,  8.18s/it]
{'loss': 1.0662, 'grad_norm': 0.19339112667607017, 'learning_rate': 0.0001368140218918168, 'epoch': 0.4}


 40%|███▉      | 3067/7689 [7:36:31<12:06:03,  9.43s/it]

 40%|███▉      | 3068/7689 [7:36:38<10:49:00,  8.43s/it]

 40%|███▉      | 3069/7689 [7:36:46<10:46:32,  8.40s/it]

 40%|███▉      | 3070/7689 [7:36:52<9:50:52,  7.68s/it]

 40%|███▉      | 3071/7689 [7:37:01<10:31:01,  8.20s/it]
{'loss': 1.0625, 'grad_norm': 0.195860680028277, 'learning_rate': 0.0001365789118416066, 'epoch': 0.4}


 40%|███▉      | 3073/7689 [7:37:21<11:30:54,  8.98s/it]
{'loss': 0.9797, 'grad_norm': 0.18571522298198995, 'learning_rate': 0.0001365004898257782, 'epoch': 0.4}

 40%|███▉      | 3074/7689 [7:37:29<11:14:59,  8.78s/it]


 40%|████      | 3076/7689 [7:37:48<11:42:33,  9.14s/it]
{'loss': 0.9508, 'grad_norm': 0.1847749179490835, 'learning_rate': 0.000136382808244199, 'epoch': 0.4}


 40%|████      | 3078/7689 [7:38:08<13:06:15, 10.23s/it]
{'loss': 0.8322, 'grad_norm': 0.20120986671948443, 'learning_rate': 0.0001363043215657946, 'epoch': 0.4}


 40%|████      | 3080/7689 [7:38:35<14:37:39, 11.43s/it]

 40%|████      | 3081/7689 [7:38:47<15:05:35, 11.79s/it]

 40%|████      | 3082/7689 [7:38:55<13:17:32, 10.39s/it]

 40%|████      | 3083/7689 [7:39:00<11:26:29,  8.94s/it]

 40%|████      | 3084/7689 [7:39:11<12:02:55,  9.42s/it]

 40%|████      | 3085/7689 [7:39:17<11:01:44,  8.62s/it]

 40%|████      | 3086/7689 [7:39:27<11:20:13,  8.87s/it]

 40%|████      | 3087/7689 [7:39:33<10:29:02,  8.20s/it]

 40%|████      | 3088/7689 [7:39:39<9:21:52,  7.33s/it]

 40%|████      | 3089/7689 [7:39:46<9:13:24,  7.22s/it]

 40%|████      | 3090/7689 [7:39:54<9:35:53,  7.51s/it]

 40%|████      | 3091/7689 [7:40:04<10:26:08,  8.17s/it]

 40%|████      | 3092/7689 [7:40:15<11:35:38,  9.08s/it]
{'loss': 0.9842, 'grad_norm': 0.16371120871725853, 'learning_rate': 0.00013575419644721258, 'epoch': 0.4}


 40%|████      | 3094/7689 [7:40:30<10:31:20,  8.24s/it]

 40%|████      | 3095/7689 [7:40:44<12:38:57,  9.91s/it]

 40%|████      | 3096/7689 [7:41:00<15:04:32, 11.82s/it]
[2024-05-24 21:12:10,407] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|████      | 3097/7689 [7:41:06<12:40:44,  9.94s/it]

 40%|████      | 3098/7689 [7:41:15<12:26:28,  9.76s/it]

 40%|████      | 3099/7689 [7:41:21<11:02:19,  8.66s/it]

 40%|████      | 3100/7689 [7:41:34<12:43:33,  9.98s/it]

 40%|████      | 3101/7689 [7:41:45<12:54:02, 10.12s/it]

 40%|████      | 3102/7689 [7:41:54<12:32:23,  9.84s/it]

 40%|████      | 3103/7689 [7:42:07<13:44:57, 10.79s/it]

 40%|████      | 3104/7689 [7:42:19<14:16:05, 11.20s/it]

 40%|████      | 3105/7689 [7:42:37<16:55:33, 13.29s/it]

 40%|████      | 3106/7689 [7:42:44<14:40:05, 11.52s/it]

 40%|████      | 3107/7689 [7:42:52<13:13:17, 10.39s/it]

 40%|████      | 3108/7689 [7:43:01<12:35:07,  9.89s/it]

 40%|████      | 3109/7689 [7:43:10<12:06:41,  9.52s/it]

 40%|████      | 3110/7689 [7:43:21<12:42:30,  9.99s/it]

 40%|████      | 3111/7689 [7:43:27<11:09:43,  8.78s/it]
{'loss': 1.0353, 'grad_norm': 0.195051700684359, 'learning_rate': 0.00013500561279754686, 'epoch': 0.4}

 40%|████      | 3112/7689 [7:43:35<11:07:27,  8.75s/it]


 40%|████      | 3114/7689 [7:43:58<12:38:49,  9.95s/it]

 41%|████      | 3115/7689 [7:44:06<11:36:02,  9.13s/it]

 41%|████      | 3116/7689 [7:44:10<9:54:27,  7.80s/it]

 41%|████      | 3117/7689 [7:44:20<10:44:15,  8.45s/it]

 41%|████      | 3118/7689 [7:44:32<12:03:24,  9.50s/it]

 41%|████      | 3119/7689 [7:44:42<12:15:19,  9.65s/it]

 41%|████      | 3120/7689 [7:44:50<11:27:26,  9.03s/it]

 41%|████      | 3121/7689 [7:44:57<10:39:31,  8.40s/it]

 41%|████      | 3122/7689 [7:45:05<10:32:42,  8.31s/it]

 41%|████      | 3123/7689 [7:45:10<9:27:30,  7.46s/it]

 41%|████      | 3124/7689 [7:45:18<9:31:53,  7.52s/it]

 41%|████      | 3125/7689 [7:45:27<10:09:26,  8.01s/it]

 41%|████      | 3126/7689 [7:45:36<10:39:11,  8.40s/it]

 41%|████      | 3127/7689 [7:45:43<10:00:18,  7.90s/it]

 41%|████      | 3128/7689 [7:45:53<10:44:36,  8.48s/it]

 41%|████      | 3129/7689 [7:46:08<13:06:30, 10.35s/it]

 41%|████      | 3130/7689 [7:46:14<11:42:59,  9.25s/it]

 41%|████      | 3131/7689 [7:46:20<10:14:59,  8.10s/it]
{'loss': 1.1461, 'grad_norm': 0.2059377856355566, 'learning_rate': 0.00013421520846856644, 'epoch': 0.41}


 41%|████      | 3133/7689 [7:46:37<10:04:23,  7.96s/it]

 41%|████      | 3134/7689 [7:46:44<10:00:21,  7.91s/it]

 41%|████      | 3135/7689 [7:46:52<10:01:43,  7.93s/it]

 41%|████      | 3136/7689 [7:47:00<9:57:08,  7.87s/it]

 41%|████      | 3137/7689 [7:47:08<10:05:49,  7.99s/it]

 41%|████      | 3138/7689 [7:47:18<10:46:39,  8.53s/it]

 41%|████      | 3139/7689 [7:47:25<9:59:26,  7.90s/it]
{'loss': 0.9748, 'grad_norm': 0.19726143458333306, 'learning_rate': 0.00013389836361973697, 'epoch': 0.41}

 41%|████      | 3140/7689 [7:47:31<9:35:59,  7.60s/it]


 41%|████      | 3142/7689 [7:47:47<9:35:03,  7.59s/it]

 41%|████      | 3143/7689 [7:47:55<9:38:23,  7.63s/it]

 41%|████      | 3144/7689 [7:48:05<10:35:20,  8.39s/it]
{'loss': 0.997, 'grad_norm': 0.19914004041421546, 'learning_rate': 0.00013370013987418162, 'epoch': 0.41}


 41%|████      | 3146/7689 [7:48:27<12:11:28,  9.66s/it]

 41%|████      | 3147/7689 [7:48:33<10:55:28,  8.66s/it]

 41%|████      | 3148/7689 [7:48:42<11:13:52,  8.90s/it]

 41%|████      | 3149/7689 [7:48:55<12:33:23,  9.96s/it]
{'loss': 1.118, 'grad_norm': 0.21132464249300723, 'learning_rate': 0.0001335017666336706, 'epoch': 0.41}


 41%|████      | 3151/7689 [7:49:21<15:08:41, 12.01s/it]

 41%|████      | 3152/7689 [7:49:31<14:36:21, 11.59s/it]

 41%|████      | 3153/7689 [7:49:41<13:56:17, 11.06s/it]
{'loss': 0.8153, 'grad_norm': 0.18534370598519204, 'learning_rate': 0.00013334296099622498, 'epoch': 0.41}


 41%|████      | 3155/7689 [7:49:55<11:00:21,  8.74s/it]

 41%|████      | 3156/7689 [7:50:02<10:33:00,  8.38s/it]

 41%|████      | 3157/7689 [7:50:19<13:54:32, 11.05s/it]

 41%|████      | 3158/7689 [7:50:28<12:57:39, 10.30s/it]
{'loss': 1.0108, 'grad_norm': 0.207886351002265, 'learning_rate': 0.00013314432088281253, 'epoch': 0.41}


 41%|████      | 3160/7689 [7:50:41<10:26:31,  8.30s/it]

 41%|████      | 3161/7689 [7:50:47<9:19:51,  7.42s/it]

 41%|████      | 3162/7689 [7:50:58<10:41:13,  8.50s/it]

 41%|████      | 3163/7689 [7:51:07<10:55:39,  8.69s/it]

 41%|████      | 3164/7689 [7:51:12<9:46:56,  7.78s/it]
{'loss': 1.1481, 'grad_norm': 0.2288656296625138, 'learning_rate': 0.00013290575874559926, 'epoch': 0.41}


 41%|████      | 3166/7689 [7:51:23<8:16:19,  6.58s/it]

 41%|████      | 3167/7689 [7:51:30<8:20:33,  6.64s/it]

 41%|████      | 3168/7689 [7:51:38<8:54:12,  7.09s/it]

 41%|████      | 3169/7689 [7:51:49<10:12:01,  8.12s/it]

 41%|████      | 3170/7689 [7:51:55<9:19:54,  7.43s/it]

 41%|████      | 3171/7689 [7:52:08<11:42:27,  9.33s/it]

 41%|████▏     | 3172/7689 [7:52:14<10:24:34,  8.30s/it]

 41%|████▏     | 3173/7689 [7:52:21<9:49:41,  7.83s/it]
{'loss': 0.9684, 'grad_norm': 0.19074342978759296, 'learning_rate': 0.0001325475218945856, 'epoch': 0.41}


 41%|████▏     | 3175/7689 [7:52:41<11:12:33,  8.94s/it]
{'loss': 0.9335, 'grad_norm': 0.18859082348425216, 'learning_rate': 0.00013246784999607376, 'epoch': 0.41}


 41%|████▏     | 3177/7689 [7:53:03<12:29:51,  9.97s/it]

 41%|████▏     | 3178/7689 [7:53:09<10:51:23,  8.66s/it]

 41%|████▏     | 3179/7689 [7:53:21<12:09:17,  9.70s/it]
{'loss': 0.8685, 'grad_norm': 0.1935656289749626, 'learning_rate': 0.00013230843712192463, 'epoch': 0.41}


 41%|████▏     | 3181/7689 [7:53:40<12:43:51, 10.17s/it]

 41%|████▏     | 3182/7689 [7:53:49<12:07:44,  9.69s/it]
{'loss': 1.145, 'grad_norm': 0.19219658749738358, 'learning_rate': 0.00013218881724659187, 'epoch': 0.41}


 41%|████▏     | 3184/7689 [7:54:15<14:12:26, 11.35s/it]

 41%|████▏     | 3185/7689 [7:54:25<13:24:34, 10.72s/it]

 41%|████▏     | 3186/7689 [7:54:31<11:51:34,  9.48s/it]

 41%|████▏     | 3187/7689 [7:54:38<10:58:22,  8.77s/it]

 41%|████▏     | 3188/7689 [7:54:49<11:31:24,  9.22s/it]

 41%|████▏     | 3189/7689 [7:55:00<12:12:34,  9.77s/it]

 41%|████▏     | 3190/7689 [7:55:09<11:50:04,  9.47s/it]

 42%|████▏     | 3191/7689 [7:55:19<12:19:43,  9.87s/it]

 42%|████▏     | 3192/7689 [7:55:25<10:37:35,  8.51s/it]
{'loss': 1.0446, 'grad_norm': 0.2022344130845364, 'learning_rate': 0.0001317897141466483, 'epoch': 0.42}


 42%|████▏     | 3194/7689 [7:55:40<9:57:54,  7.98s/it]

 42%|████▏     | 3195/7689 [7:55:48<10:14:11,  8.20s/it]

 42%|████▏     | 3196/7689 [7:55:53<8:57:09,  7.17s/it]
{'loss': 1.1397, 'grad_norm': 0.21366240885387275, 'learning_rate': 0.00013162991456725943, 'epoch': 0.42}


 42%|████▏     | 3198/7689 [7:56:05<8:09:28,  6.54s/it]
{'loss': 0.9783, 'grad_norm': 0.2138071016686404, 'learning_rate': 0.00013154998107446753, 'epoch': 0.42}


 42%|████▏     | 3200/7689 [7:56:27<10:40:31,  8.56s/it]

 42%|████▏     | 3201/7689 [7:56:38<11:52:25,  9.52s/it]

 42%|████▏     | 3202/7689 [7:56:47<11:35:37,  9.30s/it]

 42%|████▏     | 3203/7689 [7:56:53<10:17:24,  8.26s/it]

 42%|████▏     | 3204/7689 [7:56:59<9:33:59,  7.68s/it]
{'loss': 0.9191, 'grad_norm': 0.2040808299838502, 'learning_rate': 0.000131310046464558, 'epoch': 0.42}


 42%|████▏     | 3206/7689 [7:57:14<9:19:05,  7.48s/it]

 42%|████▏     | 3207/7689 [7:57:24<10:15:18,  8.24s/it]

 42%|████▏     | 3208/7689 [7:57:41<13:32:59, 10.89s/it]

 42%|████▏     | 3209/7689 [7:57:53<13:59:25, 11.24s/it]

 42%|████▏     | 3210/7689 [7:57:58<11:42:02,  9.40s/it]

 42%|████▏     | 3211/7689 [7:58:07<11:37:57,  9.35s/it]

 42%|████▏     | 3212/7689 [7:58:13<10:23:46,  8.36s/it]

 42%|████▏     | 3213/7689 [7:58:29<13:10:52, 10.60s/it]

 42%|████▏     | 3214/7689 [7:58:37<12:13:04,  9.83s/it]
{'loss': 0.8652, 'grad_norm': 0.19360868922602342, 'learning_rate': 0.000130909711749823, 'epoch': 0.42}


 42%|████▏     | 3216/7689 [7:58:52<10:31:01,  8.46s/it]

 42%|████▏     | 3217/7689 [7:58:59<9:50:58,  7.93s/it]

 42%|████▏     | 3218/7689 [7:59:05<9:08:14,  7.36s/it]

 42%|████▏     | 3219/7689 [7:59:19<11:33:24,  9.31s/it]

 42%|████▏     | 3220/7689 [7:59:27<11:14:13,  9.05s/it]

 42%|████▏     | 3221/7689 [7:59:32<9:44:11,  7.84s/it]
{'loss': 0.9229, 'grad_norm': 0.1936330474788969, 'learning_rate': 0.00013062915068949118, 'epoch': 0.42}


 42%|████▏     | 3223/7689 [7:59:46<9:08:15,  7.37s/it]

 42%|████▏     | 3224/7689 [7:59:57<10:42:43,  8.64s/it]

 42%|████▏     | 3225/7689 [8:00:04<9:59:13,  8.05s/it]

 42%|████▏     | 3226/7689 [8:00:09<8:58:24,  7.24s/it]

 42%|████▏     | 3227/7689 [8:00:18<9:28:40,  7.65s/it]

 42%|████▏     | 3228/7689 [8:00:29<10:41:35,  8.63s/it]
{'loss': 0.9641, 'grad_norm': 0.1730014015253547, 'learning_rate': 0.0001303483233202431, 'epoch': 0.42}


 42%|████▏     | 3230/7689 [8:00:45<9:54:10,  8.00s/it]
{'loss': 1.0181, 'grad_norm': 0.1868226996039037, 'learning_rate': 0.0001302680383567833, 'epoch': 0.42}


 42%|████▏     | 3232/7689 [8:01:07<12:31:36, 10.12s/it]

 42%|████▏     | 3233/7689 [8:01:15<11:42:58,  9.47s/it]

 42%|████▏     | 3234/7689 [8:01:28<12:55:39, 10.45s/it]
{'loss': 1.0374, 'grad_norm': 0.17758704865236827, 'learning_rate': 0.0001301074040372242, 'epoch': 0.42}


 42%|████▏     | 3236/7689 [8:01:54<14:32:04, 11.75s/it]

 42%|████▏     | 3237/7689 [8:02:07<14:58:38, 12.11s/it]

 42%|████▏     | 3238/7689 [8:02:17<14:15:26, 11.53s/it]

 42%|████▏     | 3239/7689 [8:02:22<11:51:22,  9.59s/it]

 42%|████▏     | 3240/7689 [8:02:28<10:35:42,  8.57s/it]

 42%|████▏     | 3241/7689 [8:02:36<10:11:52,  8.25s/it]

 42%|████▏     | 3242/7689 [8:02:47<11:26:28,  9.26s/it]

 42%|████▏     | 3243/7689 [8:02:59<12:23:49, 10.04s/it]

 42%|████▏     | 3244/7689 [8:03:06<11:14:33,  9.11s/it]

 42%|████▏     | 3245/7689 [8:03:13<10:22:10,  8.40s/it]

 42%|████▏     | 3246/7689 [8:03:26<12:09:40,  9.85s/it]

 42%|████▏     | 3247/7689 [8:03:33<11:11:35,  9.07s/it]

 42%|████▏     | 3248/7689 [8:03:52<14:38:32, 11.87s/it]

 42%|████▏     | 3249/7689 [8:03:57<12:14:34,  9.93s/it]

 42%|████▏     | 3250/7689 [8:04:04<11:01:02,  8.93s/it]

 42%|████▏     | 3251/7689 [8:04:09<9:28:08,  7.68s/it]

 42%|████▏     | 3252/7689 [8:04:17<9:43:13,  7.89s/it]

 42%|████▏     | 3253/7689 [8:04:26<10:00:49,  8.13s/it]

 42%|████▏     | 3254/7689 [8:04:35<10:40:40,  8.67s/it]

 42%|████▏     | 3255/7689 [8:04:42<10:02:43,  8.16s/it]

 42%|████▏     | 3256/7689 [8:04:50<9:49:40,  7.98s/it]

 42%|████▏     | 3257/7689 [8:05:00<10:38:29,  8.64s/it]

 42%|████▏     | 3258/7689 [8:05:07<10:01:06,  8.14s/it]

 42%|████▏     | 3259/7689 [8:05:13<9:19:25,  7.58s/it]

 42%|████▏     | 3260/7689 [8:05:20<8:57:27,  7.28s/it]
{'loss': 0.8503, 'grad_norm': 0.1828896588730269, 'learning_rate': 0.00012906121786616294, 'epoch': 0.42}


 42%|████▏     | 3262/7689 [8:05:39<10:06:03,  8.21s/it]
{'loss': 1.0836, 'grad_norm': 0.20293545939314123, 'learning_rate': 0.00012898059601959268, 'epoch': 0.42}

 42%|████▏     | 3263/7689 [8:05:51<11:19:27,  9.21s/it]


 42%|████▏     | 3265/7689 [8:06:11<12:05:33,  9.84s/it]

 42%|████▏     | 3266/7689 [8:06:22<12:09:34,  9.90s/it]

 42%|████▏     | 3267/7689 [8:06:36<13:52:36, 11.30s/it]

 43%|████▎     | 3268/7689 [8:06:44<12:39:14, 10.30s/it]

 43%|████▎     | 3269/7689 [8:06:52<11:46:05,  9.58s/it]

 43%|████▎     | 3270/7689 [8:07:09<14:32:14, 11.84s/it]

 43%|████▎     | 3271/7689 [8:07:19<13:59:46, 11.40s/it]

 43%|████▎     | 3272/7689 [8:07:34<15:16:30, 12.45s/it]

 43%|████▎     | 3273/7689 [8:07:43<13:57:15, 11.38s/it]

 43%|████▎     | 3274/7689 [8:07:56<14:31:57, 11.85s/it]

 43%|████▎     | 3275/7689 [8:08:06<13:43:13, 11.19s/it]

 43%|████▎     | 3276/7689 [8:08:14<12:25:28, 10.14s/it]

 43%|████▎     | 3277/7689 [8:08:20<11:05:09,  9.05s/it]

 43%|████▎     | 3278/7689 [8:08:26<9:55:48,  8.10s/it]

 43%|████▎     | 3279/7689 [8:08:33<9:27:03,  7.72s/it]

 43%|████▎     | 3280/7689 [8:08:40<9:07:38,  7.45s/it]

 43%|████▎     | 3281/7689 [8:08:44<8:05:29,  6.61s/it]

 43%|████▎     | 3282/7689 [8:08:52<8:36:06,  7.03s/it]

 43%|████▎     | 3283/7689 [8:08:59<8:40:22,  7.09s/it]

 43%|████▎     | 3284/7689 [8:09:11<10:10:49,  8.32s/it]

 43%|████▎     | 3285/7689 [8:09:20<10:39:29,  8.71s/it]

 43%|████▎     | 3286/7689 [8:09:30<11:02:25,  9.03s/it]

 43%|████▎     | 3287/7689 [8:09:50<15:02:02, 12.30s/it]

 43%|████▎     | 3288/7689 [8:09:56<12:48:29, 10.48s/it]

 43%|████▎     | 3289/7689 [8:10:10<13:55:15, 11.39s/it]

 43%|████▎     | 3290/7689 [8:10:20<13:29:25, 11.04s/it]

 43%|████▎     | 3291/7689 [8:10:28<12:21:33, 10.12s/it]

 43%|████▎     | 3292/7689 [8:10:38<12:30:49, 10.25s/it]

 43%|████▎     | 3293/7689 [8:10:49<12:34:04, 10.29s/it]

 43%|████▎     | 3294/7689 [8:11:00<13:02:19, 10.68s/it]

 43%|████▎     | 3295/7689 [8:11:10<12:29:50, 10.24s/it]

 43%|████▎     | 3296/7689 [8:11:16<11:10:35,  9.16s/it]

 43%|████▎     | 3297/7689 [8:11:22<9:46:59,  8.02s/it]

 43%|████▎     | 3298/7689 [8:11:28<9:17:30,  7.62s/it]

 43%|████▎     | 3299/7689 [8:11:37<9:34:41,  7.85s/it]

 43%|████▎     | 3300/7689 [8:11:53<12:31:42, 10.28s/it]
 43%|████▎     | 3300/7689 [8:11:53<12:31:42, 10.28s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 43%|████▎     | 3301/7689 [8:12:31<22:37:04, 18.56s/it]

 43%|████▎     | 3302/7689 [8:12:37<18:11:32, 14.93s/it]

 43%|████▎     | 3303/7689 [8:12:54<19:01:25, 15.61s/it]

 43%|████▎     | 3304/7689 [8:13:02<16:05:32, 13.21s/it]

 43%|████▎     | 3305/7689 [8:13:06<12:58:47, 10.66s/it]

 43%|████▎     | 3306/7689 [8:13:16<12:27:31, 10.23s/it]

 43%|████▎     | 3307/7689 [8:13:22<10:57:37,  9.00s/it]

 43%|████▎     | 3308/7689 [8:13:28<9:48:13,  8.06s/it]

 43%|████▎     | 3309/7689 [8:13:39<10:49:15,  8.89s/it]

 43%|████▎     | 3310/7689 [8:13:47<10:31:03,  8.65s/it]
{'loss': 1.1239, 'grad_norm': 0.1813492169217026, 'learning_rate': 0.00012703963270038076, 'epoch': 0.43}

 43%|████▎     | 3311/7689 [8:13:57<11:12:57,  9.22s/it]

 43%|████▎     | 3312/7689 [8:14:03<10:07:26,  8.33s/it]

 43%|████▎     | 3313/7689 [8:14:13<10:42:58,  8.82s/it]

 43%|████▎     | 3314/7689 [8:14:21<10:24:04,  8.56s/it]


 43%|████▎     | 3316/7689 [8:14:42<11:41:58,  9.63s/it]

 43%|████▎     | 3317/7689 [8:14:50<10:49:46,  8.92s/it]

 43%|████▎     | 3318/7689 [8:14:59<10:52:23,  8.96s/it]

 43%|████▎     | 3319/7689 [8:15:07<10:34:39,  8.71s/it]

 43%|████▎     | 3320/7689 [8:15:14<10:00:59,  8.25s/it]

 43%|████▎     | 3321/7689 [8:15:23<10:16:41,  8.47s/it]

 43%|████▎     | 3322/7689 [8:15:28<9:03:18,  7.46s/it]

 43%|████▎     | 3323/7689 [8:15:36<9:10:04,  7.56s/it]

 43%|████▎     | 3324/7689 [8:15:50<11:32:31,  9.52s/it]

 43%|████▎     | 3325/7689 [8:16:00<11:48:25,  9.74s/it]

 43%|████▎     | 3326/7689 [8:16:09<11:20:58,  9.36s/it]
{'loss': 1.1296, 'grad_norm': 0.19134267536358904, 'learning_rate': 0.00012639014914241312, 'epoch': 0.43}


 43%|████▎     | 3328/7689 [8:16:32<12:46:31, 10.55s/it]
[2024-05-24 21:47:42,324] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 3329/7689 [8:16:39<11:26:55,  9.45s/it]

 43%|████▎     | 3330/7689 [8:16:45<10:17:54,  8.51s/it]

 43%|████▎     | 3331/7689 [8:16:55<10:53:51,  9.00s/it]

 43%|████▎     | 3332/7689 [8:17:02<10:05:06,  8.33s/it]

 43%|████▎     | 3333/7689 [8:17:13<10:54:22,  9.01s/it]
{'loss': 0.952, 'grad_norm': 0.18331457403983617, 'learning_rate': 0.00012610562138799978, 'epoch': 0.43}


 43%|████▎     | 3335/7689 [8:17:27<9:24:45,  7.78s/it]
{'loss': 1.1271, 'grad_norm': 0.19512467292048133, 'learning_rate': 0.0001260242859457109, 'epoch': 0.43}


 43%|████▎     | 3337/7689 [8:17:40<8:45:14,  7.24s/it]

 43%|████▎     | 3338/7689 [8:17:49<9:10:42,  7.59s/it]
{'loss': 0.9951, 'grad_norm': 0.18152631480037093, 'learning_rate': 0.00012590224816692798, 'epoch': 0.43}

 43%|████▎     | 3339/7689 [8:18:02<11:07:42,  9.21s/it]

 43%|████▎     | 3340/7689 [8:18:12<11:25:34,  9.46s/it]


 43%|████▎     | 3342/7689 [8:18:36<13:35:03, 11.25s/it]

 43%|████▎     | 3343/7689 [8:18:45<12:56:06, 10.71s/it]

 43%|████▎     | 3344/7689 [8:18:58<13:44:39, 11.39s/it]

 44%|████▎     | 3345/7689 [8:19:04<11:40:20,  9.67s/it]
{'loss': 1.0379, 'grad_norm': 0.17915916966258244, 'learning_rate': 0.00012561733282215852, 'epoch': 0.44}


 44%|████▎     | 3347/7689 [8:19:19<10:16:57,  8.53s/it]

 44%|████▎     | 3348/7689 [8:19:25<9:31:18,  7.90s/it]

 44%|████▎     | 3349/7689 [8:19:33<9:31:43,  7.90s/it]

 44%|████▎     | 3350/7689 [8:19:39<8:38:04,  7.16s/it]

 44%|████▎     | 3351/7689 [8:19:45<8:26:24,  7.00s/it]

 44%|████▎     | 3352/7689 [8:19:51<8:03:18,  6.69s/it]
{'loss': 0.9819, 'grad_norm': 0.20419534359020805, 'learning_rate': 0.00012533219474433941, 'epoch': 0.44}


 44%|████▎     | 3354/7689 [8:20:17<11:29:28,  9.54s/it]

 44%|████▎     | 3355/7689 [8:20:25<11:10:59,  9.29s/it]

 44%|████▎     | 3356/7689 [8:20:32<10:06:42,  8.40s/it]

 44%|████▎     | 3357/7689 [8:20:36<8:49:01,  7.33s/it]

 44%|████▎     | 3358/7689 [8:20:45<9:14:49,  7.69s/it]

 44%|████▎     | 3359/7689 [8:20:58<11:17:17,  9.39s/it]

 44%|████▎     | 3360/7689 [8:21:07<11:06:25,  9.24s/it]

 44%|████▎     | 3361/7689 [8:21:18<11:30:47,  9.58s/it]

 44%|████▎     | 3362/7689 [8:21:24<10:24:37,  8.66s/it]
{'loss': 1.1754, 'grad_norm': 0.21798090422020272, 'learning_rate': 0.00012492447317488783, 'epoch': 0.44}


 44%|████▍     | 3364/7689 [8:21:39<9:22:41,  7.81s/it]

 44%|████▍     | 3365/7689 [8:21:50<10:37:13,  8.84s/it]

 44%|████▍     | 3366/7689 [8:22:03<12:11:56, 10.16s/it]

 44%|████▍     | 3367/7689 [8:22:10<10:47:40,  8.99s/it]

 44%|████▍     | 3368/7689 [8:22:15<9:28:04,  7.89s/it]

 44%|████▍     | 3369/7689 [8:22:23<9:32:05,  7.95s/it]

 44%|████▍     | 3370/7689 [8:22:30<9:03:08,  7.55s/it]
{'loss': 0.9941, 'grad_norm': 0.19677996701918166, 'learning_rate': 0.00012459797714263978, 'epoch': 0.44}


 44%|████▍     | 3372/7689 [8:22:43<8:17:55,  6.92s/it]

 44%|████▍     | 3373/7689 [8:23:00<12:02:32, 10.04s/it]

 44%|████▍     | 3374/7689 [8:23:09<11:25:47,  9.54s/it]

 44%|████▍     | 3375/7689 [8:23:16<10:35:04,  8.83s/it]

 44%|████▍     | 3376/7689 [8:23:21<9:20:30,  7.80s/it]

 44%|████▍     | 3377/7689 [8:23:35<11:36:27,  9.69s/it]

 44%|████▍     | 3378/7689 [8:23:46<12:04:05, 10.08s/it]

 44%|████▍     | 3379/7689 [8:23:52<10:41:24,  8.93s/it]

 44%|████▍     | 3380/7689 [8:23:59<9:49:12,  8.20s/it]
{'loss': 1.0282, 'grad_norm': 0.20339298096064812, 'learning_rate': 0.0001241894647145241, 'epoch': 0.44}


 44%|████▍     | 3382/7689 [8:24:19<10:45:26,  8.99s/it]

 44%|████▍     | 3383/7689 [8:24:25<9:43:41,  8.13s/it]

 44%|████▍     | 3384/7689 [8:24:33<9:36:47,  8.04s/it]

 44%|████▍     | 3385/7689 [8:24:39<9:06:48,  7.62s/it]

 44%|████▍     | 3386/7689 [8:24:49<9:40:43,  8.10s/it]

 44%|████▍     | 3387/7689 [8:24:57<9:50:26,  8.23s/it]
{'loss': 0.8813, 'grad_norm': 0.1866195138165318, 'learning_rate': 0.00012390325019695476, 'epoch': 0.44}


 44%|████▍     | 3389/7689 [8:25:13<9:47:47,  8.20s/it]

 44%|████▍     | 3390/7689 [8:25:19<8:47:06,  7.36s/it]

 44%|████▍     | 3391/7689 [8:25:27<9:04:46,  7.60s/it]

 44%|████▍     | 3392/7689 [8:25:34<8:46:44,  7.35s/it]

 44%|████▍     | 3393/7689 [8:25:45<10:04:55,  8.45s/it]
{'loss': 0.9513, 'grad_norm': 0.1907465280289126, 'learning_rate': 0.0001236577579577299, 'epoch': 0.44}


 44%|████▍     | 3395/7689 [8:26:12<13:09:58, 11.04s/it]
{'loss': 0.9883, 'grad_norm': 0.18978676273698197, 'learning_rate': 0.00012357589355094275, 'epoch': 0.44}

 44%|████▍     | 3396/7689 [8:26:18<11:34:50,  9.71s/it]

 44%|████▍     | 3397/7689 [8:26:26<10:59:05,  9.21s/it]


 44%|████▍     | 3399/7689 [8:26:49<12:24:46, 10.42s/it]
{'loss': 1.0063, 'grad_norm': 0.1974606085431682, 'learning_rate': 0.0001234121145954094, 'epoch': 0.44}


 44%|████▍     | 3401/7689 [8:27:05<11:04:53,  9.30s/it]

 44%|████▍     | 3402/7689 [8:27:11<9:51:47,  8.28s/it]

 44%|████▍     | 3403/7689 [8:27:19<9:42:26,  8.15s/it]

 44%|████▍     | 3404/7689 [8:27:27<9:33:36,  8.03s/it]

 44%|████▍     | 3405/7689 [8:27:39<11:02:47,  9.28s/it]
{'loss': 0.9409, 'grad_norm': 0.19103002638032834, 'learning_rate': 0.00012316632167913544, 'epoch': 0.44}


 44%|████▍     | 3407/7689 [8:27:52<9:26:57,  7.94s/it]

 44%|████▍     | 3408/7689 [8:28:01<9:45:58,  8.21s/it]

 44%|████▍     | 3409/7689 [8:28:07<9:15:53,  7.79s/it]

 44%|████▍     | 3410/7689 [8:28:15<9:08:08,  7.69s/it]

 44%|████▍     | 3411/7689 [8:28:21<8:35:24,  7.23s/it]

 44%|████▍     | 3412/7689 [8:28:28<8:22:19,  7.05s/it]

 44%|████▍     | 3413/7689 [8:28:38<9:29:52,  8.00s/it]

 44%|████▍     | 3414/7689 [8:28:48<10:14:33,  8.63s/it]

 44%|████▍     | 3415/7689 [8:28:55<9:35:10,  8.07s/it]

 44%|████▍     | 3416/7689 [8:29:06<10:46:07,  9.07s/it]

 44%|████▍     | 3417/7689 [8:29:24<13:52:10, 11.69s/it]
[2024-05-24 22:00:34,377] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 3418/7689 [8:29:32<12:30:20, 10.54s/it]

 44%|████▍     | 3419/7689 [8:29:40<11:33:16,  9.74s/it]

 44%|████▍     | 3420/7689 [8:29:58<14:31:34, 12.25s/it]
[2024-05-24 22:01:08,218] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 3421/7689 [8:30:04<12:11:52, 10.29s/it]

 45%|████▍     | 3422/7689 [8:30:15<12:47:08, 10.79s/it]

 45%|████▍     | 3423/7689 [8:30:24<11:49:31,  9.98s/it]

 45%|████▍     | 3424/7689 [8:30:31<10:55:30,  9.22s/it]

 45%|████▍     | 3425/7689 [8:30:38<10:04:30,  8.51s/it]

 45%|████▍     | 3426/7689 [8:30:46<9:50:33,  8.31s/it]
{'loss': 0.9932, 'grad_norm': 0.20834638216093526, 'learning_rate': 0.00012230489141157826, 'epoch': 0.45}


 45%|████▍     | 3428/7689 [8:30:57<8:25:06,  7.11s/it]
{'loss': 1.0549, 'grad_norm': 0.18619733824172982, 'learning_rate': 0.00012222275834324216, 'epoch': 0.45}

 45%|████▍     | 3429/7689 [8:31:03<7:44:56,  6.55s/it]


 45%|████▍     | 3431/7689 [8:31:23<10:06:27,  8.55s/it]
{'loss': 1.022, 'grad_norm': 0.1780178517435543, 'learning_rate': 0.00012209952918467768, 'epoch': 0.45}

 45%|████▍     | 3432/7689 [8:31:31<9:50:05,  8.32s/it]

 45%|████▍     | 3433/7689 [8:31:41<10:27:49,  8.85s/it]


 45%|████▍     | 3435/7689 [8:31:56<9:45:30,  8.26s/it]

 45%|████▍     | 3436/7689 [8:32:05<10:05:35,  8.54s/it]

 45%|████▍     | 3437/7689 [8:32:13<9:43:51,  8.24s/it]

 45%|████▍     | 3438/7689 [8:32:23<10:27:35,  8.86s/it]
{'loss': 0.7889, 'grad_norm': 0.16916954224370587, 'learning_rate': 0.00012181185757361247, 'epoch': 0.45}


 45%|████▍     | 3440/7689 [8:32:42<11:15:32,  9.54s/it]

 45%|████▍     | 3441/7689 [8:32:49<10:19:12,  8.75s/it]

 45%|████▍     | 3442/7689 [8:32:55<9:20:53,  7.92s/it]

 45%|████▍     | 3443/7689 [8:33:04<9:34:24,  8.12s/it]

 45%|████▍     | 3444/7689 [8:33:14<10:18:02,  8.74s/it]
{'loss': 0.9252, 'grad_norm': 0.19716383270429816, 'learning_rate': 0.00012156513086950584, 'epoch': 0.45}

 45%|████▍     | 3445/7689 [8:33:27<11:47:24, 10.00s/it]


 45%|████▍     | 3447/7689 [8:33:38<8:58:53,  7.62s/it]

 45%|████▍     | 3448/7689 [8:33:48<10:00:52,  8.50s/it]

 45%|████▍     | 3449/7689 [8:34:00<11:03:49,  9.39s/it]

 45%|████▍     | 3450/7689 [8:34:07<10:25:15,  8.85s/it]

 45%|████▍     | 3451/7689 [8:34:15<10:04:33,  8.56s/it]

 45%|████▍     | 3452/7689 [8:34:22<9:39:00,  8.20s/it]

 45%|████▍     | 3453/7689 [8:34:32<10:02:38,  8.54s/it]
{'loss': 0.8365, 'grad_norm': 0.18957886349909914, 'learning_rate': 0.00012119478301430147, 'epoch': 0.45}


 45%|████▍     | 3455/7689 [8:34:46<9:15:43,  7.88s/it]

 45%|████▍     | 3456/7689 [8:34:52<8:52:26,  7.55s/it]
{'loss': 1.2102, 'grad_norm': 0.20767895608414827, 'learning_rate': 0.00012107126577134722, 'epoch': 0.45}


 45%|████▍     | 3458/7689 [8:35:14<10:41:57,  9.10s/it]

 45%|████▍     | 3459/7689 [8:35:20<9:45:52,  8.31s/it]
{'loss': 0.8052, 'grad_norm': 0.19423052565742505, 'learning_rate': 0.0001209477148781609, 'epoch': 0.45}

 45%|████▍     | 3460/7689 [8:35:27<9:06:27,  7.75s/it]


 45%|████▌     | 3462/7689 [8:35:41<8:38:21,  7.36s/it]

 45%|████▌     | 3463/7689 [8:35:49<9:02:18,  7.70s/it]

 45%|████▌     | 3464/7689 [8:36:00<10:09:43,  8.66s/it]

 45%|████▌     | 3465/7689 [8:36:08<9:50:43,  8.39s/it]
{'loss': 1.0223, 'grad_norm': 0.1775813296293012, 'learning_rate': 0.00012070051293037492, 'epoch': 0.45}


 45%|████▌     | 3467/7689 [8:36:24<9:55:02,  8.46s/it]

 45%|████▌     | 3468/7689 [8:36:29<8:47:37,  7.50s/it]

 45%|████▌     | 3469/7689 [8:36:39<9:41:39,  8.27s/it]

 45%|████▌     | 3470/7689 [8:36:46<9:01:50,  7.71s/it]

 45%|████▌     | 3471/7689 [8:36:54<9:12:24,  7.86s/it]

 45%|████▌     | 3472/7689 [8:37:00<8:46:24,  7.49s/it]

 45%|████▌     | 3473/7689 [8:37:08<8:57:13,  7.65s/it]

 45%|████▌     | 3474/7689 [8:37:20<10:16:56,  8.78s/it]

 45%|████▌     | 3475/7689 [8:37:32<11:27:54,  9.79s/it]
{'loss': 0.7413, 'grad_norm': 0.19449504233746892, 'learning_rate': 0.00012028821661414447, 'epoch': 0.45}


 45%|████▌     | 3477/7689 [8:38:00<13:56:33, 11.92s/it]
{'loss': 1.0084, 'grad_norm': 0.18522833601719518, 'learning_rate': 0.00012020571391710445, 'epoch': 0.45}


 45%|████▌     | 3479/7689 [8:38:14<10:50:20,  9.27s/it]
{'loss': 0.9947, 'grad_norm': 0.20656823502959507, 'learning_rate': 0.00012012319687874356, 'epoch': 0.45}


 45%|████▌     | 3481/7689 [8:38:32<10:23:37,  8.89s/it]

 45%|████▌     | 3482/7689 [8:38:40<10:14:45,  8.77s/it]

 45%|████▌     | 3483/7689 [8:38:46<9:14:36,  7.91s/it]

 45%|████▌     | 3484/7689 [8:38:52<8:28:10,  7.25s/it]

 45%|████▌     | 3485/7689 [8:38:58<8:03:36,  6.90s/it]

 45%|████▌     | 3486/7689 [8:39:05<8:15:22,  7.07s/it]

 45%|████▌     | 3487/7689 [8:39:15<9:18:53,  7.98s/it]

 45%|████▌     | 3488/7689 [8:39:22<8:44:56,  7.50s/it]

 45%|████▌     | 3489/7689 [8:39:26<7:42:19,  6.60s/it]

 45%|████▌     | 3490/7689 [8:39:34<8:05:04,  6.93s/it]
{'loss': 1.0942, 'grad_norm': 0.18967393998939908, 'learning_rate': 0.0001196690994344029, 'epoch': 0.45}


 45%|████▌     | 3492/7689 [8:39:52<9:44:34,  8.36s/it]

 45%|████▌     | 3493/7689 [8:40:10<12:56:58, 11.11s/it]

 45%|████▌     | 3494/7689 [8:40:15<10:50:44,  9.31s/it]

 45%|████▌     | 3495/7689 [8:40:22<10:05:18,  8.66s/it]

 45%|████▌     | 3496/7689 [8:40:28<9:08:23,  7.85s/it]

 45%|████▌     | 3497/7689 [8:40:38<10:05:33,  8.67s/it]

 45%|████▌     | 3498/7689 [8:40:44<8:56:27,  7.68s/it]
{'loss': 1.065, 'grad_norm': 0.18773994130642005, 'learning_rate': 0.00011933858094245281, 'epoch': 0.45}


 46%|████▌     | 3500/7689 [8:40:55<7:52:15,  6.76s/it]

 46%|████▌     | 3501/7689 [8:41:03<8:04:07,  6.94s/it]

 46%|████▌     | 3502/7689 [8:41:12<8:54:09,  7.65s/it]

 46%|████▌     | 3503/7689 [8:41:24<10:24:58,  8.96s/it]

 46%|████▌     | 3504/7689 [8:41:34<10:39:34,  9.17s/it]

 46%|████▌     | 3505/7689 [8:41:40<9:49:23,  8.45s/it]
{'loss': 1.1313, 'grad_norm': 0.19478422054654682, 'learning_rate': 0.00011904919698189455, 'epoch': 0.46}


 46%|████▌     | 3507/7689 [8:42:03<11:17:46,  9.72s/it]

 46%|████▌     | 3508/7689 [8:42:09<10:01:06,  8.63s/it]
{'loss': 1.0588, 'grad_norm': 0.19407083361215008, 'learning_rate': 0.00011892512443604102, 'epoch': 0.46}

 46%|████▌     | 3509/7689 [8:42:15<9:09:45,  7.89s/it]


 46%|████▌     | 3511/7689 [8:42:28<8:12:24,  7.07s/it]
{'loss': 0.9743, 'grad_norm': 0.21445675736045602, 'learning_rate': 0.00011880102166728413, 'epoch': 0.46}


 46%|████▌     | 3513/7689 [8:42:42<8:09:44,  7.04s/it]

 46%|████▌     | 3514/7689 [8:42:49<8:05:04,  6.97s/it]

 46%|████▌     | 3515/7689 [8:42:54<7:15:58,  6.27s/it]

 46%|████▌     | 3516/7689 [8:43:04<8:35:09,  7.41s/it]

 46%|████▌     | 3517/7689 [8:43:14<9:44:28,  8.41s/it]
{'loss': 0.9711, 'grad_norm': 0.17204587294776394, 'learning_rate': 0.000118552726253863, 'epoch': 0.46}


 46%|████▌     | 3519/7689 [8:43:28<9:00:12,  7.77s/it]

 46%|████▌     | 3520/7689 [8:43:38<9:39:57,  8.35s/it]
{'loss': 1.032, 'grad_norm': 0.17750220927833563, 'learning_rate': 0.00011842853400571971, 'epoch': 0.46}

 46%|████▌     | 3521/7689 [8:43:53<12:10:43, 10.52s/it]


 46%|████▌     | 3523/7689 [8:44:08<10:20:31,  8.94s/it]

 46%|████▌     | 3524/7689 [8:44:17<10:20:39,  8.94s/it]

 46%|████▌     | 3525/7689 [8:44:23<9:18:59,  8.05s/it]

 46%|████▌     | 3526/7689 [8:44:32<9:44:53,  8.43s/it]

 46%|████▌     | 3527/7689 [8:44:44<10:52:09,  9.40s/it]

 46%|████▌     | 3528/7689 [8:44:50<9:51:36,  8.53s/it]

 46%|████▌     | 3529/7689 [8:44:59<9:59:37,  8.65s/it]

 46%|████▌     | 3530/7689 [8:45:09<10:28:59,  9.07s/it]

 46%|████▌     | 3531/7689 [8:45:14<9:05:14,  7.87s/it]
{'loss': 1.0735, 'grad_norm': 0.21161079445654174, 'learning_rate': 0.00011797291214917881, 'epoch': 0.46}


 46%|████▌     | 3533/7689 [8:45:42<12:47:44, 11.08s/it]

 46%|████▌     | 3534/7689 [8:45:51<12:00:13, 10.40s/it]

 46%|████▌     | 3535/7689 [8:45:55<10:03:33,  8.72s/it]
{'loss': 1.0572, 'grad_norm': 0.20339203866947708, 'learning_rate': 0.00011780713528534194, 'epoch': 0.46}


 46%|████▌     | 3537/7689 [8:46:11<9:38:17,  8.36s/it]

 46%|████▌     | 3538/7689 [8:46:21<10:18:57,  8.95s/it]

 46%|████▌     | 3539/7689 [8:46:29<9:50:29,  8.54s/it]

 46%|████▌     | 3540/7689 [8:46:36<9:17:39,  8.06s/it]
{'loss': 1.0518, 'grad_norm': 0.18992612755116756, 'learning_rate': 0.00011759984316695765, 'epoch': 0.46}


 46%|████▌     | 3542/7689 [8:46:51<9:09:20,  7.95s/it]

 46%|████▌     | 3543/7689 [8:46:58<8:42:00,  7.55s/it]

 46%|████▌     | 3544/7689 [8:47:03<7:55:37,  6.88s/it]

 46%|████▌     | 3545/7689 [8:47:09<7:34:28,  6.58s/it]

 46%|████▌     | 3546/7689 [8:47:18<8:31:32,  7.41s/it]

 46%|████▌     | 3547/7689 [8:47:30<10:04:14,  8.75s/it]

 46%|████▌     | 3548/7689 [8:47:48<13:03:06, 11.35s/it]

 46%|████▌     | 3549/7689 [8:47:55<11:45:06, 10.22s/it]

 46%|████▌     | 3550/7689 [8:48:02<10:28:06,  9.11s/it]

 46%|████▌     | 3551/7689 [8:48:09<9:44:03,  8.47s/it]

 46%|████▌     | 3552/7689 [8:48:18<10:00:50,  8.71s/it]
{'loss': 1.2827, 'grad_norm': 0.22045445871306538, 'learning_rate': 0.00011710202529452398, 'epoch': 0.46}


 46%|████▌     | 3554/7689 [8:48:36<10:02:33,  8.74s/it]

 46%|████▌     | 3555/7689 [8:48:47<10:42:55,  9.33s/it]

 46%|████▌     | 3556/7689 [8:48:52<9:18:48,  8.11s/it]

 46%|████▋     | 3557/7689 [8:48:59<8:54:27,  7.76s/it]

 46%|████▋     | 3558/7689 [8:49:07<9:13:32,  8.04s/it]

 46%|████▋     | 3559/7689 [8:49:22<11:26:36,  9.98s/it]
{'loss': 0.9055, 'grad_norm': 0.18561122327554583, 'learning_rate': 0.00011681142891831034, 'epoch': 0.46}


 46%|████▋     | 3561/7689 [8:49:42<11:21:33,  9.91s/it]

 46%|████▋     | 3562/7689 [8:49:58<13:24:15, 11.69s/it]

 46%|████▋     | 3563/7689 [8:50:04<11:33:16, 10.08s/it]

 46%|████▋     | 3564/7689 [8:50:15<11:38:59, 10.17s/it]

 46%|████▋     | 3565/7689 [8:50:24<11:28:04, 10.01s/it]

 46%|████▋     | 3566/7689 [8:50:31<10:22:53,  9.06s/it]

 46%|████▋     | 3567/7689 [8:50:37<9:22:42,  8.19s/it]

 46%|████▋     | 3568/7689 [8:50:46<9:29:33,  8.29s/it]

 46%|████▋     | 3569/7689 [8:50:52<8:46:37,  7.67s/it]

 46%|████▋     | 3570/7689 [8:50:58<8:17:58,  7.25s/it]

 46%|████▋     | 3571/7689 [8:51:11<10:16:21,  8.98s/it]

 46%|████▋     | 3572/7689 [8:51:18<9:31:59,  8.34s/it]

 46%|████▋     | 3573/7689 [8:51:30<10:40:24,  9.34s/it]

 46%|████▋     | 3574/7689 [8:51:36<9:24:06,  8.23s/it]

 46%|████▋     | 3575/7689 [8:51:40<8:06:31,  7.10s/it]

 47%|████▋     | 3576/7689 [8:51:52<9:39:16,  8.45s/it]

 47%|████▋     | 3577/7689 [8:51:58<9:03:36,  7.93s/it]

 47%|████▋     | 3578/7689 [8:52:06<8:51:08,  7.75s/it]
{'loss': 1.0509, 'grad_norm': 0.180005220348301, 'learning_rate': 0.00011602193779663473, 'epoch': 0.47}

 47%|████▋     | 3579/7689 [8:52:20<11:04:02,  9.69s/it]


 47%|████▋     | 3581/7689 [8:52:36<10:08:53,  8.89s/it]

 47%|████▋     | 3582/7689 [8:52:42<9:11:20,  8.05s/it]

 47%|████▋     | 3583/7689 [8:52:52<10:07:38,  8.88s/it]

 47%|████▋     | 3584/7689 [8:52:58<9:10:07,  8.04s/it]

 47%|████▋     | 3585/7689 [8:53:05<8:33:14,  7.50s/it]

 47%|████▋     | 3586/7689 [8:53:13<8:49:48,  7.75s/it]

 47%|████▋     | 3587/7689 [8:53:27<11:06:06,  9.74s/it]

 47%|████▋     | 3588/7689 [8:53:36<10:31:18,  9.24s/it]

 47%|████▋     | 3589/7689 [8:53:42<9:40:03,  8.49s/it]

 47%|████▋     | 3590/7689 [8:53:53<10:36:18,  9.31s/it]

 47%|████▋     | 3591/7689 [8:54:02<10:11:11,  8.95s/it]

 47%|████▋     | 3592/7689 [8:54:08<9:13:12,  8.10s/it]

 47%|████▋     | 3593/7689 [8:54:19<10:23:43,  9.14s/it]

 47%|████▋     | 3594/7689 [8:54:29<10:46:00,  9.47s/it]
{'loss': 0.8721, 'grad_norm': 0.19395466273949302, 'learning_rate': 0.00011535630507824682, 'epoch': 0.47}


 47%|████▋     | 3596/7689 [8:54:48<10:34:45,  9.31s/it]

 47%|████▋     | 3597/7689 [8:54:55<9:41:18,  8.52s/it]

 47%|████▋     | 3598/7689 [8:55:02<9:04:09,  7.98s/it]

 47%|████▋     | 3599/7689 [8:55:08<8:41:00,  7.64s/it]

 47%|████▋     | 3600/7689 [8:55:17<8:50:26,  7.78s/it]
 47%|████▋     | 3600/7689 [8:55:17<8:50:26,  7.78s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 47%|████▋     | 3601/7689 [8:55:52<18:21:56, 16.17s/it]
{'loss': 0.968, 'grad_norm': 0.19848706614425668, 'learning_rate': 0.00011506486963092021, 'epoch': 0.47}


 47%|████▋     | 3603/7689 [8:56:05<12:27:46, 10.98s/it]

 47%|████▋     | 3604/7689 [8:56:16<12:39:11, 11.15s/it]

 47%|████▋     | 3605/7689 [8:56:23<11:09:46,  9.84s/it]

 47%|████▋     | 3606/7689 [8:56:29<9:49:11,  8.66s/it]

 47%|████▋     | 3607/7689 [8:56:43<11:39:44, 10.29s/it]

 47%|████▋     | 3608/7689 [8:56:49<10:12:46,  9.01s/it]

 47%|████▋     | 3609/7689 [8:56:55<9:06:17,  8.03s/it]

 47%|████▋     | 3610/7689 [8:57:09<11:10:28,  9.86s/it]

 47%|████▋     | 3611/7689 [8:57:16<10:11:35,  9.00s/it]

 47%|████▋     | 3612/7689 [8:57:29<11:40:50, 10.31s/it]

 47%|████▋     | 3613/7689 [8:57:37<10:50:54,  9.58s/it]

 47%|████▋     | 3614/7689 [8:57:54<13:14:24, 11.70s/it]

 47%|████▋     | 3615/7689 [8:58:00<11:15:21,  9.95s/it]

 47%|████▋     | 3616/7689 [8:58:07<10:11:59,  9.02s/it]

 47%|████▋     | 3617/7689 [8:58:19<11:26:43, 10.12s/it]

 47%|████▋     | 3618/7689 [8:58:36<13:36:33, 12.03s/it]

 47%|████▋     | 3619/7689 [8:58:44<12:27:01, 11.01s/it]

 47%|████▋     | 3620/7689 [8:58:55<12:09:44, 10.76s/it]

 47%|████▋     | 3621/7689 [8:59:01<10:32:51,  9.33s/it]

 47%|████▋     | 3622/7689 [8:59:09<10:14:55,  9.07s/it]

 47%|████▋     | 3623/7689 [8:59:22<11:24:27, 10.10s/it]

 47%|████▋     | 3624/7689 [8:59:27<9:41:30,  8.58s/it]

 47%|████▋     | 3625/7689 [8:59:35<9:36:00,  8.50s/it]

 47%|████▋     | 3626/7689 [8:59:47<10:46:16,  9.54s/it]

 47%|████▋     | 3627/7689 [8:59:53<9:33:58,  8.48s/it]

 47%|████▋     | 3628/7689 [8:59:59<8:52:55,  7.87s/it]

 47%|████▋     | 3629/7689 [9:00:05<7:58:34,  7.07s/it]

 47%|████▋     | 3630/7689 [9:00:12<8:00:32,  7.10s/it]
{'loss': 0.7933, 'grad_norm': 0.17449223543612644, 'learning_rate': 0.0001138561271313219, 'epoch': 0.47}


 47%|████▋     | 3632/7689 [9:00:24<7:15:32,  6.44s/it]

 47%|████▋     | 3633/7689 [9:00:29<6:51:25,  6.09s/it]

 47%|████▋     | 3634/7689 [9:00:41<9:01:55,  8.02s/it]

 47%|████▋     | 3635/7689 [9:00:47<8:21:28,  7.42s/it]

 47%|████▋     | 3636/7689 [9:00:53<7:49:42,  6.95s/it]

 47%|████▋     | 3637/7689 [9:01:05<9:32:07,  8.47s/it]

 47%|████▋     | 3638/7689 [9:01:24<12:59:52, 11.55s/it]
{'loss': 0.9564, 'grad_norm': 0.18178815319551173, 'learning_rate': 0.00011352230937901152, 'epoch': 0.47}


 47%|████▋     | 3640/7689 [9:01:46<12:59:43, 11.55s/it]

 47%|████▋     | 3641/7689 [9:01:51<10:48:09,  9.61s/it]

 47%|████▋     | 3642/7689 [9:02:01<10:47:39,  9.60s/it]

 47%|████▋     | 3643/7689 [9:02:14<11:50:53, 10.54s/it]

 47%|████▋     | 3644/7689 [9:02:24<11:52:38, 10.57s/it]
{'loss': 1.0411, 'grad_norm': 0.1772036668260333, 'learning_rate': 0.00011327184508194975, 'epoch': 0.47}


 47%|████▋     | 3646/7689 [9:02:43<11:33:18, 10.29s/it]
{'loss': 0.8643, 'grad_norm': 0.1888004269185151, 'learning_rate': 0.00011318833806411467, 'epoch': 0.47}


 47%|████▋     | 3648/7689 [9:03:03<11:03:45,  9.86s/it]

 47%|████▋     | 3649/7689 [9:03:16<12:03:10, 10.74s/it]
{'loss': 1.0012, 'grad_norm': 0.21649636356754728, 'learning_rate': 0.00011306306000470684, 'epoch': 0.47}


 47%|████▋     | 3651/7689 [9:03:28<9:13:43,  8.23s/it]

 47%|████▋     | 3652/7689 [9:03:33<8:07:37,  7.25s/it]

 48%|████▊     | 3653/7689 [9:03:47<10:24:20,  9.28s/it]
{'loss': 1.0724, 'grad_norm': 0.19902225983907632, 'learning_rate': 0.00011289599017576184, 'epoch': 0.48}


 48%|████▊     | 3655/7689 [9:04:05<10:43:55,  9.58s/it]

 48%|████▊     | 3656/7689 [9:04:14<10:24:56,  9.30s/it]

 48%|████▊     | 3657/7689 [9:04:24<10:34:12,  9.44s/it]

 48%|████▊     | 3658/7689 [9:04:30<9:32:32,  8.52s/it]
{'loss': 1.0446, 'grad_norm': 0.2206069220316338, 'learning_rate': 0.00011268710145882608, 'epoch': 0.48}


 48%|████▊     | 3660/7689 [9:04:50<10:07:41,  9.05s/it]

 48%|████▊     | 3661/7689 [9:04:59<10:19:45,  9.23s/it]
{'loss': 1.0682, 'grad_norm': 0.2152844026710988, 'learning_rate': 0.00011256174115475287, 'epoch': 0.48}


 48%|████▊     | 3663/7689 [9:05:14<9:09:00,  8.18s/it]

 48%|████▊     | 3664/7689 [9:05:24<9:47:32,  8.76s/it]

 48%|████▊     | 3665/7689 [9:05:36<11:03:49,  9.90s/it]

 48%|████▊     | 3666/7689 [9:05:47<11:29:06, 10.28s/it]

 48%|████▊     | 3667/7689 [9:05:58<11:42:06, 10.47s/it]

 48%|████▊     | 3668/7689 [9:06:05<10:24:41,  9.32s/it]

 48%|████▊     | 3669/7689 [9:06:13<9:47:17,  8.77s/it]

 48%|████▊     | 3670/7689 [9:06:20<9:23:02,  8.41s/it]

 48%|████▊     | 3671/7689 [9:06:28<9:14:26,  8.28s/it]

 48%|████▊     | 3672/7689 [9:06:44<11:47:31, 10.57s/it]

 48%|████▊     | 3673/7689 [9:06:50<10:11:59,  9.14s/it]

 48%|████▊     | 3674/7689 [9:06:55<9:01:35,  8.09s/it]

 48%|████▊     | 3675/7689 [9:07:06<9:51:54,  8.85s/it]
{'loss': 0.9194, 'grad_norm': 0.17597605519151133, 'learning_rate': 0.00011197646439078746, 'epoch': 0.48}


 48%|████▊     | 3677/7689 [9:07:21<9:16:49,  8.33s/it]

 48%|████▊     | 3678/7689 [9:07:26<8:11:36,  7.35s/it]

 48%|████▊     | 3679/7689 [9:07:40<10:19:17,  9.27s/it]

 48%|████▊     | 3680/7689 [9:07:53<11:27:40, 10.29s/it]
{'loss': 0.8919, 'grad_norm': 0.1909505320669842, 'learning_rate': 0.00011176733497462592, 'epoch': 0.48}


 48%|████▊     | 3682/7689 [9:08:18<12:43:13, 11.43s/it]

 48%|████▊     | 3683/7689 [9:08:24<10:54:35,  9.80s/it]

 48%|████▊     | 3684/7689 [9:08:39<12:37:55, 11.35s/it]

 48%|████▊     | 3685/7689 [9:08:47<11:43:02, 10.54s/it]

 48%|████▊     | 3686/7689 [9:08:58<11:37:34, 10.46s/it]

 48%|████▊     | 3687/7689 [9:09:04<10:21:24,  9.32s/it]

 48%|████▊     | 3688/7689 [9:09:16<11:05:24,  9.98s/it]

 48%|████▊     | 3689/7689 [9:09:22<9:44:06,  8.76s/it]

 48%|████▊     | 3690/7689 [9:09:33<10:22:57,  9.35s/it]

 48%|████▊     | 3691/7689 [9:09:37<8:50:52,  7.97s/it]
{'loss': 1.0876, 'grad_norm': 0.19751985779252246, 'learning_rate': 0.00011130706782058545, 'epoch': 0.48}


 48%|████▊     | 3693/7689 [9:09:55<9:09:53,  8.26s/it]

 48%|████▊     | 3694/7689 [9:10:03<9:16:51,  8.36s/it]
{'loss': 0.86, 'grad_norm': 0.20177627671912177, 'learning_rate': 0.00011118149786592702, 'epoch': 0.48}

 48%|████▊     | 3695/7689 [9:10:09<8:22:07,  7.54s/it]


 48%|████▊     | 3697/7689 [9:10:29<10:05:36,  9.10s/it]

 48%|████▊     | 3698/7689 [9:10:35<9:01:16,  8.14s/it]

 48%|████▊     | 3699/7689 [9:10:49<10:52:03,  9.81s/it]
{'loss': 0.9965, 'grad_norm': 0.16813669080314733, 'learning_rate': 0.0001109721750259936, 'epoch': 0.48}


 48%|████▊     | 3701/7689 [9:11:13<12:02:07, 10.86s/it]

 48%|████▊     | 3702/7689 [9:11:19<10:33:05,  9.53s/it]
{'loss': 1.0713, 'grad_norm': 0.17465198937535978, 'learning_rate': 0.00011084655789958622, 'epoch': 0.48}

 48%|████▊     | 3703/7689 [9:11:26<9:36:45,  8.68s/it]


 48%|████▊     | 3705/7689 [9:11:43<9:37:26,  8.70s/it]
{'loss': 0.951, 'grad_norm': 0.192359849396229, 'learning_rate': 0.0001107209234515241, 'epoch': 0.48}


 48%|████▊     | 3707/7689 [9:11:55<8:04:17,  7.30s/it]

 48%|████▊     | 3708/7689 [9:12:08<9:56:01,  8.98s/it]

 48%|████▊     | 3709/7689 [9:12:18<10:18:17,  9.32s/it]

 48%|████▊     | 3710/7689 [9:12:25<9:24:24,  8.51s/it]
{'loss': 1.2359, 'grad_norm': 0.18910763630865254, 'learning_rate': 0.00011051149475713409, 'epoch': 0.48}


 48%|████▊     | 3712/7689 [9:12:37<8:04:33,  7.31s/it]
{'loss': 1.0752, 'grad_norm': 0.21030063963731488, 'learning_rate': 0.00011042771017112874, 'epoch': 0.48}

 48%|████▊     | 3713/7689 [9:12:47<9:02:49,  8.19s/it]

 48%|████▊     | 3714/7689 [9:12:53<8:23:05,  7.59s/it]


 48%|████▊     | 3716/7689 [9:13:04<7:07:18,  6.45s/it]

 48%|████▊     | 3717/7689 [9:13:23<11:02:26, 10.01s/it]

 48%|████▊     | 3718/7689 [9:13:35<11:52:10, 10.76s/it]
{'loss': 0.9156, 'grad_norm': 0.1650978872434049, 'learning_rate': 0.00011017631224362803, 'epoch': 0.48}


 48%|████▊     | 3720/7689 [9:13:50<9:57:11,  9.03s/it]

 48%|████▊     | 3721/7689 [9:14:02<11:03:51, 10.04s/it]

 48%|████▊     | 3722/7689 [9:14:08<9:41:31,  8.80s/it]
{'loss': 0.8388, 'grad_norm': 0.22241291330266233, 'learning_rate': 0.00011000867741218203, 'epoch': 0.48}


 48%|████▊     | 3724/7689 [9:14:26<9:46:35,  8.88s/it]
{'loss': 0.9545, 'grad_norm': 0.16891572406322755, 'learning_rate': 0.00010992484931098988, 'epoch': 0.48}


 48%|████▊     | 3726/7689 [9:14:46<10:37:32,  9.65s/it]
{'loss': 0.7689, 'grad_norm': 0.17166751854985454, 'learning_rate': 0.00010984101416548098, 'epoch': 0.48}


 48%|████▊     | 3728/7689 [9:15:00<9:09:18,  8.32s/it]

 48%|████▊     | 3729/7689 [9:15:11<9:59:53,  9.09s/it]

 49%|████▊     | 3730/7689 [9:15:17<8:54:25,  8.10s/it]
{'loss': 1.0304, 'grad_norm': 0.1990494584182609, 'learning_rate': 0.00010967332297953117, 'epoch': 0.49}


 49%|████▊     | 3732/7689 [9:15:31<8:09:22,  7.42s/it]

 49%|████▊     | 3733/7689 [9:15:37<7:39:41,  6.97s/it]

 49%|████▊     | 3734/7689 [9:15:42<7:07:14,  6.48s/it]
{'loss': 0.9973, 'grad_norm': 0.21393549702961207, 'learning_rate': 0.00010950560433041826, 'epoch': 0.49}


 49%|████▊     | 3736/7689 [9:16:01<8:45:58,  7.98s/it]
{'loss': 0.9861, 'grad_norm': 0.19800281785419271, 'learning_rate': 0.00010942173485597382, 'epoch': 0.49}


 49%|████▊     | 3738/7689 [9:16:11<7:16:48,  6.63s/it]

 49%|████▊     | 3739/7689 [9:16:17<6:53:54,  6.29s/it]

 49%|████▊     | 3740/7689 [9:16:27<8:19:31,  7.59s/it]

 49%|████▊     | 3741/7689 [9:16:37<9:05:38,  8.29s/it]
{'loss': 0.727, 'grad_norm': 0.1782632351344884, 'learning_rate': 0.00010921203204348874, 'epoch': 0.49}


 49%|████▊     | 3743/7689 [9:16:57<10:02:36,  9.16s/it]
{'loss': 0.9118, 'grad_norm': 0.2145978520822775, 'learning_rate': 0.00010912813942422702, 'epoch': 0.49}

 49%|████▊     | 3744/7689 [9:17:04<9:17:20,  8.48s/it]


 49%|████▊     | 3746/7689 [9:17:25<10:09:52,  9.28s/it]
{'loss': 0.93, 'grad_norm': 0.19924026704832815, 'learning_rate': 0.00010900228836611923, 'epoch': 0.49}

 49%|████▊     | 3747/7689 [9:17:34<9:54:00,  9.04s/it]


 49%|████▉     | 3749/7689 [9:17:49<8:54:15,  8.14s/it]
{'loss': 0.9368, 'grad_norm': 0.23175660586669114, 'learning_rate': 0.0001088764229316044, 'epoch': 0.49}

 49%|████▉     | 3750/7689 [9:17:58<9:12:15,  8.41s/it]

 49%|████▉     | 3751/7689 [9:18:06<9:07:53,  8.35s/it]


 49%|████▉     | 3753/7689 [9:18:26<9:59:03,  9.13s/it]
{'loss': 0.9199, 'grad_norm': 0.18814097259832777, 'learning_rate': 0.00010870858033636855, 'epoch': 0.49}


 49%|████▉     | 3755/7689 [9:18:49<11:05:06, 10.14s/it]

 49%|████▉     | 3756/7689 [9:18:57<10:34:13,  9.68s/it]

 49%|████▉     | 3757/7689 [9:19:05<9:57:08,  9.11s/it]
{'loss': 1.0827, 'grad_norm': 0.18943352171016264, 'learning_rate': 0.00010854071301693369, 'epoch': 0.49}


 49%|████▉     | 3759/7689 [9:19:24<10:43:54,  9.83s/it]

 49%|████▉     | 3760/7689 [9:19:33<10:18:35,  9.45s/it]

 49%|████▉     | 3761/7689 [9:19:47<11:49:16, 10.83s/it]
{'loss': 0.9848, 'grad_norm': 0.19587581955318314, 'learning_rate': 0.00010837282144988564, 'epoch': 0.49}


 49%|████▉     | 3763/7689 [9:20:15<13:22:44, 12.27s/it]

 49%|████▉     | 3764/7689 [9:20:23<12:05:43, 11.09s/it]

 49%|████▉     | 3765/7689 [9:20:33<11:47:22, 10.82s/it]
{'loss': 0.8758, 'grad_norm': 0.18535433071785334, 'learning_rate': 0.00010820490611187883, 'epoch': 0.49}

 49%|████▉     | 3766/7689 [9:20:44<11:49:29, 10.85s/it]


 49%|████▉     | 3768/7689 [9:21:05<11:20:24, 10.41s/it]

 49%|████▉     | 3769/7689 [9:21:17<11:51:42, 10.89s/it]
{'loss': 0.8349, 'grad_norm': 0.18360612611878144, 'learning_rate': 0.00010803696747963543, 'epoch': 0.49}

 49%|████▉     | 3770/7689 [9:21:30<12:36:05, 11.58s/it]


 49%|████▉     | 3772/7689 [9:21:46<10:45:12,  9.88s/it]

 49%|████▉     | 3773/7689 [9:21:51<9:17:23,  8.54s/it]
{'loss': 1.2358, 'grad_norm': 0.1868144434538243, 'learning_rate': 0.00010786900602994359, 'epoch': 0.49}


 49%|████▉     | 3775/7689 [9:22:01<7:24:32,  6.81s/it]

 49%|████▉     | 3776/7689 [9:22:11<8:24:18,  7.73s/it]
{'loss': 0.9521, 'grad_norm': 0.17918194990780456, 'learning_rate': 0.00010774302025557856, 'epoch': 0.49}


 49%|████▉     | 3778/7689 [9:22:37<10:35:21,  9.75s/it]
{'loss': 0.8864, 'grad_norm': 0.18855149554244857, 'learning_rate': 0.00010765902285725356, 'epoch': 0.49}

 49%|████▉     | 3779/7689 [9:22:44<9:52:19,  9.09s/it]

 49%|████▉     | 3780/7689 [9:22:52<9:34:47,  8.82s/it]


 49%|████▉     | 3782/7689 [9:23:21<12:12:04, 11.24s/it]
{'loss': 0.8374, 'grad_norm': 0.17828850809230715, 'learning_rate': 0.00010749101181189278, 'epoch': 0.49}


 49%|████▉     | 3784/7689 [9:23:33<9:21:24,  8.63s/it]
{'loss': 1.1, 'grad_norm': 0.2003116930297531, 'learning_rate': 0.00010740699828410545, 'epoch': 0.49}


 49%|████▉     | 3786/7689 [9:23:46<8:19:06,  7.67s/it]

 49%|████▉     | 3787/7689 [9:23:53<8:09:22,  7.53s/it]

 49%|████▉     | 3788/7689 [9:24:01<8:12:06,  7.57s/it]

 49%|████▉     | 3789/7689 [9:24:12<9:21:30,  8.64s/it]

 49%|████▉     | 3790/7689 [9:24:20<9:08:52,  8.45s/it]
{'loss': 0.8724, 'grad_norm': 0.1852988899940805, 'learning_rate': 0.00010715492639588582, 'epoch': 0.49}


 49%|████▉     | 3792/7689 [9:24:34<8:23:42,  7.76s/it]

 49%|████▉     | 3793/7689 [9:24:41<8:11:59,  7.58s/it]

 49%|████▉     | 3794/7689 [9:24:47<7:45:24,  7.17s/it]

 49%|████▉     | 3795/7689 [9:24:53<7:17:30,  6.74s/it]

 49%|████▉     | 3796/7689 [9:25:01<7:43:22,  7.14s/it]
{'loss': 0.8845, 'grad_norm': 0.18271703485091134, 'learning_rate': 0.00010690280880280151, 'epoch': 0.49}


 49%|████▉     | 3798/7689 [9:25:14<7:09:51,  6.63s/it]

 49%|████▉     | 3799/7689 [9:25:20<7:04:00,  6.54s/it]

 49%|████▉     | 3800/7689 [9:25:27<7:11:40,  6.66s/it]
{'loss': 0.9173, 'grad_norm': 0.1967053992959946, 'learning_rate': 0.00010673470581111687, 'epoch': 0.49}


 49%|████▉     | 3802/7689 [9:25:40<7:06:31,  6.58s/it]

 49%|████▉     | 3803/7689 [9:25:47<7:30:52,  6.96s/it]
{'loss': 0.9757, 'grad_norm': 0.17549070545884077, 'learning_rate': 0.00010660861599358938, 'epoch': 0.49}


 49%|████▉     | 3805/7689 [9:26:01<7:22:07,  6.83s/it]

 49%|████▉     | 3806/7689 [9:26:10<7:57:26,  7.38s/it]
{'loss': 0.9317, 'grad_norm': 0.19353096514299586, 'learning_rate': 0.00010648251562228386, 'epoch': 0.49}


 50%|████▉     | 3808/7689 [9:26:30<9:20:00,  8.66s/it]

 50%|████▉     | 3809/7689 [9:26:42<10:23:39,  9.64s/it]
{'loss': 0.8677, 'grad_norm': 0.19990545132381318, 'learning_rate': 0.00010635640489857914, 'epoch': 0.5}


 50%|████▉     | 3811/7689 [9:26:58<9:29:46,  8.82s/it]

 50%|████▉     | 3812/7689 [9:27:04<8:29:03,  7.88s/it]

 50%|████▉     | 3813/7689 [9:27:12<8:36:45,  8.00s/it]
{'loss': 0.9736, 'grad_norm': 0.18715937150335046, 'learning_rate': 0.00010618824151133385, 'epoch': 0.5}


 50%|████▉     | 3815/7689 [9:27:30<9:07:16,  8.48s/it]

 50%|████▉     | 3816/7689 [9:27:39<9:25:40,  8.76s/it]
{'loss': 0.9493, 'grad_norm': 0.19576313846238574, 'learning_rate': 0.00010606210741526307, 'epoch': 0.5}


 50%|████▉     | 3818/7689 [9:27:52<8:11:13,  7.61s/it]

 50%|████▉     | 3819/7689 [9:27:58<7:37:56,  7.10s/it]

 50%|████▉     | 3820/7689 [9:28:03<7:15:17,  6.75s/it]
{'loss': 1.074, 'grad_norm': 0.1983217910485788, 'learning_rate': 0.0001058939135959572, 'epoch': 0.5}

 50%|████▉     | 3821/7689 [9:28:11<7:24:42,  6.90s/it]


 50%|████▉     | 3823/7689 [9:28:30<9:13:39,  8.59s/it]

 50%|████▉     | 3824/7689 [9:28:38<9:06:58,  8.49s/it]

 50%|████▉     | 3825/7689 [9:28:47<9:16:24,  8.64s/it]

 50%|████▉     | 3826/7689 [9:28:54<8:37:58,  8.05s/it]

 50%|████▉     | 3827/7689 [9:29:04<9:16:57,  8.65s/it]

 50%|████▉     | 3828/7689 [9:29:18<11:05:41, 10.34s/it]
{'loss': 0.928, 'grad_norm': 0.193277751301465, 'learning_rate': 0.00010555747623534831, 'epoch': 0.5}


 50%|████▉     | 3830/7689 [9:29:38<10:32:26,  9.83s/it]

 50%|████▉     | 3831/7689 [9:29:47<10:30:48,  9.81s/it]
{'loss': 0.9591, 'grad_norm': 0.18672541511863136, 'learning_rate': 0.00010543129574881445, 'epoch': 0.5}


 50%|████▉     | 3833/7689 [9:30:10<10:49:05, 10.10s/it]

 50%|████▉     | 3834/7689 [9:30:22<11:14:19, 10.50s/it]
{'loss': 0.7402, 'grad_norm': 0.18676028371600528, 'learning_rate': 0.00010530510658865065, 'epoch': 0.5}


 50%|████▉     | 3836/7689 [9:30:40<10:01:27,  9.37s/it]
{'loss': 1.0803, 'grad_norm': 0.1825363737554202, 'learning_rate': 0.00010522097576270719, 'epoch': 0.5}


 50%|████▉     | 3838/7689 [9:30:57<10:04:34,  9.42s/it]

 50%|████▉     | 3839/7689 [9:31:04<9:11:27,  8.59s/it]

 50%|████▉     | 3840/7689 [9:31:10<8:22:26,  7.83s/it]

 50%|████▉     | 3841/7689 [9:31:16<7:51:35,  7.35s/it]
{'loss': 1.0206, 'grad_norm': 0.21049028145837204, 'learning_rate': 0.0001050106326161756, 'epoch': 0.5}


 50%|████▉     | 3843/7689 [9:31:34<8:45:14,  8.19s/it]

 50%|████▉     | 3844/7689 [9:31:44<9:21:13,  8.76s/it]
{'loss': 0.9434, 'grad_norm': 0.19745570127651513, 'learning_rate': 0.00010488441599941396, 'epoch': 0.5}

 50%|█████     | 3845/7689 [9:31:57<10:36:45,  9.94s/it]

 50%|█████     | 3846/7689 [9:32:09<11:14:54, 10.54s/it]

 50%|█████     | 3847/7689 [9:32:18<10:59:48, 10.30s/it]


 50%|█████     | 3849/7689 [9:32:45<13:13:36, 12.40s/it]

 50%|█████     | 3850/7689 [9:32:52<11:22:06, 10.66s/it]
{'loss': 1.0439, 'grad_norm': 0.2155981773092087, 'learning_rate': 0.00010463195956663338, 'epoch': 0.5}


 50%|█████     | 3852/7689 [9:33:07<9:40:01,  9.07s/it]

 50%|█████     | 3853/7689 [9:33:16<9:27:05,  8.87s/it]

 50%|█████     | 3854/7689 [9:33:30<10:59:13, 10.31s/it]
{'loss': 0.9232, 'grad_norm': 0.1915328356568175, 'learning_rate': 0.00010446363874053778, 'epoch': 0.5}

 50%|█████     | 3855/7689 [9:33:41<11:17:00, 10.59s/it]


 50%|█████     | 3857/7689 [9:33:56<9:37:48,  9.05s/it]
{'loss': 1.0023, 'grad_norm': 0.19741845286008808, 'learning_rate': 0.00010433738977847207, 'epoch': 0.5}


 50%|█████     | 3859/7689 [9:34:14<9:39:54,  9.08s/it]
{'loss': 1.0371, 'grad_norm': 0.18794717850600623, 'learning_rate': 0.0001042532199431534, 'epoch': 0.5}

 50%|█████     | 3860/7689 [9:34:25<10:07:07,  9.51s/it]


 50%|█████     | 3862/7689 [9:34:40<9:02:56,  8.51s/it]
{'loss': 1.0505, 'grad_norm': 0.22195563974945806, 'learning_rate': 0.00010412695954861514, 'epoch': 0.5}


 50%|█████     | 3864/7689 [9:35:00<10:02:52,  9.46s/it]
{'loss': 1.065, 'grad_norm': 0.158431404692854, 'learning_rate': 0.00010404278227834214, 'epoch': 0.5}


 50%|█████     | 3866/7689 [9:35:22<11:06:21, 10.46s/it]

 50%|█████     | 3867/7689 [9:35:28<9:32:24,  8.99s/it]

 50%|█████     | 3868/7689 [9:35:34<8:51:07,  8.34s/it]

 50%|█████     | 3869/7689 [9:35:44<9:18:37,  8.77s/it]

 50%|█████     | 3870/7689 [9:35:52<8:54:33,  8.40s/it]
{'loss': 0.9514, 'grad_norm': 0.1938795637617898, 'learning_rate': 0.00010379023348995072, 'epoch': 0.5}


 50%|█████     | 3872/7689 [9:36:06<8:29:29,  8.01s/it]
{'loss': 0.8095, 'grad_norm': 0.19096501026154883, 'learning_rate': 0.00010370604510046331, 'epoch': 0.5}


 50%|█████     | 3874/7689 [9:36:36<12:17:35, 11.60s/it]
{'loss': 0.9843, 'grad_norm': 0.18794950948110278, 'learning_rate': 0.00010362185408055254, 'epoch': 0.5}

 50%|█████     | 3875/7689 [9:36:53<14:00:37, 13.22s/it]


 50%|█████     | 3877/7689 [9:37:12<12:05:32, 11.42s/it]
{'loss': 0.9194, 'grad_norm': 0.17688407089034663, 'learning_rate': 0.0001034955627493591, 'epoch': 0.5}


 50%|█████     | 3879/7689 [9:37:31<11:06:04, 10.49s/it]
{'loss': 1.0274, 'grad_norm': 0.18717560077313983, 'learning_rate': 0.00010341136541482592, 'epoch': 0.5}

 50%|█████     | 3880/7689 [9:37:37<9:49:23,  9.28s/it]


 50%|█████     | 3882/7689 [9:38:04<11:41:54, 11.06s/it]
{'loss': 1.0211, 'grad_norm': 0.21934861276713863, 'learning_rate': 0.00010328506489182077, 'epoch': 0.5}


 51%|█████     | 3884/7689 [9:38:20<10:01:48,  9.49s/it]

 51%|█████     | 3885/7689 [9:38:28<9:40:52,  9.16s/it]
{'loss': 0.9628, 'grad_norm': 0.18404283230774887, 'learning_rate': 0.00010315875912265732, 'epoch': 0.51}

 51%|█████     | 3886/7689 [9:38:39<10:07:29,  9.58s/it]


 51%|█████     | 3888/7689 [9:38:52<8:17:07,  7.85s/it]
{'loss': 0.7242, 'grad_norm': 0.2074325966592926, 'learning_rate': 0.00010303244830904237, 'epoch': 0.51}


 51%|█████     | 3890/7689 [9:39:10<9:02:21,  8.57s/it]

 51%|█████     | 3891/7689 [9:39:16<8:07:08,  7.70s/it]

 51%|█████     | 3892/7689 [9:39:30<9:57:09,  9.44s/it]

 51%|█████     | 3893/7689 [9:39:36<9:06:52,  8.64s/it]

 51%|█████     | 3894/7689 [9:39:44<8:40:10,  8.22s/it]
{'loss': 0.9115, 'grad_norm': 0.2028713591750325, 'learning_rate': 0.00010277981235532541, 'epoch': 0.51}

 51%|█████     | 3895/7689 [9:39:51<8:28:11,  8.04s/it]

 51%|█████     | 3896/7689 [9:40:01<9:02:57,  8.59s/it]


 51%|█████     | 3898/7689 [9:40:14<7:48:14,  7.41s/it]
{'loss': 1.015, 'grad_norm': 0.20102182464692145, 'learning_rate': 0.00010261137842148669, 'epoch': 0.51}


 51%|█████     | 3900/7689 [9:40:37<9:47:31,  9.30s/it]
 51%|█████     | 3900/7689 [9:40:37<9:47:31,  9.30s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 51%|█████     | 3901/7689 [9:41:16<19:24:10, 18.44s/it]

 51%|█████     | 3902/7689 [9:41:24<15:53:31, 15.11s/it]

 51%|█████     | 3903/7689 [9:41:31<13:24:00, 12.74s/it]

 51%|█████     | 3904/7689 [9:41:37<11:10:41, 10.63s/it]

 51%|█████     | 3905/7689 [9:41:43<9:41:33,  9.22s/it]

 51%|█████     | 3906/7689 [9:41:49<8:45:47,  8.34s/it]
{'loss': 0.7393, 'grad_norm': 0.20147187346952225, 'learning_rate': 0.00010227448879042944, 'epoch': 0.51}


 51%|█████     | 3908/7689 [9:42:11<10:12:19,  9.72s/it]

 51%|█████     | 3909/7689 [9:42:21<10:12:45,  9.73s/it]

 51%|█████     | 3910/7689 [9:42:33<11:00:42, 10.49s/it]
{'loss': 0.856, 'grad_norm': 0.1731268470784454, 'learning_rate': 0.00010210603404966142, 'epoch': 0.51}

 51%|█████     | 3911/7689 [9:42:39<9:47:48,  9.34s/it]


 51%|█████     | 3913/7689 [9:42:53<8:29:42,  8.10s/it]

 51%|█████     | 3914/7689 [9:43:02<8:48:26,  8.40s/it]
{'loss': 0.979, 'grad_norm': 0.19720208767456787, 'learning_rate': 0.0001019375733297323, 'epoch': 0.51}


 51%|█████     | 3916/7689 [9:43:17<8:29:58,  8.11s/it]

 51%|█████     | 3917/7689 [9:43:25<8:19:51,  7.95s/it]
{'loss': 1.1835, 'grad_norm': 0.22407412806609497, 'learning_rate': 0.00010181122415366995, 'epoch': 0.51}

 51%|█████     | 3918/7689 [9:43:40<10:32:46, 10.07s/it]

 51%|█████     | 3919/7689 [9:43:49<10:25:25,  9.95s/it]


 51%|█████     | 3921/7689 [9:44:10<10:32:24, 10.07s/it]
{'loss': 1.2176, 'grad_norm': 0.22540181910862714, 'learning_rate': 0.00010164275412105652, 'epoch': 0.51}


 51%|█████     | 3923/7689 [9:44:27<9:32:56,  9.13s/it]
{'loss': 1.027, 'grad_norm': 0.18567831348566036, 'learning_rate': 0.00010155851732590047, 'epoch': 0.51}


 51%|█████     | 3925/7689 [9:44:51<10:59:54, 10.52s/it]

 51%|█████     | 3926/7689 [9:44:57<9:31:38,  9.11s/it]

 51%|█████     | 3927/7689 [9:45:03<8:39:28,  8.29s/it]
{'loss': 1.0924, 'grad_norm': 0.21668043889609684, 'learning_rate': 0.00010139004047683151, 'epoch': 0.51}


 51%|█████     | 3929/7689 [9:45:27<10:38:35, 10.19s/it]

 51%|█████     | 3930/7689 [9:45:43<12:35:38, 12.06s/it]
{'loss': 0.7963, 'grad_norm': 0.16913881532089894, 'learning_rate': 0.00010126368022403923, 'epoch': 0.51}


 51%|█████     | 3932/7689 [9:45:59<10:17:34,  9.86s/it]
{'loss': 1.129, 'grad_norm': 0.19971363747248438, 'learning_rate': 0.00010117943892190863, 'epoch': 0.51}


 51%|█████     | 3934/7689 [9:46:21<10:34:28, 10.14s/it]

 51%|█████     | 3935/7689 [9:46:28<9:42:54,  9.32s/it]
{'loss': 1.0839, 'grad_norm': 0.18368337830353387, 'learning_rate': 0.00010105307541778802, 'epoch': 0.51}

 51%|█████     | 3936/7689 [9:46:38<9:51:04,  9.45s/it]

 51%|█████     | 3937/7689 [9:46:46<9:17:31,  8.92s/it]

 51%|█████     | 3938/7689 [9:46:51<8:17:02,  7.95s/it]


 51%|█████     | 3940/7689 [9:47:05<7:35:19,  7.29s/it]
{'loss': 1.015, 'grad_norm': 0.1935387096655455, 'learning_rate': 0.00010084246594005875, 'epoch': 0.51}

 51%|█████▏    | 3941/7689 [9:47:12<7:37:53,  7.33s/it]

 51%|█████▏    | 3942/7689 [9:47:18<7:07:21,  6.84s/it]


 51%|█████▏    | 3944/7689 [9:47:39<9:01:12,  8.67s/it]

 51%|█████▏    | 3945/7689 [9:47:48<9:10:18,  8.82s/it]

 51%|█████▏    | 3946/7689 [9:47:55<8:38:57,  8.32s/it]
{'loss': 1.1119, 'grad_norm': 0.19145387825417073, 'learning_rate': 0.00010058972971588708, 'epoch': 0.51}


 51%|█████▏    | 3948/7689 [9:48:06<7:06:54,  6.85s/it]

 51%|█████▏    | 3949/7689 [9:48:15<7:37:19,  7.34s/it]

 51%|█████▏    | 3950/7689 [9:48:21<7:04:21,  6.81s/it]
{'loss': 1.0422, 'grad_norm': 0.22301488041255776, 'learning_rate': 0.00010042123670726567, 'epoch': 0.51}


 51%|█████▏    | 3952/7689 [9:48:41<8:59:54,  8.67s/it]

 51%|█████▏    | 3953/7689 [9:48:47<8:17:37,  7.99s/it]
{'loss': 1.0728, 'grad_norm': 0.20096183276417154, 'learning_rate': 0.00010029486613981849, 'epoch': 0.51}

 51%|█████▏    | 3954/7689 [9:48:58<9:15:47,  8.93s/it]


 51%|█████▏    | 3956/7689 [9:49:21<11:02:02, 10.64s/it]
[2024-05-24 23:20:31,798] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9841, 'grad_norm': 0.18625408143370684, 'learning_rate': 0.00010016849510147816, 'epoch': 0.51}


 51%|█████▏    | 3958/7689 [9:49:34<8:53:16,  8.58s/it]
{'loss': 0.8911, 'grad_norm': 0.21575052702219374, 'learning_rate': 0.0001000842475806371, 'epoch': 0.51}


 52%|█████▏    | 3960/7689 [9:49:49<7:53:15,  7.61s/it]

 52%|█████▏    | 3961/7689 [9:49:54<7:19:19,  7.07s/it]

 52%|█████▏    | 3962/7689 [9:50:01<7:17:38,  7.05s/it]

 52%|█████▏    | 3963/7689 [9:50:14<8:50:45,  8.55s/it]

 52%|█████▏    | 3964/7689 [9:50:19<7:52:15,  7.61s/it]

 52%|█████▏    | 3965/7689 [9:50:25<7:19:06,  7.07s/it]

 52%|█████▏    | 3966/7689 [9:50:35<8:15:59,  7.99s/it]

 52%|█████▏    | 3967/7689 [9:50:41<7:43:48,  7.48s/it]

 52%|█████▏    | 3968/7689 [9:50:49<7:50:22,  7.58s/it]
{'loss': 0.9777, 'grad_norm': 0.22492586996907973, 'learning_rate': 9.966301027541173e-05, 'epoch': 0.52}


 52%|█████▏    | 3970/7689 [9:51:03<7:39:32,  7.41s/it]
{'loss': 0.9447, 'grad_norm': 0.1790544619442886, 'learning_rate': 9.957876329273432e-05, 'epoch': 0.52}

 52%|█████▏    | 3971/7689 [9:51:10<7:21:35,  7.13s/it]


 52%|█████▏    | 3973/7689 [9:51:31<9:08:54,  8.86s/it]

 52%|█████▏    | 3974/7689 [9:51:37<8:08:24,  7.89s/it]
{'loss': 0.9386, 'grad_norm': 0.19748502409866298, 'learning_rate': 9.941027028411297e-05, 'epoch': 0.52}


 52%|█████▏    | 3976/7689 [9:51:55<8:46:42,  8.51s/it]

 52%|█████▏    | 3977/7689 [9:52:05<9:13:55,  8.95s/it]

 52%|█████▏    | 3978/7689 [9:52:10<7:51:35,  7.62s/it]

 52%|█████▏    | 3979/7689 [9:52:15<7:13:26,  7.01s/it]

 52%|█████▏    | 3980/7689 [9:52:25<7:59:08,  7.75s/it]

 52%|█████▏    | 3981/7689 [9:52:31<7:29:46,  7.28s/it]
{'loss': 1.1168, 'grad_norm': 0.20166749021042782, 'learning_rate': 9.91154118355219e-05, 'epoch': 0.52}


 52%|█████▏    | 3983/7689 [9:52:51<8:58:16,  8.71s/it]
{'loss': 0.9667, 'grad_norm': 0.20553153228650392, 'learning_rate': 9.903116786504511e-05, 'epoch': 0.52}


 52%|█████▏    | 3985/7689 [9:53:07<8:29:54,  8.26s/it]

 52%|█████▏    | 3986/7689 [9:53:12<7:15:43,  7.06s/it]

 52%|█████▏    | 3987/7689 [9:53:17<6:47:19,  6.60s/it]
{'loss': 1.1839, 'grad_norm': 0.2169496709045246, 'learning_rate': 9.886268204681565e-05, 'epoch': 0.52}

 52%|█████▏    | 3988/7689 [9:53:24<6:56:26,  6.75s/it]

 52%|█████▏    | 3989/7689 [9:53:32<7:12:57,  7.02s/it]

 52%|█████▏    | 3990/7689 [9:53:46<9:27:01,  9.20s/it]

 52%|█████▏    | 3991/7689 [9:53:56<9:36:10,  9.35s/it]


 52%|█████▏    | 3993/7689 [9:54:13<8:59:16,  8.75s/it]

 52%|█████▏    | 3994/7689 [9:54:21<8:40:47,  8.46s/it]
{'loss': 0.8798, 'grad_norm': 0.18802516047832873, 'learning_rate': 9.856783992224077e-05, 'epoch': 0.52}


 52%|█████▏    | 3996/7689 [9:54:37<8:24:55,  8.20s/it]

 52%|█████▏    | 3997/7689 [9:54:42<7:38:45,  7.46s/it]
{'loss': 0.8994, 'grad_norm': 0.19795977008492982, 'learning_rate': 9.844148267409953e-05, 'epoch': 0.52}

 52%|█████▏    | 3998/7689 [9:54:48<7:12:22,  7.03s/it]

 52%|█████▏    | 3999/7689 [9:54:54<6:43:38,  6.56s/it]


 52%|█████▏    | 4001/7689 [9:55:13<8:10:05,  7.97s/it]

 52%|█████▏    | 4002/7689 [9:55:23<8:34:17,  8.37s/it]

 52%|█████▏    | 4003/7689 [9:55:39<11:01:35, 10.77s/it]
{'loss': 1.082, 'grad_norm': 0.1842886378796253, 'learning_rate': 9.818877584633005e-05, 'epoch': 0.52}


 52%|█████▏    | 4005/7689 [9:55:56<9:57:48,  9.74s/it]
{'loss': 0.9209, 'grad_norm': 0.1824487043614597, 'learning_rate': 9.810454272844694e-05, 'epoch': 0.52}
[2024-05-24 23:27:20,534] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 4006/7689 [9:56:10<11:26:06, 11.18s/it]


 52%|█████▏    | 4008/7689 [9:56:27<10:01:37,  9.81s/it]

 52%|█████▏    | 4009/7689 [9:56:37<9:58:22,  9.76s/it]

 52%|█████▏    | 4010/7689 [9:56:47<10:12:50,  9.99s/it]
{'loss': 1.0742, 'grad_norm': 0.17314163507300434, 'learning_rate': 9.789396595033859e-05, 'epoch': 0.52}

 52%|█████▏    | 4011/7689 [9:56:55<9:24:50,  9.21s/it]


 52%|█████▏    | 4013/7689 [9:57:12<9:20:11,  9.14s/it]
{'loss': 0.9448, 'grad_norm': 0.18441967995190284, 'learning_rate': 9.776762430806145e-05, 'epoch': 0.52}

 52%|█████▏    | 4014/7689 [9:57:20<9:04:41,  8.89s/it]


 52%|█████▏    | 4016/7689 [9:57:38<9:12:46,  9.03s/it]
{'loss': 0.7972, 'grad_norm': 0.18370086019798154, 'learning_rate': 9.764128623082689e-05, 'epoch': 0.52}


 52%|█████▏    | 4018/7689 [9:57:52<8:06:28,  7.95s/it]

 52%|█████▏    | 4019/7689 [9:58:06<9:48:16,  9.62s/it]

 52%|█████▏    | 4020/7689 [9:58:12<8:50:36,  8.68s/it]

 52%|█████▏    | 4021/7689 [9:58:17<7:46:19,  7.63s/it]
{'loss': 0.9805, 'grad_norm': 0.21331197189294848, 'learning_rate': 9.743073123906934e-05, 'epoch': 0.52}
[2024-05-24 23:29:46,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 52%|█████▏    | 4023/7689 [9:58:43<10:01:20,  9.84s/it]

 52%|█████▏    | 4024/7689 [9:58:53<10:04:15,  9.89s/it]

 52%|█████▏    | 4025/7689 [9:58:59<8:52:44,  8.72s/it]

 52%|█████▏    | 4026/7689 [9:59:07<8:38:09,  8.49s/it]

 52%|█████▏    | 4027/7689 [9:59:15<8:30:24,  8.36s/it]
{'loss': 1.0468, 'grad_norm': 0.2099515007513137, 'learning_rate': 9.717808037567021e-05, 'epoch': 0.52}

 52%|█████▏    | 4028/7689 [9:59:24<8:40:56,  8.54s/it]

 52%|█████▏    | 4029/7689 [9:59:37<9:49:32,  9.66s/it]


 52%|█████▏    | 4031/7689 [10:00:00<10:43:24, 10.55s/it]
{'loss': 0.86, 'grad_norm': 0.17258808460274808, 'learning_rate': 9.700965638162111e-05, 'epoch': 0.52}


 52%|█████▏    | 4033/7689 [10:00:15<9:25:34,  9.28s/it]
{'loss': 0.9468, 'grad_norm': 0.17816595843122465, 'learning_rate': 9.692544753837611e-05, 'epoch': 0.52}


 52%|█████▏    | 4035/7689 [10:00:29<8:26:00,  8.31s/it]

 52%|█████▏    | 4036/7689 [10:00:37<8:20:52,  8.23s/it]

 53%|█████▎    | 4037/7689 [10:00:44<7:44:20,  7.63s/it]
{'loss': 1.0453, 'grad_norm': 0.18812637645817923, 'learning_rate': 9.675703645828794e-05, 'epoch': 0.53}

 53%|█████▎    | 4038/7689 [10:00:51<7:32:17,  7.43s/it]

 53%|█████▎    | 4039/7689 [10:00:57<7:09:01,  7.05s/it]


 53%|█████▎    | 4041/7689 [10:01:09<6:42:59,  6.63s/it]
{'loss': 0.9031, 'grad_norm': 0.20992233298172222, 'learning_rate': 9.65886345851741e-05, 'epoch': 0.53}


 53%|█████▎    | 4043/7689 [10:01:23<6:52:59,  6.80s/it]

 53%|█████▎    | 4044/7689 [10:01:30<6:49:23,  6.74s/it]

 53%|█████▎    | 4045/7689 [10:01:36<6:37:47,  6.55s/it]

 53%|█████▎    | 4046/7689 [10:01:44<7:04:30,  6.99s/it]

 53%|█████▎    | 4047/7689 [10:01:51<7:13:51,  7.15s/it]
{'loss': 0.8668, 'grad_norm': 0.20996331717472244, 'learning_rate': 9.633605008442388e-05, 'epoch': 0.53}

 53%|█████▎    | 4048/7689 [10:01:58<7:09:53,  7.08s/it]

 53%|█████▎    | 4049/7689 [10:02:05<6:55:51,  6.85s/it]


 53%|█████▎    | 4051/7689 [10:02:23<8:29:47,  8.41s/it]

 53%|█████▎    | 4052/7689 [10:02:30<7:58:30,  7.89s/it]
{'loss': 1.015, 'grad_norm': 0.19561864493122522, 'learning_rate': 9.612558081073928e-05, 'epoch': 0.53}

 53%|█████▎    | 4053/7689 [10:02:35<7:03:08,  6.98s/it]


 53%|█████▎    | 4055/7689 [10:02:54<8:30:16,  8.43s/it]

 53%|█████▎    | 4056/7689 [10:03:08<10:17:44, 10.20s/it]
{'loss': 0.9749, 'grad_norm': 0.18831365125938326, 'learning_rate': 9.595721772165788e-05, 'epoch': 0.53}


 53%|█████▎    | 4058/7689 [10:03:21<8:26:11,  8.36s/it]
{'loss': 0.8319, 'grad_norm': 0.20837850281089332, 'learning_rate': 9.587304045138489e-05, 'epoch': 0.53}


 53%|█████▎    | 4060/7689 [10:03:42<9:42:59,  9.64s/it]

 53%|█████▎    | 4061/7689 [10:03:52<9:52:57,  9.81s/it]

 53%|█████▎    | 4062/7689 [10:04:00<9:16:18,  9.20s/it]

 53%|█████▎    | 4063/7689 [10:04:08<8:56:45,  8.88s/it]
{'loss': 0.872, 'grad_norm': 0.16027190678234787, 'learning_rate': 9.566261022152794e-05, 'epoch': 0.53}


 53%|█████▎    | 4065/7689 [10:04:26<9:01:07,  8.96s/it]

 53%|█████▎    | 4066/7689 [10:04:40<10:35:15, 10.52s/it]

 53%|█████▎    | 4067/7689 [10:04:50<10:21:22, 10.29s/it]

 53%|█████▎    | 4068/7689 [10:04:59<10:09:35, 10.10s/it]

 53%|█████▎    | 4069/7689 [10:05:08<9:47:45,  9.74s/it]

 53%|█████▎    | 4070/7689 [10:05:24<11:31:22, 11.46s/it]
{'loss': 1.0471, 'grad_norm': 0.2027841306639746, 'learning_rate': 9.536804043336664e-05, 'epoch': 0.53}

 53%|█████▎    | 4071/7689 [10:05:29<9:42:32,  9.66s/it]

 53%|█████▎    | 4072/7689 [10:05:39<9:50:49,  9.80s/it]


 53%|█████▎    | 4074/7689 [10:05:54<8:40:25,  8.64s/it]

 53%|█████▎    | 4075/7689 [10:06:00<8:06:50,  8.08s/it]
{'loss': 0.9464, 'grad_norm': 0.1950441848791221, 'learning_rate': 9.51576579495226e-05, 'epoch': 0.53}

 53%|█████▎    | 4076/7689 [10:06:07<7:33:03,  7.52s/it]


 53%|█████▎    | 4078/7689 [10:06:22<7:28:25,  7.45s/it]

 53%|█████▎    | 4079/7689 [10:06:28<7:10:31,  7.16s/it]

 53%|█████▎    | 4080/7689 [10:06:35<7:10:27,  7.16s/it]

 53%|█████▎    | 4081/7689 [10:06:42<7:08:02,  7.12s/it]

 53%|█████▎    | 4082/7689 [10:06:52<7:50:25,  7.83s/it]
{'loss': 0.8885, 'grad_norm': 0.2005032979516282, 'learning_rate': 9.486315876890527e-05, 'epoch': 0.53}

 53%|█████▎    | 4083/7689 [10:06:58<7:26:14,  7.43s/it]


 53%|█████▎    | 4085/7689 [10:07:20<9:15:42,  9.25s/it]

 53%|█████▎    | 4086/7689 [10:07:30<9:27:48,  9.46s/it]
{'loss': 0.96, 'grad_norm': 0.17188094989706892, 'learning_rate': 9.469489341134939e-05, 'epoch': 0.53}


 53%|█████▎    | 4088/7689 [10:07:48<8:55:24,  8.92s/it]

 53%|█████▎    | 4089/7689 [10:07:58<9:25:22,  9.42s/it]

 53%|█████▎    | 4090/7689 [10:08:09<9:55:38,  9.93s/it]

 53%|█████▎    | 4091/7689 [10:08:17<9:05:13,  9.09s/it]

 53%|█████▎    | 4092/7689 [10:08:28<9:51:42,  9.87s/it]

 53%|█████▎    | 4093/7689 [10:08:38<9:44:11,  9.75s/it]
{'loss': 0.8725, 'grad_norm': 0.2232016387664367, 'learning_rate': 9.440046556477585e-05, 'epoch': 0.53}


 53%|█████▎    | 4095/7689 [10:08:54<8:42:44,  8.73s/it]

 53%|█████▎    | 4096/7689 [10:09:02<8:24:38,  8.43s/it]
{'loss': 1.0198, 'grad_norm': 0.21738764534020186, 'learning_rate': 9.42742969565345e-05, 'epoch': 0.53}


 53%|█████▎    | 4098/7689 [10:09:15<7:18:29,  7.33s/it]

 53%|█████▎    | 4099/7689 [10:09:30<9:40:08,  9.70s/it]
{'loss': 0.7967, 'grad_norm': 0.18826322470578596, 'learning_rate': 9.41481374920835e-05, 'epoch': 0.53}

 53%|█████▎    | 4100/7689 [10:09:39<9:32:42,  9.57s/it]


 53%|█████▎    | 4102/7689 [10:09:54<8:28:33,  8.51s/it]
{'loss': 0.8874, 'grad_norm': 0.19386783600218493, 'learning_rate': 9.402198737289601e-05, 'epoch': 0.53}

 53%|█████▎    | 4103/7689 [10:10:05<9:23:15,  9.42s/it]


 53%|█████▎    | 4105/7689 [10:10:21<8:42:16,  8.74s/it]

 53%|█████▎    | 4106/7689 [10:10:26<7:41:42,  7.73s/it]

 53%|█████▎    | 4107/7689 [10:10:32<6:59:34,  7.03s/it]
{'loss': 0.9058, 'grad_norm': 0.21149746069709846, 'learning_rate': 9.381175848866619e-05, 'epoch': 0.53}


 53%|█████▎    | 4109/7689 [10:10:53<8:28:26,  8.52s/it]
{'loss': 1.0245, 'grad_norm': 0.19955122150592355, 'learning_rate': 9.372767456910308e-05, 'epoch': 0.53}


 53%|█████▎    | 4111/7689 [10:11:08<8:04:37,  8.13s/it]

 53%|█████▎    | 4112/7689 [10:11:14<7:32:07,  7.58s/it]
{'loss': 0.9327, 'grad_norm': 0.207874229834829, 'learning_rate': 9.36015570556842e-05, 'epoch': 0.53}

 53%|█████▎    | 4113/7689 [10:11:21<7:27:22,  7.51s/it]

 54%|█████▎    | 4114/7689 [10:11:34<8:49:16,  8.88s/it]


 54%|█████▎    | 4116/7689 [10:11:48<8:04:52,  8.14s/it]

 54%|█████▎    | 4117/7689 [10:12:02<9:56:31, 10.02s/it]

 54%|█████▎    | 4118/7689 [10:12:12<9:50:45,  9.93s/it]

 54%|█████▎    | 4119/7689 [10:12:20<9:17:49,  9.38s/it]
{'loss': 0.9101, 'grad_norm': 0.1837265472258178, 'learning_rate': 9.330732294298687e-05, 'epoch': 0.54}

 54%|█████▎    | 4120/7689 [10:12:29<9:16:49,  9.36s/it]

 54%|█████▎    | 4121/7689 [10:12:41<10:03:48, 10.15s/it]

 54%|█████▎    | 4122/7689 [10:12:48<8:55:16,  9.00s/it]


 54%|█████▎    | 4124/7689 [10:13:04<8:21:44,  8.44s/it]

 54%|█████▎    | 4125/7689 [10:13:23<11:27:52, 11.58s/it]

 54%|█████▎    | 4126/7689 [10:13:34<11:16:45, 11.40s/it]
{'loss': 0.8517, 'grad_norm': 0.18094676876700005, 'learning_rate': 9.301314702059306e-05, 'epoch': 0.54}


 54%|█████▎    | 4128/7689 [10:13:50<9:47:32,  9.90s/it]
{'loss': 0.9424, 'grad_norm': 0.17285228532808128, 'learning_rate': 9.292910780301556e-05, 'epoch': 0.54}


 54%|█████▎    | 4130/7689 [10:14:02<7:38:02,  7.72s/it]

 54%|█████▎    | 4131/7689 [10:14:21<10:55:32, 11.05s/it]
[2024-05-24 23:45:31,481] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9529, 'grad_norm': 0.19170407942735618, 'learning_rate': 9.280305840530612e-05, 'epoch': 0.54}

 54%|█████▎    | 4132/7689 [10:14:35<11:49:33, 11.97s/it]

 54%|█████▍    | 4133/7689 [10:14:47<11:55:45, 12.08s/it]


 54%|█████▍    | 4135/7689 [10:15:06<10:29:50, 10.63s/it]

 54%|█████▍    | 4136/7689 [10:15:14<9:34:29,  9.70s/it]

 54%|█████▍    | 4137/7689 [10:15:19<8:16:55,  8.39s/it]

 54%|█████▍    | 4138/7689 [10:15:31<9:09:50,  9.29s/it]

 54%|█████▍    | 4139/7689 [10:15:37<8:22:02,  8.49s/it]

 54%|█████▍    | 4140/7689 [10:15:45<8:11:26,  8.31s/it]
{'loss': 0.9834, 'grad_norm': 0.19809851023036582, 'learning_rate': 9.242497997718249e-05, 'epoch': 0.54}


 54%|█████▍    | 4142/7689 [10:15:58<7:19:25,  7.43s/it]

 54%|█████▍    | 4143/7689 [10:16:07<7:40:20,  7.79s/it]
{'loss': 1.0161, 'grad_norm': 0.2101698844053175, 'learning_rate': 9.229897776034382e-05, 'epoch': 0.54}


 54%|█████▍    | 4145/7689 [10:16:27<8:32:29,  8.68s/it]
{'loss': 0.9003, 'grad_norm': 0.20758280636847176, 'learning_rate': 9.221498310243152e-05, 'epoch': 0.54}


 54%|█████▍    | 4147/7689 [10:16:41<7:50:49,  7.98s/it]

 54%|█████▍    | 4148/7689 [10:16:51<8:20:22,  8.48s/it]

 54%|█████▍    | 4149/7689 [10:17:00<8:39:44,  8.81s/it]

 54%|█████▍    | 4150/7689 [10:17:09<8:30:21,  8.65s/it]

 54%|█████▍    | 4151/7689 [10:17:19<8:55:03,  9.07s/it]
{'loss': 0.9388, 'grad_norm': 0.2108643921514247, 'learning_rate': 9.19630325203646e-05, 'epoch': 0.54}


 54%|█████▍    | 4153/7689 [10:17:30<7:20:26,  7.47s/it]
{'loss': 1.2334, 'grad_norm': 0.201385989990663, 'learning_rate': 9.187906032226104e-05, 'epoch': 0.54}


 54%|█████▍    | 4155/7689 [10:17:42<6:38:52,  6.77s/it]

 54%|█████▍    | 4156/7689 [10:17:48<6:24:49,  6.54s/it]

 54%|█████▍    | 4157/7689 [10:17:55<6:27:03,  6.58s/it]

 54%|█████▍    | 4158/7689 [10:18:01<6:11:18,  6.31s/it]

 54%|█████▍    | 4159/7689 [10:18:08<6:36:27,  6.74s/it]

 54%|█████▍    | 4160/7689 [10:18:23<8:51:15,  9.03s/it]
{'loss': 0.9788, 'grad_norm': 0.18962288133400734, 'learning_rate': 9.158520341120452e-05, 'epoch': 0.54}


 54%|█████▍    | 4162/7689 [10:18:51<11:11:04, 11.42s/it]

 54%|█████▍    | 4163/7689 [10:18:56<9:32:19,  9.74s/it]

 54%|█████▍    | 4164/7689 [10:19:15<12:02:45, 12.30s/it]

 54%|█████▍    | 4165/7689 [10:19:29<12:31:54, 12.80s/it]

 54%|█████▍    | 4166/7689 [10:19:38<11:34:38, 11.83s/it]
{'loss': 0.9407, 'grad_norm': 0.20344335432319277, 'learning_rate': 9.133338419421198e-05, 'epoch': 0.54}

 54%|█████▍    | 4167/7689 [10:19:47<10:41:49, 10.93s/it]


 54%|█████▍    | 4169/7689 [10:20:04<9:23:48,  9.61s/it]
{'loss': 0.8973, 'grad_norm': 0.20469429215638615, 'learning_rate': 9.12074952457048e-05, 'epoch': 0.54}

 54%|█████▍    | 4170/7689 [10:20:10<8:13:47,  8.42s/it]

 54%|█████▍    | 4171/7689 [10:20:17<7:53:07,  8.07s/it]


 54%|█████▍    | 4173/7689 [10:20:33<7:38:11,  7.82s/it]

 54%|█████▍    | 4174/7689 [10:20:45<8:49:01,  9.03s/it]
{'loss': 1.0399, 'grad_norm': 0.20191525817331346, 'learning_rate': 9.099771163388082e-05, 'epoch': 0.54}

 54%|█████▍    | 4175/7689 [10:20:49<7:29:50,  7.68s/it]


 54%|█████▍    | 4177/7689 [10:21:05<7:34:28,  7.76s/it]
{'loss': 0.9327, 'grad_norm': 0.2219064602253265, 'learning_rate': 9.087186057577303e-05, 'epoch': 0.54}

 54%|█████▍    | 4178/7689 [10:21:10<6:45:16,  6.93s/it]

 54%|█████▍    | 4179/7689 [10:21:21<8:12:21,  8.42s/it]

 54%|█████▍    | 4180/7689 [10:21:33<9:15:31,  9.50s/it]

 54%|█████▍    | 4181/7689 [10:21:42<8:50:24,  9.07s/it]


 54%|█████▍    | 4183/7689 [10:21:57<8:18:31,  8.53s/it]
{'loss': 0.926, 'grad_norm': 0.18603805050946245, 'learning_rate': 9.06202023926779e-05, 'epoch': 0.54}

 54%|█████▍    | 4184/7689 [10:22:04<7:44:11,  7.95s/it]


 54%|█████▍    | 4186/7689 [10:22:21<7:54:40,  8.13s/it]

 54%|█████▍    | 4187/7689 [10:22:27<7:19:19,  7.53s/it]
{'loss': 1.0409, 'grad_norm': 0.19867128308412385, 'learning_rate': 9.045246345867085e-05, 'epoch': 0.54}

 54%|█████▍    | 4188/7689 [10:22:38<8:22:25,  8.61s/it]

 54%|█████▍    | 4189/7689 [10:22:49<9:12:55,  9.48s/it]

 54%|█████▍    | 4190/7689 [10:22:58<8:55:46,  9.19s/it]


 55%|█████▍    | 4192/7689 [10:23:13<8:16:18,  8.52s/it]

 55%|█████▍    | 4193/7689 [10:23:25<9:15:31,  9.53s/it]
{'loss': 1.1033, 'grad_norm': 0.21012128293691087, 'learning_rate': 9.020090603029767e-05, 'epoch': 0.55}

 55%|█████▍    | 4194/7689 [10:23:34<8:59:52,  9.27s/it]
[2024-05-24 23:55:01,736] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 55%|█████▍    | 4195/7689 [10:23:51<11:25:45, 11.78s/it]

 55%|█████▍    | 4196/7689 [10:24:00<10:37:06, 10.94s/it]

 55%|█████▍    | 4197/7689 [10:24:08<9:32:26,  9.84s/it]

 55%|█████▍    | 4198/7689 [10:24:12<8:02:00,  8.28s/it]


 55%|█████▍    | 4200/7689 [10:24:29<7:55:20,  8.17s/it]
 55%|█████▍    | 4200/7689 [10:24:29<7:55:20,  8.17s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 55%|█████▍    | 4201/7689 [10:25:09<17:20:06, 17.89s/it]

 55%|█████▍    | 4202/7689 [10:25:23<16:08:04, 16.66s/it]
{'loss': 1.0927, 'grad_norm': 0.19240668249676138, 'learning_rate': 8.9823687756372e-05, 'epoch': 0.55}

 55%|█████▍    | 4203/7689 [10:25:29<13:02:03, 13.46s/it]


 55%|█████▍    | 4205/7689 [10:25:57<13:28:52, 13.93s/it]
{'loss': 1.0778, 'grad_norm': 0.15537868154838824, 'learning_rate': 8.969798056660075e-05, 'epoch': 0.55}

 55%|█████▍    | 4206/7689 [10:26:08<12:25:46, 12.85s/it]

 55%|█████▍    | 4207/7689 [10:26:23<13:08:44, 13.59s/it]

 55%|█████▍    | 4208/7689 [10:26:30<11:06:52, 11.49s/it]

 55%|█████▍    | 4209/7689 [10:26:38<10:06:00, 10.45s/it]

 55%|█████▍    | 4210/7689 [10:26:45<9:05:01,  9.40s/it]

 55%|█████▍    | 4211/7689 [10:26:51<8:08:56,  8.43s/it]

 55%|█████▍    | 4212/7689 [10:26:59<8:09:29,  8.45s/it]

 55%|█████▍    | 4213/7689 [10:27:08<8:07:42,  8.42s/it]

 55%|█████▍    | 4214/7689 [10:27:16<8:13:59,  8.53s/it]

 55%|█████▍    | 4215/7689 [10:27:24<7:54:47,  8.20s/it]

 55%|█████▍    | 4216/7689 [10:27:36<9:00:32,  9.34s/it]

 55%|█████▍    | 4217/7689 [10:27:40<7:40:22,  7.96s/it]

 55%|█████▍    | 4218/7689 [10:27:52<8:39:35,  8.98s/it]

 55%|█████▍    | 4219/7689 [10:27:59<8:07:49,  8.44s/it]


 55%|█████▍    | 4221/7689 [10:28:13<7:38:20,  7.93s/it]
{'loss': 1.0231, 'grad_norm': 0.19928032931540698, 'learning_rate': 8.902782497400639e-05, 'epoch': 0.55}

 55%|█████▍    | 4222/7689 [10:28:19<6:51:14,  7.12s/it]

 55%|█████▍    | 4223/7689 [10:28:27<7:14:48,  7.53s/it]

 55%|█████▍    | 4224/7689 [10:28:35<7:17:11,  7.57s/it]

 55%|█████▍    | 4225/7689 [10:28:41<6:48:29,  7.08s/it]

 55%|█████▍    | 4226/7689 [10:28:51<7:38:36,  7.95s/it]

 55%|█████▍    | 4227/7689 [10:29:00<8:09:34,  8.48s/it]

 55%|█████▍    | 4228/7689 [10:29:14<9:42:30, 10.10s/it]

 55%|█████▌    | 4229/7689 [10:29:27<10:20:14, 10.76s/it]

 55%|█████▌    | 4230/7689 [10:29:38<10:39:12, 11.09s/it]

 55%|█████▌    | 4231/7689 [10:29:44<9:07:52,  9.51s/it]

 55%|█████▌    | 4232/7689 [10:29:52<8:32:23,  8.89s/it]

 55%|█████▌    | 4233/7689 [10:29:58<7:48:53,  8.14s/it]

 55%|█████▌    | 4234/7689 [10:30:06<7:52:46,  8.21s/it]

 55%|█████▌    | 4235/7689 [10:30:13<7:15:31,  7.57s/it]

 55%|█████▌    | 4236/7689 [10:30:21<7:29:24,  7.81s/it]

 55%|█████▌    | 4237/7689 [10:30:27<6:55:08,  7.22s/it]

 55%|█████▌    | 4238/7689 [10:30:33<6:34:48,  6.86s/it]

 55%|█████▌    | 4239/7689 [10:30:39<6:21:11,  6.63s/it]

 55%|█████▌    | 4240/7689 [10:30:45<6:05:54,  6.37s/it]

 55%|█████▌    | 4241/7689 [10:30:54<7:02:54,  7.36s/it]

 55%|█████▌    | 4242/7689 [10:30:59<6:20:31,  6.62s/it]

 55%|█████▌    | 4243/7689 [10:31:11<7:50:26,  8.19s/it]

 55%|█████▌    | 4244/7689 [10:31:17<7:05:33,  7.41s/it]

 55%|█████▌    | 4245/7689 [10:31:24<7:04:13,  7.39s/it]

 55%|█████▌    | 4246/7689 [10:31:35<8:06:53,  8.48s/it]

 55%|█████▌    | 4247/7689 [10:31:41<7:29:50,  7.84s/it]

 55%|█████▌    | 4248/7689 [10:31:47<6:54:42,  7.23s/it]

 55%|█████▌    | 4249/7689 [10:31:56<7:19:45,  7.67s/it]


 55%|█████▌    | 4251/7689 [10:32:14<7:41:57,  8.06s/it]
{'loss': 0.9695, 'grad_norm': 0.19154261807284537, 'learning_rate': 8.777265052039962e-05, 'epoch': 0.55}

 55%|█████▌    | 4252/7689 [10:32:20<7:19:27,  7.67s/it]

 55%|█████▌    | 4253/7689 [10:32:30<7:52:58,  8.26s/it]

 55%|█████▌    | 4254/7689 [10:32:41<8:32:59,  8.96s/it]

 55%|█████▌    | 4255/7689 [10:32:47<7:45:58,  8.14s/it]

 55%|█████▌    | 4256/7689 [10:32:55<7:54:48,  8.30s/it]

 55%|█████▌    | 4257/7689 [10:33:07<8:44:50,  9.18s/it]


 55%|█████▌    | 4259/7689 [10:33:22<7:43:15,  8.10s/it]
{'loss': 0.9366, 'grad_norm': 0.1984187699686478, 'learning_rate': 8.743825884524714e-05, 'epoch': 0.55}

 55%|█████▌    | 4260/7689 [10:33:27<6:52:27,  7.22s/it]

 55%|█████▌    | 4261/7689 [10:33:41<8:56:38,  9.39s/it]

 55%|█████▌    | 4262/7689 [10:33:50<8:44:12,  9.18s/it]

 55%|█████▌    | 4263/7689 [10:34:01<9:14:18,  9.71s/it]

 55%|█████▌    | 4264/7689 [10:34:08<8:36:15,  9.04s/it]

 55%|█████▌    | 4265/7689 [10:34:15<7:48:46,  8.21s/it]


 55%|█████▌    | 4267/7689 [10:34:34<8:35:06,  9.03s/it]
{'loss': 0.997, 'grad_norm': 0.19576812728573492, 'learning_rate': 8.710400982423817e-05, 'epoch': 0.55}

 56%|█████▌    | 4268/7689 [10:34:51<11:00:02, 11.58s/it]

 56%|█████▌    | 4269/7689 [10:34:57<9:26:13,  9.93s/it]


 56%|█████▌    | 4271/7689 [10:35:14<8:40:27,  9.14s/it]
{'loss': 0.9919, 'grad_norm': 0.19347480593168004, 'learning_rate': 8.693693999529316e-05, 'epoch': 0.56}

 56%|█████▌    | 4272/7689 [10:35:19<7:34:55,  7.99s/it]

 56%|█████▌    | 4273/7689 [10:35:29<8:02:48,  8.48s/it]

 56%|█████▌    | 4274/7689 [10:35:33<6:59:11,  7.36s/it]

 56%|█████▌    | 4275/7689 [10:35:44<7:45:55,  8.19s/it]


 56%|█████▌    | 4277/7689 [10:36:00<7:56:49,  8.38s/it]
{'loss': 1.1111, 'grad_norm': 0.18537043719727705, 'learning_rate': 8.668640493789077e-05, 'epoch': 0.56}

 56%|█████▌    | 4278/7689 [10:36:06<7:21:59,  7.77s/it]

 56%|█████▌    | 4279/7689 [10:36:11<6:33:44,  6.93s/it]


 56%|█████▌    | 4281/7689 [10:36:35<9:35:11, 10.13s/it]
{'loss': 0.987, 'grad_norm': 0.18409410628208886, 'learning_rate': 8.651942871515921e-05, 'epoch': 0.56}

 56%|█████▌    | 4282/7689 [10:36:44<8:59:57,  9.51s/it]


 56%|█████▌    | 4284/7689 [10:37:02<8:58:24,  9.49s/it]

 56%|█████▌    | 4285/7689 [10:37:16<10:17:35, 10.89s/it]
{'loss': 0.9821, 'grad_norm': 0.18752548043459996, 'learning_rate': 8.635249076460443e-05, 'epoch': 0.56}


 56%|█████▌    | 4287/7689 [10:37:42<11:02:02, 11.68s/it]

 56%|█████▌    | 4288/7689 [10:37:48<9:26:58, 10.00s/it]

 56%|█████▌    | 4289/7689 [10:38:00<9:56:34, 10.53s/it]
{'loss': 0.9298, 'grad_norm': 0.19476349512881655, 'learning_rate': 8.618559156017369e-05, 'epoch': 0.56}

 56%|█████▌    | 4290/7689 [10:38:09<9:32:47, 10.11s/it]

 56%|█████▌    | 4291/7689 [10:38:18<9:18:12,  9.86s/it]

 56%|█████▌    | 4292/7689 [10:38:27<9:01:41,  9.57s/it]


 56%|█████▌    | 4294/7689 [10:38:42<8:10:50,  8.67s/it]
{'loss': 0.9853, 'grad_norm': 0.17277838450879335, 'learning_rate': 8.597702276322289e-05, 'epoch': 0.56}

 56%|█████▌    | 4295/7689 [10:38:49<7:32:58,  8.01s/it]


 56%|█████▌    | 4297/7689 [10:39:10<8:38:24,  9.17s/it]
{'loss': 0.9988, 'grad_norm': 0.19679615546524762, 'learning_rate': 8.585191128492159e-05, 'epoch': 0.56}

 56%|█████▌    | 4298/7689 [10:39:19<8:38:19,  9.17s/it]

 56%|█████▌    | 4299/7689 [10:39:27<8:18:57,  8.83s/it]


 56%|█████▌    | 4301/7689 [10:39:40<7:08:53,  7.60s/it]
{'loss': 0.933, 'grad_norm': 0.17949792906720657, 'learning_rate': 8.568513116143919e-05, 'epoch': 0.56}


 56%|█████▌    | 4303/7689 [10:39:54<6:58:07,  7.41s/it]
{'loss': 1.0718, 'grad_norm': 0.21619343966447896, 'learning_rate': 8.560175631040854e-05, 'epoch': 0.56}

 56%|█████▌    | 4304/7689 [10:40:06<8:07:45,  8.65s/it]

 56%|█████▌    | 4305/7689 [10:40:17<8:51:04,  9.42s/it]

 56%|█████▌    | 4306/7689 [10:40:21<7:29:47,  7.98s/it]

 56%|█████▌    | 4307/7689 [10:40:34<8:45:01,  9.31s/it]

 56%|█████▌    | 4308/7689 [10:40:40<7:48:34,  8.32s/it]

 56%|█████▌    | 4309/7689 [10:40:52<8:46:34,  9.35s/it]

 56%|█████▌    | 4310/7689 [10:41:01<8:48:15,  9.38s/it]

 56%|█████▌    | 4311/7689 [10:41:09<8:16:01,  8.81s/it]


 56%|█████▌    | 4313/7689 [10:41:26<8:09:32,  8.70s/it]
{'loss': 0.9229, 'grad_norm': 0.1981495306382115, 'learning_rate': 8.518503652920575e-05, 'epoch': 0.56}

 56%|█████▌    | 4314/7689 [10:41:31<7:07:17,  7.60s/it]


 56%|█████▌    | 4316/7689 [10:41:42<6:07:16,  6.53s/it]
{'loss': 0.9458, 'grad_norm': 0.2139140487731815, 'learning_rate': 8.506007151981527e-05, 'epoch': 0.56}

 56%|█████▌    | 4317/7689 [10:41:49<6:07:59,  6.55s/it]

 56%|█████▌    | 4318/7689 [10:41:55<6:00:02,  6.41s/it]

 56%|█████▌    | 4319/7689 [10:42:08<7:46:59,  8.31s/it]

 56%|█████▌    | 4320/7689 [10:42:14<7:02:12,  7.52s/it]

 56%|█████▌    | 4321/7689 [10:42:19<6:30:12,  6.95s/it]

 56%|█████▌    | 4322/7689 [10:42:31<7:51:21,  8.40s/it]

 56%|█████▌    | 4323/7689 [10:42:37<7:13:54,  7.73s/it]

 56%|█████▌    | 4324/7689 [10:42:45<7:15:16,  7.76s/it]

 56%|█████▌    | 4325/7689 [10:42:58<8:37:44,  9.23s/it]

 56%|█████▋    | 4326/7689 [10:43:07<8:45:06,  9.37s/it]

 56%|█████▋    | 4327/7689 [10:43:23<10:32:30, 11.29s/it]

 56%|█████▋    | 4328/7689 [10:43:29<9:03:38,  9.71s/it]

 56%|█████▋    | 4329/7689 [10:43:39<9:01:53,  9.68s/it]

 56%|█████▋    | 4330/7689 [10:43:45<8:01:07,  8.59s/it]

 56%|█████▋    | 4331/7689 [10:43:52<7:33:13,  8.10s/it]

 56%|█████▋    | 4332/7689 [10:44:00<7:29:29,  8.03s/it]

 56%|█████▋    | 4333/7689 [10:44:08<7:37:20,  8.18s/it]

 56%|█████▋    | 4334/7689 [10:44:19<8:22:15,  8.98s/it]


 56%|█████▋    | 4336/7689 [10:44:34<7:44:49,  8.32s/it]
{'loss': 0.7918, 'grad_norm': 0.16598288954168863, 'learning_rate': 8.422759080773548e-05, 'epoch': 0.56}


 56%|█████▋    | 4338/7689 [10:44:52<8:16:44,  8.89s/it]

 56%|█████▋    | 4339/7689 [10:44:59<7:30:16,  8.06s/it]
{'loss': 0.8536, 'grad_norm': 0.19023181342531464, 'learning_rate': 8.410281381186577e-05, 'epoch': 0.56}

 56%|█████▋    | 4340/7689 [10:45:09<8:15:58,  8.89s/it]


 56%|█████▋    | 4342/7689 [10:45:34<9:53:01, 10.63s/it]
{'loss': 0.9005, 'grad_norm': 0.17320551704131057, 'learning_rate': 8.397806220336529e-05, 'epoch': 0.56}

 56%|█████▋    | 4343/7689 [10:45:52<11:42:38, 12.60s/it]

 56%|█████▋    | 4344/7689 [10:45:57<9:42:07, 10.44s/it]

 57%|█████▋    | 4345/7689 [10:46:07<9:35:27, 10.33s/it]

 57%|█████▋    | 4346/7689 [10:46:14<8:36:18,  9.27s/it]

 57%|█████▋    | 4347/7689 [10:46:19<7:30:04,  8.08s/it]


 57%|█████▋    | 4349/7689 [10:46:43<9:32:37, 10.29s/it]
{'loss': 0.8424, 'grad_norm': 0.18876510621086257, 'learning_rate': 8.36870749645486e-05, 'epoch': 0.57}

 57%|█████▋    | 4350/7689 [10:46:49<8:29:36,  9.16s/it]


 57%|█████▋    | 4352/7689 [10:47:03<7:19:33,  7.90s/it]
{'loss': 0.9526, 'grad_norm': 0.2005550786096145, 'learning_rate': 8.356240941922858e-05, 'epoch': 0.57}

 57%|█████▋    | 4353/7689 [10:47:10<7:11:12,  7.76s/it]

 57%|█████▋    | 4354/7689 [10:47:17<6:57:16,  7.51s/it]

 57%|█████▋    | 4355/7689 [10:47:34<9:36:22, 10.37s/it]

 57%|█████▋    | 4356/7689 [10:47:44<9:24:11, 10.16s/it]

 57%|█████▋    | 4357/7689 [10:47:51<8:38:10,  9.33s/it]

 57%|█████▋    | 4358/7689 [10:48:00<8:35:31,  9.29s/it]

 57%|█████▋    | 4359/7689 [10:48:07<7:56:30,  8.59s/it]

 57%|█████▋    | 4360/7689 [10:48:21<9:23:22, 10.15s/it]
[2024-05-25 00:19:50,473] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 4361/7689 [10:48:40<11:51:58, 12.84s/it]

 57%|█████▋    | 4362/7689 [10:48:49<10:52:07, 11.76s/it]


 57%|█████▋    | 4364/7689 [10:49:07<9:33:35, 10.35s/it]
{'loss': 1.2066, 'grad_norm': 0.19717822768492213, 'learning_rate': 8.306401173199437e-05, 'epoch': 0.57}

 57%|█████▋    | 4365/7689 [10:49:14<8:35:37,  9.31s/it]

 57%|█████▋    | 4366/7689 [10:49:24<8:44:59,  9.48s/it]

 57%|█████▋    | 4367/7689 [10:49:36<9:34:53, 10.38s/it]

 57%|█████▋    | 4368/7689 [10:49:42<8:26:04,  9.14s/it]


 57%|█████▋    | 4370/7689 [10:49:53<6:35:43,  7.15s/it]
{'loss': 1.0552, 'grad_norm': 0.21961346732827794, 'learning_rate': 8.281497437040127e-05, 'epoch': 0.57}


 57%|█████▋    | 4372/7689 [10:50:11<7:16:39,  7.90s/it]
{'loss': 1.0085, 'grad_norm': 0.18556658501788897, 'learning_rate': 8.273198623266672e-05, 'epoch': 0.57}

 57%|█████▋    | 4373/7689 [10:50:22<8:08:59,  8.85s/it]
[2024-05-25 00:21:44,117] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 4374/7689 [10:50:34<8:57:18,  9.72s/it]


 57%|█████▋    | 4376/7689 [10:50:51<8:25:11,  9.15s/it]
{'loss': 1.0061, 'grad_norm': 0.2033352485129028, 'learning_rate': 8.256604678481803e-05, 'epoch': 0.57}


 57%|█████▋    | 4378/7689 [10:51:09<8:13:48,  8.95s/it]
{'loss': 0.9387, 'grad_norm': 0.1926085988241648, 'learning_rate': 8.248309559248203e-05, 'epoch': 0.57}


 57%|█████▋    | 4380/7689 [10:51:37<10:43:35, 11.67s/it]
{'loss': 0.7656, 'grad_norm': 0.19783350884165082, 'learning_rate': 8.240015683304236e-05, 'epoch': 0.57}

 57%|█████▋    | 4381/7689 [10:51:44<9:33:08, 10.40s/it]

 57%|█████▋    | 4382/7689 [10:51:52<8:44:24,  9.51s/it]

 57%|█████▋    | 4383/7689 [10:52:02<8:59:23,  9.79s/it]

 57%|█████▋    | 4384/7689 [10:52:08<7:52:06,  8.57s/it]

 57%|█████▋    | 4385/7689 [10:52:16<7:41:17,  8.38s/it]

 57%|█████▋    | 4386/7689 [10:52:24<7:26:53,  8.12s/it]

 57%|█████▋    | 4387/7689 [10:52:29<6:46:56,  7.39s/it]

 57%|█████▋    | 4388/7689 [10:52:41<7:53:36,  8.61s/it]

 57%|█████▋    | 4389/7689 [10:52:53<8:47:46,  9.60s/it]

 57%|█████▋    | 4390/7689 [10:52:58<7:45:07,  8.46s/it]


 57%|█████▋    | 4392/7689 [10:53:17<8:13:25,  8.98s/it]
{'loss': 0.7395, 'grad_norm': 0.17086999324182764, 'learning_rate': 8.190278866316111e-05, 'epoch': 0.57}

 57%|█████▋    | 4393/7689 [10:53:25<8:04:46,  8.82s/it]

 57%|█████▋    | 4394/7689 [10:53:31<7:04:07,  7.72s/it]


 57%|█████▋    | 4396/7689 [10:53:45<6:49:44,  7.47s/it]
{'loss': 1.0444, 'grad_norm': 0.1943805578293548, 'learning_rate': 8.173710140397057e-05, 'epoch': 0.57}

 57%|█████▋    | 4397/7689 [10:53:51<6:20:50,  6.94s/it]

 57%|█████▋    | 4398/7689 [10:54:06<8:33:05,  9.35s/it]

 57%|█████▋    | 4399/7689 [10:54:11<7:34:46,  8.29s/it]

 57%|█████▋    | 4400/7689 [10:54:23<8:22:19,  9.16s/it]

 57%|█████▋    | 4401/7689 [10:54:31<8:01:25,  8.79s/it]


 57%|█████▋    | 4403/7689 [10:54:51<8:29:47,  9.31s/it]

 57%|█████▋    | 4404/7689 [10:55:01<8:43:15,  9.56s/it]
{'loss': 1.1928, 'grad_norm': 0.1810753973746874, 'learning_rate': 8.140588290433951e-05, 'epoch': 0.57}

 57%|█████▋    | 4405/7689 [10:55:10<8:36:29,  9.44s/it]

 57%|█████▋    | 4406/7689 [10:55:16<7:27:40,  8.18s/it]

 57%|█████▋    | 4407/7689 [10:55:22<7:03:06,  7.73s/it]

 57%|█████▋    | 4408/7689 [10:55:31<7:10:08,  7.87s/it]

 57%|█████▋    | 4409/7689 [10:55:40<7:38:47,  8.39s/it]

 57%|█████▋    | 4410/7689 [10:55:46<6:49:43,  7.50s/it]

 57%|█████▋    | 4411/7689 [10:55:52<6:29:49,  7.14s/it]


 57%|█████▋    | 4413/7689 [10:56:07<6:39:49,  7.32s/it]
{'loss': 1.0464, 'grad_norm': 0.2006329963154273, 'learning_rate': 8.103351468078841e-05, 'epoch': 0.57}


 57%|█████▋    | 4415/7689 [10:56:29<8:09:42,  8.97s/it]
{'loss': 0.9667, 'grad_norm': 0.19727255949846847, 'learning_rate': 8.09508030181055e-05, 'epoch': 0.57}

 57%|█████▋    | 4416/7689 [10:56:36<7:40:45,  8.45s/it]

 57%|█████▋    | 4417/7689 [10:56:48<8:27:46,  9.31s/it]


 57%|█████▋    | 4419/7689 [10:57:03<7:32:55,  8.31s/it]
{'loss': 1.1109, 'grad_norm': 0.202984032276613, 'learning_rate': 8.078542031283069e-05, 'epoch': 0.57}

 57%|█████▋    | 4420/7689 [10:57:10<7:01:41,  7.74s/it]


 58%|█████▊    | 4422/7689 [10:57:29<8:06:45,  8.94s/it]

 58%|█████▊    | 4423/7689 [10:57:35<7:18:49,  8.06s/it]
{'loss': 0.9995, 'grad_norm': 0.21355874172341108, 'learning_rate': 8.062009215893772e-05, 'epoch': 0.58}

 58%|█████▊    | 4424/7689 [10:57:42<7:01:04,  7.74s/it]

 58%|█████▊    | 4425/7689 [10:57:56<8:44:44,  9.65s/it]

 58%|█████▊    | 4426/7689 [10:58:11<10:02:57, 11.09s/it]

 58%|█████▊    | 4427/7689 [10:58:18<8:57:22,  9.88s/it]

 58%|█████▊    | 4428/7689 [10:58:31<9:42:16, 10.71s/it]

 58%|█████▊    | 4429/7689 [10:58:41<9:33:54, 10.56s/it]

 58%|█████▊    | 4430/7689 [10:58:54<10:14:12, 11.31s/it]

 58%|█████▊    | 4431/7689 [10:59:02<9:28:05, 10.46s/it]


 58%|█████▊    | 4433/7689 [10:59:15<7:38:57,  8.46s/it]
{'loss': 1.0449, 'grad_norm': 0.19762735359236352, 'learning_rate': 8.020701351640182e-05, 'epoch': 0.58}


 58%|█████▊    | 4435/7689 [10:59:42<9:55:16, 10.98s/it]
{'loss': 0.7857, 'grad_norm': 0.16302898746612085, 'learning_rate': 8.012443969853616e-05, 'epoch': 0.58}

 58%|█████▊    | 4436/7689 [10:59:53<9:59:35, 11.06s/it]

 58%|█████▊    | 4437/7689 [11:00:06<10:39:37, 11.80s/it]

 58%|█████▊    | 4438/7689 [11:00:14<9:34:53, 10.61s/it]

 58%|█████▊    | 4439/7689 [11:00:19<8:04:52,  8.95s/it]

 58%|█████▊    | 4440/7689 [11:00:27<7:45:22,  8.59s/it]

 58%|█████▊    | 4441/7689 [11:00:32<6:44:14,  7.47s/it]

 58%|█████▊    | 4442/7689 [11:00:37<6:04:27,  6.73s/it]

 58%|█████▊    | 4443/7689 [11:00:44<6:14:14,  6.92s/it]

 58%|█████▊    | 4444/7689 [11:00:51<6:03:42,  6.73s/it]

 58%|█████▊    | 4445/7689 [11:01:03<7:34:36,  8.41s/it]

 58%|█████▊    | 4446/7689 [11:01:09<6:53:56,  7.66s/it]

 58%|█████▊    | 4447/7689 [11:01:15<6:29:16,  7.20s/it]

 58%|█████▊    | 4448/7689 [11:01:27<7:53:36,  8.77s/it]


 58%|█████▊    | 4450/7689 [11:01:41<7:12:03,  8.00s/it]

 58%|█████▊    | 4451/7689 [11:01:50<7:13:42,  8.04s/it]
{'loss': 1.1225, 'grad_norm': 0.21403802114639667, 'learning_rate': 7.946436192818534e-05, 'epoch': 0.58}

 58%|█████▊    | 4452/7689 [11:01:56<6:51:14,  7.62s/it]

 58%|█████▊    | 4453/7689 [11:02:02<6:26:05,  7.16s/it]

 58%|█████▊    | 4454/7689 [11:02:07<5:44:35,  6.39s/it]

 58%|█████▊    | 4455/7689 [11:02:15<6:13:34,  6.93s/it]

 58%|█████▊    | 4456/7689 [11:02:20<5:48:09,  6.46s/it]

 58%|█████▊    | 4457/7689 [11:02:26<5:38:01,  6.28s/it]

 58%|█████▊    | 4458/7689 [11:02:38<7:07:29,  7.94s/it]

 58%|█████▊    | 4459/7689 [11:02:52<8:45:44,  9.77s/it]

 58%|█████▊    | 4460/7689 [11:03:03<8:55:48,  9.96s/it]

 58%|█████▊    | 4461/7689 [11:03:15<9:34:17, 10.67s/it]

 58%|█████▊    | 4462/7689 [11:03:32<11:19:07, 12.63s/it]

 58%|█████▊    | 4463/7689 [11:03:41<10:10:43, 11.36s/it]

 58%|█████▊    | 4464/7689 [11:03:48<9:07:06, 10.18s/it]

 58%|█████▊    | 4465/7689 [11:03:54<8:04:13,  9.01s/it]

 58%|█████▊    | 4466/7689 [11:04:02<7:50:19,  8.76s/it]

 58%|█████▊    | 4467/7689 [11:04:13<8:22:45,  9.36s/it]

 58%|█████▊    | 4468/7689 [11:04:19<7:30:54,  8.40s/it]

 58%|█████▊    | 4469/7689 [11:04:25<6:39:08,  7.44s/it]

 58%|█████▊    | 4470/7689 [11:04:32<6:47:44,  7.60s/it]


 58%|█████▊    | 4472/7689 [11:04:54<8:27:06,  9.46s/it]
{'loss': 1.0981, 'grad_norm': 0.17076949273012787, 'learning_rate': 7.859943022812786e-05, 'epoch': 0.58}

 58%|█████▊    | 4473/7689 [11:04:59<7:19:09,  8.19s/it]

 58%|█████▊    | 4474/7689 [11:05:07<7:13:17,  8.09s/it]

 58%|█████▊    | 4475/7689 [11:05:14<6:59:29,  7.83s/it]

 58%|█████▊    | 4476/7689 [11:05:19<6:02:13,  6.76s/it]

 58%|█████▊    | 4477/7689 [11:05:29<7:05:44,  7.95s/it]

 58%|█████▊    | 4478/7689 [11:05:35<6:32:39,  7.34s/it]

 58%|█████▊    | 4479/7689 [11:05:45<7:12:16,  8.08s/it]

 58%|█████▊    | 4480/7689 [11:05:52<6:57:44,  7.81s/it]

 58%|█████▊    | 4481/7689 [11:05:59<6:35:15,  7.39s/it]

 58%|█████▊    | 4482/7689 [11:06:06<6:38:50,  7.46s/it]

 58%|█████▊    | 4483/7689 [11:06:14<6:49:22,  7.66s/it]

 58%|█████▊    | 4484/7689 [11:06:21<6:39:28,  7.48s/it]

 58%|█████▊    | 4485/7689 [11:06:29<6:42:14,  7.53s/it]

 58%|█████▊    | 4486/7689 [11:06:39<7:21:29,  8.27s/it]

 58%|█████▊    | 4487/7689 [11:06:45<6:42:46,  7.55s/it]


 58%|█████▊    | 4489/7689 [11:07:02<6:55:34,  7.79s/it]

 58%|█████▊    | 4490/7689 [11:07:12<7:29:16,  8.43s/it]

 58%|█████▊    | 4491/7689 [11:07:18<6:49:18,  7.68s/it]

 58%|█████▊    | 4492/7689 [11:07:32<8:35:33,  9.68s/it]
{'loss': 0.9293, 'grad_norm': 0.19464617490542732, 'learning_rate': 7.777724165675788e-05, 'epoch': 0.58}


 58%|█████▊    | 4494/7689 [11:07:56<9:39:04, 10.87s/it]
{'loss': 0.6638, 'grad_norm': 0.16090407948654661, 'learning_rate': 7.769510858842175e-05, 'epoch': 0.58}

 58%|█████▊    | 4495/7689 [11:08:02<8:27:28,  9.53s/it]


 58%|█████▊    | 4497/7689 [11:08:26<9:40:27, 10.91s/it]

 58%|█████▊    | 4498/7689 [11:08:34<8:54:30, 10.05s/it]
{'loss': 0.9211, 'grad_norm': 0.20922564296943283, 'learning_rate': 7.753089000376807e-05, 'epoch': 0.58}


 59%|█████▊    | 4500/7689 [11:09:00<10:24:06, 11.74s/it]
 59%|█████▊    | 4500/7689 [11:09:00<10:24:06, 11.74s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.9765, 'grad_norm': 0.21382342455350198, 'learning_rate': 7.740776790275954e-05, 'epoch': 0.59}
 59%|█████▊    | 4501/7689 [11:09:40<17:57:46, 20.28s/it]

 59%|█████▊    | 4502/7689 [11:09:50<14:59:41, 16.94s/it]

 59%|█████▊    | 4503/7689 [11:09:56<12:20:35, 13.95s/it]


 59%|█████▊    | 4505/7689 [11:10:18<11:01:35, 12.47s/it]
{'loss': 0.7935, 'grad_norm': 0.22540242455330944, 'learning_rate': 7.724366125854947e-05, 'epoch': 0.59}


 59%|█████▊    | 4507/7689 [11:10:34<9:03:56, 10.26s/it]

 59%|█████▊    | 4508/7689 [11:10:40<7:54:59,  8.96s/it]
{'loss': 1.0546, 'grad_norm': 0.20098169365850166, 'learning_rate': 7.712062364803684e-05, 'epoch': 0.59}

 59%|█████▊    | 4509/7689 [11:10:47<7:21:16,  8.33s/it]

 59%|█████▊    | 4510/7689 [11:10:56<7:36:05,  8.61s/it]

 59%|█████▊    | 4511/7689 [11:11:04<7:26:42,  8.43s/it]

 59%|█████▊    | 4512/7689 [11:11:13<7:29:05,  8.48s/it]

 59%|█████▊    | 4513/7689 [11:11:23<7:49:55,  8.88s/it]

 59%|█████▊    | 4514/7689 [11:11:31<7:33:42,  8.57s/it]

 59%|█████▊    | 4515/7689 [11:11:35<6:32:54,  7.43s/it]

 59%|█████▊    | 4516/7689 [11:11:42<6:11:31,  7.03s/it]

 59%|█████▊    | 4517/7689 [11:11:48<6:02:17,  6.85s/it]

 59%|█████▉    | 4518/7689 [11:11:57<6:29:04,  7.36s/it]

 59%|█████▉    | 4519/7689 [11:12:07<7:23:24,  8.39s/it]

 59%|█████▉    | 4520/7689 [11:12:15<7:13:03,  8.20s/it]


 59%|█████▉    | 4522/7689 [11:12:30<6:51:19,  7.79s/it]
{'loss': 0.9954, 'grad_norm': 0.20069264446938093, 'learning_rate': 7.654693441612425e-05, 'epoch': 0.59}

 59%|█████▉    | 4523/7689 [11:12:37<6:35:15,  7.49s/it]

 59%|█████▉    | 4524/7689 [11:12:46<6:56:39,  7.90s/it]

 59%|█████▉    | 4525/7689 [11:12:55<7:21:52,  8.38s/it]

 59%|█████▉    | 4526/7689 [11:13:05<7:39:37,  8.72s/it]


 59%|█████▉    | 4528/7689 [11:13:18<6:50:56,  7.80s/it]

 59%|█████▉    | 4529/7689 [11:13:26<6:54:57,  7.88s/it]
{'loss': 1.0197, 'grad_norm': 0.18661772448226524, 'learning_rate': 7.626039442694597e-05, 'epoch': 0.59}

 59%|█████▉    | 4530/7689 [11:13:31<6:02:16,  6.88s/it]


 59%|█████▉    | 4532/7689 [11:14:06<10:51:14, 12.38s/it]
[2024-05-25 00:45:16,804] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9659, 'grad_norm': 0.19908499129733734, 'learning_rate': 7.613765461502724e-05, 'epoch': 0.59}

 59%|█████▉    | 4533/7689 [11:14:23<12:01:09, 13.71s/it]

 59%|█████▉    | 4534/7689 [11:14:31<10:31:47, 12.02s/it]


 59%|█████▉    | 4536/7689 [11:14:58<10:51:29, 12.40s/it]
{'loss': 0.9847, 'grad_norm': 0.2134373091273348, 'learning_rate': 7.59740608446869e-05, 'epoch': 0.59}

 59%|█████▉    | 4537/7689 [11:15:10<10:38:24, 12.15s/it]

 59%|█████▉    | 4538/7689 [11:15:23<10:47:19, 12.33s/it]

 59%|█████▉    | 4539/7689 [11:15:31<9:47:33, 11.19s/it]

 59%|█████▉    | 4540/7689 [11:15:37<8:21:28,  9.55s/it]

 59%|█████▉    | 4541/7689 [11:15:54<10:17:59, 11.78s/it]

 59%|█████▉    | 4542/7689 [11:16:03<9:30:01, 10.87s/it]

 59%|█████▉    | 4543/7689 [11:16:09<8:21:19,  9.56s/it]

 59%|█████▉    | 4544/7689 [11:16:17<7:49:46,  8.96s/it]


 59%|█████▉    | 4546/7689 [11:16:33<7:07:43,  8.17s/it]
{'loss': 1.2527, 'grad_norm': 0.18731633124773536, 'learning_rate': 7.55653758580407e-05, 'epoch': 0.59}

 59%|█████▉    | 4547/7689 [11:16:39<6:44:21,  7.72s/it]

 59%|█████▉    | 4548/7689 [11:16:54<8:38:24,  9.90s/it]


 59%|█████▉    | 4550/7689 [11:17:11<7:57:49,  9.13s/it]
{'loss': 0.9222, 'grad_norm': 0.19807388291959946, 'learning_rate': 7.540202285736024e-05, 'epoch': 0.59}

 59%|█████▉    | 4551/7689 [11:17:19<7:42:39,  8.85s/it]


 59%|█████▉    | 4553/7689 [11:17:40<8:30:30,  9.77s/it]
{'loss': 1.0823, 'grad_norm': 0.17954774981167687, 'learning_rate': 7.527955391083221e-05, 'epoch': 0.59}

 59%|█████▉    | 4554/7689 [11:17:50<8:28:04,  9.72s/it]

 59%|█████▉    | 4555/7689 [11:17:57<7:46:29,  8.93s/it]

 59%|█████▉    | 4556/7689 [11:18:12<9:17:01, 10.67s/it]

 59%|█████▉    | 4557/7689 [11:18:24<9:42:52, 11.17s/it]


 59%|█████▉    | 4559/7689 [11:18:37<7:30:52,  8.64s/it]
{'loss': 1.0151, 'grad_norm': 0.20160763459654912, 'learning_rate': 7.503473464691085e-05, 'epoch': 0.59}

 59%|█████▉    | 4560/7689 [11:18:44<7:12:34,  8.29s/it]

 59%|█████▉    | 4561/7689 [11:18:59<8:50:24, 10.17s/it]

 59%|█████▉    | 4562/7689 [11:19:05<7:38:47,  8.80s/it]

 59%|█████▉    | 4563/7689 [11:19:16<8:18:05,  9.56s/it]

 59%|█████▉    | 4564/7689 [11:19:28<8:56:38, 10.30s/it]

 59%|█████▉    | 4565/7689 [11:19:45<10:35:34, 12.21s/it]

 59%|█████▉    | 4566/7689 [11:19:55<10:12:59, 11.78s/it]

 59%|█████▉    | 4567/7689 [11:20:12<11:31:54, 13.30s/it]

 59%|█████▉    | 4568/7689 [11:20:23<10:55:46, 12.61s/it]

 59%|█████▉    | 4569/7689 [11:20:30<9:29:33, 10.95s/it]


 59%|█████▉    | 4571/7689 [11:20:45<8:00:26,  9.25s/it]
{'loss': 1.1043, 'grad_norm': 0.21519010726461837, 'learning_rate': 7.454557610784452e-05, 'epoch': 0.59}

 59%|█████▉    | 4572/7689 [11:20:56<8:20:58,  9.64s/it]

 59%|█████▉    | 4573/7689 [11:21:12<9:59:45, 11.55s/it]

 59%|█████▉    | 4574/7689 [11:21:18<8:33:51,  9.90s/it]

 60%|█████▉    | 4575/7689 [11:21:25<7:58:12,  9.21s/it]

 60%|█████▉    | 4576/7689 [11:21:30<6:56:06,  8.02s/it]

 60%|█████▉    | 4577/7689 [11:21:42<7:58:22,  9.22s/it]

 60%|█████▉    | 4578/7689 [11:21:49<7:10:47,  8.31s/it]

 60%|█████▉    | 4579/7689 [11:21:54<6:23:50,  7.41s/it]

 60%|█████▉    | 4580/7689 [11:22:01<6:22:24,  7.38s/it]

 60%|█████▉    | 4581/7689 [11:22:08<6:18:05,  7.30s/it]

 60%|█████▉    | 4582/7689 [11:22:14<5:56:34,  6.89s/it]

 60%|█████▉    | 4583/7689 [11:22:25<6:50:59,  7.94s/it]

 60%|█████▉    | 4584/7689 [11:22:36<7:41:28,  8.92s/it]


 60%|█████▉    | 4586/7689 [11:22:51<6:53:47,  8.00s/it]
{'loss': 1.0499, 'grad_norm': 0.23065661643399515, 'learning_rate': 7.39350440206434e-05, 'epoch': 0.6}


 60%|█████▉    | 4588/7689 [11:23:07<7:00:49,  8.14s/it]
{'loss': 0.8692, 'grad_norm': 0.18920687187029184, 'learning_rate': 7.385371783557538e-05, 'epoch': 0.6}


 60%|█████▉    | 4590/7689 [11:23:21<6:20:18,  7.36s/it]
{'loss': 1.0107, 'grad_norm': 0.22796716617910137, 'learning_rate': 7.377241020823921e-05, 'epoch': 0.6}

 60%|█████▉    | 4591/7689 [11:23:27<6:03:53,  7.05s/it]

 60%|█████▉    | 4592/7689 [11:23:37<6:40:32,  7.76s/it]


 60%|█████▉    | 4594/7689 [11:23:53<6:48:49,  7.93s/it]
{'loss': 1.1604, 'grad_norm': 0.21706428265176322, 'learning_rate': 7.360985085758692e-05, 'epoch': 0.6}

 60%|█████▉    | 4595/7689 [11:24:03<7:12:38,  8.39s/it]


 60%|█████▉    | 4597/7689 [11:24:19<7:16:19,  8.47s/it]
{'loss': 1.3218, 'grad_norm': 0.21629419359613533, 'learning_rate': 7.34879804877611e-05, 'epoch': 0.6}

 60%|█████▉    | 4598/7689 [11:24:27<7:02:45,  8.21s/it]

 60%|█████▉    | 4599/7689 [11:24:43<9:03:41, 10.56s/it]

 60%|█████▉    | 4600/7689 [11:24:50<8:19:45,  9.71s/it]


 60%|█████▉    | 4602/7689 [11:25:05<7:14:05,  8.44s/it]
{'loss': 0.7947, 'grad_norm': 0.18910204988150056, 'learning_rate': 7.328495738738947e-05, 'epoch': 0.6}

 60%|█████▉    | 4603/7689 [11:25:15<7:42:07,  8.99s/it]


 60%|█████▉    | 4605/7689 [11:25:29<6:49:52,  7.97s/it]
{'loss': 1.038, 'grad_norm': 0.18664972308735225, 'learning_rate': 7.316320035378675e-05, 'epoch': 0.6}


 60%|█████▉    | 4607/7689 [11:25:55<8:31:54,  9.97s/it]
{'loss': 1.0638, 'grad_norm': 0.22376240181813092, 'learning_rate': 7.308205279584685e-05, 'epoch': 0.6}

 60%|█████▉    | 4608/7689 [11:26:00<7:12:34,  8.42s/it]

 60%|█████▉    | 4609/7689 [11:26:08<7:11:47,  8.41s/it]

 60%|█████▉    | 4610/7689 [11:26:17<7:09:26,  8.37s/it]

 60%|█████▉    | 4611/7689 [11:26:26<7:21:22,  8.60s/it]

 60%|█████▉    | 4612/7689 [11:26:31<6:25:35,  7.52s/it]


 60%|██████    | 4614/7689 [11:26:57<8:37:24, 10.10s/it]
{'loss': 1.0169, 'grad_norm': 0.17411431304949374, 'learning_rate': 7.27981871761935e-05, 'epoch': 0.6}

 60%|██████    | 4615/7689 [11:27:06<8:08:27,  9.53s/it]

 60%|██████    | 4616/7689 [11:27:12<7:27:06,  8.73s/it]

 60%|██████    | 4617/7689 [11:27:24<8:14:42,  9.66s/it]

 60%|██████    | 4618/7689 [11:27:32<7:49:17,  9.17s/it]

 60%|██████    | 4619/7689 [11:27:40<7:23:08,  8.66s/it]

 60%|██████    | 4620/7689 [11:27:51<7:59:00,  9.36s/it]


 60%|██████    | 4622/7689 [11:28:07<7:28:44,  8.78s/it]
{'loss': 1.0246, 'grad_norm': 0.20262115786969725, 'learning_rate': 7.247405907245633e-05, 'epoch': 0.6}

 60%|██████    | 4623/7689 [11:28:26<9:55:50, 11.66s/it]

 60%|██████    | 4624/7689 [11:28:34<9:00:21, 10.58s/it]

 60%|██████    | 4625/7689 [11:28:38<7:25:16,  8.72s/it]

 60%|██████    | 4626/7689 [11:28:45<6:57:36,  8.18s/it]

 60%|██████    | 4627/7689 [11:28:50<6:17:01,  7.39s/it]

 60%|██████    | 4628/7689 [11:28:59<6:34:37,  7.74s/it]

 60%|██████    | 4629/7689 [11:29:10<7:31:37,  8.86s/it]

 60%|██████    | 4630/7689 [11:29:15<6:24:12,  7.54s/it]

 60%|██████    | 4631/7689 [11:29:23<6:26:25,  7.58s/it]

 60%|██████    | 4632/7689 [11:29:29<6:08:15,  7.23s/it]

 60%|██████    | 4633/7689 [11:29:38<6:33:35,  7.73s/it]

 60%|██████    | 4634/7689 [11:29:46<6:46:04,  7.98s/it]

 60%|██████    | 4635/7689 [11:29:53<6:18:27,  7.44s/it]

 60%|██████    | 4636/7689 [11:30:08<8:26:48,  9.96s/it]

 60%|██████    | 4637/7689 [11:30:16<7:47:55,  9.20s/it]


 60%|██████    | 4639/7689 [11:30:38<8:29:20, 10.02s/it]
{'loss': 1.1218, 'grad_norm': 0.18146269491292866, 'learning_rate': 7.178632933098964e-05, 'epoch': 0.6}

 60%|██████    | 4640/7689 [11:30:45<7:48:11,  9.21s/it]

 60%|██████    | 4641/7689 [11:30:52<7:19:00,  8.64s/it]

 60%|██████    | 4642/7689 [11:30:58<6:42:21,  7.92s/it]

 60%|██████    | 4643/7689 [11:31:10<7:42:50,  9.12s/it]

 60%|██████    | 4644/7689 [11:31:21<8:07:38,  9.61s/it]

 60%|██████    | 4645/7689 [11:31:26<6:56:31,  8.21s/it]

 60%|██████    | 4646/7689 [11:31:34<7:00:40,  8.29s/it]

 60%|██████    | 4647/7689 [11:31:40<6:23:14,  7.56s/it]

 60%|██████    | 4648/7689 [11:31:46<5:54:17,  6.99s/it]

 60%|██████    | 4649/7689 [11:31:51<5:29:13,  6.50s/it]

 60%|██████    | 4650/7689 [11:31:58<5:33:37,  6.59s/it]

 60%|██████    | 4651/7689 [11:32:16<8:28:56, 10.05s/it]

 61%|██████    | 4652/7689 [11:32:25<8:01:11,  9.51s/it]

 61%|██████    | 4653/7689 [11:32:36<8:30:06, 10.08s/it]

 61%|██████    | 4654/7689 [11:32:43<7:38:08,  9.06s/it]

 61%|██████    | 4655/7689 [11:32:49<7:03:17,  8.37s/it]

 61%|██████    | 4656/7689 [11:33:02<8:13:10,  9.76s/it]


 61%|██████    | 4658/7689 [11:33:28<9:39:05, 11.46s/it]
{'loss': 1.0032, 'grad_norm': 0.18308955458657258, 'learning_rate': 7.101940398040735e-05, 'epoch': 0.61}

 61%|██████    | 4659/7689 [11:33:35<8:30:13, 10.10s/it]

 61%|██████    | 4660/7689 [11:33:43<8:06:55,  9.65s/it]

 61%|██████    | 4661/7689 [11:33:54<8:26:44, 10.04s/it]

 61%|██████    | 4662/7689 [11:34:00<7:22:39,  8.77s/it]

 61%|██████    | 4663/7689 [11:34:11<7:53:53,  9.40s/it]


 61%|██████    | 4665/7689 [11:34:28<7:28:33,  8.90s/it]
{'loss': 0.8916, 'grad_norm': 0.18501475414315238, 'learning_rate': 7.07373178841569e-05, 'epoch': 0.61}

 61%|██████    | 4666/7689 [11:34:42<8:50:55, 10.54s/it]


 61%|██████    | 4668/7689 [11:35:00<7:59:46,  9.53s/it]

 61%|██████    | 4669/7689 [11:35:12<8:32:50, 10.19s/it]

 61%|██████    | 4670/7689 [11:35:30<10:35:15, 12.63s/it]
{'loss': 1.0728, 'grad_norm': 0.18598056955583542, 'learning_rate': 7.053598344472035e-05, 'epoch': 0.61}

 61%|██████    | 4671/7689 [11:35:38<9:32:06, 11.37s/it]


 61%|██████    | 4673/7689 [11:35:50<7:05:28,  8.46s/it]
{'loss': 0.8774, 'grad_norm': 0.2019934948431935, 'learning_rate': 7.041524546154002e-05, 'epoch': 0.61}

 61%|██████    | 4674/7689 [11:35:58<7:02:00,  8.40s/it]

 61%|██████    | 4675/7689 [11:36:09<7:36:18,  9.08s/it]

 61%|██████    | 4676/7689 [11:36:17<7:23:20,  8.83s/it]


 61%|██████    | 4678/7689 [11:36:42<9:02:08, 10.80s/it]
{'loss': 0.9939, 'grad_norm': 0.19456159463108225, 'learning_rate': 7.021412057595504e-05, 'epoch': 0.61}


 61%|██████    | 4680/7689 [11:36:56<7:29:24,  8.96s/it]

 61%|██████    | 4681/7689 [11:37:04<7:12:45,  8.63s/it]

 61%|██████    | 4682/7689 [11:37:10<6:34:39,  7.87s/it]
{'loss': 1.0246, 'grad_norm': 0.21649741253602572, 'learning_rate': 7.005331575911975e-05, 'epoch': 0.61}

 61%|██████    | 4683/7689 [11:37:15<5:56:59,  7.13s/it]

 61%|██████    | 4684/7689 [11:37:21<5:30:26,  6.60s/it]

 61%|██████    | 4685/7689 [11:37:29<5:58:01,  7.15s/it]

 61%|██████    | 4686/7689 [11:37:40<6:50:20,  8.20s/it]

 61%|██████    | 4687/7689 [11:37:51<7:41:32,  9.22s/it]


 61%|██████    | 4689/7689 [11:38:02<6:04:32,  7.29s/it]
{'loss': 0.995, 'grad_norm': 0.1954623253742173, 'learning_rate': 6.977211218471347e-05, 'epoch': 0.61}

 61%|██████    | 4690/7689 [11:38:08<5:41:00,  6.82s/it]

 61%|██████    | 4691/7689 [11:38:13<5:17:03,  6.35s/it]

 61%|██████    | 4692/7689 [11:38:21<5:41:10,  6.83s/it]

 61%|██████    | 4693/7689 [11:38:31<6:23:20,  7.68s/it]

 61%|██████    | 4694/7689 [11:38:45<8:00:18,  9.62s/it]

 61%|██████    | 4695/7689 [11:38:53<7:37:13,  9.16s/it]

 61%|██████    | 4696/7689 [11:38:59<6:53:53,  8.30s/it]

 61%|██████    | 4697/7689 [11:39:09<7:21:59,  8.86s/it]

 61%|██████    | 4698/7689 [11:39:17<7:04:24,  8.51s/it]

 61%|██████    | 4699/7689 [11:39:23<6:29:07,  7.81s/it]

 61%|██████    | 4700/7689 [11:39:29<5:55:39,  7.14s/it]


 61%|██████    | 4702/7689 [11:39:52<7:49:24,  9.43s/it]
{'loss': 0.892, 'grad_norm': 0.16993482062603132, 'learning_rate': 6.925057610453516e-05, 'epoch': 0.61}

 61%|██████    | 4703/7689 [11:40:11<10:10:02, 12.26s/it]

 61%|██████    | 4704/7689 [11:40:18<8:47:40, 10.61s/it]

 61%|██████    | 4705/7689 [11:40:23<7:29:03,  9.03s/it]

 61%|██████    | 4706/7689 [11:40:33<7:39:12,  9.24s/it]

 61%|██████    | 4707/7689 [11:40:39<6:53:49,  8.33s/it]

 61%|██████    | 4708/7689 [11:40:52<8:03:32,  9.73s/it]

 61%|██████    | 4709/7689 [11:41:02<8:07:51,  9.82s/it]

 61%|██████▏   | 4710/7689 [11:41:07<6:56:51,  8.40s/it]

 61%|██████▏   | 4711/7689 [11:41:13<6:21:28,  7.69s/it]

 61%|██████▏   | 4712/7689 [11:41:28<8:13:14,  9.94s/it]

 61%|██████▏   | 4713/7689 [11:41:37<7:56:47,  9.61s/it]

 61%|██████▏   | 4714/7689 [11:41:45<7:32:19,  9.12s/it]

 61%|██████▏   | 4715/7689 [11:41:53<7:11:16,  8.70s/it]

 61%|██████▏   | 4716/7689 [11:42:02<7:12:48,  8.73s/it]

 61%|██████▏   | 4717/7689 [11:42:09<6:51:47,  8.31s/it]

 61%|██████▏   | 4718/7689 [11:42:15<6:15:21,  7.58s/it]

 61%|██████▏   | 4719/7689 [11:42:22<6:16:28,  7.61s/it]

 61%|██████▏   | 4720/7689 [11:42:31<6:36:51,  8.02s/it]

 61%|██████▏   | 4721/7689 [11:42:37<6:07:43,  7.43s/it]

 61%|██████▏   | 4722/7689 [11:42:43<5:45:57,  7.00s/it]

 61%|██████▏   | 4723/7689 [11:42:50<5:43:39,  6.95s/it]

 61%|██████▏   | 4724/7689 [11:43:01<6:36:45,  8.03s/it]

 61%|██████▏   | 4725/7689 [11:43:11<7:02:46,  8.56s/it]

 61%|██████▏   | 4726/7689 [11:43:18<6:37:32,  8.05s/it]

 61%|██████▏   | 4727/7689 [11:43:23<6:02:03,  7.33s/it]

 61%|██████▏   | 4728/7689 [11:43:31<6:03:22,  7.36s/it]

 62%|██████▏   | 4729/7689 [11:43:39<6:15:05,  7.60s/it]

 62%|██████▏   | 4730/7689 [11:43:45<5:49:16,  7.08s/it]

 62%|██████▏   | 4731/7689 [11:43:53<6:01:01,  7.32s/it]

 62%|██████▏   | 4732/7689 [11:44:03<6:48:44,  8.29s/it]

 62%|██████▏   | 4733/7689 [11:44:16<7:56:43,  9.68s/it]

 62%|██████▏   | 4734/7689 [11:44:26<7:55:51,  9.66s/it]

 62%|██████▏   | 4735/7689 [11:44:33<7:26:44,  9.07s/it]

 62%|██████▏   | 4736/7689 [11:44:41<7:07:23,  8.68s/it]

 62%|██████▏   | 4737/7689 [11:44:52<7:41:47,  9.39s/it]

 62%|██████▏   | 4738/7689 [11:45:01<7:31:32,  9.18s/it]

 62%|██████▏   | 4739/7689 [11:45:12<8:02:04,  9.80s/it]

 62%|██████▏   | 4740/7689 [11:45:21<7:52:05,  9.61s/it]

 62%|██████▏   | 4741/7689 [11:45:26<6:40:48,  8.16s/it]

 62%|██████▏   | 4742/7689 [11:45:31<5:49:23,  7.11s/it]

 62%|██████▏   | 4743/7689 [11:45:46<7:57:15,  9.72s/it]

 62%|██████▏   | 4744/7689 [11:45:58<8:21:59, 10.23s/it]

 62%|██████▏   | 4745/7689 [11:46:10<8:48:31, 10.77s/it]

 62%|██████▏   | 4746/7689 [11:46:16<7:39:16,  9.36s/it]

 62%|██████▏   | 4747/7689 [11:46:28<8:11:42, 10.03s/it]

 62%|██████▏   | 4748/7689 [11:46:36<7:41:02,  9.41s/it]

 62%|██████▏   | 4749/7689 [11:46:44<7:32:02,  9.23s/it]

 62%|██████▏   | 4750/7689 [11:46:51<6:58:22,  8.54s/it]

 62%|██████▏   | 4751/7689 [11:46:57<6:11:03,  7.58s/it]

 62%|██████▏   | 4752/7689 [11:47:03<5:58:27,  7.32s/it]

 62%|██████▏   | 4753/7689 [11:47:11<6:03:42,  7.43s/it]

 62%|██████▏   | 4754/7689 [11:47:20<6:27:46,  7.93s/it]

 62%|██████▏   | 4755/7689 [11:47:39<9:10:38, 11.26s/it]

 62%|██████▏   | 4756/7689 [11:47:50<9:00:06, 11.05s/it]

 62%|██████▏   | 4757/7689 [11:47:59<8:34:13, 10.52s/it]

 62%|██████▏   | 4758/7689 [11:48:13<9:30:12, 11.67s/it]

 62%|██████▏   | 4759/7689 [11:48:21<8:23:53, 10.32s/it]

 62%|██████▏   | 4760/7689 [11:48:30<8:04:26,  9.92s/it]

 62%|██████▏   | 4761/7689 [11:48:39<7:54:59,  9.73s/it]

 62%|██████▏   | 4762/7689 [11:48:51<8:26:20, 10.38s/it]

 62%|██████▏   | 4763/7689 [11:48:57<7:30:09,  9.23s/it]

 62%|██████▏   | 4764/7689 [11:49:02<6:29:22,  7.99s/it]

 62%|██████▏   | 4765/7689 [11:49:15<7:42:58,  9.50s/it]

 62%|██████▏   | 4766/7689 [11:49:28<8:26:27, 10.40s/it]

 62%|██████▏   | 4767/7689 [11:49:36<7:56:44,  9.79s/it]

 62%|██████▏   | 4768/7689 [11:49:47<8:08:17, 10.03s/it]

 62%|██████▏   | 4769/7689 [11:49:52<7:01:40,  8.66s/it]

 62%|██████▏   | 4770/7689 [11:49:58<6:11:22,  7.63s/it]

 62%|██████▏   | 4771/7689 [11:50:07<6:40:48,  8.24s/it]

 62%|██████▏   | 4772/7689 [11:50:13<6:08:02,  7.57s/it]

 62%|██████▏   | 4773/7689 [11:50:22<6:19:55,  7.82s/it]

 62%|██████▏   | 4774/7689 [11:50:31<6:48:34,  8.41s/it]

 62%|██████▏   | 4775/7689 [11:50:42<7:25:11,  9.17s/it]

 62%|██████▏   | 4776/7689 [11:50:53<7:43:19,  9.54s/it]

 62%|██████▏   | 4777/7689 [11:51:02<7:45:52,  9.60s/it]

 62%|██████▏   | 4778/7689 [11:51:09<6:59:41,  8.65s/it]

 62%|██████▏   | 4779/7689 [11:51:16<6:34:07,  8.13s/it]

 62%|██████▏   | 4780/7689 [11:51:23<6:13:38,  7.71s/it]

 62%|██████▏   | 4781/7689 [11:51:30<6:08:03,  7.59s/it]

 62%|██████▏   | 4782/7689 [11:51:38<6:10:56,  7.66s/it]

 62%|██████▏   | 4783/7689 [11:51:46<6:13:58,  7.72s/it]

 62%|██████▏   | 4784/7689 [11:51:54<6:19:56,  7.85s/it]

 62%|██████▏   | 4785/7689 [11:52:04<6:57:09,  8.62s/it]

 62%|██████▏   | 4786/7689 [11:52:13<7:07:13,  8.83s/it]

 62%|██████▏   | 4787/7689 [11:52:24<7:35:36,  9.42s/it]

 62%|██████▏   | 4788/7689 [11:52:34<7:44:53,  9.62s/it]

 62%|██████▏   | 4789/7689 [11:52:41<7:09:54,  8.89s/it]

 62%|██████▏   | 4790/7689 [11:52:56<8:30:43, 10.57s/it]

 62%|██████▏   | 4791/7689 [11:53:04<7:51:17,  9.76s/it]

 62%|██████▏   | 4792/7689 [11:53:11<7:18:33,  9.08s/it]

 62%|██████▏   | 4793/7689 [11:53:18<6:36:40,  8.22s/it]

 62%|██████▏   | 4794/7689 [11:53:25<6:22:13,  7.92s/it]

 62%|██████▏   | 4795/7689 [11:53:32<6:11:26,  7.70s/it]

 62%|██████▏   | 4796/7689 [11:53:39<6:00:07,  7.47s/it]

 62%|██████▏   | 4797/7689 [11:53:50<6:57:34,  8.66s/it]

 62%|██████▏   | 4798/7689 [11:53:55<6:03:13,  7.54s/it]

 62%|██████▏   | 4799/7689 [11:54:02<5:48:54,  7.24s/it]

 62%|██████▏   | 4800/7689 [11:54:13<6:42:45,  8.36s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0221, 'grad_norm': 0.20926309958537212, 'learning_rate': 6.531025392411399e-05, 'epoch': 0.62}
 62%|██████▏   | 4801/7689 [11:54:43<11:58:10, 14.92s/it]

 62%|██████▏   | 4802/7689 [11:54:54<10:58:00, 13.68s/it]

 62%|██████▏   | 4803/7689 [11:55:00<9:14:29, 11.53s/it]

 62%|██████▏   | 4804/7689 [11:55:08<8:15:00, 10.29s/it]

 62%|██████▏   | 4805/7689 [11:55:17<7:56:08,  9.91s/it]

 63%|██████▎   | 4806/7689 [11:55:23<7:06:50,  8.88s/it]

 63%|██████▎   | 4807/7689 [11:55:30<6:42:18,  8.38s/it]

 63%|██████▎   | 4808/7689 [11:55:37<6:16:35,  7.84s/it]

 63%|██████▎   | 4809/7689 [11:55:48<6:58:12,  8.71s/it]

 63%|██████▎   | 4810/7689 [11:55:55<6:35:37,  8.25s/it]

 63%|██████▎   | 4811/7689 [11:56:08<7:51:49,  9.84s/it]

 63%|██████▎   | 4812/7689 [11:56:19<8:07:25, 10.17s/it]

 63%|██████▎   | 4813/7689 [11:56:28<7:50:14,  9.81s/it]

 63%|██████▎   | 4814/7689 [11:56:47<9:57:42, 12.47s/it]

 63%|██████▎   | 4815/7689 [11:56:53<8:21:31, 10.47s/it]

 63%|██████▎   | 4816/7689 [11:57:02<7:59:20, 10.01s/it]

 63%|██████▎   | 4817/7689 [11:57:15<8:50:26, 11.08s/it]

 63%|██████▎   | 4818/7689 [11:57:24<8:16:31, 10.38s/it]

 63%|██████▎   | 4819/7689 [11:57:34<8:07:05, 10.18s/it]

 63%|██████▎   | 4820/7689 [11:57:39<6:52:50,  8.63s/it]

 63%|██████▎   | 4821/7689 [11:57:49<7:11:06,  9.02s/it]

 63%|██████▎   | 4822/7689 [11:58:02<8:16:52, 10.40s/it]

 63%|██████▎   | 4823/7689 [11:58:09<7:19:45,  9.21s/it]

 63%|██████▎   | 4824/7689 [11:58:16<6:54:44,  8.69s/it]

 63%|██████▎   | 4825/7689 [11:58:23<6:31:21,  8.20s/it]

 63%|██████▎   | 4826/7689 [11:58:31<6:28:45,  8.15s/it]

 63%|██████▎   | 4827/7689 [11:58:39<6:18:57,  7.94s/it]

 63%|██████▎   | 4828/7689 [11:58:48<6:34:37,  8.28s/it]

 63%|██████▎   | 4829/7689 [11:58:55<6:16:11,  7.89s/it]

 63%|██████▎   | 4830/7689 [11:59:02<6:04:43,  7.65s/it]

 63%|██████▎   | 4831/7689 [11:59:14<7:13:01,  9.09s/it]

 63%|██████▎   | 4832/7689 [11:59:21<6:39:58,  8.40s/it]

 63%|██████▎   | 4833/7689 [11:59:34<7:46:05,  9.79s/it]

 63%|██████▎   | 4834/7689 [11:59:41<7:05:15,  8.94s/it]

 63%|██████▎   | 4835/7689 [11:59:46<6:10:20,  7.79s/it]

 63%|██████▎   | 4836/7689 [11:59:52<5:47:10,  7.30s/it]

 63%|██████▎   | 4837/7689 [11:59:59<5:40:28,  7.16s/it]

 63%|██████▎   | 4838/7689 [12:00:09<6:17:03,  7.94s/it]

 63%|██████▎   | 4839/7689 [12:00:21<7:09:28,  9.04s/it]

 63%|██████▎   | 4840/7689 [12:00:29<6:54:16,  8.72s/it]

 63%|██████▎   | 4841/7689 [12:00:38<7:03:31,  8.92s/it]

 63%|██████▎   | 4842/7689 [12:00:51<8:01:57, 10.16s/it]

 63%|██████▎   | 4843/7689 [12:01:01<7:51:45,  9.95s/it]

 63%|██████▎   | 4844/7689 [12:01:10<7:47:15,  9.85s/it]

 63%|██████▎   | 4845/7689 [12:01:23<8:33:35, 10.84s/it]

 63%|██████▎   | 4846/7689 [12:01:30<7:36:40,  9.64s/it]

 63%|██████▎   | 4847/7689 [12:01:44<8:36:29, 10.90s/it]

 63%|██████▎   | 4848/7689 [12:01:58<9:27:14, 11.98s/it]

 63%|██████▎   | 4849/7689 [12:02:12<9:47:57, 12.42s/it]

 63%|██████▎   | 4850/7689 [12:02:20<8:46:18, 11.12s/it]

 63%|██████▎   | 4851/7689 [12:02:28<7:59:18, 10.13s/it]

 63%|██████▎   | 4852/7689 [12:02:39<8:08:38, 10.33s/it]

 63%|██████▎   | 4853/7689 [12:02:50<8:16:15, 10.50s/it]

 63%|██████▎   | 4854/7689 [12:02:57<7:33:33,  9.60s/it]

 63%|██████▎   | 4855/7689 [12:03:12<8:53:23, 11.29s/it]

 63%|██████▎   | 4856/7689 [12:03:22<8:25:09, 10.70s/it]

 63%|██████▎   | 4857/7689 [12:03:32<8:21:22, 10.62s/it]

 63%|██████▎   | 4858/7689 [12:03:47<9:17:10, 11.81s/it]

 63%|██████▎   | 4859/7689 [12:03:58<9:06:43, 11.59s/it]

 63%|██████▎   | 4860/7689 [12:04:04<7:47:55,  9.92s/it]

 63%|██████▎   | 4861/7689 [12:04:13<7:41:40,  9.80s/it]

 63%|██████▎   | 4862/7689 [12:04:21<7:15:56,  9.25s/it]

 63%|██████▎   | 4863/7689 [12:04:28<6:40:58,  8.51s/it]

 63%|██████▎   | 4864/7689 [12:04:41<7:37:56,  9.73s/it]

 63%|██████▎   | 4865/7689 [12:04:46<6:38:39,  8.47s/it]

 63%|██████▎   | 4866/7689 [12:04:55<6:39:14,  8.49s/it]

 63%|██████▎   | 4867/7689 [12:05:08<7:47:33,  9.94s/it]

 63%|██████▎   | 4868/7689 [12:05:13<6:37:07,  8.45s/it]

 63%|██████▎   | 4869/7689 [12:05:19<6:07:42,  7.82s/it]

 63%|██████▎   | 4870/7689 [12:05:24<5:23:13,  6.88s/it]

 63%|██████▎   | 4871/7689 [12:05:32<5:36:39,  7.17s/it]

 63%|██████▎   | 4872/7689 [12:05:42<6:15:47,  8.00s/it]

 63%|██████▎   | 4873/7689 [12:05:49<6:06:02,  7.80s/it]

 63%|██████▎   | 4874/7689 [12:05:54<5:21:38,  6.86s/it]

 63%|██████▎   | 4875/7689 [12:06:01<5:26:27,  6.96s/it]

 63%|██████▎   | 4876/7689 [12:06:09<5:42:55,  7.31s/it]

 63%|██████▎   | 4877/7689 [12:06:25<7:43:38,  9.89s/it]

 63%|██████▎   | 4878/7689 [12:06:32<6:57:39,  8.91s/it]

 63%|██████▎   | 4879/7689 [12:06:39<6:40:02,  8.54s/it]

 63%|██████▎   | 4880/7689 [12:06:49<6:59:36,  8.96s/it]

 63%|██████▎   | 4881/7689 [12:06:57<6:36:02,  8.46s/it]

 63%|██████▎   | 4882/7689 [12:07:07<6:57:18,  8.92s/it]

 64%|██████▎   | 4883/7689 [12:07:12<6:05:20,  7.81s/it]

 64%|██████▎   | 4884/7689 [12:07:18<5:38:48,  7.25s/it]

 64%|██████▎   | 4885/7689 [12:07:26<5:59:21,  7.69s/it]

 64%|██████▎   | 4886/7689 [12:07:34<5:56:52,  7.64s/it]

 64%|██████▎   | 4887/7689 [12:07:42<5:59:17,  7.69s/it]

 64%|██████▎   | 4888/7689 [12:07:47<5:28:47,  7.04s/it]

 64%|██████▎   | 4889/7689 [12:07:58<6:20:07,  8.15s/it]

 64%|██████▎   | 4890/7689 [12:08:06<6:12:44,  7.99s/it]

 64%|██████▎   | 4891/7689 [12:08:14<6:17:58,  8.11s/it]

 64%|██████▎   | 4892/7689 [12:08:20<5:47:43,  7.46s/it]

 64%|██████▎   | 4893/7689 [12:08:33<7:02:00,  9.06s/it]

 64%|██████▎   | 4894/7689 [12:08:40<6:33:56,  8.46s/it]

 64%|██████▎   | 4895/7689 [12:08:54<7:53:46, 10.17s/it]

 64%|██████▎   | 4896/7689 [12:09:04<7:52:10, 10.14s/it]

 64%|██████▎   | 4897/7689 [12:09:11<7:00:57,  9.05s/it]

 64%|██████▎   | 4898/7689 [12:09:17<6:26:03,  8.30s/it]

 64%|██████▎   | 4899/7689 [12:09:26<6:36:54,  8.54s/it]

 64%|██████▎   | 4900/7689 [12:09:33<6:11:47,  8.00s/it]

 64%|██████▎   | 4901/7689 [12:09:41<6:07:02,  7.90s/it]

 64%|██████▍   | 4902/7689 [12:09:49<6:10:19,  7.97s/it]

 64%|██████▍   | 4903/7689 [12:10:01<7:11:48,  9.30s/it]

 64%|██████▍   | 4904/7689 [12:10:09<6:53:52,  8.92s/it]

 64%|██████▍   | 4905/7689 [12:10:16<6:20:04,  8.19s/it]

 64%|██████▍   | 4906/7689 [12:10:23<6:08:35,  7.95s/it]

 64%|██████▍   | 4907/7689 [12:10:34<6:51:48,  8.88s/it]

 64%|██████▍   | 4908/7689 [12:10:42<6:44:55,  8.74s/it]

 64%|██████▍   | 4909/7689 [12:10:48<5:56:34,  7.70s/it]

 64%|██████▍   | 4910/7689 [12:10:53<5:25:00,  7.02s/it]

 64%|██████▍   | 4911/7689 [12:11:00<5:23:42,  6.99s/it]

 64%|██████▍   | 4912/7689 [12:11:09<5:45:11,  7.46s/it]

 64%|██████▍   | 4913/7689 [12:11:16<5:37:42,  7.30s/it]

 64%|██████▍   | 4914/7689 [12:11:22<5:26:17,  7.05s/it]

 64%|██████▍   | 4915/7689 [12:11:31<5:59:08,  7.77s/it]

 64%|██████▍   | 4916/7689 [12:11:40<6:08:27,  7.97s/it]

 64%|██████▍   | 4917/7689 [12:11:51<6:48:58,  8.85s/it]

 64%|██████▍   | 4918/7689 [12:11:57<6:06:57,  7.95s/it]

 64%|██████▍   | 4919/7689 [12:12:02<5:32:42,  7.21s/it]

 64%|██████▍   | 4920/7689 [12:12:12<6:02:53,  7.86s/it]

 64%|██████▍   | 4921/7689 [12:12:17<5:29:40,  7.15s/it]

 64%|██████▍   | 4922/7689 [12:12:22<5:04:30,  6.60s/it]

 64%|██████▍   | 4923/7689 [12:12:30<5:23:12,  7.01s/it]

 64%|██████▍   | 4924/7689 [12:12:41<6:11:56,  8.07s/it]

 64%|██████▍   | 4925/7689 [12:12:52<7:00:00,  9.12s/it]

 64%|██████▍   | 4926/7689 [12:13:00<6:34:11,  8.56s/it]

 64%|██████▍   | 4927/7689 [12:13:09<6:40:55,  8.71s/it]

 64%|██████▍   | 4928/7689 [12:13:14<5:58:00,  7.78s/it]

 64%|██████▍   | 4929/7689 [12:13:23<6:05:41,  7.95s/it]

 64%|██████▍   | 4930/7689 [12:13:36<7:13:11,  9.42s/it]

 64%|██████▍   | 4931/7689 [12:13:44<6:54:55,  9.03s/it]

 64%|██████▍   | 4932/7689 [12:13:56<7:43:55, 10.10s/it]

 64%|██████▍   | 4933/7689 [12:14:06<7:33:28,  9.87s/it]

 64%|██████▍   | 4934/7689 [12:14:11<6:32:13,  8.54s/it]

 64%|██████▍   | 4935/7689 [12:14:16<5:41:29,  7.44s/it]

 64%|██████▍   | 4936/7689 [12:14:31<7:22:02,  9.63s/it]

 64%|██████▍   | 4937/7689 [12:14:40<7:14:30,  9.47s/it]

 64%|██████▍   | 4938/7689 [12:14:55<8:36:35, 11.27s/it]

 64%|██████▍   | 4939/7689 [12:15:01<7:20:41,  9.62s/it]

 64%|██████▍   | 4940/7689 [12:15:09<7:05:00,  9.28s/it]

 64%|██████▍   | 4941/7689 [12:15:16<6:21:53,  8.34s/it]

 64%|██████▍   | 4942/7689 [12:15:23<6:09:28,  8.07s/it]

 64%|██████▍   | 4943/7689 [12:15:32<6:15:59,  8.22s/it]

 64%|██████▍   | 4944/7689 [12:15:39<6:03:14,  7.94s/it]

 64%|██████▍   | 4945/7689 [12:15:44<5:25:55,  7.13s/it]

 64%|██████▍   | 4946/7689 [12:15:51<5:23:02,  7.07s/it]

 64%|██████▍   | 4947/7689 [12:16:02<6:11:14,  8.12s/it]

 64%|██████▍   | 4948/7689 [12:16:08<5:43:52,  7.53s/it]

 64%|██████▍   | 4949/7689 [12:16:13<5:06:26,  6.71s/it]

 64%|██████▍   | 4950/7689 [12:16:18<4:41:52,  6.17s/it]

 64%|██████▍   | 4951/7689 [12:16:25<4:55:48,  6.48s/it]

 64%|██████▍   | 4952/7689 [12:16:33<5:21:29,  7.05s/it]

 64%|██████▍   | 4953/7689 [12:16:48<7:12:55,  9.49s/it]

 64%|██████▍   | 4954/7689 [12:17:01<7:53:10, 10.38s/it]

 64%|██████▍   | 4955/7689 [12:17:08<7:03:42,  9.30s/it]

 64%|██████▍   | 4956/7689 [12:17:19<7:34:26,  9.98s/it]

 64%|██████▍   | 4957/7689 [12:17:29<7:34:33,  9.98s/it]

 64%|██████▍   | 4958/7689 [12:17:34<6:29:48,  8.56s/it]

 64%|██████▍   | 4959/7689 [12:17:43<6:30:35,  8.58s/it]

 65%|██████▍   | 4960/7689 [12:17:57<7:48:44, 10.31s/it]

 65%|██████▍   | 4961/7689 [12:18:04<7:00:45,  9.25s/it]

 65%|██████▍   | 4962/7689 [12:18:10<6:14:49,  8.25s/it]

 65%|██████▍   | 4963/7689 [12:18:21<6:55:48,  9.15s/it]

 65%|██████▍   | 4964/7689 [12:18:28<6:16:38,  8.29s/it]

 65%|██████▍   | 4965/7689 [12:18:36<6:22:16,  8.42s/it]

 65%|██████▍   | 4966/7689 [12:18:44<6:18:28,  8.34s/it]

 65%|██████▍   | 4967/7689 [12:18:51<5:54:02,  7.80s/it]

 65%|██████▍   | 4968/7689 [12:19:03<6:50:16,  9.05s/it]

 65%|██████▍   | 4969/7689 [12:19:09<6:10:03,  8.16s/it]

 65%|██████▍   | 4970/7689 [12:19:16<5:54:39,  7.83s/it]

 65%|██████▍   | 4971/7689 [12:19:21<5:20:40,  7.08s/it]

 65%|██████▍   | 4972/7689 [12:19:26<4:51:35,  6.44s/it]

 65%|██████▍   | 4973/7689 [12:19:35<5:24:11,  7.16s/it]

 65%|██████▍   | 4974/7689 [12:19:41<5:09:31,  6.84s/it]

 65%|██████▍   | 4975/7689 [12:19:52<5:56:01,  7.87s/it]

 65%|██████▍   | 4976/7689 [12:19:59<5:56:27,  7.88s/it]

 65%|██████▍   | 4977/7689 [12:20:04<5:18:09,  7.04s/it]

 65%|██████▍   | 4978/7689 [12:20:14<5:55:26,  7.87s/it]

 65%|██████▍   | 4979/7689 [12:20:22<5:50:10,  7.75s/it]

 65%|██████▍   | 4980/7689 [12:20:27<5:20:56,  7.11s/it]

 65%|██████▍   | 4981/7689 [12:20:41<6:44:32,  8.96s/it]

 65%|██████▍   | 4982/7689 [12:20:47<6:13:51,  8.29s/it]

 65%|██████▍   | 4983/7689 [12:20:53<5:41:24,  7.57s/it]

 65%|██████▍   | 4984/7689 [12:21:07<7:05:47,  9.44s/it]

 65%|██████▍   | 4985/7689 [12:21:21<8:08:25, 10.84s/it]

 65%|██████▍   | 4986/7689 [12:21:27<6:55:42,  9.23s/it]

 65%|██████▍   | 4987/7689 [12:21:34<6:30:59,  8.68s/it]
[2024-05-25 01:53:01,778] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▍   | 4988/7689 [12:21:51<8:27:07, 11.27s/it]

 65%|██████▍   | 4989/7689 [12:21:57<7:13:03,  9.62s/it]

 65%|██████▍   | 4990/7689 [12:22:03<6:23:02,  8.52s/it]

 65%|██████▍   | 4991/7689 [12:22:13<6:39:50,  8.89s/it]

 65%|██████▍   | 4992/7689 [12:22:20<6:22:09,  8.50s/it]

 65%|██████▍   | 4993/7689 [12:22:34<7:25:48,  9.92s/it]

 65%|██████▍   | 4994/7689 [12:22:41<6:50:41,  9.14s/it]

 65%|██████▍   | 4995/7689 [12:22:49<6:38:00,  8.86s/it]

 65%|██████▍   | 4996/7689 [12:22:58<6:30:01,  8.69s/it]

 65%|██████▍   | 4997/7689 [12:23:04<5:55:52,  7.93s/it]

 65%|██████▌   | 4998/7689 [12:23:10<5:30:01,  7.36s/it]

 65%|██████▌   | 4999/7689 [12:23:22<6:30:00,  8.70s/it]

 65%|██████▌   | 5000/7689 [12:23:29<6:19:36,  8.47s/it]

 65%|██████▌   | 5001/7689 [12:23:40<6:48:51,  9.13s/it]

 65%|██████▌   | 5002/7689 [12:23:50<6:52:09,  9.20s/it]


 65%|██████▌   | 5004/7689 [12:24:07<6:52:27,  9.22s/it]
{'loss': 0.9508, 'grad_norm': 0.211183756870532, 'learning_rate': 5.742664466783242e-05, 'epoch': 0.65}

 65%|██████▌   | 5005/7689 [12:24:16<6:46:49,  9.09s/it]

 65%|██████▌   | 5006/7689 [12:24:25<6:39:58,  8.94s/it]

 65%|██████▌   | 5007/7689 [12:24:32<6:22:02,  8.55s/it]

 65%|██████▌   | 5008/7689 [12:24:40<6:12:15,  8.33s/it]

 65%|██████▌   | 5009/7689 [12:24:47<5:47:33,  7.78s/it]

 65%|██████▌   | 5010/7689 [12:24:53<5:26:29,  7.31s/it]

 65%|██████▌   | 5011/7689 [12:24:59<5:12:11,  6.99s/it]

 65%|██████▌   | 5012/7689 [12:25:07<5:21:40,  7.21s/it]

 65%|██████▌   | 5013/7689 [12:25:15<5:29:56,  7.40s/it]

 65%|██████▌   | 5014/7689 [12:25:21<5:17:26,  7.12s/it]

 65%|██████▌   | 5015/7689 [12:25:26<4:54:22,  6.61s/it]

 65%|██████▌   | 5016/7689 [12:25:36<5:28:26,  7.37s/it]

 65%|██████▌   | 5017/7689 [12:25:41<5:04:08,  6.83s/it]

 65%|██████▌   | 5018/7689 [12:25:48<5:01:10,  6.77s/it]

 65%|██████▌   | 5019/7689 [12:26:02<6:33:32,  8.84s/it]

 65%|██████▌   | 5020/7689 [12:26:20<8:47:49, 11.87s/it]

 65%|██████▌   | 5021/7689 [12:26:32<8:43:31, 11.77s/it]

 65%|██████▌   | 5022/7689 [12:26:36<7:03:13,  9.52s/it]

 65%|██████▌   | 5023/7689 [12:26:45<6:55:03,  9.34s/it]

 65%|██████▌   | 5024/7689 [12:26:51<6:03:37,  8.19s/it]

 65%|██████▌   | 5025/7689 [12:27:03<6:57:53,  9.41s/it]

 65%|██████▌   | 5026/7689 [12:27:08<5:59:10,  8.09s/it]

 65%|██████▌   | 5027/7689 [12:27:19<6:37:14,  8.95s/it]

 65%|██████▌   | 5028/7689 [12:27:31<7:17:32,  9.87s/it]

 65%|██████▌   | 5029/7689 [12:27:49<9:00:56, 12.20s/it]

 65%|██████▌   | 5030/7689 [12:28:01<8:59:03, 12.16s/it]

 65%|██████▌   | 5031/7689 [12:28:12<8:51:24, 12.00s/it]

 65%|██████▌   | 5032/7689 [12:28:18<7:22:29,  9.99s/it]

 65%|██████▌   | 5033/7689 [12:28:25<6:44:36,  9.14s/it]

 65%|██████▌   | 5034/7689 [12:28:31<6:10:34,  8.37s/it]

 65%|██████▌   | 5035/7689 [12:28:41<6:30:35,  8.83s/it]

 65%|██████▌   | 5036/7689 [12:28:49<6:18:02,  8.55s/it]

 66%|██████▌   | 5037/7689 [12:29:01<7:06:04,  9.64s/it]

 66%|██████▌   | 5038/7689 [12:29:07<6:16:33,  8.52s/it]

 66%|██████▌   | 5039/7689 [12:29:16<6:19:20,  8.59s/it]

 66%|██████▌   | 5040/7689 [12:29:26<6:44:00,  9.15s/it]

 66%|██████▌   | 5041/7689 [12:29:37<7:07:42,  9.69s/it]

 66%|██████▌   | 5042/7689 [12:29:44<6:28:27,  8.81s/it]

 66%|██████▌   | 5043/7689 [12:29:52<6:12:13,  8.44s/it]

 66%|██████▌   | 5044/7689 [12:29:57<5:34:23,  7.59s/it]

 66%|██████▌   | 5045/7689 [12:30:04<5:16:54,  7.19s/it]

 66%|██████▌   | 5046/7689 [12:30:09<4:50:15,  6.59s/it]

 66%|██████▌   | 5047/7689 [12:30:14<4:36:34,  6.28s/it]

 66%|██████▌   | 5048/7689 [12:30:20<4:22:30,  5.96s/it]

 66%|██████▌   | 5049/7689 [12:30:27<4:43:22,  6.44s/it]

 66%|██████▌   | 5050/7689 [12:30:32<4:22:57,  5.98s/it]

 66%|██████▌   | 5051/7689 [12:30:40<4:44:59,  6.48s/it]

 66%|██████▌   | 5052/7689 [12:30:45<4:30:37,  6.16s/it]

 66%|██████▌   | 5053/7689 [12:30:58<5:58:33,  8.16s/it]

 66%|██████▌   | 5054/7689 [12:31:07<6:10:33,  8.44s/it]

 66%|██████▌   | 5055/7689 [12:31:12<5:30:16,  7.52s/it]

 66%|██████▌   | 5056/7689 [12:31:17<4:58:55,  6.81s/it]

 66%|██████▌   | 5057/7689 [12:31:24<4:50:13,  6.62s/it]

 66%|██████▌   | 5058/7689 [12:31:29<4:39:27,  6.37s/it]

 66%|██████▌   | 5059/7689 [12:31:37<4:52:12,  6.67s/it]

 66%|██████▌   | 5060/7689 [12:31:45<5:12:19,  7.13s/it]

 66%|██████▌   | 5061/7689 [12:31:54<5:31:14,  7.56s/it]

 66%|██████▌   | 5062/7689 [12:32:00<5:12:46,  7.14s/it]

 66%|██████▌   | 5063/7689 [12:32:06<5:03:06,  6.93s/it]

 66%|██████▌   | 5064/7689 [12:32:14<5:09:56,  7.08s/it]

 66%|██████▌   | 5065/7689 [12:32:23<5:38:28,  7.74s/it]

 66%|██████▌   | 5066/7689 [12:32:31<5:37:29,  7.72s/it]

 66%|██████▌   | 5067/7689 [12:32:36<5:10:35,  7.11s/it]

 66%|██████▌   | 5068/7689 [12:32:50<6:36:46,  9.08s/it]

 66%|██████▌   | 5069/7689 [12:32:57<6:03:41,  8.33s/it]

 66%|██████▌   | 5070/7689 [12:33:03<5:45:12,  7.91s/it]

 66%|██████▌   | 5071/7689 [12:33:10<5:24:21,  7.43s/it]


 66%|██████▌   | 5073/7689 [12:33:22<5:02:47,  6.94s/it]

 66%|██████▌   | 5074/7689 [12:33:32<5:44:34,  7.91s/it]

 66%|██████▌   | 5075/7689 [12:33:42<6:05:24,  8.39s/it]

 66%|██████▌   | 5076/7689 [12:33:51<6:19:19,  8.71s/it]

 66%|██████▌   | 5077/7689 [12:34:01<6:31:05,  8.98s/it]

 66%|██████▌   | 5078/7689 [12:34:11<6:43:34,  9.27s/it]

 66%|██████▌   | 5079/7689 [12:34:18<6:08:41,  8.48s/it]

 66%|██████▌   | 5080/7689 [12:34:30<6:59:14,  9.64s/it]

 66%|██████▌   | 5081/7689 [12:34:38<6:33:34,  9.05s/it]

 66%|██████▌   | 5082/7689 [12:34:48<6:49:35,  9.43s/it]

 66%|██████▌   | 5083/7689 [12:34:55<6:15:35,  8.65s/it]

 66%|██████▌   | 5084/7689 [12:35:02<6:03:28,  8.37s/it]

 66%|██████▌   | 5085/7689 [12:35:09<5:39:30,  7.82s/it]

 66%|██████▌   | 5086/7689 [12:35:26<7:44:07, 10.70s/it]

 66%|██████▌   | 5087/7689 [12:35:33<6:52:46,  9.52s/it]

 66%|██████▌   | 5088/7689 [12:35:38<5:56:22,  8.22s/it]

 66%|██████▌   | 5089/7689 [12:35:47<5:59:07,  8.29s/it]

 66%|██████▌   | 5090/7689 [12:35:57<6:24:32,  8.88s/it]

 66%|██████▌   | 5091/7689 [12:36:02<5:32:01,  7.67s/it]

 66%|██████▌   | 5092/7689 [12:36:07<5:00:14,  6.94s/it]

 66%|██████▌   | 5093/7689 [12:36:19<6:07:03,  8.48s/it]

 66%|██████▋   | 5094/7689 [12:36:34<7:30:24, 10.41s/it]

 66%|██████▋   | 5095/7689 [12:36:43<7:08:55,  9.92s/it]

 66%|██████▋   | 5096/7689 [12:36:57<8:03:47, 11.19s/it]

 66%|██████▋   | 5097/7689 [12:37:04<7:08:44,  9.92s/it]

 66%|██████▋   | 5098/7689 [12:37:14<7:05:15,  9.85s/it]

 66%|██████▋   | 5099/7689 [12:37:26<7:36:42, 10.58s/it]

 66%|██████▋   | 5100/7689 [12:37:32<6:37:48,  9.22s/it]
 66%|██████▋   | 5100/7689 [12:37:32<6:37:48,  9.22s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 66%|██████▋   | 5101/7689 [12:38:10<12:50:08, 17.85s/it]

 66%|██████▋   | 5102/7689 [12:38:25<12:05:37, 16.83s/it]

 66%|██████▋   | 5103/7689 [12:38:31<9:51:50, 13.73s/it]

 66%|██████▋   | 5104/7689 [12:38:40<8:54:38, 12.41s/it]

 66%|██████▋   | 5105/7689 [12:38:48<7:49:46, 10.91s/it]

 66%|██████▋   | 5106/7689 [12:38:57<7:27:11, 10.39s/it]

 66%|██████▋   | 5107/7689 [12:39:11<8:15:47, 11.52s/it]

 66%|██████▋   | 5108/7689 [12:39:19<7:25:12, 10.35s/it]

 66%|██████▋   | 5109/7689 [12:39:31<7:51:22, 10.96s/it]

 66%|██████▋   | 5110/7689 [12:39:40<7:22:01, 10.28s/it]

 66%|██████▋   | 5111/7689 [12:39:45<6:21:49,  8.89s/it]

 66%|██████▋   | 5112/7689 [12:39:54<6:22:51,  8.91s/it]

 66%|██████▋   | 5113/7689 [12:40:05<6:45:00,  9.43s/it]

 67%|██████▋   | 5114/7689 [12:40:17<7:12:59, 10.09s/it]

 67%|██████▋   | 5115/7689 [12:40:23<6:21:17,  8.89s/it]

 67%|██████▋   | 5116/7689 [12:40:29<5:47:42,  8.11s/it]

 67%|██████▋   | 5117/7689 [12:40:36<5:29:31,  7.69s/it]

 67%|██████▋   | 5118/7689 [12:40:44<5:35:42,  7.83s/it]

 67%|██████▋   | 5119/7689 [12:40:50<5:14:30,  7.34s/it]

 67%|██████▋   | 5120/7689 [12:41:02<6:06:49,  8.57s/it]

 67%|██████▋   | 5121/7689 [12:41:07<5:23:23,  7.56s/it]

 67%|██████▋   | 5122/7689 [12:41:12<4:54:47,  6.89s/it]

 67%|██████▋   | 5123/7689 [12:41:20<5:11:09,  7.28s/it]

 67%|██████▋   | 5124/7689 [12:41:25<4:42:03,  6.60s/it]

 67%|██████▋   | 5125/7689 [12:41:36<5:31:59,  7.77s/it]

 67%|██████▋   | 5126/7689 [12:41:44<5:31:53,  7.77s/it]

 67%|██████▋   | 5127/7689 [12:41:54<6:05:17,  8.55s/it]

 67%|██████▋   | 5128/7689 [12:42:00<5:33:15,  7.81s/it]

 67%|██████▋   | 5129/7689 [12:42:06<5:12:40,  7.33s/it]

 67%|██████▋   | 5130/7689 [12:42:14<5:17:06,  7.43s/it]

 67%|██████▋   | 5131/7689 [12:42:21<5:13:01,  7.34s/it]

 67%|██████▋   | 5132/7689 [12:42:29<5:20:56,  7.53s/it]

 67%|██████▋   | 5133/7689 [12:42:35<5:03:56,  7.13s/it]

 67%|██████▋   | 5134/7689 [12:42:47<5:58:42,  8.42s/it]

 67%|██████▋   | 5135/7689 [12:42:54<5:43:01,  8.06s/it]

 67%|██████▋   | 5136/7689 [12:43:07<6:52:17,  9.69s/it]

 67%|██████▋   | 5137/7689 [12:43:17<6:48:32,  9.61s/it]

 67%|██████▋   | 5138/7689 [12:43:25<6:29:01,  9.15s/it]

 67%|██████▋   | 5139/7689 [12:43:33<6:16:03,  8.85s/it]

 67%|██████▋   | 5140/7689 [12:43:44<6:47:38,  9.60s/it]

 67%|██████▋   | 5141/7689 [12:43:51<6:04:48,  8.59s/it]

 67%|██████▋   | 5142/7689 [12:43:59<6:08:20,  8.68s/it]

 67%|██████▋   | 5143/7689 [12:44:05<5:31:53,  7.82s/it]

 67%|██████▋   | 5144/7689 [12:44:13<5:33:18,  7.86s/it]

 67%|██████▋   | 5145/7689 [12:44:24<6:16:31,  8.88s/it]

 67%|██████▋   | 5146/7689 [12:44:29<5:25:03,  7.67s/it]

 67%|██████▋   | 5147/7689 [12:44:36<5:17:21,  7.49s/it]

 67%|██████▋   | 5148/7689 [12:44:51<6:53:49,  9.77s/it]

 67%|██████▋   | 5149/7689 [12:44:59<6:19:04,  8.95s/it]

 67%|██████▋   | 5150/7689 [12:45:13<7:29:17, 10.62s/it]

 67%|██████▋   | 5151/7689 [12:45:22<7:10:14, 10.17s/it]

 67%|██████▋   | 5152/7689 [12:45:29<6:27:15,  9.16s/it]

 67%|██████▋   | 5153/7689 [12:45:36<5:54:47,  8.39s/it]

 67%|██████▋   | 5154/7689 [12:45:47<6:31:02,  9.26s/it]

 67%|██████▋   | 5155/7689 [12:45:53<5:55:36,  8.42s/it]

 67%|██████▋   | 5156/7689 [12:45:58<5:13:37,  7.43s/it]

 67%|██████▋   | 5157/7689 [12:46:03<4:33:16,  6.48s/it]

 67%|██████▋   | 5158/7689 [12:46:09<4:26:13,  6.31s/it]

 67%|██████▋   | 5159/7689 [12:46:16<4:41:26,  6.67s/it]

 67%|██████▋   | 5160/7689 [12:46:24<5:00:19,  7.13s/it]

 67%|██████▋   | 5161/7689 [12:46:32<5:01:39,  7.16s/it]

 67%|██████▋   | 5162/7689 [12:46:36<4:32:31,  6.47s/it]

 67%|██████▋   | 5163/7689 [12:46:43<4:37:17,  6.59s/it]

 67%|██████▋   | 5164/7689 [12:46:49<4:24:41,  6.29s/it]

 67%|██████▋   | 5165/7689 [12:46:55<4:19:40,  6.17s/it]

 67%|██████▋   | 5166/7689 [12:47:09<6:06:51,  8.72s/it]

 67%|██████▋   | 5167/7689 [12:47:16<5:43:37,  8.18s/it]

 67%|██████▋   | 5168/7689 [12:47:32<7:12:09, 10.29s/it]

 67%|██████▋   | 5169/7689 [12:47:42<7:10:47, 10.26s/it]

 67%|██████▋   | 5170/7689 [12:47:49<6:38:17,  9.49s/it]

 67%|██████▋   | 5171/7689 [12:47:57<6:16:32,  8.97s/it]

 67%|██████▋   | 5172/7689 [12:48:06<6:14:50,  8.94s/it]

 67%|██████▋   | 5173/7689 [12:48:13<5:49:58,  8.35s/it]

 67%|██████▋   | 5174/7689 [12:48:20<5:38:21,  8.07s/it]

 67%|██████▋   | 5175/7689 [12:48:26<5:08:07,  7.35s/it]

 67%|██████▋   | 5176/7689 [12:48:33<5:06:37,  7.32s/it]

 67%|██████▋   | 5177/7689 [12:48:44<5:46:24,  8.27s/it]

 67%|██████▋   | 5178/7689 [12:48:57<6:43:30,  9.64s/it]

 67%|██████▋   | 5179/7689 [12:49:05<6:20:50,  9.10s/it]

 67%|██████▋   | 5180/7689 [12:49:09<5:22:39,  7.72s/it]

 67%|██████▋   | 5181/7689 [12:49:14<4:50:26,  6.95s/it]

 67%|██████▋   | 5182/7689 [12:49:21<4:54:44,  7.05s/it]

 67%|██████▋   | 5183/7689 [12:49:29<4:57:59,  7.13s/it]

 67%|██████▋   | 5184/7689 [12:49:39<5:37:17,  8.08s/it]

 67%|██████▋   | 5185/7689 [12:49:48<5:47:27,  8.33s/it]

 67%|██████▋   | 5186/7689 [12:50:00<6:30:20,  9.36s/it]

 67%|██████▋   | 5187/7689 [12:50:09<6:31:18,  9.38s/it]

 67%|██████▋   | 5188/7689 [12:50:20<6:52:44,  9.90s/it]

 67%|██████▋   | 5189/7689 [12:50:35<7:53:32, 11.36s/it]

 67%|██████▋   | 5190/7689 [12:50:45<7:40:02, 11.05s/it]

 68%|██████▊   | 5191/7689 [12:50:53<6:57:40, 10.03s/it]

 68%|██████▊   | 5192/7689 [12:51:09<8:14:47, 11.89s/it]

 68%|██████▊   | 5193/7689 [12:51:16<7:08:13, 10.29s/it]

 68%|██████▊   | 5194/7689 [12:51:26<7:12:32, 10.40s/it]

 68%|██████▊   | 5195/7689 [12:51:36<6:58:50, 10.08s/it]

 68%|██████▊   | 5196/7689 [12:51:43<6:22:28,  9.21s/it]

 68%|██████▊   | 5197/7689 [12:51:51<6:05:49,  8.81s/it]

 68%|██████▊   | 5198/7689 [12:51:58<5:46:20,  8.34s/it]

 68%|██████▊   | 5199/7689 [12:52:06<5:44:47,  8.31s/it]

 68%|██████▊   | 5200/7689 [12:52:11<4:56:14,  7.14s/it]

 68%|██████▊   | 5201/7689 [12:52:29<7:11:52, 10.41s/it]

 68%|██████▊   | 5202/7689 [12:52:44<8:11:50, 11.87s/it]

 68%|██████▊   | 5203/7689 [12:52:53<7:37:37, 11.04s/it]

 68%|██████▊   | 5204/7689 [12:53:01<6:53:57,  9.99s/it]

 68%|██████▊   | 5205/7689 [12:53:07<6:05:24,  8.83s/it]

 68%|██████▊   | 5206/7689 [12:53:12<5:24:41,  7.85s/it]
{'loss': 0.9403, 'grad_norm': 0.19043626912543926, 'learning_rate': 4.989059912293675e-05, 'epoch': 0.68}


 68%|██████▊   | 5208/7689 [12:53:32<6:20:37,  9.20s/it]

 68%|██████▊   | 5209/7689 [12:53:42<6:32:00,  9.48s/it]

 68%|██████▊   | 5210/7689 [12:53:53<6:47:40,  9.87s/it]

 68%|██████▊   | 5211/7689 [12:54:00<6:08:13,  8.92s/it]

 68%|██████▊   | 5212/7689 [12:54:07<5:40:56,  8.26s/it]

 68%|██████▊   | 5213/7689 [12:54:14<5:29:49,  7.99s/it]

 68%|██████▊   | 5214/7689 [12:54:22<5:25:43,  7.90s/it]

 68%|██████▊   | 5215/7689 [12:54:27<4:51:41,  7.07s/it]
{'loss': 0.988, 'grad_norm': 0.21670453975513998, 'learning_rate': 4.9562877329961186e-05, 'epoch': 0.68}


 68%|██████▊   | 5217/7689 [12:54:40<4:45:27,  6.93s/it]

 68%|██████▊   | 5218/7689 [12:54:51<5:29:17,  8.00s/it]

 68%|██████▊   | 5219/7689 [12:54:56<4:51:05,  7.07s/it]

 68%|██████▊   | 5220/7689 [12:55:00<4:20:46,  6.34s/it]
{'loss': 1.2019, 'grad_norm': 0.207742462903432, 'learning_rate': 4.9381122602922715e-05, 'epoch': 0.68}


 68%|██████▊   | 5222/7689 [12:55:17<4:56:35,  7.21s/it]
{'loss': 1.0939, 'grad_norm': 0.18401009954566286, 'learning_rate': 4.930848354016232e-05, 'epoch': 0.68}


 68%|██████▊   | 5224/7689 [12:55:36<5:42:58,  8.35s/it]

 68%|██████▊   | 5225/7689 [12:55:43<5:18:13,  7.75s/it]

 68%|██████▊   | 5226/7689 [12:55:49<4:56:09,  7.21s/it]
{'loss': 0.9561, 'grad_norm': 0.1940782210525895, 'learning_rate': 4.916331340345801e-05, 'epoch': 0.68}


 68%|██████▊   | 5228/7689 [12:56:09<6:06:36,  8.94s/it]

 68%|██████▊   | 5229/7689 [12:56:15<5:23:03,  7.88s/it]

 68%|██████▊   | 5230/7689 [12:56:33<7:36:08, 11.13s/it]
{'loss': 0.9438, 'grad_norm': 0.2044999920529109, 'learning_rate': 4.901828759525572e-05, 'epoch': 0.68}


 68%|██████▊   | 5232/7689 [12:56:51<6:46:04,  9.92s/it]

 68%|██████▊   | 5233/7689 [12:56:58<6:12:38,  9.10s/it]

 68%|██████▊   | 5234/7689 [12:57:05<5:44:05,  8.41s/it]

 68%|██████▊   | 5235/7689 [12:57:16<6:19:43,  9.28s/it]

 68%|██████▊   | 5236/7689 [12:57:28<6:51:26, 10.06s/it]
{'loss': 0.874, 'grad_norm': 0.2064305617713196, 'learning_rate': 4.880102039945624e-05, 'epoch': 0.68}


 68%|██████▊   | 5238/7689 [12:57:42<5:51:49,  8.61s/it]

 68%|██████▊   | 5239/7689 [12:57:48<5:19:19,  7.82s/it]

 68%|██████▊   | 5240/7689 [12:57:54<5:00:12,  7.36s/it]

 68%|██████▊   | 5241/7689 [12:58:02<5:03:09,  7.43s/it]

 68%|██████▊   | 5242/7689 [12:58:08<4:48:28,  7.07s/it]

 68%|██████▊   | 5243/7689 [12:58:21<6:01:28,  8.87s/it]

 68%|██████▊   | 5244/7689 [12:58:32<6:23:34,  9.41s/it]

 68%|██████▊   | 5245/7689 [12:58:47<7:39:41, 11.29s/it]

 68%|██████▊   | 5246/7689 [12:58:56<7:01:13, 10.35s/it]

 68%|██████▊   | 5247/7689 [12:59:03<6:26:19,  9.49s/it]

 68%|██████▊   | 5248/7689 [12:59:09<5:40:48,  8.38s/it]

 68%|██████▊   | 5249/7689 [12:59:16<5:29:08,  8.09s/it]

 68%|██████▊   | 5250/7689 [12:59:25<5:37:40,  8.31s/it]

 68%|██████▊   | 5251/7689 [12:59:37<6:17:24,  9.29s/it]

 68%|██████▊   | 5252/7689 [12:59:47<6:34:22,  9.71s/it]

 68%|██████▊   | 5253/7689 [12:59:53<5:42:36,  8.44s/it]

 68%|██████▊   | 5254/7689 [13:00:05<6:31:54,  9.66s/it]
{'loss': 0.7287, 'grad_norm': 0.19541259481023696, 'learning_rate': 4.815118667303817e-05, 'epoch': 0.68}


 68%|██████▊   | 5256/7689 [13:00:21<5:59:10,  8.86s/it]

 68%|██████▊   | 5257/7689 [13:00:27<5:26:55,  8.07s/it]

 68%|██████▊   | 5258/7689 [13:00:40<6:25:56,  9.53s/it]

 68%|██████▊   | 5259/7689 [13:00:48<6:07:11,  9.07s/it]

 68%|██████▊   | 5260/7689 [13:00:56<5:52:33,  8.71s/it]

 68%|██████▊   | 5261/7689 [13:01:04<5:36:40,  8.32s/it]

 68%|██████▊   | 5262/7689 [13:01:13<5:53:07,  8.73s/it]
{'loss': 1.0122, 'grad_norm': 0.19168575474550772, 'learning_rate': 4.786332627758826e-05, 'epoch': 0.68}


 68%|██████▊   | 5264/7689 [13:01:32<6:11:32,  9.19s/it]

 68%|██████▊   | 5265/7689 [13:01:42<6:12:15,  9.21s/it]

 68%|██████▊   | 5266/7689 [13:01:51<6:17:01,  9.34s/it]

 69%|██████▊   | 5267/7689 [13:02:05<7:15:57, 10.80s/it]

 69%|██████▊   | 5268/7689 [13:02:12<6:30:20,  9.67s/it]

 69%|██████▊   | 5269/7689 [13:02:19<5:49:38,  8.67s/it]
{'loss': 0.9919, 'grad_norm': 0.18836685215833587, 'learning_rate': 4.7611934007806666e-05, 'epoch': 0.69}


 69%|██████▊   | 5271/7689 [13:02:36<5:49:02,  8.66s/it]
{'loss': 1.0115, 'grad_norm': 0.19422462140450672, 'learning_rate': 4.754019121177498e-05, 'epoch': 0.69}


 69%|██████▊   | 5273/7689 [13:02:50<5:17:10,  7.88s/it]

 69%|██████▊   | 5274/7689 [13:02:55<4:40:47,  6.98s/it]

 69%|██████▊   | 5275/7689 [13:03:06<5:32:17,  8.26s/it]

 69%|██████▊   | 5276/7689 [13:03:17<6:01:40,  8.99s/it]

 69%|██████▊   | 5277/7689 [13:03:23<5:29:23,  8.19s/it]
{'loss': 0.9702, 'grad_norm': 0.21235749982609634, 'learning_rate': 4.73251864322393e-05, 'epoch': 0.69}


 69%|██████▊   | 5279/7689 [13:03:46<6:38:39,  9.93s/it]

 69%|██████▊   | 5280/7689 [13:03:52<5:55:57,  8.87s/it]

 69%|██████▊   | 5281/7689 [13:04:02<6:01:32,  9.01s/it]

 69%|██████▊   | 5282/7689 [13:04:15<6:55:13, 10.35s/it]

 69%|██████▊   | 5283/7689 [13:04:24<6:33:38,  9.82s/it]
{'loss': 1.1594, 'grad_norm': 0.20343994559896106, 'learning_rate': 4.711051813348518e-05, 'epoch': 0.69}

 69%|██████▊   | 5284/7689 [13:04:42<8:15:25, 12.36s/it]


 69%|██████▊   | 5286/7689 [13:04:59<6:51:16, 10.27s/it]

 69%|██████▉   | 5287/7689 [13:05:09<6:48:58, 10.22s/it]

 69%|██████▉   | 5288/7689 [13:05:16<6:09:01,  9.22s/it]

 69%|██████▉   | 5289/7689 [13:05:23<5:49:16,  8.73s/it]

 69%|██████▉   | 5290/7689 [13:05:30<5:23:38,  8.09s/it]

 69%|██████▉   | 5291/7689 [13:05:35<4:55:28,  7.39s/it]

 69%|██████▉   | 5292/7689 [13:05:42<4:42:32,  7.07s/it]

 69%|██████▉   | 5293/7689 [13:05:48<4:28:27,  6.72s/it]

 69%|██████▉   | 5294/7689 [13:05:53<4:05:36,  6.15s/it]

 69%|██████▉   | 5295/7689 [13:05:59<4:05:46,  6.16s/it]

 69%|██████▉   | 5296/7689 [13:06:13<5:44:52,  8.65s/it]
{'loss': 1.0789, 'grad_norm': 0.2168524935705904, 'learning_rate': 4.664656433286858e-05, 'epoch': 0.69}


 69%|██████▉   | 5298/7689 [13:06:33<6:19:33,  9.52s/it]

 69%|██████▉   | 5299/7689 [13:06:41<6:08:22,  9.25s/it]

 69%|██████▉   | 5300/7689 [13:06:48<5:36:14,  8.44s/it]

 69%|██████▉   | 5301/7689 [13:06:57<5:44:02,  8.64s/it]

 69%|██████▉   | 5302/7689 [13:07:02<5:00:37,  7.56s/it]

 69%|██████▉   | 5303/7689 [13:07:09<4:51:33,  7.33s/it]

 69%|██████▉   | 5304/7689 [13:07:20<5:30:11,  8.31s/it]

 69%|██████▉   | 5305/7689 [13:07:32<6:17:57,  9.51s/it]

 69%|██████▉   | 5306/7689 [13:07:38<5:39:32,  8.55s/it]

 69%|██████▉   | 5307/7689 [13:07:48<5:50:50,  8.84s/it]

 69%|██████▉   | 5308/7689 [13:08:01<6:47:54, 10.28s/it]

 69%|██████▉   | 5309/7689 [13:08:10<6:27:47,  9.78s/it]

 69%|██████▉   | 5310/7689 [13:08:21<6:38:45, 10.06s/it]

 69%|██████▉   | 5311/7689 [13:08:27<5:50:52,  8.85s/it]

 69%|██████▉   | 5312/7689 [13:08:35<5:49:48,  8.83s/it]

 69%|██████▉   | 5313/7689 [13:08:43<5:32:02,  8.38s/it]

 69%|██████▉   | 5314/7689 [13:08:53<5:51:57,  8.89s/it]

 69%|██████▉   | 5315/7689 [13:09:01<5:43:16,  8.68s/it]

 69%|██████▉   | 5316/7689 [13:09:07<5:09:15,  7.82s/it]
{'loss': 1.0339, 'grad_norm': 0.20894291183179947, 'learning_rate': 4.593591825444028e-05, 'epoch': 0.69}


 69%|██████▉   | 5318/7689 [13:09:24<5:16:58,  8.02s/it]

 69%|██████▉   | 5319/7689 [13:09:32<5:17:32,  8.04s/it]

 69%|██████▉   | 5320/7689 [13:09:40<5:23:20,  8.19s/it]

 69%|██████▉   | 5321/7689 [13:09:48<5:13:30,  7.94s/it]

 69%|██████▉   | 5322/7689 [13:09:58<5:41:07,  8.65s/it]

 69%|██████▉   | 5323/7689 [13:10:07<5:42:27,  8.68s/it]

 69%|██████▉   | 5324/7689 [13:10:13<5:11:04,  7.89s/it]

 69%|██████▉   | 5325/7689 [13:10:20<5:01:37,  7.66s/it]

 69%|██████▉   | 5326/7689 [13:10:32<5:50:38,  8.90s/it]
{'loss': 0.811, 'grad_norm': 0.18675535187135228, 'learning_rate': 4.55820310459386e-05, 'epoch': 0.69}


 69%|██████▉   | 5328/7689 [13:10:46<5:17:52,  8.08s/it]

 69%|██████▉   | 5329/7689 [13:10:59<6:25:52,  9.81s/it]
{'loss': 0.8615, 'grad_norm': 0.18315001947561985, 'learning_rate': 4.547605288986907e-05, 'epoch': 0.69}

 69%|██████▉   | 5330/7689 [13:11:07<5:54:06,  9.01s/it]

 69%|██████▉   | 5331/7689 [13:11:19<6:29:08,  9.90s/it]

 69%|██████▉   | 5332/7689 [13:11:25<5:44:47,  8.78s/it]

 69%|██████▉   | 5333/7689 [13:11:31<5:12:37,  7.96s/it]


 69%|██████▉   | 5335/7689 [13:12:03<7:58:00, 12.18s/it]

 69%|██████▉   | 5336/7689 [13:12:12<7:13:41, 11.06s/it]
{'loss': 1.0173, 'grad_norm': 0.20435328503690378, 'learning_rate': 4.522910943613986e-05, 'epoch': 0.69}


 69%|██████▉   | 5338/7689 [13:12:29<6:36:28, 10.12s/it]

 69%|██████▉   | 5339/7689 [13:12:40<6:43:01, 10.29s/it]

 69%|██████▉   | 5340/7689 [13:12:49<6:32:11, 10.02s/it]

 69%|██████▉   | 5341/7689 [13:13:04<7:21:07, 11.27s/it]

 69%|██████▉   | 5342/7689 [13:13:11<6:39:54, 10.22s/it]

 69%|██████▉   | 5343/7689 [13:13:21<6:33:05, 10.05s/it]

 70%|██████▉   | 5344/7689 [13:13:28<6:01:16,  9.24s/it]
{'loss': 0.9317, 'grad_norm': 0.18095042050871293, 'learning_rate': 4.4947471588198764e-05, 'epoch': 0.7}


 70%|██████▉   | 5346/7689 [13:13:42<5:09:55,  7.94s/it]

 70%|██████▉   | 5347/7689 [13:13:49<5:07:34,  7.88s/it]

 70%|██████▉   | 5348/7689 [13:13:55<4:45:33,  7.32s/it]

 70%|██████▉   | 5349/7689 [13:14:04<4:55:17,  7.57s/it]

 70%|██████▉   | 5350/7689 [13:14:10<4:41:54,  7.23s/it]

 70%|██████▉   | 5351/7689 [13:14:16<4:31:11,  6.96s/it]

 70%|██████▉   | 5352/7689 [13:14:21<4:04:24,  6.27s/it]

 70%|██████▉   | 5353/7689 [13:14:30<4:31:26,  6.97s/it]

 70%|██████▉   | 5354/7689 [13:14:49<6:56:14, 10.70s/it]

 70%|██████▉   | 5355/7689 [13:14:55<5:56:49,  9.17s/it]

 70%|██████▉   | 5356/7689 [13:15:02<5:39:29,  8.73s/it]

 70%|██████▉   | 5357/7689 [13:15:09<5:09:24,  7.96s/it]

 70%|██████▉   | 5358/7689 [13:15:14<4:40:14,  7.21s/it]

 70%|██████▉   | 5359/7689 [13:15:23<5:02:08,  7.78s/it]

 70%|██████▉   | 5360/7689 [13:15:37<6:14:08,  9.64s/it]

 70%|██████▉   | 5361/7689 [13:15:43<5:35:21,  8.64s/it]
{'loss': 0.9755, 'grad_norm': 0.2129368278954968, 'learning_rate': 4.435107095972585e-05, 'epoch': 0.7}


 70%|██████▉   | 5363/7689 [13:15:54<4:23:22,  6.79s/it]

 70%|██████▉   | 5364/7689 [13:16:02<4:43:16,  7.31s/it]

 70%|██████▉   | 5365/7689 [13:16:08<4:24:26,  6.83s/it]

 70%|██████▉   | 5366/7689 [13:16:15<4:33:57,  7.08s/it]

 70%|██████▉   | 5367/7689 [13:16:34<6:44:56, 10.46s/it]

 70%|██████▉   | 5368/7689 [13:16:40<5:57:58,  9.25s/it]

 70%|██████▉   | 5369/7689 [13:16:47<5:30:08,  8.54s/it]

 70%|██████▉   | 5370/7689 [13:16:54<5:10:11,  8.03s/it]
{'loss': 0.9013, 'grad_norm': 0.18647338374417483, 'learning_rate': 4.403648281692092e-05, 'epoch': 0.7}


 70%|██████▉   | 5372/7689 [13:17:12<5:24:33,  8.40s/it]

 70%|██████▉   | 5373/7689 [13:17:22<5:44:15,  8.92s/it]

 70%|██████▉   | 5374/7689 [13:17:26<4:53:20,  7.60s/it]
{'loss': 1.0781, 'grad_norm': 0.22617534080062582, 'learning_rate': 4.3896923782290613e-05, 'epoch': 0.7}


 70%|██████▉   | 5376/7689 [13:17:43<5:10:11,  8.05s/it]

 70%|██████▉   | 5377/7689 [13:17:50<5:06:28,  7.95s/it]

 70%|██████▉   | 5378/7689 [13:17:55<4:31:53,  7.06s/it]

 70%|██████▉   | 5379/7689 [13:18:05<5:05:26,  7.93s/it]

 70%|██████▉   | 5380/7689 [13:18:15<5:24:13,  8.42s/it]
{'loss': 1.0204, 'grad_norm': 0.18916209640091802, 'learning_rate': 4.3687884004242295e-05, 'epoch': 0.7}


 70%|██████▉   | 5382/7689 [13:18:30<5:00:22,  7.81s/it]

 70%|███████   | 5383/7689 [13:18:39<5:10:41,  8.08s/it]

 70%|███████   | 5384/7689 [13:18:46<5:01:28,  7.85s/it]

 70%|███████   | 5385/7689 [13:18:54<5:01:56,  7.86s/it]

 70%|███████   | 5386/7689 [13:19:03<5:07:51,  8.02s/it]

 70%|███████   | 5387/7689 [13:19:13<5:31:09,  8.63s/it]

 70%|███████   | 5388/7689 [13:19:19<5:10:49,  8.11s/it]
{'loss': 1.1167, 'grad_norm': 0.20567286798285553, 'learning_rate': 4.340972408802605e-05, 'epoch': 0.7}


 70%|███████   | 5390/7689 [13:19:37<5:30:54,  8.64s/it]

 70%|███████   | 5391/7689 [13:19:46<5:31:25,  8.65s/it]

 70%|███████   | 5392/7689 [13:19:58<6:07:28,  9.60s/it]
{'loss': 1.1164, 'grad_norm': 0.2155196983434, 'learning_rate': 4.327088492755091e-05, 'epoch': 0.7}


 70%|███████   | 5394/7689 [13:20:17<5:57:45,  9.35s/it]

 70%|███████   | 5395/7689 [13:20:31<6:49:15, 10.70s/it]

 70%|███████   | 5396/7689 [13:20:36<5:48:33,  9.12s/it]

 70%|███████   | 5397/7689 [13:20:42<5:12:16,  8.17s/it]
{'loss': 1.0678, 'grad_norm': 0.21653600113205765, 'learning_rate': 4.30975625101747e-05, 'epoch': 0.7}


 70%|███████   | 5399/7689 [13:21:04<6:01:38,  9.48s/it]

 70%|███████   | 5400/7689 [13:21:12<5:55:30,  9.32s/it]
 70%|███████   | 5400/7689 [13:21:12<5:55:30,  9.32s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|███████   | 5401/7689 [13:21:51<11:30:09, 18.10s/it]

 70%|███████   | 5402/7689 [13:22:03<10:20:41, 16.28s/it]

 70%|███████   | 5403/7689 [13:22:13<9:02:26, 14.24s/it]

 70%|███████   | 5404/7689 [13:22:17<7:12:12, 11.35s/it]

 70%|███████   | 5405/7689 [13:22:26<6:44:24, 10.62s/it]

 70%|███████   | 5406/7689 [13:22:32<5:45:55,  9.09s/it]

 70%|███████   | 5407/7689 [13:22:38<5:09:54,  8.15s/it]

 70%|███████   | 5408/7689 [13:22:50<5:54:38,  9.33s/it]

 70%|███████   | 5409/7689 [13:23:00<6:11:11,  9.77s/it]
{'loss': 0.8534, 'grad_norm': 0.19494849597019032, 'learning_rate': 4.2682620048251765e-05, 'epoch': 0.7}


 70%|███████   | 5411/7689 [13:23:18<5:52:54,  9.30s/it]
{'loss': 0.991, 'grad_norm': 0.21003916842778164, 'learning_rate': 4.261360507185347e-05, 'epoch': 0.7}

 70%|███████   | 5412/7689 [13:23:25<5:29:18,  8.68s/it]


 70%|███████   | 5414/7689 [13:23:47<6:12:23,  9.82s/it]

 70%|███████   | 5415/7689 [13:23:57<6:16:13,  9.93s/it]

 70%|███████   | 5416/7689 [13:24:07<6:09:14,  9.75s/it]

 70%|███████   | 5417/7689 [13:24:16<6:04:28,  9.63s/it]

 70%|███████   | 5418/7689 [13:24:24<5:48:29,  9.21s/it]

 70%|███████   | 5419/7689 [13:24:32<5:30:06,  8.73s/it]

 70%|███████   | 5420/7689 [13:24:36<4:41:50,  7.45s/it]

 71%|███████   | 5421/7689 [13:24:45<4:59:30,  7.92s/it]

 71%|███████   | 5422/7689 [13:24:53<5:02:10,  8.00s/it]

 71%|███████   | 5423/7689 [13:25:01<4:52:16,  7.74s/it]
{'loss': 1.1629, 'grad_norm': 0.230817898338303, 'learning_rate': 4.2200372274576e-05, 'epoch': 0.71}


 71%|███████   | 5425/7689 [13:25:21<5:25:34,  8.63s/it]

 71%|███████   | 5426/7689 [13:25:27<5:01:10,  7.99s/it]

 71%|███████   | 5427/7689 [13:25:33<4:36:23,  7.33s/it]
{'loss': 0.9698, 'grad_norm': 0.2120320515455208, 'learning_rate': 4.206295568130791e-05, 'epoch': 0.71}

 71%|███████   | 5428/7689 [13:25:42<4:53:20,  7.78s/it]


 71%|███████   | 5430/7689 [13:25:55<4:28:30,  7.13s/it]

 71%|███████   | 5431/7689 [13:26:03<4:39:32,  7.43s/it]

 71%|███████   | 5432/7689 [13:26:15<5:25:56,  8.66s/it]

 71%|███████   | 5433/7689 [13:26:22<5:10:51,  8.27s/it]

 71%|███████   | 5434/7689 [13:26:35<6:06:52,  9.76s/it]

 71%|███████   | 5435/7689 [13:26:41<5:19:05,  8.49s/it]

 71%|███████   | 5436/7689 [13:26:48<5:07:05,  8.18s/it]

 71%|███████   | 5437/7689 [13:26:55<4:49:48,  7.72s/it]

 71%|███████   | 5438/7689 [13:27:02<4:45:27,  7.61s/it]

 71%|███████   | 5439/7689 [13:27:11<4:55:18,  7.87s/it]

 71%|███████   | 5440/7689 [13:27:19<4:56:52,  7.92s/it]

 71%|███████   | 5441/7689 [13:27:34<6:20:26, 10.15s/it]

 71%|███████   | 5442/7689 [13:27:43<6:08:01,  9.83s/it]

 71%|███████   | 5443/7689 [13:27:53<6:06:49,  9.80s/it]
{'loss': 0.9798, 'grad_norm': 0.1809742542472967, 'learning_rate': 4.1514938071139155e-05, 'epoch': 0.71}


 71%|███████   | 5445/7689 [13:28:12<5:51:45,  9.41s/it]

 71%|███████   | 5446/7689 [13:28:20<5:42:06,  9.15s/it]

 71%|███████   | 5447/7689 [13:28:28<5:22:18,  8.63s/it]
{'loss': 1.1637, 'grad_norm': 0.20963493106089662, 'learning_rate': 4.137834780414329e-05, 'epoch': 0.71}


 71%|███████   | 5449/7689 [13:28:56<7:09:37, 11.51s/it]

 71%|███████   | 5450/7689 [13:29:01<5:56:16,  9.55s/it]

 71%|███████   | 5451/7689 [13:29:07<5:19:13,  8.56s/it]

 71%|███████   | 5452/7689 [13:29:15<5:13:34,  8.41s/it]

 71%|███████   | 5453/7689 [13:29:21<4:38:30,  7.47s/it]

 71%|███████   | 5454/7689 [13:29:28<4:38:18,  7.47s/it]

 71%|███████   | 5455/7689 [13:29:34<4:21:37,  7.03s/it]

 71%|███████   | 5456/7689 [13:29:40<4:09:28,  6.70s/it]
{'loss': 1.0292, 'grad_norm': 0.18171894943856257, 'learning_rate': 4.107162880492984e-05, 'epoch': 0.71}


 71%|███████   | 5458/7689 [13:29:55<4:30:03,  7.26s/it]

 71%|███████   | 5459/7689 [13:30:03<4:34:33,  7.39s/it]

 71%|███████   | 5460/7689 [13:30:11<4:43:46,  7.64s/it]

 71%|███████   | 5461/7689 [13:30:17<4:22:29,  7.07s/it]

 71%|███████   | 5462/7689 [13:30:25<4:36:45,  7.46s/it]

 71%|███████   | 5463/7689 [13:30:33<4:44:43,  7.67s/it]

 71%|███████   | 5464/7689 [13:30:43<5:10:53,  8.38s/it]
{'loss': 1.0179, 'grad_norm': 0.18108583004492676, 'learning_rate': 4.079970058754485e-05, 'epoch': 0.71}

 71%|███████   | 5465/7689 [13:30:52<5:15:11,  8.50s/it]


 71%|███████   | 5467/7689 [13:31:13<5:28:53,  8.88s/it]
{'loss': 1.0569, 'grad_norm': 0.220044703364025, 'learning_rate': 4.069790066588967e-05, 'epoch': 0.71}


 71%|███████   | 5469/7689 [13:31:25<4:40:52,  7.59s/it]

 71%|███████   | 5470/7689 [13:31:31<4:25:16,  7.17s/it]

 71%|███████   | 5471/7689 [13:31:40<4:41:18,  7.61s/it]

 71%|███████   | 5472/7689 [13:31:45<4:13:04,  6.85s/it]
{'loss': 1.167, 'grad_norm': 0.19821341762933872, 'learning_rate': 4.052844466294965e-05, 'epoch': 0.71}


 71%|███████   | 5474/7689 [13:32:00<4:31:41,  7.36s/it]

 71%|███████   | 5475/7689 [13:32:14<5:38:47,  9.18s/it]

 71%|███████   | 5476/7689 [13:32:21<5:17:30,  8.61s/it]

 71%|███████   | 5477/7689 [13:32:29<5:04:45,  8.27s/it]

 71%|███████   | 5478/7689 [13:32:41<5:55:46,  9.65s/it]

 71%|███████▏  | 5479/7689 [13:32:47<5:08:16,  8.37s/it]

 71%|███████▏  | 5480/7689 [13:32:55<5:07:39,  8.36s/it]

 71%|███████▏  | 5481/7689 [13:33:03<5:02:26,  8.22s/it]

 71%|███████▏  | 5482/7689 [13:33:14<5:30:10,  8.98s/it]

 71%|███████▏  | 5483/7689 [13:33:23<5:31:52,  9.03s/it]

 71%|███████▏  | 5484/7689 [13:33:31<5:18:13,  8.66s/it]
{'loss': 0.9105, 'grad_norm': 0.2099714688167499, 'learning_rate': 4.01228280614199e-05, 'epoch': 0.71}


 71%|███████▏  | 5486/7689 [13:33:55<6:46:34, 11.07s/it]

 71%|███████▏  | 5487/7689 [13:34:03<6:13:37, 10.18s/it]

 71%|███████▏  | 5488/7689 [13:34:09<5:27:56,  8.94s/it]

 71%|███████▏  | 5489/7689 [13:34:28<7:16:06, 11.89s/it]

 71%|███████▏  | 5490/7689 [13:34:34<6:14:38, 10.22s/it]

 71%|███████▏  | 5491/7689 [13:34:39<5:12:42,  8.54s/it]
{'loss': 0.9518, 'grad_norm': 0.22016636354510072, 'learning_rate': 3.988692425444004e-05, 'epoch': 0.71}


 71%|███████▏  | 5493/7689 [13:34:57<5:23:31,  8.84s/it]

 71%|███████▏  | 5494/7689 [13:35:15<7:10:01, 11.75s/it]
{'loss': 0.8479, 'grad_norm': 0.18885213497834547, 'learning_rate': 3.978598250156118e-05, 'epoch': 0.71}


 71%|███████▏  | 5496/7689 [13:35:39<7:19:37, 12.03s/it]

 71%|███████▏  | 5497/7689 [13:35:44<6:09:32, 10.12s/it]

 72%|███████▏  | 5498/7689 [13:35:52<5:38:12,  9.26s/it]

 72%|███████▏  | 5499/7689 [13:36:00<5:24:00,  8.88s/it]
{'loss': 0.8822, 'grad_norm': 0.18623648161445033, 'learning_rate': 3.961796001547775e-05, 'epoch': 0.72}


 72%|███████▏  | 5501/7689 [13:36:14<4:49:59,  7.95s/it]

 72%|███████▏  | 5502/7689 [13:36:27<5:50:28,  9.62s/it]

 72%|███████▏  | 5503/7689 [13:36:36<5:39:08,  9.31s/it]

 72%|███████▏  | 5504/7689 [13:36:48<6:05:37, 10.04s/it]

 72%|███████▏  | 5505/7689 [13:37:03<7:09:44, 11.81s/it]

 72%|███████▏  | 5506/7689 [13:37:15<7:05:27, 11.69s/it]

 72%|███████▏  | 5507/7689 [13:37:26<6:56:54, 11.46s/it]

 72%|███████▏  | 5508/7689 [13:37:35<6:29:04, 10.70s/it]

 72%|███████▏  | 5509/7689 [13:37:44<6:13:28, 10.28s/it]

 72%|███████▏  | 5510/7689 [13:37:59<7:04:47, 11.70s/it]
{'loss': 0.9393, 'grad_norm': 0.21814850081207648, 'learning_rate': 3.924925444960218e-05, 'epoch': 0.72}


 72%|███████▏  | 5512/7689 [13:38:15<5:54:13,  9.76s/it]
{'loss': 0.9683, 'grad_norm': 0.1912329624021016, 'learning_rate': 3.9182356978303944e-05, 'epoch': 0.72}


 72%|███████▏  | 5514/7689 [13:38:31<5:22:46,  8.90s/it]

 72%|███████▏  | 5515/7689 [13:38:36<4:38:37,  7.69s/it]

 72%|███████▏  | 5516/7689 [13:38:49<5:30:53,  9.14s/it]

 72%|███████▏  | 5517/7689 [13:38:54<4:52:23,  8.08s/it]
{'loss': 1.1374, 'grad_norm': 0.18464591630427707, 'learning_rate': 3.9015302256286655e-05, 'epoch': 0.72}


 72%|███████▏  | 5519/7689 [13:39:20<6:12:38, 10.30s/it]

 72%|███████▏  | 5520/7689 [13:39:28<5:48:41,  9.65s/it]

 72%|███████▏  | 5521/7689 [13:39:40<6:15:11, 10.38s/it]

 72%|███████▏  | 5522/7689 [13:39:47<5:39:30,  9.40s/it]

 72%|███████▏  | 5523/7689 [13:40:00<6:21:29, 10.57s/it]

 72%|███████▏  | 5524/7689 [13:40:15<7:08:46, 11.88s/it]
[2024-05-25 03:11:25,703] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 5525/7689 [13:40:22<6:16:30, 10.44s/it]

 72%|███████▏  | 5526/7689 [13:40:31<5:57:36,  9.92s/it]

 72%|███████▏  | 5527/7689 [13:40:39<5:38:28,  9.39s/it]

 72%|███████▏  | 5528/7689 [13:40:46<5:14:52,  8.74s/it]

 72%|███████▏  | 5529/7689 [13:40:53<4:55:08,  8.20s/it]

 72%|███████▏  | 5530/7689 [13:41:05<5:31:30,  9.21s/it]

 72%|███████▏  | 5531/7689 [13:41:16<5:53:07,  9.82s/it]

 72%|███████▏  | 5532/7689 [13:41:25<5:44:14,  9.58s/it]

 72%|███████▏  | 5533/7689 [13:41:30<4:54:08,  8.19s/it]

 72%|███████▏  | 5534/7689 [13:41:35<4:18:31,  7.20s/it]

 72%|███████▏  | 5535/7689 [13:41:42<4:17:16,  7.17s/it]

 72%|███████▏  | 5536/7689 [13:41:57<5:44:29,  9.60s/it]

 72%|███████▏  | 5537/7689 [13:42:04<5:11:04,  8.67s/it]

 72%|███████▏  | 5538/7689 [13:42:13<5:13:51,  8.75s/it]

 72%|███████▏  | 5539/7689 [13:42:25<5:51:31,  9.81s/it]

 72%|███████▏  | 5540/7689 [13:42:32<5:17:49,  8.87s/it]

 72%|███████▏  | 5541/7689 [13:42:38<4:46:49,  8.01s/it]

 72%|███████▏  | 5542/7689 [13:42:44<4:23:55,  7.38s/it]

 72%|███████▏  | 5543/7689 [13:42:51<4:26:38,  7.46s/it]

 72%|███████▏  | 5544/7689 [13:42:56<4:01:24,  6.75s/it]

 72%|███████▏  | 5545/7689 [13:43:12<5:33:26,  9.33s/it]

 72%|███████▏  | 5546/7689 [13:43:19<5:11:58,  8.73s/it]
{'loss': 0.9321, 'grad_norm': 0.19177083077472276, 'learning_rate': 3.8051743031556054e-05, 'epoch': 0.72}


 72%|███████▏  | 5548/7689 [13:43:38<5:27:43,  9.18s/it]

 72%|███████▏  | 5549/7689 [13:43:45<5:03:46,  8.52s/it]

 72%|███████▏  | 5550/7689 [13:43:50<4:24:26,  7.42s/it]

 72%|███████▏  | 5551/7689 [13:44:03<5:18:05,  8.93s/it]

 72%|███████▏  | 5552/7689 [13:44:13<5:28:41,  9.23s/it]

 72%|███████▏  | 5553/7689 [13:44:17<4:37:25,  7.79s/it]

 72%|███████▏  | 5554/7689 [13:44:28<5:13:10,  8.80s/it]

 72%|███████▏  | 5555/7689 [13:44:35<4:57:33,  8.37s/it]

 72%|███████▏  | 5556/7689 [13:44:49<5:49:14,  9.82s/it]

 72%|███████▏  | 5557/7689 [13:44:56<5:25:10,  9.15s/it]
{'loss': 1.0479, 'grad_norm': 0.19799176421335538, 'learning_rate': 3.768866556262515e-05, 'epoch': 0.72}


 72%|███████▏  | 5559/7689 [13:45:14<5:12:36,  8.81s/it]

 72%|███████▏  | 5560/7689 [13:45:19<4:35:27,  7.76s/it]

 72%|███████▏  | 5561/7689 [13:45:26<4:22:49,  7.41s/it]

 72%|███████▏  | 5562/7689 [13:45:35<4:36:46,  7.81s/it]

 72%|███████▏  | 5563/7689 [13:45:44<4:51:54,  8.24s/it]

 72%|███████▏  | 5564/7689 [13:45:54<5:15:12,  8.90s/it]

 72%|███████▏  | 5565/7689 [13:46:00<4:41:49,  7.96s/it]

 72%|███████▏  | 5566/7689 [13:46:10<5:01:33,  8.52s/it]

 72%|███████▏  | 5567/7689 [13:46:20<5:14:39,  8.90s/it]

 72%|███████▏  | 5568/7689 [13:46:33<6:04:20, 10.31s/it]

 72%|███████▏  | 5569/7689 [13:46:46<6:30:29, 11.05s/it]

 72%|███████▏  | 5570/7689 [13:46:58<6:39:07, 11.30s/it]

 72%|███████▏  | 5571/7689 [13:47:10<6:45:23, 11.48s/it]

 72%|███████▏  | 5572/7689 [13:47:17<6:03:55, 10.31s/it]

 72%|███████▏  | 5573/7689 [13:47:27<6:01:08, 10.24s/it]

 72%|███████▏  | 5574/7689 [13:47:39<6:17:58, 10.72s/it]

 73%|███████▎  | 5575/7689 [13:47:47<5:42:38,  9.72s/it]

 73%|███████▎  | 5576/7689 [13:47:53<5:11:21,  8.84s/it]

 73%|███████▎  | 5577/7689 [13:48:00<4:43:42,  8.06s/it]

 73%|███████▎  | 5578/7689 [13:48:05<4:10:23,  7.12s/it]

 73%|███████▎  | 5579/7689 [13:48:12<4:10:29,  7.12s/it]

 73%|███████▎  | 5580/7689 [13:48:17<3:49:33,  6.53s/it]

 73%|███████▎  | 5581/7689 [13:48:23<3:44:52,  6.40s/it]

 73%|███████▎  | 5582/7689 [13:48:32<4:10:51,  7.14s/it]

 73%|███████▎  | 5583/7689 [13:48:40<4:23:53,  7.52s/it]

 73%|███████▎  | 5584/7689 [13:48:45<3:58:07,  6.79s/it]

 73%|███████▎  | 5585/7689 [13:48:53<4:05:44,  7.01s/it]

 73%|███████▎  | 5586/7689 [13:49:00<4:06:45,  7.04s/it]

 73%|███████▎  | 5587/7689 [13:49:09<4:31:38,  7.75s/it]

 73%|███████▎  | 5588/7689 [13:49:28<6:29:45, 11.13s/it]
[2024-05-25 03:20:38,804] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 5589/7689 [13:49:39<6:19:49, 10.85s/it]

 73%|███████▎  | 5590/7689 [13:49:52<6:41:52, 11.49s/it]

 73%|███████▎  | 5591/7689 [13:50:03<6:41:20, 11.48s/it]

 73%|███████▎  | 5592/7689 [13:50:17<7:05:34, 12.18s/it]

 73%|███████▎  | 5593/7689 [13:50:27<6:43:36, 11.55s/it]

 73%|███████▎  | 5594/7689 [13:50:34<5:57:11, 10.23s/it]

 73%|███████▎  | 5595/7689 [13:50:42<5:34:56,  9.60s/it]

 73%|███████▎  | 5596/7689 [13:50:48<4:50:24,  8.33s/it]

 73%|███████▎  | 5597/7689 [13:50:54<4:34:40,  7.88s/it]

 73%|███████▎  | 5598/7689 [13:51:03<4:40:32,  8.05s/it]

 73%|███████▎  | 5599/7689 [13:51:11<4:46:44,  8.23s/it]
{'loss': 0.9469, 'grad_norm': 0.19603521118486977, 'learning_rate': 3.631474275578754e-05, 'epoch': 0.73}


 73%|███████▎  | 5601/7689 [13:51:27<4:42:55,  8.13s/it]
{'loss': 0.8342, 'grad_norm': 0.19288528713064118, 'learning_rate': 3.6249811684780096e-05, 'epoch': 0.73}


 73%|███████▎  | 5603/7689 [13:51:42<4:29:23,  7.75s/it]

 73%|███████▎  | 5604/7689 [13:51:55<5:20:55,  9.24s/it]

 73%|███████▎  | 5605/7689 [13:52:01<4:51:15,  8.39s/it]

 73%|███████▎  | 5606/7689 [13:52:06<4:18:04,  7.43s/it]

 73%|███████▎  | 5607/7689 [13:52:15<4:30:59,  7.81s/it]

 73%|███████▎  | 5608/7689 [13:52:21<4:12:50,  7.29s/it]

 73%|███████▎  | 5609/7689 [13:52:28<4:06:01,  7.10s/it]

 73%|███████▎  | 5610/7689 [13:52:33<3:48:43,  6.60s/it]
{'loss': 1.0935, 'grad_norm': 0.21094145554626043, 'learning_rate': 3.595818247003713e-05, 'epoch': 0.73}


 73%|███████▎  | 5612/7689 [13:52:47<3:56:17,  6.83s/it]

 73%|███████▎  | 5613/7689 [13:52:52<3:42:46,  6.44s/it]
{'loss': 1.0463, 'grad_norm': 0.23311830656100677, 'learning_rate': 3.5861177071007624e-05, 'epoch': 0.73}


 73%|███████▎  | 5615/7689 [13:53:08<4:06:00,  7.12s/it]

 73%|███████▎  | 5616/7689 [13:53:24<5:39:58,  9.84s/it]

 73%|███████▎  | 5617/7689 [13:53:34<5:46:08, 10.02s/it]

 73%|███████▎  | 5618/7689 [13:53:45<5:50:13, 10.15s/it]

 73%|███████▎  | 5619/7689 [13:53:55<5:51:08, 10.18s/it]

 73%|███████▎  | 5620/7689 [13:54:01<5:09:42,  8.98s/it]

 73%|███████▎  | 5621/7689 [13:54:09<4:55:21,  8.57s/it]

 73%|███████▎  | 5622/7689 [13:54:14<4:23:38,  7.65s/it]

 73%|███████▎  | 5623/7689 [13:54:20<4:04:49,  7.11s/it]

 73%|███████▎  | 5624/7689 [13:54:29<4:19:20,  7.54s/it]

 73%|███████▎  | 5625/7689 [13:54:38<4:37:11,  8.06s/it]

 73%|███████▎  | 5626/7689 [13:54:45<4:24:07,  7.68s/it]

 73%|███████▎  | 5627/7689 [13:54:59<5:30:45,  9.62s/it]

 73%|███████▎  | 5628/7689 [13:55:06<5:05:16,  8.89s/it]

 73%|███████▎  | 5629/7689 [13:55:16<5:18:33,  9.28s/it]

 73%|███████▎  | 5630/7689 [13:55:26<5:22:06,  9.39s/it]

 73%|███████▎  | 5631/7689 [13:55:36<5:29:40,  9.61s/it]

 73%|███████▎  | 5632/7689 [13:55:43<5:00:13,  8.76s/it]

 73%|███████▎  | 5633/7689 [13:55:57<5:52:47, 10.30s/it]

 73%|███████▎  | 5634/7689 [13:56:03<5:09:22,  9.03s/it]

 73%|███████▎  | 5635/7689 [13:56:09<4:41:18,  8.22s/it]

 73%|███████▎  | 5636/7689 [13:56:14<4:08:43,  7.27s/it]

 73%|███████▎  | 5637/7689 [13:56:20<3:52:00,  6.78s/it]

 73%|███████▎  | 5638/7689 [13:56:30<4:29:06,  7.87s/it]

 73%|███████▎  | 5639/7689 [13:56:36<4:06:41,  7.22s/it]

 73%|███████▎  | 5640/7689 [13:56:45<4:26:48,  7.81s/it]
{'loss': 1.0798, 'grad_norm': 0.1962609139643487, 'learning_rate': 3.499275627259533e-05, 'epoch': 0.73}


 73%|███████▎  | 5642/7689 [13:57:06<5:19:34,  9.37s/it]

 73%|███████▎  | 5643/7689 [13:57:20<6:10:37, 10.87s/it]

 73%|███████▎  | 5644/7689 [13:57:32<6:12:56, 10.94s/it]

 73%|███████▎  | 5645/7689 [13:57:44<6:28:52, 11.42s/it]

 73%|███████▎  | 5646/7689 [13:57:51<5:42:16, 10.05s/it]

 73%|███████▎  | 5647/7689 [13:58:01<5:40:06,  9.99s/it]
{'loss': 0.9453, 'grad_norm': 0.192700247417701, 'learning_rate': 3.4768978284163764e-05, 'epoch': 0.73}


 73%|███████▎  | 5649/7689 [13:58:23<6:03:41, 10.70s/it]

 73%|███████▎  | 5650/7689 [13:58:31<5:35:52,  9.88s/it]
{'loss': 1.0383, 'grad_norm': 0.20350186601673217, 'learning_rate': 3.4673246938847535e-05, 'epoch': 0.73}


 74%|███████▎  | 5652/7689 [13:58:46<4:53:32,  8.65s/it]

 74%|███████▎  | 5653/7689 [13:58:59<5:33:39,  9.83s/it]

 74%|███████▎  | 5654/7689 [13:59:08<5:24:15,  9.56s/it]

 74%|███████▎  | 5655/7689 [13:59:19<5:37:44,  9.96s/it]

 74%|███████▎  | 5656/7689 [13:59:25<4:58:44,  8.82s/it]

 74%|███████▎  | 5657/7689 [13:59:31<4:37:21,  8.19s/it]

 74%|███████▎  | 5658/7689 [13:59:41<4:50:02,  8.57s/it]

 74%|███████▎  | 5659/7689 [13:59:51<5:05:53,  9.04s/it]

 74%|███████▎  | 5660/7689 [13:59:59<4:50:03,  8.58s/it]

 74%|███████▎  | 5661/7689 [14:00:10<5:14:14,  9.30s/it]

 74%|███████▎  | 5662/7689 [14:00:16<4:45:42,  8.46s/it]

 74%|███████▎  | 5663/7689 [14:00:34<6:24:52, 11.40s/it]

 74%|███████▎  | 5664/7689 [14:00:43<6:00:07, 10.67s/it]

 74%|███████▎  | 5665/7689 [14:00:56<6:24:14, 11.39s/it]

 74%|███████▎  | 5666/7689 [14:01:01<5:19:24,  9.47s/it]

 74%|███████▎  | 5667/7689 [14:01:07<4:38:55,  8.28s/it]

 74%|███████▎  | 5668/7689 [14:01:12<4:05:39,  7.29s/it]

 74%|███████▎  | 5669/7689 [14:01:17<3:47:46,  6.77s/it]

 74%|███████▎  | 5670/7689 [14:01:23<3:33:17,  6.34s/it]

 74%|███████▍  | 5671/7689 [14:01:39<5:16:38,  9.41s/it]
{'loss': 0.8643, 'grad_norm': 0.20220651926406, 'learning_rate': 3.400605716270693e-05, 'epoch': 0.74}


 74%|███████▍  | 5673/7689 [14:01:57<5:08:22,  9.18s/it]

 74%|███████▍  | 5674/7689 [14:02:08<5:26:45,  9.73s/it]
{'loss': 0.9617, 'grad_norm': 0.17948955264895133, 'learning_rate': 3.391116468424227e-05, 'epoch': 0.74}


 74%|███████▍  | 5676/7689 [14:02:30<5:54:03, 10.55s/it]

 74%|███████▍  | 5677/7689 [14:02:37<5:20:12,  9.55s/it]

 74%|███████▍  | 5678/7689 [14:02:42<4:30:47,  8.08s/it]

 74%|███████▍  | 5679/7689 [14:02:57<5:48:07, 10.39s/it]

 74%|███████▍  | 5680/7689 [14:03:06<5:34:01,  9.98s/it]

 74%|███████▍  | 5681/7689 [14:03:14<5:05:47,  9.14s/it]

 74%|███████▍  | 5682/7689 [14:03:22<4:56:27,  8.86s/it]

 74%|███████▍  | 5683/7689 [14:03:26<4:13:35,  7.58s/it]

 74%|███████▍  | 5684/7689 [14:03:35<4:26:56,  7.99s/it]

 74%|███████▍  | 5685/7689 [14:03:41<3:59:18,  7.16s/it]

 74%|███████▍  | 5686/7689 [14:03:47<3:52:26,  6.96s/it]

 74%|███████▍  | 5687/7689 [14:03:52<3:31:17,  6.33s/it]

 74%|███████▍  | 5688/7689 [14:03:58<3:28:00,  6.24s/it]

 74%|███████▍  | 5689/7689 [14:04:04<3:22:22,  6.07s/it]

 74%|███████▍  | 5690/7689 [14:04:10<3:27:30,  6.23s/it]

 74%|███████▍  | 5691/7689 [14:04:18<3:47:01,  6.82s/it]

 74%|███████▍  | 5692/7689 [14:04:28<4:19:18,  7.79s/it]

 74%|███████▍  | 5693/7689 [14:04:38<4:34:44,  8.26s/it]

 74%|███████▍  | 5694/7689 [14:04:45<4:24:24,  7.95s/it]

 74%|███████▍  | 5695/7689 [14:04:51<4:06:02,  7.40s/it]
{'loss': 1.1278, 'grad_norm': 0.21386147803272226, 'learning_rate': 3.3249880977480954e-05, 'epoch': 0.74}


 74%|███████▍  | 5697/7689 [14:05:06<4:04:28,  7.36s/it]

 74%|███████▍  | 5698/7689 [14:05:14<4:11:09,  7.57s/it]

 74%|███████▍  | 5699/7689 [14:05:23<4:31:47,  8.19s/it]

 74%|███████▍  | 5700/7689 [14:05:36<5:13:41,  9.46s/it]
 74%|███████▍  | 5700/7689 [14:05:36<5:13:41,  9.46s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.8633, 'grad_norm': 0.18211091787544742, 'learning_rate': 3.306189989958929e-05, 'epoch': 0.74}

 74%|███████▍  | 5702/7689 [14:06:32<9:31:07, 17.25s/it]

 74%|███████▍  | 5703/7689 [14:06:41<8:13:39, 14.91s/it]

 74%|███████▍  | 5704/7689 [14:06:49<7:04:47, 12.84s/it]

 74%|███████▍  | 5705/7689 [14:06:54<5:45:00, 10.43s/it]

 74%|███████▍  | 5706/7689 [14:07:00<5:04:41,  9.22s/it]

 74%|███████▍  | 5707/7689 [14:07:10<5:05:38,  9.25s/it]

 74%|███████▍  | 5708/7689 [14:07:26<6:10:23, 11.22s/it]

 74%|███████▍  | 5709/7689 [14:07:40<6:42:19, 12.19s/it]

 74%|███████▍  | 5710/7689 [14:07:49<6:07:51, 11.15s/it]

 74%|███████▍  | 5711/7689 [14:07:55<5:14:43,  9.55s/it]

 74%|███████▍  | 5712/7689 [14:08:02<4:52:29,  8.88s/it]
{'loss': 1.098, 'grad_norm': 0.2110309012689465, 'learning_rate': 3.271837934205936e-05, 'epoch': 0.74}


 74%|███████▍  | 5714/7689 [14:08:13<3:56:01,  7.17s/it]

 74%|███████▍  | 5715/7689 [14:08:17<3:30:02,  6.38s/it]

 74%|███████▍  | 5716/7689 [14:08:28<4:09:30,  7.59s/it]

 74%|███████▍  | 5717/7689 [14:08:43<5:27:17,  9.96s/it]

 74%|███████▍  | 5718/7689 [14:08:49<4:47:24,  8.75s/it]
{'loss': 0.8323, 'grad_norm': 0.2087083162607741, 'learning_rate': 3.253161288749146e-05, 'epoch': 0.74}


 74%|███████▍  | 5720/7689 [14:08:59<3:44:21,  6.84s/it]

 74%|███████▍  | 5721/7689 [14:09:06<3:43:48,  6.82s/it]

 74%|███████▍  | 5722/7689 [14:09:13<3:42:43,  6.79s/it]

 74%|███████▍  | 5723/7689 [14:09:20<3:50:58,  7.05s/it]

 74%|███████▍  | 5724/7689 [14:09:29<4:10:18,  7.64s/it]

 74%|███████▍  | 5725/7689 [14:09:36<3:55:57,  7.21s/it]

 74%|███████▍  | 5726/7689 [14:09:41<3:37:29,  6.65s/it]

 74%|███████▍  | 5727/7689 [14:09:49<3:50:50,  7.06s/it]

 74%|███████▍  | 5728/7689 [14:09:56<3:48:38,  7.00s/it]

 75%|███████▍  | 5729/7689 [14:10:02<3:44:43,  6.88s/it]

 75%|███████▍  | 5730/7689 [14:10:10<3:54:08,  7.17s/it]

 75%|███████▍  | 5731/7689 [14:10:15<3:34:39,  6.58s/it]

 75%|███████▍  | 5732/7689 [14:10:22<3:38:19,  6.69s/it]

 75%|███████▍  | 5733/7689 [14:10:30<3:48:09,  7.00s/it]

 75%|███████▍  | 5734/7689 [14:10:39<4:04:20,  7.50s/it]

 75%|███████▍  | 5735/7689 [14:10:48<4:20:56,  8.01s/it]

 75%|███████▍  | 5736/7689 [14:10:54<4:03:50,  7.49s/it]

 75%|███████▍  | 5737/7689 [14:11:03<4:14:15,  7.82s/it]

 75%|███████▍  | 5738/7689 [14:11:13<4:40:47,  8.64s/it]

 75%|███████▍  | 5739/7689 [14:11:23<4:53:42,  9.04s/it]

 75%|███████▍  | 5740/7689 [14:11:37<5:36:15, 10.35s/it]

 75%|███████▍  | 5741/7689 [14:11:48<5:40:55, 10.50s/it]

 75%|███████▍  | 5742/7689 [14:11:55<5:06:00,  9.43s/it]

 75%|███████▍  | 5743/7689 [14:12:03<4:51:33,  8.99s/it]

 75%|███████▍  | 5744/7689 [14:12:12<4:53:35,  9.06s/it]

 75%|███████▍  | 5745/7689 [14:12:26<5:46:22, 10.69s/it]
{'loss': 0.9706, 'grad_norm': 0.17384113563861842, 'learning_rate': 3.16965143850671e-05, 'epoch': 0.75}


 75%|███████▍  | 5747/7689 [14:12:41<4:55:31,  9.13s/it]

 75%|███████▍  | 5748/7689 [14:12:53<5:23:23, 10.00s/it]

 75%|███████▍  | 5749/7689 [14:13:02<5:13:50,  9.71s/it]

 75%|███████▍  | 5750/7689 [14:13:08<4:39:45,  8.66s/it]

 75%|███████▍  | 5751/7689 [14:13:19<5:02:59,  9.38s/it]

 75%|███████▍  | 5752/7689 [14:13:25<4:27:32,  8.29s/it]

 75%|███████▍  | 5753/7689 [14:13:32<4:09:07,  7.72s/it]

 75%|███████▍  | 5754/7689 [14:13:42<4:33:45,  8.49s/it]

 75%|███████▍  | 5755/7689 [14:13:48<4:05:57,  7.63s/it]
{'loss': 0.9544, 'grad_norm': 0.210092498838058, 'learning_rate': 3.138945507719554e-05, 'epoch': 0.75}


 75%|███████▍  | 5757/7689 [14:14:11<4:54:09,  9.14s/it]
{'loss': 1.0276, 'grad_norm': 0.2010836250824381, 'learning_rate': 3.1328189133657394e-05, 'epoch': 0.75}


 75%|███████▍  | 5759/7689 [14:14:23<3:55:50,  7.33s/it]

 75%|███████▍  | 5760/7689 [14:14:32<4:13:23,  7.88s/it]

 75%|███████▍  | 5761/7689 [14:14:38<3:54:26,  7.30s/it]

 75%|███████▍  | 5762/7689 [14:14:49<4:33:44,  8.52s/it]

 75%|███████▍  | 5763/7689 [14:14:55<4:13:59,  7.91s/it]

 75%|███████▍  | 5764/7689 [14:15:02<4:00:15,  7.49s/it]

 75%|███████▍  | 5765/7689 [14:15:07<3:41:08,  6.90s/it]

 75%|███████▍  | 5766/7689 [14:15:14<3:35:02,  6.71s/it]
{'loss': 0.869, 'grad_norm': 0.19546613249578332, 'learning_rate': 3.1053096183241394e-05, 'epoch': 0.75}


 75%|███████▌  | 5768/7689 [14:15:29<3:53:11,  7.28s/it]

 75%|███████▌  | 5769/7689 [14:15:34<3:29:09,  6.54s/it]
{'loss': 1.047, 'grad_norm': 0.21379623103765477, 'learning_rate': 3.0961618550646145e-05, 'epoch': 0.75}


 75%|███████▌  | 5771/7689 [14:15:51<4:06:16,  7.70s/it]

 75%|███████▌  | 5772/7689 [14:16:00<4:18:01,  8.08s/it]

 75%|███████▌  | 5773/7689 [14:16:06<3:57:45,  7.45s/it]

 75%|███████▌  | 5774/7689 [14:16:12<3:43:36,  7.01s/it]

 75%|███████▌  | 5775/7689 [14:16:21<3:59:55,  7.52s/it]

 75%|███████▌  | 5776/7689 [14:16:29<4:03:55,  7.65s/it]

 75%|███████▌  | 5777/7689 [14:16:36<4:04:06,  7.66s/it]

 75%|███████▌  | 5778/7689 [14:16:44<4:04:10,  7.67s/it]

 75%|███████▌  | 5779/7689 [14:16:52<4:02:11,  7.61s/it]

 75%|███████▌  | 5780/7689 [14:16:59<3:58:23,  7.49s/it]

 75%|███████▌  | 5781/7689 [14:17:08<4:17:21,  8.09s/it]

 75%|███████▌  | 5782/7689 [14:17:18<4:30:47,  8.52s/it]

 75%|███████▌  | 5783/7689 [14:17:25<4:15:35,  8.05s/it]

 75%|███████▌  | 5784/7689 [14:17:41<5:38:49, 10.67s/it]
{'loss': 0.785, 'grad_norm': 0.19024201141545893, 'learning_rate': 3.0505887089202256e-05, 'epoch': 0.75}


 75%|███████▌  | 5786/7689 [14:17:57<4:51:01,  9.18s/it]

 75%|███████▌  | 5787/7689 [14:18:06<4:53:53,  9.27s/it]

 75%|███████▌  | 5788/7689 [14:18:12<4:22:39,  8.29s/it]

 75%|███████▌  | 5789/7689 [14:18:18<3:55:37,  7.44s/it]

 75%|███████▌  | 5790/7689 [14:18:24<3:43:17,  7.05s/it]
{'loss': 1.1141, 'grad_norm': 0.20337241678038015, 'learning_rate': 3.0324370348266983e-05, 'epoch': 0.75}

 75%|███████▌  | 5791/7689 [14:18:31<3:42:56,  7.05s/it]


 75%|███████▌  | 5793/7689 [14:18:58<5:28:01, 10.38s/it]

 75%|███████▌  | 5794/7689 [14:19:15<6:33:02, 12.44s/it]

 75%|███████▌  | 5795/7689 [14:19:23<5:48:47, 11.05s/it]

 75%|███████▌  | 5796/7689 [14:19:29<4:59:54,  9.51s/it]

 75%|███████▌  | 5797/7689 [14:19:34<4:23:13,  8.35s/it]

 75%|███████▌  | 5798/7689 [14:19:41<4:07:12,  7.84s/it]

 75%|███████▌  | 5799/7689 [14:19:53<4:43:22,  9.00s/it]

 75%|███████▌  | 5800/7689 [14:19:59<4:13:07,  8.04s/it]

 75%|███████▌  | 5801/7689 [14:20:08<4:27:06,  8.49s/it]

 75%|███████▌  | 5802/7689 [14:20:14<3:58:59,  7.60s/it]

 75%|███████▌  | 5803/7689 [14:20:21<3:58:12,  7.58s/it]
{'loss': 0.9922, 'grad_norm': 0.19692879657224363, 'learning_rate': 2.9932612491141787e-05, 'epoch': 0.75}


 75%|███████▌  | 5805/7689 [14:20:34<3:37:58,  6.94s/it]

 76%|███████▌  | 5806/7689 [14:20:43<3:56:51,  7.55s/it]

 76%|███████▌  | 5807/7689 [14:20:54<4:30:50,  8.63s/it]

 76%|███████▌  | 5808/7689 [14:21:00<4:06:11,  7.85s/it]

 76%|███████▌  | 5809/7689 [14:21:07<3:56:04,  7.53s/it]

 76%|███████▌  | 5810/7689 [14:21:13<3:41:58,  7.09s/it]

 76%|███████▌  | 5811/7689 [14:21:22<4:03:53,  7.79s/it]

 76%|███████▌  | 5812/7689 [14:21:34<4:39:45,  8.94s/it]

 76%|███████▌  | 5813/7689 [14:21:41<4:19:23,  8.30s/it]

 76%|███████▌  | 5814/7689 [14:21:48<4:12:00,  8.06s/it]

 76%|███████▌  | 5815/7689 [14:21:59<4:37:45,  8.89s/it]

 76%|███████▌  | 5816/7689 [14:22:08<4:38:34,  8.92s/it]

 76%|███████▌  | 5817/7689 [14:22:14<4:16:01,  8.21s/it]

 76%|███████▌  | 5818/7689 [14:22:26<4:48:40,  9.26s/it]

 76%|███████▌  | 5819/7689 [14:22:33<4:22:46,  8.43s/it]

 76%|███████▌  | 5820/7689 [14:22:42<4:32:51,  8.76s/it]

 76%|███████▌  | 5821/7689 [14:22:47<3:57:55,  7.64s/it]

 76%|███████▌  | 5822/7689 [14:22:55<3:54:49,  7.55s/it]

 76%|███████▌  | 5823/7689 [14:23:05<4:21:54,  8.42s/it]

 76%|███████▌  | 5824/7689 [14:23:13<4:18:54,  8.33s/it]

 76%|███████▌  | 5825/7689 [14:23:19<3:53:02,  7.50s/it]

 76%|███████▌  | 5826/7689 [14:23:28<4:11:21,  8.10s/it]

 76%|███████▌  | 5827/7689 [14:23:40<4:44:13,  9.16s/it]

 76%|███████▌  | 5828/7689 [14:23:50<4:49:57,  9.35s/it]

 76%|███████▌  | 5829/7689 [14:23:58<4:38:12,  8.97s/it]

 76%|███████▌  | 5830/7689 [14:24:09<4:58:17,  9.63s/it]

 76%|███████▌  | 5831/7689 [14:24:17<4:40:28,  9.06s/it]

 76%|███████▌  | 5832/7689 [14:24:24<4:27:58,  8.66s/it]

 76%|███████▌  | 5833/7689 [14:24:37<5:04:45,  9.85s/it]

 76%|███████▌  | 5834/7689 [14:24:48<5:17:52, 10.28s/it]

 76%|███████▌  | 5835/7689 [14:25:02<5:49:33, 11.31s/it]

 76%|███████▌  | 5836/7689 [14:25:15<6:04:41, 11.81s/it]

 76%|███████▌  | 5837/7689 [14:25:25<5:45:50, 11.20s/it]
{'loss': 1.1389, 'grad_norm': 0.18279624989453597, 'learning_rate': 2.891797856778833e-05, 'epoch': 0.76}


 76%|███████▌  | 5839/7689 [14:25:34<4:04:15,  7.92s/it]

 76%|███████▌  | 5840/7689 [14:25:39<3:37:55,  7.07s/it]

 76%|███████▌  | 5841/7689 [14:25:47<3:44:59,  7.30s/it]

 76%|███████▌  | 5842/7689 [14:26:02<4:51:28,  9.47s/it]

 76%|███████▌  | 5843/7689 [14:26:09<4:27:17,  8.69s/it]

 76%|███████▌  | 5844/7689 [14:26:14<4:00:10,  7.81s/it]

 76%|███████▌  | 5845/7689 [14:26:27<4:40:04,  9.11s/it]

 76%|███████▌  | 5846/7689 [14:26:39<5:11:03, 10.13s/it]

 76%|███████▌  | 5847/7689 [14:26:46<4:42:55,  9.22s/it]

 76%|███████▌  | 5848/7689 [14:26:52<4:11:28,  8.20s/it]

 76%|███████▌  | 5849/7689 [14:26:57<3:38:57,  7.14s/it]

 76%|███████▌  | 5850/7689 [14:27:03<3:29:57,  6.85s/it]

 76%|███████▌  | 5851/7689 [14:27:08<3:18:23,  6.48s/it]

 76%|███████▌  | 5852/7689 [14:27:17<3:37:59,  7.12s/it]

 76%|███████▌  | 5853/7689 [14:27:22<3:14:30,  6.36s/it]

 76%|███████▌  | 5854/7689 [14:27:27<3:09:17,  6.19s/it]

 76%|███████▌  | 5855/7689 [14:27:35<3:26:00,  6.74s/it]

 76%|███████▌  | 5856/7689 [14:27:47<4:07:30,  8.10s/it]

 76%|███████▌  | 5857/7689 [14:27:53<3:52:22,  7.61s/it]

 76%|███████▌  | 5858/7689 [14:28:05<4:34:28,  8.99s/it]

 76%|███████▌  | 5859/7689 [14:28:13<4:21:15,  8.57s/it]

 76%|███████▌  | 5860/7689 [14:28:21<4:15:45,  8.39s/it]

 76%|███████▌  | 5861/7689 [14:28:37<5:22:59, 10.60s/it]

 76%|███████▌  | 5862/7689 [14:28:47<5:22:50, 10.60s/it]

 76%|███████▋  | 5863/7689 [14:28:53<4:34:01,  9.00s/it]

 76%|███████▋  | 5864/7689 [14:29:01<4:32:27,  8.96s/it]

 76%|███████▋  | 5865/7689 [14:29:11<4:39:43,  9.20s/it]

 76%|███████▋  | 5866/7689 [14:29:18<4:14:48,  8.39s/it]

 76%|███████▋  | 5867/7689 [14:29:24<3:59:41,  7.89s/it]

 76%|███████▋  | 5868/7689 [14:29:34<4:15:47,  8.43s/it]

 76%|███████▋  | 5869/7689 [14:29:41<4:05:39,  8.10s/it]
{'loss': 1.1441, 'grad_norm': 0.19067831270394905, 'learning_rate': 2.7976344293091962e-05, 'epoch': 0.76}


 76%|███████▋  | 5871/7689 [14:30:00<4:22:44,  8.67s/it]

 76%|███████▋  | 5872/7689 [14:30:08<4:21:30,  8.64s/it]

 76%|███████▋  | 5873/7689 [14:30:15<4:02:11,  8.00s/it]

 76%|███████▋  | 5874/7689 [14:30:30<5:10:03, 10.25s/it]

 76%|███████▋  | 5875/7689 [14:30:36<4:27:51,  8.86s/it]

 76%|███████▋  | 5876/7689 [14:30:44<4:20:19,  8.62s/it]

 76%|███████▋  | 5877/7689 [14:30:51<4:08:00,  8.21s/it]

 76%|███████▋  | 5878/7689 [14:30:58<3:59:53,  7.95s/it]

 76%|███████▋  | 5879/7689 [14:31:13<4:58:36,  9.90s/it]

 76%|███████▋  | 5880/7689 [14:31:22<4:48:31,  9.57s/it]

 76%|███████▋  | 5881/7689 [14:31:29<4:30:40,  8.98s/it]

 76%|███████▋  | 5882/7689 [14:31:35<4:02:30,  8.05s/it]

 77%|███████▋  | 5883/7689 [14:31:45<4:17:16,  8.55s/it]

 77%|███████▋  | 5884/7689 [14:31:51<3:54:21,  7.79s/it]
{'loss': 1.0943, 'grad_norm': 0.2018107040113506, 'learning_rate': 2.753944776080728e-05, 'epoch': 0.77}


 77%|███████▋  | 5886/7689 [14:32:11<4:24:43,  8.81s/it]

 77%|███████▋  | 5887/7689 [14:32:22<4:41:24,  9.37s/it]

 77%|███████▋  | 5888/7689 [14:32:29<4:16:18,  8.54s/it]
{'loss': 1.0866, 'grad_norm': 0.20760282246181924, 'learning_rate': 2.7423429884936013e-05, 'epoch': 0.77}


 77%|███████▋  | 5890/7689 [14:32:42<3:47:54,  7.60s/it]

 77%|███████▋  | 5891/7689 [14:32:50<3:47:55,  7.61s/it]

 77%|███████▋  | 5892/7689 [14:32:56<3:32:56,  7.11s/it]

 77%|███████▋  | 5893/7689 [14:33:05<3:48:53,  7.65s/it]

 77%|███████▋  | 5894/7689 [14:33:11<3:36:39,  7.24s/it]
{'loss': 0.988, 'grad_norm': 0.23303319223189412, 'learning_rate': 2.7249789516484368e-05, 'epoch': 0.77}


 77%|███████▋  | 5896/7689 [14:33:25<3:41:29,  7.41s/it]

 77%|███████▋  | 5897/7689 [14:33:36<4:06:03,  8.24s/it]
{'loss': 1.0257, 'grad_norm': 0.21645408204893957, 'learning_rate': 2.716314353310898e-05, 'epoch': 0.77}


 77%|███████▋  | 5899/7689 [14:34:04<5:28:18, 11.00s/it]

 77%|███████▋  | 5900/7689 [14:34:15<5:26:18, 10.94s/it]

 77%|███████▋  | 5901/7689 [14:34:27<5:40:23, 11.42s/it]

 77%|███████▋  | 5902/7689 [14:34:36<5:20:57, 10.78s/it]

 77%|███████▋  | 5903/7689 [14:34:44<4:51:14,  9.78s/it]

 77%|███████▋  | 5904/7689 [14:34:48<4:04:36,  8.22s/it]

 77%|███████▋  | 5905/7689 [14:34:56<3:56:54,  7.97s/it]

 77%|███████▋  | 5906/7689 [14:35:07<4:24:33,  8.90s/it]

 77%|███████▋  | 5907/7689 [14:35:21<5:07:15, 10.35s/it]

 77%|███████▋  | 5908/7689 [14:35:26<4:22:43,  8.85s/it]
{'loss': 0.9001, 'grad_norm': 0.1981447956097924, 'learning_rate': 2.684643781339069e-05, 'epoch': 0.77}


 77%|███████▋  | 5910/7689 [14:35:43<4:26:24,  8.98s/it]

 77%|███████▋  | 5911/7689 [14:35:59<5:31:25, 11.18s/it]

 77%|███████▋  | 5912/7689 [14:36:10<5:32:52, 11.24s/it]

 77%|███████▋  | 5913/7689 [14:36:17<4:54:10,  9.94s/it]

 77%|███████▋  | 5914/7689 [14:36:24<4:21:34,  8.84s/it]

 77%|███████▋  | 5915/7689 [14:36:30<4:00:33,  8.14s/it]

 77%|███████▋  | 5916/7689 [14:36:48<5:26:08, 11.04s/it]

 77%|███████▋  | 5917/7689 [14:36:58<5:14:57, 10.66s/it]

 77%|███████▋  | 5918/7689 [14:37:05<4:46:39,  9.71s/it]

 77%|███████▋  | 5919/7689 [14:37:13<4:30:16,  9.16s/it]

 77%|███████▋  | 5920/7689 [14:37:25<4:54:51, 10.00s/it]

 77%|███████▋  | 5921/7689 [14:37:37<5:10:47, 10.55s/it]

 77%|███████▋  | 5922/7689 [14:37:48<5:17:16, 10.77s/it]

 77%|███████▋  | 5923/7689 [14:37:54<4:35:29,  9.36s/it]
{'loss': 1.1672, 'grad_norm': 0.2089457225599902, 'learning_rate': 2.6417098887561376e-05, 'epoch': 0.77}


 77%|███████▋  | 5925/7689 [14:38:06<3:37:54,  7.41s/it]

 77%|███████▋  | 5926/7689 [14:38:13<3:35:22,  7.33s/it]

 77%|███████▋  | 5927/7689 [14:38:19<3:30:58,  7.18s/it]

 77%|███████▋  | 5928/7689 [14:38:31<4:05:15,  8.36s/it]

 77%|███████▋  | 5929/7689 [14:38:36<3:39:39,  7.49s/it]

 77%|███████▋  | 5930/7689 [14:38:51<4:41:37,  9.61s/it]

 77%|███████▋  | 5931/7689 [14:39:00<4:42:04,  9.63s/it]

 77%|███████▋  | 5932/7689 [14:39:08<4:27:50,  9.15s/it]
{'loss': 0.8575, 'grad_norm': 0.20407094113121593, 'learning_rate': 2.616090455384568e-05, 'epoch': 0.77}


 77%|███████▋  | 5934/7689 [14:39:37<5:38:13, 11.56s/it]

 77%|███████▋  | 5935/7689 [14:39:44<4:55:32, 10.11s/it]

 77%|███████▋  | 5936/7689 [14:39:50<4:21:23,  8.95s/it]

 77%|███████▋  | 5937/7689 [14:40:01<4:39:23,  9.57s/it]
{'loss': 0.9833, 'grad_norm': 0.1901174910041154, 'learning_rate': 2.6019032706776712e-05, 'epoch': 0.77}

 77%|███████▋  | 5938/7689 [14:40:09<4:18:41,  8.86s/it]


 77%|███████▋  | 5940/7689 [14:40:31<4:54:18, 10.10s/it]

 77%|███████▋  | 5941/7689 [14:40:36<4:13:14,  8.69s/it]

 77%|███████▋  | 5942/7689 [14:40:42<3:48:49,  7.86s/it]

 77%|███████▋  | 5943/7689 [14:40:47<3:18:53,  6.84s/it]
{'loss': 1.1376, 'grad_norm': 0.20572708847438453, 'learning_rate': 2.584921974589738e-05, 'epoch': 0.77}


 77%|███████▋  | 5945/7689 [14:41:02<3:34:11,  7.37s/it]

 77%|███████▋  | 5946/7689 [14:41:10<3:40:59,  7.61s/it]

 77%|███████▋  | 5947/7689 [14:41:19<3:55:41,  8.12s/it]

 77%|███████▋  | 5948/7689 [14:41:26<3:40:00,  7.58s/it]

 77%|███████▋  | 5949/7689 [14:41:45<5:23:55, 11.17s/it]

 77%|███████▋  | 5950/7689 [14:41:52<4:42:31,  9.75s/it]

 77%|███████▋  | 5951/7689 [14:42:01<4:41:43,  9.73s/it]

 77%|███████▋  | 5952/7689 [14:42:07<4:11:08,  8.68s/it]

 77%|███████▋  | 5953/7689 [14:42:20<4:43:05,  9.78s/it]

 77%|███████▋  | 5954/7689 [14:42:31<4:56:01, 10.24s/it]

 77%|███████▋  | 5955/7689 [14:42:38<4:25:35,  9.19s/it]

 77%|███████▋  | 5956/7689 [14:42:53<5:19:45, 11.07s/it]

 77%|███████▋  | 5957/7689 [14:43:01<4:49:33, 10.03s/it]

 77%|███████▋  | 5958/7689 [14:43:12<5:02:42, 10.49s/it]
{'loss': 0.8921, 'grad_norm': 0.23851160603580704, 'learning_rate': 2.5426762001558613e-05, 'epoch': 0.77}


 78%|███████▊  | 5960/7689 [14:43:29<4:34:25,  9.52s/it]

 78%|███████▊  | 5961/7689 [14:43:39<4:37:06,  9.62s/it]

 78%|███████▊  | 5962/7689 [14:43:50<4:50:32, 10.09s/it]

 78%|███████▊  | 5963/7689 [14:43:58<4:35:25,  9.57s/it]

 78%|███████▊  | 5964/7689 [14:44:13<5:21:13, 11.17s/it]

 78%|███████▊  | 5965/7689 [14:44:20<4:40:40,  9.77s/it]

 78%|███████▊  | 5966/7689 [14:44:29<4:33:31,  9.53s/it]

 78%|███████▊  | 5967/7689 [14:44:37<4:19:07,  9.03s/it]

 78%|███████▊  | 5968/7689 [14:44:43<3:52:00,  8.09s/it]

 78%|███████▊  | 5969/7689 [14:44:53<4:11:56,  8.79s/it]

 78%|███████▊  | 5970/7689 [14:45:01<4:07:14,  8.63s/it]

 78%|███████▊  | 5971/7689 [14:45:14<4:41:43,  9.84s/it]

 78%|███████▊  | 5972/7689 [14:45:24<4:44:02,  9.93s/it]

 78%|███████▊  | 5973/7689 [14:45:31<4:21:30,  9.14s/it]

 78%|███████▊  | 5974/7689 [14:45:36<3:47:02,  7.94s/it]

 78%|███████▊  | 5975/7689 [14:45:45<3:53:59,  8.19s/it]

 78%|███████▊  | 5976/7689 [14:45:52<3:40:25,  7.72s/it]

 78%|███████▊  | 5977/7689 [14:46:10<5:05:36, 10.71s/it]

 78%|███████▊  | 5978/7689 [14:46:16<4:31:30,  9.52s/it]

 78%|███████▊  | 5979/7689 [14:46:24<4:14:07,  8.92s/it]

 78%|███████▊  | 5980/7689 [14:46:32<4:05:21,  8.61s/it]

 78%|███████▊  | 5981/7689 [14:46:42<4:17:00,  9.03s/it]

 78%|███████▊  | 5982/7689 [14:46:50<4:13:12,  8.90s/it]

 78%|███████▊  | 5983/7689 [14:46:55<3:38:26,  7.68s/it]

 78%|███████▊  | 5984/7689 [14:47:03<3:36:05,  7.60s/it]

 78%|███████▊  | 5985/7689 [14:47:09<3:28:24,  7.34s/it]

 78%|███████▊  | 5986/7689 [14:47:19<3:46:15,  7.97s/it]

 78%|███████▊  | 5987/7689 [14:47:26<3:41:11,  7.80s/it]

 78%|███████▊  | 5988/7689 [14:47:40<4:31:43,  9.58s/it]

 78%|███████▊  | 5989/7689 [14:47:47<4:11:59,  8.89s/it]

 78%|███████▊  | 5990/7689 [14:47:57<4:24:21,  9.34s/it]

 78%|███████▊  | 5991/7689 [14:48:03<3:50:49,  8.16s/it]

 78%|███████▊  | 5992/7689 [14:48:10<3:44:56,  7.95s/it]

 78%|███████▊  | 5993/7689 [14:48:24<4:31:15,  9.60s/it]

 78%|███████▊  | 5994/7689 [14:48:32<4:19:08,  9.17s/it]

 78%|███████▊  | 5995/7689 [14:48:38<3:48:45,  8.10s/it]

 78%|███████▊  | 5996/7689 [14:48:45<3:42:02,  7.87s/it]

 78%|███████▊  | 5997/7689 [14:48:56<4:07:40,  8.78s/it]

 78%|███████▊  | 5998/7689 [14:49:02<3:43:27,  7.93s/it]

 78%|███████▊  | 5999/7689 [14:49:10<3:47:47,  8.09s/it]

 78%|███████▊  | 6000/7689 [14:49:16<3:31:58,  7.53s/it]
 78%|███████▊  | 6000/7689 [14:49:16<3:31:58,  7.53s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 78%|███████▊  | 6001/7689 [14:49:52<7:28:39, 15.95s/it]

 78%|███████▊  | 6002/7689 [14:49:59<6:15:23, 13.35s/it]

 78%|███████▊  | 6003/7689 [14:50:06<5:18:27, 11.33s/it]

 78%|███████▊  | 6004/7689 [14:50:15<5:02:38, 10.78s/it]

 78%|███████▊  | 6005/7689 [14:50:22<4:23:58,  9.41s/it]
{'loss': 1.0637, 'grad_norm': 0.20226854576508718, 'learning_rate': 2.412241754705532e-05, 'epoch': 0.78}


 78%|███████▊  | 6007/7689 [14:50:53<5:58:35, 12.79s/it]

 78%|███████▊  | 6008/7689 [14:51:04<5:42:54, 12.24s/it]

 78%|███████▊  | 6009/7689 [14:51:10<4:52:12, 10.44s/it]

 78%|███████▊  | 6010/7689 [14:51:19<4:41:31, 10.06s/it]

 78%|███████▊  | 6011/7689 [14:51:31<4:55:06, 10.55s/it]

 78%|███████▊  | 6012/7689 [14:51:39<4:32:53,  9.76s/it]

 78%|███████▊  | 6013/7689 [14:51:49<4:38:37,  9.97s/it]

 78%|███████▊  | 6014/7689 [14:52:02<4:56:30, 10.62s/it]

 78%|███████▊  | 6015/7689 [14:52:09<4:29:14,  9.65s/it]

 78%|███████▊  | 6016/7689 [14:52:25<5:24:05, 11.62s/it]

 78%|███████▊  | 6017/7689 [14:52:32<4:39:40, 10.04s/it]

 78%|███████▊  | 6018/7689 [14:52:51<5:55:27, 12.76s/it]
{'loss': 0.8743, 'grad_norm': 0.20617211278401232, 'learning_rate': 2.3766870657609376e-05, 'epoch': 0.78}


 78%|███████▊  | 6020/7689 [14:53:04<4:30:10,  9.71s/it]

 78%|███████▊  | 6021/7689 [14:53:09<3:46:02,  8.13s/it]

 78%|███████▊  | 6022/7689 [14:53:21<4:22:52,  9.46s/it]

 78%|███████▊  | 6023/7689 [14:53:39<5:32:36, 11.98s/it]
{'loss': 0.9854, 'grad_norm': 0.2145197520047735, 'learning_rate': 2.3630729983466516e-05, 'epoch': 0.78}


 78%|███████▊  | 6025/7689 [14:53:58<5:00:52, 10.85s/it]

 78%|███████▊  | 6026/7689 [14:54:07<4:46:18, 10.33s/it]

 78%|███████▊  | 6027/7689 [14:54:14<4:19:51,  9.38s/it]

 78%|███████▊  | 6028/7689 [14:54:31<5:25:55, 11.77s/it]

 78%|███████▊  | 6029/7689 [14:54:38<4:43:12, 10.24s/it]

 78%|███████▊  | 6030/7689 [14:54:44<4:09:57,  9.04s/it]

 78%|███████▊  | 6031/7689 [14:54:54<4:15:34,  9.25s/it]

 78%|███████▊  | 6032/7689 [14:55:05<4:33:04,  9.89s/it]

 78%|███████▊  | 6033/7689 [14:55:14<4:22:28,  9.51s/it]

 78%|███████▊  | 6034/7689 [14:55:22<4:08:23,  9.01s/it]

 78%|███████▊  | 6035/7689 [14:55:27<3:34:51,  7.79s/it]

 79%|███████▊  | 6036/7689 [14:55:38<4:04:29,  8.87s/it]

 79%|███████▊  | 6037/7689 [14:55:53<4:53:06, 10.65s/it]
[2024-05-25 04:27:03,240] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▊  | 6038/7689 [14:56:01<4:28:27,  9.76s/it]
{'loss': 0.8132, 'grad_norm': 0.19074445479996352, 'learning_rate': 2.3224343029146168e-05, 'epoch': 0.79}


 79%|███████▊  | 6040/7689 [14:56:16<4:04:46,  8.91s/it]

 79%|███████▊  | 6041/7689 [14:56:32<5:07:39, 11.20s/it]

 79%|███████▊  | 6042/7689 [14:56:42<4:53:03, 10.68s/it]

 79%|███████▊  | 6043/7689 [14:56:51<4:37:23, 10.11s/it]

 79%|███████▊  | 6044/7689 [14:57:01<4:41:57, 10.28s/it]

 79%|███████▊  | 6045/7689 [14:57:09<4:20:31,  9.51s/it]

 79%|███████▊  | 6046/7689 [14:57:20<4:32:53,  9.97s/it]

 79%|███████▊  | 6047/7689 [14:57:30<4:32:04,  9.94s/it]

 79%|███████▊  | 6048/7689 [14:57:43<4:57:26, 10.88s/it]

 79%|███████▊  | 6049/7689 [14:57:51<4:37:56, 10.17s/it]

 79%|███████▊  | 6050/7689 [14:58:10<5:48:52, 12.77s/it]

 79%|███████▊  | 6051/7689 [14:58:29<6:39:46, 14.64s/it]

 79%|███████▊  | 6052/7689 [14:58:42<6:20:01, 13.93s/it]

 79%|███████▊  | 6053/7689 [14:59:01<7:00:36, 15.43s/it]

 79%|███████▊  | 6054/7689 [14:59:07<5:44:38, 12.65s/it]

 79%|███████▊  | 6055/7689 [14:59:13<4:55:41, 10.86s/it]

 79%|███████▉  | 6056/7689 [14:59:24<4:50:27, 10.67s/it]

 79%|███████▉  | 6057/7689 [14:59:29<4:09:41,  9.18s/it]

 79%|███████▉  | 6058/7689 [14:59:34<3:35:25,  7.92s/it]

 79%|███████▉  | 6059/7689 [14:59:41<3:24:45,  7.54s/it]

 79%|███████▉  | 6060/7689 [14:59:49<3:28:05,  7.66s/it]

 79%|███████▉  | 6061/7689 [14:59:55<3:12:22,  7.09s/it]

 79%|███████▉  | 6062/7689 [15:00:04<3:31:42,  7.81s/it]

 79%|███████▉  | 6063/7689 [15:00:10<3:17:16,  7.28s/it]

 79%|███████▉  | 6064/7689 [15:00:22<3:52:20,  8.58s/it]

 79%|███████▉  | 6065/7689 [15:00:27<3:23:41,  7.53s/it]

 79%|███████▉  | 6066/7689 [15:00:33<3:12:40,  7.12s/it]

 79%|███████▉  | 6067/7689 [15:00:40<3:08:41,  6.98s/it]

 79%|███████▉  | 6068/7689 [15:00:46<3:05:36,  6.87s/it]

 79%|███████▉  | 6069/7689 [15:00:53<3:03:40,  6.80s/it]

 79%|███████▉  | 6070/7689 [15:01:02<3:24:16,  7.57s/it]

 79%|███████▉  | 6071/7689 [15:01:13<3:51:25,  8.58s/it]

 79%|███████▉  | 6072/7689 [15:01:24<4:06:09,  9.13s/it]

 79%|███████▉  | 6073/7689 [15:01:29<3:34:40,  7.97s/it]

 79%|███████▉  | 6074/7689 [15:01:35<3:21:05,  7.47s/it]

 79%|███████▉  | 6075/7689 [15:01:41<3:08:48,  7.02s/it]

 79%|███████▉  | 6076/7689 [15:01:55<4:01:31,  8.98s/it]

 79%|███████▉  | 6077/7689 [15:02:05<4:14:14,  9.46s/it]
{'loss': 0.886, 'grad_norm': 0.1749257230942678, 'learning_rate': 2.218212225512637e-05, 'epoch': 0.79}


 79%|███████▉  | 6079/7689 [15:02:23<4:04:20,  9.11s/it]

 79%|███████▉  | 6080/7689 [15:02:30<3:46:21,  8.44s/it]

 79%|███████▉  | 6081/7689 [15:02:41<4:07:08,  9.22s/it]

 79%|███████▉  | 6082/7689 [15:02:47<3:34:50,  8.02s/it]

 79%|███████▉  | 6083/7689 [15:02:54<3:26:03,  7.70s/it]

 79%|███████▉  | 6084/7689 [15:03:04<3:48:50,  8.56s/it]

 79%|███████▉  | 6085/7689 [15:03:14<4:03:32,  9.11s/it]
{'loss': 1.0106, 'grad_norm': 0.16908162713173086, 'learning_rate': 2.1970920135656858e-05, 'epoch': 0.79}


 79%|███████▉  | 6087/7689 [15:03:31<3:58:51,  8.95s/it]

 79%|███████▉  | 6088/7689 [15:03:37<3:31:21,  7.92s/it]

 79%|███████▉  | 6089/7689 [15:03:51<4:21:24,  9.80s/it]

 79%|███████▉  | 6090/7689 [15:04:00<4:18:18,  9.69s/it]
{'loss': 0.8498, 'grad_norm': 0.1664358270559948, 'learning_rate': 2.1839368640416524e-05, 'epoch': 0.79}


 79%|███████▉  | 6092/7689 [15:04:20<4:29:22, 10.12s/it]

 79%|███████▉  | 6093/7689 [15:04:26<3:53:07,  8.76s/it]

 79%|███████▉  | 6094/7689 [15:04:32<3:38:42,  8.23s/it]

 79%|███████▉  | 6095/7689 [15:04:46<4:18:42,  9.74s/it]

 79%|███████▉  | 6096/7689 [15:04:57<4:34:23, 10.34s/it]

 79%|███████▉  | 6097/7689 [15:05:05<4:11:01,  9.46s/it]

 79%|███████▉  | 6098/7689 [15:05:22<5:10:34, 11.71s/it]

 79%|███████▉  | 6099/7689 [15:05:27<4:20:40,  9.84s/it]

 79%|███████▉  | 6100/7689 [15:05:34<3:54:20,  8.85s/it]

 79%|███████▉  | 6101/7689 [15:05:41<3:37:03,  8.20s/it]

 79%|███████▉  | 6102/7689 [15:05:48<3:33:35,  8.08s/it]

 79%|███████▉  | 6103/7689 [15:05:59<3:54:55,  8.89s/it]

 79%|███████▉  | 6104/7689 [15:06:08<3:54:44,  8.89s/it]

 79%|███████▉  | 6105/7689 [15:06:17<3:55:34,  8.92s/it]

 79%|███████▉  | 6106/7689 [15:06:25<3:47:32,  8.62s/it]

 79%|███████▉  | 6107/7689 [15:06:32<3:36:42,  8.22s/it]

 79%|███████▉  | 6108/7689 [15:06:41<3:40:26,  8.37s/it]

 79%|███████▉  | 6109/7689 [15:06:53<4:05:43,  9.33s/it]

 79%|███████▉  | 6110/7689 [15:07:04<4:23:26, 10.01s/it]

 79%|███████▉  | 6111/7689 [15:07:24<5:39:55, 12.93s/it]

 79%|███████▉  | 6112/7689 [15:07:31<4:55:58, 11.26s/it]

 80%|███████▉  | 6113/7689 [15:07:42<4:53:18, 11.17s/it]

 80%|███████▉  | 6114/7689 [15:07:49<4:19:57,  9.90s/it]

 80%|███████▉  | 6115/7689 [15:07:57<4:04:00,  9.30s/it]

 80%|███████▉  | 6116/7689 [15:08:10<4:31:42, 10.36s/it]

 80%|███████▉  | 6117/7689 [15:08:19<4:24:19, 10.09s/it]

 80%|███████▉  | 6118/7689 [15:08:37<5:23:47, 12.37s/it]

 80%|███████▉  | 6119/7689 [15:08:45<4:52:29, 11.18s/it]

 80%|███████▉  | 6120/7689 [15:08:58<5:04:05, 11.63s/it]

 80%|███████▉  | 6121/7689 [15:09:04<4:20:22,  9.96s/it]

 80%|███████▉  | 6122/7689 [15:09:11<3:58:12,  9.12s/it]

 80%|███████▉  | 6123/7689 [15:09:16<3:24:16,  7.83s/it]

 80%|███████▉  | 6124/7689 [15:09:21<3:02:26,  6.99s/it]

 80%|███████▉  | 6125/7689 [15:09:28<3:02:26,  7.00s/it]

 80%|███████▉  | 6126/7689 [15:09:33<2:46:55,  6.41s/it]
{'loss': 1.1122, 'grad_norm': 0.2172154251647701, 'learning_rate': 2.0902468529414543e-05, 'epoch': 0.8}


 80%|███████▉  | 6128/7689 [15:09:50<3:12:01,  7.38s/it]

 80%|███████▉  | 6129/7689 [15:09:55<2:54:52,  6.73s/it]

 80%|███████▉  | 6130/7689 [15:10:12<4:20:34, 10.03s/it]
[2024-05-25 04:41:22,862] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 6131/7689 [15:10:21<4:09:32,  9.61s/it]

 80%|███████▉  | 6132/7689 [15:10:39<5:12:08, 12.03s/it]

 80%|███████▉  | 6133/7689 [15:10:45<4:25:22, 10.23s/it]

 80%|███████▉  | 6134/7689 [15:10:51<3:55:33,  9.09s/it]

 80%|███████▉  | 6135/7689 [15:11:07<4:43:42, 10.95s/it]

 80%|███████▉  | 6136/7689 [15:11:25<5:42:49, 13.25s/it]

 80%|███████▉  | 6137/7689 [15:11:37<5:31:13, 12.81s/it]

 80%|███████▉  | 6138/7689 [15:11:45<4:53:31, 11.35s/it]

 80%|███████▉  | 6139/7689 [15:11:52<4:19:38, 10.05s/it]

 80%|███████▉  | 6140/7689 [15:12:00<4:06:59,  9.57s/it]

 80%|███████▉  | 6141/7689 [15:12:08<3:53:40,  9.06s/it]

 80%|███████▉  | 6142/7689 [15:12:17<3:50:58,  8.96s/it]

 80%|███████▉  | 6143/7689 [15:12:32<4:38:04, 10.79s/it]

 80%|███████▉  | 6144/7689 [15:12:38<3:58:25,  9.26s/it]

 80%|███████▉  | 6145/7689 [15:12:49<4:13:59,  9.87s/it]

 80%|███████▉  | 6146/7689 [15:12:57<4:03:12,  9.46s/it]

 80%|███████▉  | 6147/7689 [15:13:02<3:26:53,  8.05s/it]

 80%|███████▉  | 6148/7689 [15:13:08<3:10:08,  7.40s/it]

 80%|███████▉  | 6149/7689 [15:13:18<3:31:30,  8.24s/it]

 80%|███████▉  | 6150/7689 [15:13:25<3:22:52,  7.91s/it]

 80%|███████▉  | 6151/7689 [15:13:33<3:19:47,  7.79s/it]

 80%|████████  | 6152/7689 [15:13:40<3:12:38,  7.52s/it]

 80%|████████  | 6153/7689 [15:13:47<3:06:24,  7.28s/it]

 80%|████████  | 6154/7689 [15:13:58<3:38:10,  8.53s/it]

 80%|████████  | 6155/7689 [15:14:10<4:00:55,  9.42s/it]

 80%|████████  | 6156/7689 [15:14:15<3:33:31,  8.36s/it]

 80%|████████  | 6157/7689 [15:14:24<3:34:09,  8.39s/it]

 80%|████████  | 6158/7689 [15:14:36<4:03:07,  9.53s/it]

 80%|████████  | 6159/7689 [15:14:42<3:32:13,  8.32s/it]

 80%|████████  | 6160/7689 [15:14:48<3:18:40,  7.80s/it]

 80%|████████  | 6161/7689 [15:15:00<3:50:52,  9.07s/it]

 80%|████████  | 6162/7689 [15:15:07<3:36:22,  8.50s/it]

 80%|████████  | 6163/7689 [15:15:16<3:38:22,  8.59s/it]

 80%|████████  | 6164/7689 [15:15:21<3:10:45,  7.51s/it]

 80%|████████  | 6165/7689 [15:15:28<3:08:01,  7.40s/it]

 80%|████████  | 6166/7689 [15:15:34<2:58:30,  7.03s/it]

 80%|████████  | 6167/7689 [15:15:46<3:33:51,  8.43s/it]

 80%|████████  | 6168/7689 [15:16:00<4:18:35, 10.20s/it]

 80%|████████  | 6169/7689 [15:16:07<3:50:37,  9.10s/it]

 80%|████████  | 6170/7689 [15:16:13<3:29:22,  8.27s/it]

 80%|████████  | 6171/7689 [15:16:23<3:38:25,  8.63s/it]

 80%|████████  | 6172/7689 [15:16:30<3:24:11,  8.08s/it]

 80%|████████  | 6173/7689 [15:16:39<3:30:25,  8.33s/it]

 80%|████████  | 6174/7689 [15:16:57<4:45:20, 11.30s/it]

 80%|████████  | 6175/7689 [15:17:03<4:10:05,  9.91s/it]

 80%|████████  | 6176/7689 [15:17:13<4:09:57,  9.91s/it]

 80%|████████  | 6177/7689 [15:17:21<3:51:29,  9.19s/it]

 80%|████████  | 6178/7689 [15:17:29<3:46:03,  8.98s/it]

 80%|████████  | 6179/7689 [15:17:36<3:32:09,  8.43s/it]

 80%|████████  | 6180/7689 [15:17:41<3:05:46,  7.39s/it]

 80%|████████  | 6181/7689 [15:17:50<3:15:19,  7.77s/it]

 80%|████████  | 6182/7689 [15:18:06<4:14:09, 10.12s/it]

 80%|████████  | 6183/7689 [15:18:18<4:32:59, 10.88s/it]

 80%|████████  | 6184/7689 [15:18:26<4:11:36, 10.03s/it]

 80%|████████  | 6185/7689 [15:18:33<3:43:06,  8.90s/it]

 80%|████████  | 6186/7689 [15:18:41<3:40:07,  8.79s/it]

 80%|████████  | 6187/7689 [15:18:55<4:16:12, 10.23s/it]

 80%|████████  | 6188/7689 [15:19:01<3:46:06,  9.04s/it]

 80%|████████  | 6189/7689 [15:19:08<3:29:05,  8.36s/it]

 81%|████████  | 6190/7689 [15:19:23<4:19:52, 10.40s/it]

 81%|████████  | 6191/7689 [15:19:32<4:06:07,  9.86s/it]

 81%|████████  | 6192/7689 [15:19:37<3:31:24,  8.47s/it]
{'loss': 1.0397, 'grad_norm': 0.19533764654387792, 'learning_rate': 1.9232212374065282e-05, 'epoch': 0.81}


 81%|████████  | 6194/7689 [15:19:50<3:06:39,  7.49s/it]

 81%|████████  | 6195/7689 [15:20:02<3:39:49,  8.83s/it]

 81%|████████  | 6196/7689 [15:20:07<3:10:34,  7.66s/it]

 81%|████████  | 6197/7689 [15:20:15<3:12:44,  7.75s/it]

 81%|████████  | 6198/7689 [15:20:32<4:22:50, 10.58s/it]

 81%|████████  | 6199/7689 [15:20:38<3:46:12,  9.11s/it]

 81%|████████  | 6200/7689 [15:20:47<3:45:56,  9.10s/it]

 81%|████████  | 6201/7689 [15:20:55<3:40:56,  8.91s/it]

 81%|████████  | 6202/7689 [15:21:02<3:26:02,  8.31s/it]

 81%|████████  | 6203/7689 [15:21:12<3:36:24,  8.74s/it]
{'loss': 1.0763, 'grad_norm': 0.18552233831851386, 'learning_rate': 1.8959871206649437e-05, 'epoch': 0.81}


 81%|████████  | 6205/7689 [15:21:24<3:05:59,  7.52s/it]

 81%|████████  | 6206/7689 [15:21:35<3:28:50,  8.45s/it]

 81%|████████  | 6207/7689 [15:21:41<3:12:23,  7.79s/it]

 81%|████████  | 6208/7689 [15:21:52<3:38:38,  8.86s/it]

 81%|████████  | 6209/7689 [15:22:03<3:49:15,  9.29s/it]

 81%|████████  | 6210/7689 [15:22:14<4:04:21,  9.91s/it]

 81%|████████  | 6211/7689 [15:22:27<4:23:04, 10.68s/it]

 81%|████████  | 6212/7689 [15:22:34<3:58:32,  9.69s/it]

 81%|████████  | 6213/7689 [15:22:43<3:50:43,  9.38s/it]

 81%|████████  | 6214/7689 [15:22:54<4:08:29, 10.11s/it]

 81%|████████  | 6215/7689 [15:23:01<3:42:00,  9.04s/it]

 81%|████████  | 6216/7689 [15:23:07<3:18:45,  8.10s/it]

 81%|████████  | 6217/7689 [15:23:21<4:01:15,  9.83s/it]

 81%|████████  | 6218/7689 [15:23:28<3:44:22,  9.15s/it]

 81%|████████  | 6219/7689 [15:23:38<3:49:56,  9.39s/it]

 81%|████████  | 6220/7689 [15:23:43<3:14:29,  7.94s/it]

 81%|████████  | 6221/7689 [15:23:49<3:01:40,  7.43s/it]

 81%|████████  | 6222/7689 [15:23:58<3:13:49,  7.93s/it]

 81%|████████  | 6223/7689 [15:24:03<2:53:29,  7.10s/it]

 81%|████████  | 6224/7689 [15:24:13<3:15:24,  8.00s/it]

 81%|████████  | 6225/7689 [15:24:27<3:56:52,  9.71s/it]

 81%|████████  | 6226/7689 [15:24:43<4:43:08, 11.61s/it]

 81%|████████  | 6227/7689 [15:24:52<4:19:58, 10.67s/it]

 81%|████████  | 6228/7689 [15:24:59<3:58:36,  9.80s/it]

 81%|████████  | 6229/7689 [15:25:09<3:59:29,  9.84s/it]

 81%|████████  | 6230/7689 [15:25:20<4:04:21, 10.05s/it]

 81%|████████  | 6231/7689 [15:25:26<3:34:09,  8.81s/it]

 81%|████████  | 6232/7689 [15:25:32<3:14:16,  8.00s/it]

 81%|████████  | 6233/7689 [15:25:41<3:22:29,  8.34s/it]

 81%|████████  | 6234/7689 [15:25:46<2:57:38,  7.33s/it]

 81%|████████  | 6235/7689 [15:25:55<3:10:53,  7.88s/it]

 81%|████████  | 6236/7689 [15:26:00<2:51:51,  7.10s/it]

 81%|████████  | 6237/7689 [15:26:07<2:47:56,  6.94s/it]

 81%|████████  | 6238/7689 [15:26:17<3:10:40,  7.88s/it]

 81%|████████  | 6239/7689 [15:26:26<3:16:02,  8.11s/it]

 81%|████████  | 6240/7689 [15:26:36<3:30:43,  8.73s/it]

 81%|████████  | 6241/7689 [15:26:43<3:17:18,  8.18s/it]

 81%|████████  | 6242/7689 [15:26:58<4:06:03, 10.20s/it]

 81%|████████  | 6243/7689 [15:27:04<3:39:08,  9.09s/it]

 81%|████████  | 6244/7689 [15:27:15<3:54:31,  9.74s/it]

 81%|████████  | 6245/7689 [15:27:20<3:18:43,  8.26s/it]

 81%|████████  | 6246/7689 [15:27:30<3:29:57,  8.73s/it]

 81%|████████  | 6247/7689 [15:27:43<4:02:30, 10.09s/it]

 81%|████████▏ | 6248/7689 [15:27:52<3:49:01,  9.54s/it]

 81%|████████▏ | 6249/7689 [15:28:11<4:57:13, 12.38s/it]

 81%|████████▏ | 6250/7689 [15:28:21<4:41:59, 11.76s/it]

 81%|████████▏ | 6251/7689 [15:28:27<4:01:22, 10.07s/it]

 81%|████████▏ | 6252/7689 [15:28:35<3:47:36,  9.50s/it]

 81%|████████▏ | 6253/7689 [15:28:50<4:27:27, 11.18s/it]

 81%|████████▏ | 6254/7689 [15:28:56<3:45:23,  9.42s/it]

 81%|████████▏ | 6255/7689 [15:29:05<3:42:35,  9.31s/it]

 81%|████████▏ | 6256/7689 [15:29:11<3:18:50,  8.33s/it]

 81%|████████▏ | 6257/7689 [15:29:17<3:02:25,  7.64s/it]

 81%|████████▏ | 6258/7689 [15:29:23<2:50:42,  7.16s/it]

 81%|████████▏ | 6259/7689 [15:29:41<4:08:21, 10.42s/it]

 81%|████████▏ | 6260/7689 [15:29:48<3:44:14,  9.42s/it]

 81%|████████▏ | 6261/7689 [15:29:55<3:29:25,  8.80s/it]

 81%|████████▏ | 6262/7689 [15:30:05<3:34:58,  9.04s/it]

 81%|████████▏ | 6263/7689 [15:30:20<4:15:15, 10.74s/it]

 81%|████████▏ | 6264/7689 [15:30:26<3:42:33,  9.37s/it]

 81%|████████▏ | 6265/7689 [15:30:31<3:16:42,  8.29s/it]

 81%|████████▏ | 6266/7689 [15:30:38<3:04:41,  7.79s/it]

 82%|████████▏ | 6267/7689 [15:30:43<2:46:11,  7.01s/it]

 82%|████████▏ | 6268/7689 [15:30:50<2:42:25,  6.86s/it]

 82%|████████▏ | 6269/7689 [15:30:58<2:54:12,  7.36s/it]

 82%|████████▏ | 6270/7689 [15:31:05<2:49:12,  7.15s/it]

 82%|████████▏ | 6271/7689 [15:31:11<2:37:49,  6.68s/it]

 82%|████████▏ | 6272/7689 [15:31:18<2:45:16,  7.00s/it]

 82%|████████▏ | 6273/7689 [15:31:22<2:25:06,  6.15s/it]

 82%|████████▏ | 6274/7689 [15:31:33<2:55:59,  7.46s/it]

 82%|████████▏ | 6275/7689 [15:31:41<3:01:54,  7.72s/it]

 82%|████████▏ | 6276/7689 [15:31:48<2:55:32,  7.45s/it]

 82%|████████▏ | 6277/7689 [15:31:55<2:49:18,  7.19s/it]

 82%|████████▏ | 6278/7689 [15:32:06<3:14:32,  8.27s/it]

 82%|████████▏ | 6279/7689 [15:32:13<3:08:35,  8.03s/it]

 82%|████████▏ | 6280/7689 [15:32:22<3:13:38,  8.25s/it]

 82%|████████▏ | 6281/7689 [15:32:34<3:41:06,  9.42s/it]

 82%|████████▏ | 6282/7689 [15:32:46<3:56:19, 10.08s/it]

 82%|████████▏ | 6283/7689 [15:32:59<4:17:58, 11.01s/it]
[2024-05-25 05:04:09,139] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 6284/7689 [15:33:08<4:03:13, 10.39s/it]

 82%|████████▏ | 6285/7689 [15:33:14<3:32:03,  9.06s/it]

 82%|████████▏ | 6286/7689 [15:33:23<3:34:29,  9.17s/it]

 82%|████████▏ | 6287/7689 [15:33:30<3:17:33,  8.45s/it]

 82%|████████▏ | 6288/7689 [15:33:39<3:19:40,  8.55s/it]

 82%|████████▏ | 6289/7689 [15:33:44<2:57:24,  7.60s/it]

 82%|████████▏ | 6290/7689 [15:33:54<3:12:19,  8.25s/it]

 82%|████████▏ | 6291/7689 [15:34:00<2:56:19,  7.57s/it]

 82%|████████▏ | 6292/7689 [15:34:08<2:57:53,  7.64s/it]

 82%|████████▏ | 6293/7689 [15:34:13<2:39:38,  6.86s/it]

 82%|████████▏ | 6294/7689 [15:34:20<2:41:26,  6.94s/it]

 82%|████████▏ | 6295/7689 [15:34:25<2:29:36,  6.44s/it]

 82%|████████▏ | 6296/7689 [15:34:31<2:27:02,  6.33s/it]

 82%|████████▏ | 6297/7689 [15:34:40<2:44:05,  7.07s/it]

 82%|████████▏ | 6298/7689 [15:34:52<3:22:02,  8.71s/it]

 82%|████████▏ | 6299/7689 [15:35:00<3:15:15,  8.43s/it]

 82%|████████▏ | 6300/7689 [15:35:09<3:17:25,  8.53s/it]
 82%|████████▏ | 6300/7689 [15:35:09<3:17:25,  8.53s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 82%|████████▏ | 6301/7689 [15:35:46<6:37:01, 17.16s/it]

 82%|████████▏ | 6302/7689 [15:35:56<5:42:01, 14.80s/it]

 82%|████████▏ | 6303/7689 [15:36:06<5:14:17, 13.61s/it]

 82%|████████▏ | 6304/7689 [15:36:12<4:19:02, 11.22s/it]

 82%|████████▏ | 6305/7689 [15:36:20<3:56:33, 10.26s/it]

 82%|████████▏ | 6306/7689 [15:36:28<3:40:39,  9.57s/it]

 82%|████████▏ | 6307/7689 [15:36:37<3:38:54,  9.50s/it]

 82%|████████▏ | 6308/7689 [15:36:56<4:44:04, 12.34s/it]

 82%|████████▏ | 6309/7689 [15:37:03<4:03:05, 10.57s/it]

 82%|████████▏ | 6310/7689 [15:37:11<3:49:58, 10.01s/it]

 82%|████████▏ | 6311/7689 [15:37:17<3:20:34,  8.73s/it]

 82%|████████▏ | 6312/7689 [15:37:25<3:16:31,  8.56s/it]

 82%|████████▏ | 6313/7689 [15:37:30<2:51:17,  7.47s/it]

 82%|████████▏ | 6314/7689 [15:37:41<3:11:33,  8.36s/it]

 82%|████████▏ | 6315/7689 [15:37:54<3:48:25,  9.97s/it]

 82%|████████▏ | 6316/7689 [15:38:05<3:50:17, 10.06s/it]

 82%|████████▏ | 6317/7689 [15:38:16<3:59:44, 10.48s/it]

 82%|████████▏ | 6318/7689 [15:38:25<3:46:42,  9.92s/it]

 82%|████████▏ | 6319/7689 [15:38:40<4:22:05, 11.48s/it]

 82%|████████▏ | 6320/7689 [15:38:48<3:55:35, 10.33s/it]

 82%|████████▏ | 6321/7689 [15:38:54<3:32:12,  9.31s/it]

 82%|████████▏ | 6322/7689 [15:39:00<3:09:28,  8.32s/it]

 82%|████████▏ | 6323/7689 [15:39:08<3:00:41,  7.94s/it]

 82%|████████▏ | 6324/7689 [15:39:15<2:58:03,  7.83s/it]

 82%|████████▏ | 6325/7689 [15:39:27<3:28:50,  9.19s/it]

 82%|████████▏ | 6326/7689 [15:39:37<3:30:42,  9.28s/it]

 82%|████████▏ | 6327/7689 [15:39:49<3:51:08, 10.18s/it]

 82%|████████▏ | 6328/7689 [15:40:01<3:59:34, 10.56s/it]

 82%|████████▏ | 6329/7689 [15:40:11<3:58:55, 10.54s/it]

 82%|████████▏ | 6330/7689 [15:40:17<3:25:14,  9.06s/it]

 82%|████████▏ | 6331/7689 [15:40:26<3:26:09,  9.11s/it]

 82%|████████▏ | 6332/7689 [15:40:31<2:59:56,  7.96s/it]

 82%|████████▏ | 6333/7689 [15:40:51<4:17:19, 11.39s/it]

 82%|████████▏ | 6334/7689 [15:40:57<3:44:51,  9.96s/it]

 82%|████████▏ | 6335/7689 [15:41:05<3:31:46,  9.38s/it]

 82%|████████▏ | 6336/7689 [15:41:12<3:10:28,  8.45s/it]

 82%|████████▏ | 6337/7689 [15:41:19<3:01:57,  8.08s/it]

 82%|████████▏ | 6338/7689 [15:41:27<3:02:29,  8.11s/it]

 82%|████████▏ | 6339/7689 [15:41:34<2:52:22,  7.66s/it]

 82%|████████▏ | 6340/7689 [15:41:49<3:41:19,  9.84s/it]

 82%|████████▏ | 6341/7689 [15:41:55<3:16:06,  8.73s/it]

 82%|████████▏ | 6342/7689 [15:42:06<3:31:30,  9.42s/it]

 82%|████████▏ | 6343/7689 [15:42:11<3:04:39,  8.23s/it]

 83%|████████▎ | 6344/7689 [15:42:20<3:05:42,  8.28s/it]

 83%|████████▎ | 6345/7689 [15:42:27<3:01:44,  8.11s/it]

 83%|████████▎ | 6346/7689 [15:42:36<3:06:47,  8.35s/it]

 83%|████████▎ | 6347/7689 [15:42:49<3:37:28,  9.72s/it]

 83%|████████▎ | 6348/7689 [15:42:56<3:17:37,  8.84s/it]

 83%|████████▎ | 6349/7689 [15:43:06<3:23:57,  9.13s/it]

 83%|████████▎ | 6350/7689 [15:43:14<3:17:03,  8.83s/it]

 83%|████████▎ | 6351/7689 [15:43:20<2:56:29,  7.91s/it]

 83%|████████▎ | 6352/7689 [15:43:29<3:04:20,  8.27s/it]

 83%|████████▎ | 6353/7689 [15:43:39<3:19:45,  8.97s/it]

 83%|████████▎ | 6354/7689 [15:43:49<3:24:56,  9.21s/it]

 83%|████████▎ | 6355/7689 [15:44:01<3:43:06, 10.03s/it]

 83%|████████▎ | 6356/7689 [15:44:15<4:06:28, 11.09s/it]

 83%|████████▎ | 6357/7689 [15:44:24<3:56:07, 10.64s/it]

 83%|████████▎ | 6358/7689 [15:44:36<4:04:48, 11.04s/it]

 83%|████████▎ | 6359/7689 [15:44:42<3:31:35,  9.55s/it]

 83%|████████▎ | 6360/7689 [15:44:48<3:09:37,  8.56s/it]

 83%|████████▎ | 6361/7689 [15:44:56<3:01:17,  8.19s/it]

 83%|████████▎ | 6362/7689 [15:45:07<3:19:08,  9.00s/it]

 83%|████████▎ | 6363/7689 [15:45:12<2:55:03,  7.92s/it]

 83%|████████▎ | 6364/7689 [15:45:22<3:04:52,  8.37s/it]

 83%|████████▎ | 6365/7689 [15:45:28<2:53:07,  7.85s/it]

 83%|████████▎ | 6366/7689 [15:45:40<3:19:54,  9.07s/it]

 83%|████████▎ | 6367/7689 [15:45:49<3:22:12,  9.18s/it]

 83%|████████▎ | 6368/7689 [15:45:57<3:08:03,  8.54s/it]

 83%|████████▎ | 6369/7689 [15:46:02<2:47:49,  7.63s/it]

 83%|████████▎ | 6370/7689 [15:46:15<3:22:06,  9.19s/it]

 83%|████████▎ | 6371/7689 [15:46:24<3:19:21,  9.08s/it]

 83%|████████▎ | 6372/7689 [15:46:31<3:08:45,  8.60s/it]

 83%|████████▎ | 6373/7689 [15:46:40<3:07:34,  8.55s/it]

 83%|████████▎ | 6374/7689 [15:46:45<2:48:19,  7.68s/it]

 83%|████████▎ | 6375/7689 [15:46:52<2:44:47,  7.52s/it]

 83%|████████▎ | 6376/7689 [15:47:01<2:49:52,  7.76s/it]

 83%|████████▎ | 6377/7689 [15:47:08<2:45:35,  7.57s/it]

 83%|████████▎ | 6378/7689 [15:47:22<3:28:00,  9.52s/it]

 83%|████████▎ | 6379/7689 [15:47:30<3:17:09,  9.03s/it]

 83%|████████▎ | 6380/7689 [15:47:39<3:14:46,  8.93s/it]

 83%|████████▎ | 6381/7689 [15:47:50<3:33:39,  9.80s/it]

 83%|████████▎ | 6382/7689 [15:47:56<3:05:40,  8.52s/it]

 83%|████████▎ | 6383/7689 [15:48:03<2:56:58,  8.13s/it]

 83%|████████▎ | 6384/7689 [15:48:11<2:56:29,  8.11s/it]

 83%|████████▎ | 6385/7689 [15:48:24<3:24:36,  9.41s/it]

 83%|████████▎ | 6386/7689 [15:48:30<3:02:24,  8.40s/it]

 83%|████████▎ | 6387/7689 [15:48:34<2:38:34,  7.31s/it]

 83%|████████▎ | 6388/7689 [15:48:49<3:22:52,  9.36s/it]

 83%|████████▎ | 6389/7689 [15:48:57<3:18:02,  9.14s/it]

 83%|████████▎ | 6390/7689 [15:49:03<2:57:52,  8.22s/it]

 83%|████████▎ | 6391/7689 [15:49:15<3:17:47,  9.14s/it]

 83%|████████▎ | 6392/7689 [15:49:22<3:08:29,  8.72s/it]

 83%|████████▎ | 6393/7689 [15:49:29<2:55:06,  8.11s/it]

 83%|████████▎ | 6394/7689 [15:49:38<3:03:05,  8.48s/it]

 83%|████████▎ | 6395/7689 [15:49:46<2:56:52,  8.20s/it]

 83%|████████▎ | 6396/7689 [15:50:02<3:45:28, 10.46s/it]

 83%|████████▎ | 6397/7689 [15:50:10<3:32:10,  9.85s/it]

 83%|████████▎ | 6398/7689 [15:50:23<3:52:15, 10.79s/it]

 83%|████████▎ | 6399/7689 [15:50:30<3:25:20,  9.55s/it]

 83%|████████▎ | 6400/7689 [15:50:37<3:08:45,  8.79s/it]

 83%|████████▎ | 6401/7689 [15:50:46<3:12:48,  8.98s/it]

 83%|████████▎ | 6402/7689 [15:50:53<3:01:22,  8.46s/it]

 83%|████████▎ | 6403/7689 [15:51:04<3:14:32,  9.08s/it]

 83%|████████▎ | 6404/7689 [15:51:14<3:21:19,  9.40s/it]

 83%|████████▎ | 6405/7689 [15:51:28<3:50:58, 10.79s/it]

 83%|████████▎ | 6406/7689 [15:51:39<3:48:55, 10.71s/it]

 83%|████████▎ | 6407/7689 [15:51:45<3:21:07,  9.41s/it]

 83%|████████▎ | 6408/7689 [15:51:52<3:04:17,  8.63s/it]

 83%|████████▎ | 6409/7689 [15:52:03<3:22:52,  9.51s/it]

 83%|████████▎ | 6410/7689 [15:52:10<3:05:49,  8.72s/it]

 83%|████████▎ | 6411/7689 [15:52:16<2:49:58,  7.98s/it]

 83%|████████▎ | 6412/7689 [15:52:23<2:37:28,  7.40s/it]

 83%|████████▎ | 6413/7689 [15:52:30<2:36:01,  7.34s/it]

 83%|████████▎ | 6414/7689 [15:52:35<2:21:15,  6.65s/it]

 83%|████████▎ | 6415/7689 [15:52:42<2:27:33,  6.95s/it]

 83%|████████▎ | 6416/7689 [15:52:51<2:38:06,  7.45s/it]

 83%|████████▎ | 6417/7689 [15:52:59<2:38:48,  7.49s/it]

 83%|████████▎ | 6418/7689 [15:53:04<2:27:32,  6.96s/it]

 83%|████████▎ | 6419/7689 [15:53:14<2:45:45,  7.83s/it]

 83%|████████▎ | 6420/7689 [15:53:23<2:50:39,  8.07s/it]

 84%|████████▎ | 6421/7689 [15:53:33<3:06:07,  8.81s/it]

 84%|████████▎ | 6422/7689 [15:53:40<2:53:41,  8.23s/it]
{'loss': 1.0184, 'grad_norm': 0.21778288203799923, 'learning_rate': 1.3907362177510241e-05, 'epoch': 0.84}


 84%|████████▎ | 6424/7689 [15:53:53<2:34:32,  7.33s/it]

 84%|████████▎ | 6425/7689 [15:54:03<2:51:58,  8.16s/it]

 84%|████████▎ | 6426/7689 [15:54:15<3:13:51,  9.21s/it]

 84%|████████▎ | 6427/7689 [15:54:31<3:57:23, 11.29s/it]

 84%|████████▎ | 6428/7689 [15:54:37<3:26:31,  9.83s/it]

 84%|████████▎ | 6429/7689 [15:54:49<3:41:01, 10.52s/it]

 84%|████████▎ | 6430/7689 [15:54:55<3:09:42,  9.04s/it]

 84%|████████▎ | 6431/7689 [15:55:03<3:01:03,  8.64s/it]

 84%|████████▎ | 6432/7689 [15:55:12<3:07:54,  8.97s/it]

 84%|████████▎ | 6433/7689 [15:55:21<3:05:40,  8.87s/it]

 84%|████████▎ | 6434/7689 [15:55:28<2:56:35,  8.44s/it]

 84%|████████▎ | 6435/7689 [15:55:38<3:04:12,  8.81s/it]
{'loss': 0.8573, 'grad_norm': 0.17312769861557778, 'learning_rate': 1.363006936107183e-05, 'epoch': 0.84}


 84%|████████▎ | 6437/7689 [15:55:56<3:03:12,  8.78s/it]

 84%|████████▎ | 6438/7689 [15:56:02<2:46:00,  7.96s/it]

 84%|████████▎ | 6439/7689 [15:56:18<3:35:11, 10.33s/it]

 84%|████████▍ | 6440/7689 [15:56:31<3:52:16, 11.16s/it]

 84%|████████▍ | 6441/7689 [15:56:39<3:29:54, 10.09s/it]

 84%|████████▍ | 6442/7689 [15:56:51<3:43:17, 10.74s/it]

 84%|████████▍ | 6443/7689 [15:56:59<3:25:33,  9.90s/it]

 84%|████████▍ | 6444/7689 [15:57:11<3:37:32, 10.48s/it]

 84%|████████▍ | 6445/7689 [15:57:17<3:08:17,  9.08s/it]

 84%|████████▍ | 6446/7689 [15:57:23<2:50:36,  8.24s/it]

 84%|████████▍ | 6447/7689 [15:57:30<2:43:23,  7.89s/it]
{'loss': 1.191, 'grad_norm': 0.19311169410190046, 'learning_rate': 1.3376405407966697e-05, 'epoch': 0.84}


 84%|████████▍ | 6449/7689 [15:57:42<2:20:28,  6.80s/it]

 84%|████████▍ | 6450/7689 [15:57:49<2:22:33,  6.90s/it]

 84%|████████▍ | 6451/7689 [15:58:00<2:46:40,  8.08s/it]

 84%|████████▍ | 6452/7689 [15:58:07<2:42:04,  7.86s/it]

 84%|████████▍ | 6453/7689 [15:58:21<3:17:19,  9.58s/it]

 84%|████████▍ | 6454/7689 [15:58:29<3:07:28,  9.11s/it]
{'loss': 0.9853, 'grad_norm': 0.18994557599635803, 'learning_rate': 1.322945649886509e-05, 'epoch': 0.84}


 84%|████████▍ | 6456/7689 [15:58:49<3:12:58,  9.39s/it]
{'loss': 1.0823, 'grad_norm': 0.22583015858988467, 'learning_rate': 1.3187609610644136e-05, 'epoch': 0.84}


 84%|████████▍ | 6458/7689 [15:59:06<3:07:28,  9.14s/it]

 84%|████████▍ | 6459/7689 [15:59:11<2:43:05,  7.96s/it]

 84%|████████▍ | 6460/7689 [15:59:22<2:57:20,  8.66s/it]
{'loss': 0.9689, 'grad_norm': 0.19458202796665036, 'learning_rate': 1.3104100713207712e-05, 'epoch': 0.84}


 84%|████████▍ | 6462/7689 [15:59:42<3:08:00,  9.19s/it]
{'loss': 0.8781, 'grad_norm': 0.19358222104591896, 'learning_rate': 1.3062438763263985e-05, 'epoch': 0.84}


 84%|████████▍ | 6464/7689 [16:00:04<3:14:08,  9.51s/it]

 84%|████████▍ | 6465/7689 [16:00:09<2:46:16,  8.15s/it]

 84%|████████▍ | 6466/7689 [16:00:19<2:57:19,  8.70s/it]

 84%|████████▍ | 6467/7689 [16:00:27<2:56:13,  8.65s/it]
{'loss': 0.8983, 'grad_norm': 0.20620981521029014, 'learning_rate': 1.2958553913630733e-05, 'epoch': 0.84}


 84%|████████▍ | 6469/7689 [16:00:43<2:47:48,  8.25s/it]

 84%|████████▍ | 6470/7689 [16:00:53<2:59:24,  8.83s/it]
{'loss': 0.9601, 'grad_norm': 0.19009529720896903, 'learning_rate': 1.289640831144624e-05, 'epoch': 0.84}

 84%|████████▍ | 6471/7689 [16:01:00<2:50:47,  8.41s/it]

 84%|████████▍ | 6472/7689 [16:01:06<2:33:44,  7.58s/it]


 84%|████████▍ | 6474/7689 [16:01:31<3:17:49,  9.77s/it]
{'loss': 1.1062, 'grad_norm': 0.21902544042371025, 'learning_rate': 1.281376390660768e-05, 'epoch': 0.84}

 84%|████████▍ | 6475/7689 [16:01:38<3:00:48,  8.94s/it]


 84%|████████▍ | 6477/7689 [16:01:56<3:00:24,  8.93s/it]

 84%|████████▍ | 6478/7689 [16:02:01<2:38:47,  7.87s/it]

 84%|████████▍ | 6479/7689 [16:02:10<2:42:28,  8.06s/it]

 84%|████████▍ | 6480/7689 [16:02:21<3:01:05,  8.99s/it]

 84%|████████▍ | 6481/7689 [16:02:27<2:44:51,  8.19s/it]
{'loss': 0.9345, 'grad_norm': 0.20768627664805617, 'learning_rate': 1.2669731951033014e-05, 'epoch': 0.84}


 84%|████████▍ | 6483/7689 [16:02:39<2:26:26,  7.29s/it]

 84%|████████▍ | 6484/7689 [16:02:55<3:13:25,  9.63s/it]

 84%|████████▍ | 6485/7689 [16:03:05<3:20:22,  9.99s/it]
{'loss': 1.0033, 'grad_norm': 0.20234371802739004, 'learning_rate': 1.2587768808256972e-05, 'epoch': 0.84}


 84%|████████▍ | 6487/7689 [16:03:21<2:56:25,  8.81s/it]

 84%|████████▍ | 6488/7689 [16:03:27<2:41:20,  8.06s/it]
{'loss': 0.9764, 'grad_norm': 0.198506038278412, 'learning_rate': 1.2526459299210303e-05, 'epoch': 0.84}

 84%|████████▍ | 6489/7689 [16:03:40<3:12:14,  9.61s/it]


 84%|████████▍ | 6491/7689 [16:03:58<3:08:15,  9.43s/it]

 84%|████████▍ | 6492/7689 [16:04:05<2:53:43,  8.71s/it]

 84%|████████▍ | 6493/7689 [16:04:19<3:24:15, 10.25s/it]

 84%|████████▍ | 6494/7689 [16:04:23<2:48:08,  8.44s/it]

 84%|████████▍ | 6495/7689 [16:04:40<3:38:08, 10.96s/it]

 84%|████████▍ | 6496/7689 [16:04:53<3:51:32, 11.65s/it]

 84%|████████▍ | 6497/7689 [16:04:58<3:11:46,  9.65s/it]

 85%|████████▍ | 6498/7689 [16:05:04<2:50:21,  8.58s/it]

 85%|████████▍ | 6499/7689 [16:05:12<2:47:27,  8.44s/it]

 85%|████████▍ | 6500/7689 [16:05:18<2:31:25,  7.64s/it]

 85%|████████▍ | 6501/7689 [16:05:24<2:19:26,  7.04s/it]

 85%|████████▍ | 6502/7689 [16:05:36<2:51:33,  8.67s/it]

 85%|████████▍ | 6503/7689 [16:05:51<3:30:47, 10.66s/it]

 85%|████████▍ | 6504/7689 [16:05:57<3:03:04,  9.27s/it]

 85%|████████▍ | 6505/7689 [16:06:09<3:17:43, 10.02s/it]

 85%|████████▍ | 6506/7689 [16:06:14<2:47:15,  8.48s/it]

 85%|████████▍ | 6507/7689 [16:06:26<3:06:43,  9.48s/it]

 85%|████████▍ | 6508/7689 [16:06:31<2:44:11,  8.34s/it]

 85%|████████▍ | 6509/7689 [16:06:38<2:35:19,  7.90s/it]

 85%|████████▍ | 6510/7689 [16:06:47<2:38:09,  8.05s/it]

 85%|████████▍ | 6511/7689 [16:06:53<2:27:27,  7.51s/it]

 85%|████████▍ | 6512/7689 [16:06:58<2:13:35,  6.81s/it]

 85%|████████▍ | 6513/7689 [16:07:09<2:39:47,  8.15s/it]
{'loss': 0.8729, 'grad_norm': 0.20445811753059545, 'learning_rate': 1.2020988475755068e-05, 'epoch': 0.85}


 85%|████████▍ | 6515/7689 [16:07:29<2:54:56,  8.94s/it]

 85%|████████▍ | 6516/7689 [16:07:39<2:56:24,  9.02s/it]

 85%|████████▍ | 6517/7689 [16:07:43<2:32:40,  7.82s/it]

 85%|████████▍ | 6518/7689 [16:07:55<2:53:10,  8.87s/it]

 85%|████████▍ | 6519/7689 [16:08:01<2:37:56,  8.10s/it]

 85%|████████▍ | 6520/7689 [16:08:13<2:59:12,  9.20s/it]
{'loss': 0.8725, 'grad_norm': 0.18274807879052474, 'learning_rate': 1.1881202690072546e-05, 'epoch': 0.85}


 85%|████████▍ | 6522/7689 [16:08:26<2:31:39,  7.80s/it]

 85%|████████▍ | 6523/7689 [16:08:39<3:06:34,  9.60s/it]

 85%|████████▍ | 6524/7689 [16:08:47<2:56:15,  9.08s/it]
{'loss': 0.8348, 'grad_norm': 0.2130234813374724, 'learning_rate': 1.180166901065064e-05, 'epoch': 0.85}

 85%|████████▍ | 6525/7689 [16:08:57<2:57:42,  9.16s/it]


 85%|████████▍ | 6527/7689 [16:09:12<2:44:00,  8.47s/it]

 85%|████████▍ | 6528/7689 [16:09:26<3:11:19,  9.89s/it]

 85%|████████▍ | 6529/7689 [16:09:30<2:42:10,  8.39s/it]
{'loss': 0.9185, 'grad_norm': 0.2167544780902287, 'learning_rate': 1.170260406350412e-05, 'epoch': 0.85}


 85%|████████▍ | 6531/7689 [16:09:52<3:01:56,  9.43s/it]

 85%|████████▍ | 6532/7689 [16:10:00<2:55:52,  9.12s/it]

 85%|████████▍ | 6533/7689 [16:10:06<2:36:17,  8.11s/it]
{'loss': 0.9959, 'grad_norm': 0.1893695326689581, 'learning_rate': 1.1623634101711855e-05, 'epoch': 0.85}

 85%|████████▍ | 6534/7689 [16:10:13<2:28:47,  7.73s/it]


 85%|████████▌ | 6536/7689 [16:10:26<2:17:21,  7.15s/it]
{'loss': 0.9025, 'grad_norm': 0.19952753570225334, 'learning_rate': 1.1564571275180591e-05, 'epoch': 0.85}


 85%|████████▌ | 6538/7689 [16:10:42<2:27:14,  7.68s/it]

 85%|████████▌ | 6539/7689 [16:10:48<2:20:47,  7.35s/it]
{'loss': 1.1438, 'grad_norm': 0.21681444885783804, 'learning_rate': 1.1505649677597473e-05, 'epoch': 0.85}


 85%|████████▌ | 6541/7689 [16:11:07<2:41:59,  8.47s/it]

 85%|████████▌ | 6542/7689 [16:11:18<2:56:40,  9.24s/it]

 85%|████████▌ | 6543/7689 [16:11:23<2:33:12,  8.02s/it]

 85%|████████▌ | 6544/7689 [16:11:42<3:34:32, 11.24s/it]
[2024-05-25 05:42:52,430] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 6545/7689 [16:11:47<2:59:43,  9.43s/it]

 85%|████████▌ | 6546/7689 [16:11:55<2:47:37,  8.80s/it]

 85%|████████▌ | 6547/7689 [16:12:06<2:59:52,  9.45s/it]

 85%|████████▌ | 6548/7689 [16:12:22<3:42:30, 11.70s/it]
{'loss': 1.0175, 'grad_norm': 0.1998883263610998, 'learning_rate': 1.1329733198370062e-05, 'epoch': 0.85}


 85%|████████▌ | 6550/7689 [16:12:42<3:17:48, 10.42s/it]

 85%|████████▌ | 6551/7689 [16:12:52<3:15:59, 10.33s/it]

 85%|████████▌ | 6552/7689 [16:13:00<3:01:05,  9.56s/it]

 85%|████████▌ | 6553/7689 [16:13:12<3:15:22, 10.32s/it]

 85%|████████▌ | 6554/7689 [16:13:21<3:11:41, 10.13s/it]

 85%|████████▌ | 6555/7689 [16:13:38<3:46:39, 11.99s/it]
[2024-05-25 05:44:48,223] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 6556/7689 [16:13:44<3:15:48, 10.37s/it]
{'loss': 0.9323, 'grad_norm': 0.2039768170323751, 'learning_rate': 1.1174432811992685e-05, 'epoch': 0.85}


 85%|████████▌ | 6558/7689 [16:14:03<3:01:15,  9.62s/it]
{'loss': 0.9037, 'grad_norm': 0.21127362842626027, 'learning_rate': 1.1135765259950604e-05, 'epoch': 0.85}


 85%|████████▌ | 6560/7689 [16:14:22<2:58:28,  9.49s/it]

 85%|████████▌ | 6561/7689 [16:14:32<2:59:39,  9.56s/it]

 85%|████████▌ | 6562/7689 [16:14:40<2:48:22,  8.96s/it]

 85%|████████▌ | 6563/7689 [16:14:51<3:03:55,  9.80s/it]

 85%|████████▌ | 6564/7689 [16:14:57<2:42:18,  8.66s/it]

 85%|████████▌ | 6565/7689 [16:15:06<2:43:14,  8.71s/it]

 85%|████████▌ | 6566/7689 [16:15:16<2:48:00,  8.98s/it]
{'loss': 0.8357, 'grad_norm': 0.19667737311299507, 'learning_rate': 1.098172605333756e-05, 'epoch': 0.85}

 85%|████████▌ | 6567/7689 [16:15:27<3:01:18,  9.70s/it]


 85%|████████▌ | 6569/7689 [16:15:41<2:35:14,  8.32s/it]

 85%|████████▌ | 6570/7689 [16:15:50<2:35:58,  8.36s/it]

 85%|████████▌ | 6571/7689 [16:16:00<2:44:49,  8.85s/it]

 85%|████████▌ | 6572/7689 [16:16:11<2:55:59,  9.45s/it]

 85%|████████▌ | 6573/7689 [16:16:19<2:46:12,  8.94s/it]

 85%|████████▌ | 6574/7689 [16:16:26<2:38:07,  8.51s/it]
{'loss': 0.9737, 'grad_norm': 0.20135824328489568, 'learning_rate': 1.0828697759580131e-05, 'epoch': 0.85}


 86%|████████▌ | 6576/7689 [16:16:42<2:35:31,  8.38s/it]

 86%|████████▌ | 6577/7689 [16:16:48<2:22:09,  7.67s/it]

 86%|████████▌ | 6578/7689 [16:16:59<2:42:28,  8.77s/it]

 86%|████████▌ | 6579/7689 [16:17:06<2:30:43,  8.15s/it]
{'loss': 0.8384, 'grad_norm': 0.20892448852159315, 'learning_rate': 1.0733569202823612e-05, 'epoch': 0.86}


 86%|████████▌ | 6581/7689 [16:17:26<2:50:46,  9.25s/it]

 86%|████████▌ | 6582/7689 [16:17:33<2:37:33,  8.54s/it]

 86%|████████▌ | 6583/7689 [16:17:44<2:54:29,  9.47s/it]

 86%|████████▌ | 6584/7689 [16:17:54<2:53:00,  9.39s/it]
{'loss': 0.9722, 'grad_norm': 0.19449165437730762, 'learning_rate': 1.063883663496118e-05, 'epoch': 0.86}


 86%|████████▌ | 6586/7689 [16:18:16<3:02:23,  9.92s/it]

 86%|████████▌ | 6587/7689 [16:18:25<2:54:55,  9.52s/it]

 86%|████████▌ | 6588/7689 [16:18:34<2:52:07,  9.38s/it]

 86%|████████▌ | 6589/7689 [16:18:43<2:50:41,  9.31s/it]

 86%|████████▌ | 6590/7689 [16:18:52<2:51:20,  9.35s/it]

 86%|████████▌ | 6591/7689 [16:19:00<2:44:16,  8.98s/it]

 86%|████████▌ | 6592/7689 [16:19:07<2:28:32,  8.12s/it]

 86%|████████▌ | 6593/7689 [16:19:17<2:38:26,  8.67s/it]

 86%|████████▌ | 6594/7689 [16:19:26<2:42:42,  8.92s/it]
{'loss': 0.919, 'grad_norm': 0.21615811103368748, 'learning_rate': 1.0450561145107607e-05, 'epoch': 0.86}


 86%|████████▌ | 6596/7689 [16:19:42<2:37:19,  8.64s/it]

 86%|████████▌ | 6597/7689 [16:19:51<2:34:36,  8.49s/it]

 86%|████████▌ | 6598/7689 [16:20:02<2:48:14,  9.25s/it]

 86%|████████▌ | 6599/7689 [16:20:13<2:58:17,  9.81s/it]

 86%|████████▌ | 6600/7689 [16:20:23<3:00:27,  9.94s/it]
 86%|████████▌ | 6600/7689 [16:20:23<3:00:27,  9.94s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.6673, 'grad_norm': 0.18339029272271562, 'learning_rate': 1.0319713545015997e-05, 'epoch': 0.86}

 86%|████████▌ | 6602/7689 [16:21:23<5:47:50, 19.20s/it]
[2024-05-25 05:52:33,324] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 6603/7689 [16:21:28<4:32:27, 15.05s/it]

 86%|████████▌ | 6604/7689 [16:21:35<3:44:44, 12.43s/it]
{'loss': 1.0073, 'grad_norm': 0.19590754624835524, 'learning_rate': 1.026387463080134e-05, 'epoch': 0.86}


 86%|████████▌ | 6606/7689 [16:21:50<2:58:19,  9.88s/it]

 86%|████████▌ | 6607/7689 [16:22:05<3:21:48, 11.19s/it]

 86%|████████▌ | 6608/7689 [16:22:15<3:16:16, 10.89s/it]

 86%|████████▌ | 6609/7689 [16:22:24<3:09:03, 10.50s/it]

 86%|████████▌ | 6610/7689 [16:22:33<2:57:52,  9.89s/it]
{'loss': 0.8746, 'grad_norm': 0.19673583674005907, 'learning_rate': 1.0152626809691179e-05, 'epoch': 0.86}

 86%|████████▌ | 6611/7689 [16:22:48<3:23:10, 11.31s/it]

 86%|████████▌ | 6612/7689 [16:23:00<3:26:29, 11.50s/it]


 86%|████████▌ | 6614/7689 [16:23:15<2:50:16,  9.50s/it]

 86%|████████▌ | 6615/7689 [16:23:29<3:14:40, 10.88s/it]
{'loss': 0.8657, 'grad_norm': 0.2053677392849418, 'learning_rate': 1.006035868455153e-05, 'epoch': 0.86}


 86%|████████▌ | 6617/7689 [16:23:43<2:37:29,  8.82s/it]

 86%|████████▌ | 6618/7689 [16:23:49<2:23:05,  8.02s/it]

 86%|████████▌ | 6619/7689 [16:23:57<2:23:04,  8.02s/it]
{'loss': 1.0785, 'grad_norm': 0.18303725376565055, 'learning_rate': 9.986831427053255e-06, 'epoch': 0.86}


 86%|████████▌ | 6621/7689 [16:24:18<2:42:24,  9.12s/it]

 86%|████████▌ | 6622/7689 [16:24:26<2:36:55,  8.82s/it]

 86%|████████▌ | 6623/7689 [16:24:32<2:21:55,  7.99s/it]

 86%|████████▌ | 6624/7689 [16:24:41<2:25:19,  8.19s/it]
{'loss': 1.0038, 'grad_norm': 0.19834936734532593, 'learning_rate': 9.895281750925089e-06, 'epoch': 0.86}
[2024-05-25 05:56:10,059] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 6625/7689 [16:25:00<3:22:11, 11.40s/it]


 86%|████████▌ | 6627/7689 [16:25:20<3:09:01, 10.68s/it]

 86%|████████▌ | 6628/7689 [16:25:25<2:36:42,  8.86s/it]

 86%|████████▌ | 6629/7689 [16:25:32<2:29:45,  8.48s/it]

 86%|████████▌ | 6630/7689 [16:25:40<2:25:37,  8.25s/it]
{'loss': 0.9119, 'grad_norm': 0.19695424027021616, 'learning_rate': 9.785949789171267e-06, 'epoch': 0.86}


 86%|████████▋ | 6632/7689 [16:25:51<1:58:40,  6.74s/it]
{'loss': 1.0677, 'grad_norm': 0.2404641967567418, 'learning_rate': 9.749633829137295e-06, 'epoch': 0.86}


 86%|████████▋ | 6634/7689 [16:26:05<1:57:59,  6.71s/it]

 86%|████████▋ | 6635/7689 [16:26:13<2:02:50,  6.99s/it]
{'loss': 1.1503, 'grad_norm': 0.21319222281780628, 'learning_rate': 9.695280003265528e-06, 'epoch': 0.86}


 86%|████████▋ | 6637/7689 [16:26:29<2:14:37,  7.68s/it]

 86%|████████▋ | 6638/7689 [16:26:42<2:40:31,  9.16s/it]

 86%|████████▋ | 6639/7689 [16:26:49<2:29:31,  8.54s/it]

 86%|████████▋ | 6640/7689 [16:26:56<2:20:45,  8.05s/it]

 86%|████████▋ | 6641/7689 [16:27:06<2:30:53,  8.64s/it]

 86%|████████▋ | 6642/7689 [16:27:12<2:14:19,  7.70s/it]

 86%|████████▋ | 6643/7689 [16:27:24<2:40:26,  9.20s/it]

 86%|████████▋ | 6644/7689 [16:27:36<2:50:43,  9.80s/it]
{'loss': 0.933, 'grad_norm': 0.19346185186941248, 'learning_rate': 9.533084156634242e-06, 'epoch': 0.86}


 86%|████████▋ | 6646/7689 [16:27:57<2:59:31, 10.33s/it]

 86%|████████▋ | 6647/7689 [16:28:06<2:50:33,  9.82s/it]

 86%|████████▋ | 6648/7689 [16:28:13<2:35:20,  8.95s/it]

 86%|████████▋ | 6649/7689 [16:28:19<2:20:57,  8.13s/it]

 86%|████████▋ | 6650/7689 [16:28:35<3:00:26, 10.42s/it]

 87%|████████▋ | 6651/7689 [16:28:42<2:45:56,  9.59s/it]

 87%|████████▋ | 6652/7689 [16:28:55<3:01:57, 10.53s/it]

 87%|████████▋ | 6653/7689 [16:29:01<2:39:07,  9.22s/it]
{'loss': 1.1971, 'grad_norm': 0.20577440054777163, 'learning_rate': 9.37218856721359e-06, 'epoch': 0.87}


 87%|████████▋ | 6655/7689 [16:29:15<2:17:43,  7.99s/it]
{'loss': 0.9959, 'grad_norm': 0.17786374720055242, 'learning_rate': 9.336610802918044e-06, 'epoch': 0.87}


 87%|████████▋ | 6657/7689 [16:29:27<2:04:47,  7.26s/it]

 87%|████████▋ | 6658/7689 [16:29:43<2:47:38,  9.76s/it]

 87%|████████▋ | 6659/7689 [16:29:51<2:37:05,  9.15s/it]

 87%|████████▋ | 6660/7689 [16:29:58<2:24:28,  8.42s/it]

 87%|████████▋ | 6661/7689 [16:30:05<2:21:39,  8.27s/it]

 87%|████████▋ | 6662/7689 [16:30:13<2:20:16,  8.20s/it]

 87%|████████▋ | 6663/7689 [16:30:23<2:24:38,  8.46s/it]
{'loss': 0.8953, 'grad_norm': 0.1890381393653278, 'learning_rate': 9.194943495127083e-06, 'epoch': 0.87}


 87%|████████▋ | 6665/7689 [16:30:37<2:11:35,  7.71s/it]

 87%|████████▋ | 6666/7689 [16:30:47<2:25:00,  8.51s/it]

 87%|████████▋ | 6667/7689 [16:30:55<2:21:43,  8.32s/it]

 87%|████████▋ | 6668/7689 [16:31:01<2:11:35,  7.73s/it]

 87%|████████▋ | 6669/7689 [16:31:07<2:02:40,  7.22s/it]
{'loss': 0.9834, 'grad_norm': 0.21407852329825197, 'learning_rate': 9.089369654358537e-06, 'epoch': 0.87}


 87%|████████▋ | 6671/7689 [16:31:35<2:54:05, 10.26s/it]

 87%|████████▋ | 6672/7689 [16:31:43<2:41:13,  9.51s/it]

 87%|████████▋ | 6673/7689 [16:31:59<3:12:09, 11.35s/it]
{'loss': 0.8308, 'grad_norm': 0.1844573007184576, 'learning_rate': 9.019309678412003e-06, 'epoch': 0.87}


 87%|████████▋ | 6675/7689 [16:32:14<2:36:04,  9.24s/it]

 87%|████████▋ | 6676/7689 [16:32:23<2:36:28,  9.27s/it]

 87%|████████▋ | 6677/7689 [16:32:29<2:20:38,  8.34s/it]

 87%|████████▋ | 6678/7689 [16:32:35<2:07:03,  7.54s/it]

 87%|████████▋ | 6679/7689 [16:32:48<2:32:09,  9.04s/it]

 87%|████████▋ | 6680/7689 [16:32:56<2:27:39,  8.78s/it]

 87%|████████▋ | 6681/7689 [16:33:01<2:08:51,  7.67s/it]

 87%|████████▋ | 6682/7689 [16:33:09<2:11:03,  7.81s/it]
{'loss': 0.882, 'grad_norm': 0.19386958739110965, 'learning_rate': 8.862619443062281e-06, 'epoch': 0.87}


 87%|████████▋ | 6684/7689 [16:33:24<2:09:27,  7.73s/it]
{'loss': 0.9029, 'grad_norm': 0.18502009192847294, 'learning_rate': 8.82797719862789e-06, 'epoch': 0.87}


 87%|████████▋ | 6686/7689 [16:33:35<1:55:11,  6.89s/it]

 87%|████████▋ | 6687/7689 [16:33:42<1:54:18,  6.84s/it]

 87%|████████▋ | 6688/7689 [16:33:48<1:50:08,  6.60s/it]

 87%|████████▋ | 6689/7689 [16:33:54<1:44:24,  6.26s/it]

 87%|████████▋ | 6690/7689 [16:34:07<2:19:56,  8.41s/it]

 87%|████████▋ | 6691/7689 [16:34:21<2:46:43, 10.02s/it]
{'loss': 0.8503, 'grad_norm': 0.19753213050438742, 'learning_rate': 8.70723910133635e-06, 'epoch': 0.87}


 87%|████████▋ | 6693/7689 [16:34:31<2:06:16,  7.61s/it]

 87%|████████▋ | 6694/7689 [16:34:44<2:31:08,  9.11s/it]

 87%|████████▋ | 6695/7689 [16:34:53<2:28:55,  8.99s/it]

 87%|████████▋ | 6696/7689 [16:35:07<2:54:23, 10.54s/it]

 87%|████████▋ | 6697/7689 [16:35:13<2:30:30,  9.10s/it]

 87%|████████▋ | 6698/7689 [16:35:19<2:16:40,  8.28s/it]

 87%|████████▋ | 6699/7689 [16:35:25<2:06:40,  7.68s/it]

 87%|████████▋ | 6700/7689 [16:35:37<2:26:04,  8.86s/it]

 87%|████████▋ | 6701/7689 [16:35:43<2:13:31,  8.11s/it]
{'loss': 1.0376, 'grad_norm': 0.19800818801350378, 'learning_rate': 8.536133287792047e-06, 'epoch': 0.87}


 87%|████████▋ | 6703/7689 [16:35:59<2:14:46,  8.20s/it]

 87%|████████▋ | 6704/7689 [16:36:10<2:29:44,  9.12s/it]
{'loss': 0.8517, 'grad_norm': 0.17346776112507878, 'learning_rate': 8.485117880686399e-06, 'epoch': 0.87}


 87%|████████▋ | 6706/7689 [16:36:24<2:12:42,  8.10s/it]

 87%|████████▋ | 6707/7689 [16:36:32<2:10:21,  7.97s/it]

 87%|████████▋ | 6708/7689 [16:36:44<2:31:57,  9.29s/it]

 87%|████████▋ | 6709/7689 [16:36:51<2:21:47,  8.68s/it]

 87%|████████▋ | 6710/7689 [16:37:05<2:45:43, 10.16s/it]

 87%|████████▋ | 6711/7689 [16:37:11<2:27:25,  9.04s/it]

 87%|████████▋ | 6712/7689 [16:37:22<2:35:37,  9.56s/it]

 87%|████████▋ | 6713/7689 [16:37:31<2:30:40,  9.26s/it]

 87%|████████▋ | 6714/7689 [16:37:48<3:09:13, 11.64s/it]

 87%|████████▋ | 6715/7689 [16:37:57<2:58:19, 10.98s/it]

 87%|████████▋ | 6716/7689 [16:38:05<2:42:08, 10.00s/it]

 87%|████████▋ | 6717/7689 [16:38:14<2:37:59,  9.75s/it]

 87%|████████▋ | 6718/7689 [16:38:25<2:41:54, 10.00s/it]

 87%|████████▋ | 6719/7689 [16:38:34<2:36:52,  9.70s/it]

 87%|████████▋ | 6720/7689 [16:38:41<2:25:44,  9.02s/it]

 87%|████████▋ | 6721/7689 [16:38:51<2:27:57,  9.17s/it]

 87%|████████▋ | 6722/7689 [16:38:58<2:16:33,  8.47s/it]

 87%|████████▋ | 6723/7689 [16:39:03<2:00:50,  7.51s/it]

 87%|████████▋ | 6724/7689 [16:39:12<2:06:38,  7.87s/it]

 87%|████████▋ | 6725/7689 [16:39:23<2:22:33,  8.87s/it]

 87%|████████▋ | 6726/7689 [16:39:29<2:08:34,  8.01s/it]

 87%|████████▋ | 6727/7689 [16:39:39<2:20:24,  8.76s/it]

 88%|████████▊ | 6728/7689 [16:39:44<1:59:20,  7.45s/it]

 88%|████████▊ | 6729/7689 [16:39:51<1:58:39,  7.42s/it]
{'loss': 0.9026, 'grad_norm': 0.2193086923115256, 'learning_rate': 8.065680660225194e-06, 'epoch': 0.88}


 88%|████████▊ | 6731/7689 [16:40:10<2:13:11,  8.34s/it]

 88%|████████▊ | 6732/7689 [16:40:20<2:19:21,  8.74s/it]

 88%|████████▊ | 6733/7689 [16:40:26<2:04:53,  7.84s/it]

 88%|████████▊ | 6734/7689 [16:40:40<2:35:32,  9.77s/it]

 88%|████████▊ | 6735/7689 [16:40:46<2:15:03,  8.49s/it]

 88%|████████▊ | 6736/7689 [16:40:52<2:03:52,  7.80s/it]

 88%|████████▊ | 6737/7689 [16:41:02<2:17:53,  8.69s/it]

 88%|████████▊ | 6738/7689 [16:41:14<2:33:00,  9.65s/it]

 88%|████████▊ | 6739/7689 [16:41:24<2:31:30,  9.57s/it]

 88%|████████▊ | 6740/7689 [16:41:34<2:32:23,  9.64s/it]

 88%|████████▊ | 6741/7689 [16:41:42<2:27:15,  9.32s/it]

 88%|████████▊ | 6742/7689 [16:41:51<2:25:49,  9.24s/it]

 88%|████████▊ | 6743/7689 [16:41:58<2:13:34,  8.47s/it]

 88%|████████▊ | 6744/7689 [16:42:07<2:14:12,  8.52s/it]

 88%|████████▊ | 6745/7689 [16:42:12<2:00:58,  7.69s/it]

 88%|████████▊ | 6746/7689 [16:42:18<1:53:41,  7.23s/it]

 88%|████████▊ | 6747/7689 [16:42:29<2:10:26,  8.31s/it]
{'loss': 0.889, 'grad_norm': 0.16107465635673326, 'learning_rate': 7.769996273097802e-06, 'epoch': 0.88}


 88%|████████▊ | 6749/7689 [16:42:53<2:37:05, 10.03s/it]

 88%|████████▊ | 6750/7689 [16:42:59<2:21:55,  9.07s/it]

 88%|████████▊ | 6751/7689 [16:43:05<2:07:30,  8.16s/it]

 88%|████████▊ | 6752/7689 [16:43:16<2:17:53,  8.83s/it]

 88%|████████▊ | 6753/7689 [16:43:22<2:05:46,  8.06s/it]
{'loss': 1.0258, 'grad_norm': 0.20360631281633107, 'learning_rate': 7.672612284783898e-06, 'epoch': 0.88}


 88%|████████▊ | 6755/7689 [16:43:38<2:03:04,  7.91s/it]
{'loss': 0.976, 'grad_norm': 0.20505081547295498, 'learning_rate': 7.640281986281894e-06, 'epoch': 0.88}


 88%|████████▊ | 6757/7689 [16:44:04<2:43:07, 10.50s/it]

 88%|████████▊ | 6758/7689 [16:44:13<2:32:25,  9.82s/it]

 88%|████████▊ | 6759/7689 [16:44:25<2:46:16, 10.73s/it]

 88%|████████▊ | 6760/7689 [16:44:33<2:32:56,  9.88s/it]
{'loss': 0.8851, 'grad_norm': 0.19167303053449536, 'learning_rate': 7.5597430877720976e-06, 'epoch': 0.88}


 88%|████████▊ | 6762/7689 [16:44:49<2:18:59,  9.00s/it]

 88%|████████▊ | 6763/7689 [16:44:55<2:04:15,  8.05s/it]

 88%|████████▊ | 6764/7689 [16:45:02<1:59:58,  7.78s/it]

 88%|████████▊ | 6765/7689 [16:45:14<2:17:51,  8.95s/it]

 88%|████████▊ | 6766/7689 [16:45:26<2:30:13,  9.77s/it]

 88%|████████▊ | 6767/7689 [16:45:34<2:22:19,  9.26s/it]

 88%|████████▊ | 6768/7689 [16:45:43<2:23:10,  9.33s/it]

 88%|████████▊ | 6769/7689 [16:45:50<2:09:47,  8.46s/it]

 88%|████████▊ | 6770/7689 [16:45:58<2:10:01,  8.49s/it]

 88%|████████▊ | 6771/7689 [16:46:06<2:06:49,  8.29s/it]

 88%|████████▊ | 6772/7689 [16:46:20<2:33:04, 10.02s/it]

 88%|████████▊ | 6773/7689 [16:46:29<2:26:29,  9.60s/it]

 88%|████████▊ | 6774/7689 [16:46:34<2:04:48,  8.18s/it]

 88%|████████▊ | 6775/7689 [16:46:41<1:58:56,  7.81s/it]
{'loss': 0.8765, 'grad_norm': 0.20270495599942392, 'learning_rate': 7.320588220800839e-06, 'epoch': 0.88}

 88%|████████▊ | 6776/7689 [16:46:47<1:52:29,  7.39s/it]

 88%|████████▊ | 6777/7689 [16:46:53<1:46:41,  7.02s/it]


 88%|████████▊ | 6779/7689 [16:47:09<1:48:51,  7.18s/it]
{'loss': 1.0975, 'grad_norm': 0.20120186837343454, 'learning_rate': 7.257438113100401e-06, 'epoch': 0.88}


 88%|████████▊ | 6781/7689 [16:47:22<1:45:57,  7.00s/it]

 88%|████████▊ | 6782/7689 [16:47:32<1:57:23,  7.77s/it]

 88%|████████▊ | 6783/7689 [16:47:36<1:43:38,  6.86s/it]
{'loss': 1.1132, 'grad_norm': 0.22540383659372717, 'learning_rate': 7.1945513072773865e-06, 'epoch': 0.88}

 88%|████████▊ | 6784/7689 [16:47:45<1:51:51,  7.42s/it]


 88%|████████▊ | 6786/7689 [16:48:04<2:06:10,  8.38s/it]

 88%|████████▊ | 6787/7689 [16:48:16<2:21:34,  9.42s/it]

 88%|████████▊ | 6788/7689 [16:48:21<2:00:34,  8.03s/it]
{'loss': 0.9825, 'grad_norm': 0.2294784384021901, 'learning_rate': 7.116313340186376e-06, 'epoch': 0.88}


 88%|████████▊ | 6790/7689 [16:48:36<1:54:25,  7.64s/it]

 88%|████████▊ | 6791/7689 [16:48:49<2:20:25,  9.38s/it]

 88%|████████▊ | 6792/7689 [16:49:07<2:58:15, 11.92s/it]
[2024-05-25 06:20:17,276] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 6793/7689 [16:49:13<2:31:04, 10.12s/it]

 88%|████████▊ | 6794/7689 [16:49:19<2:13:08,  8.93s/it]

 88%|████████▊ | 6795/7689 [16:49:24<1:56:09,  7.80s/it]

 88%|████████▊ | 6796/7689 [16:49:31<1:52:39,  7.57s/it]
{'loss': 1.1323, 'grad_norm': 0.21208862360948383, 'learning_rate': 6.991989769674967e-06, 'epoch': 0.88}


 88%|████████▊ | 6798/7689 [16:49:55<2:29:53, 10.09s/it]

 88%|████████▊ | 6799/7689 [16:50:09<2:46:38, 11.23s/it]
{'loss': 0.8886, 'grad_norm': 0.18025311254993467, 'learning_rate': 6.945640662326714e-06, 'epoch': 0.88}


 88%|████████▊ | 6801/7689 [16:50:36<2:56:50, 11.95s/it]

 88%|████████▊ | 6802/7689 [16:50:45<2:42:42, 11.01s/it]

 88%|████████▊ | 6803/7689 [16:50:56<2:42:22, 11.00s/it]

 88%|████████▊ | 6804/7689 [16:51:04<2:28:15, 10.05s/it]

 89%|████████▊ | 6805/7689 [16:51:10<2:12:03,  8.96s/it]

 89%|████████▊ | 6806/7689 [16:51:17<2:00:10,  8.17s/it]

 89%|████████▊ | 6807/7689 [16:51:23<1:50:29,  7.52s/it]

 89%|████████▊ | 6808/7689 [16:51:28<1:40:43,  6.86s/it]

 89%|████████▊ | 6809/7689 [16:51:37<1:50:28,  7.53s/it]

 89%|████████▊ | 6810/7689 [16:51:49<2:08:43,  8.79s/it]

 89%|████████▊ | 6811/7689 [16:51:56<2:01:06,  8.28s/it]

 89%|████████▊ | 6812/7689 [16:52:05<2:04:04,  8.49s/it]

 89%|████████▊ | 6813/7689 [16:52:12<1:58:57,  8.15s/it]

 89%|████████▊ | 6814/7689 [16:52:31<2:46:17, 11.40s/it]
[2024-05-25 06:23:41,746] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▊ | 6815/7689 [16:52:38<2:26:54, 10.09s/it]

 89%|████████▊ | 6816/7689 [16:52:45<2:10:44,  8.99s/it]

 89%|████████▊ | 6817/7689 [16:52:55<2:15:39,  9.33s/it]
{'loss': 0.872, 'grad_norm': 0.2026586958120818, 'learning_rate': 6.670669302577093e-06, 'epoch': 0.89}


 89%|████████▊ | 6819/7689 [16:53:15<2:23:15,  9.88s/it]

 89%|████████▊ | 6820/7689 [16:53:31<2:50:34, 11.78s/it]

 89%|████████▊ | 6821/7689 [16:53:38<2:31:38, 10.48s/it]

 89%|████████▊ | 6822/7689 [16:53:44<2:12:18,  9.16s/it]

 89%|████████▊ | 6823/7689 [16:53:50<1:58:41,  8.22s/it]

 89%|████████▉ | 6824/7689 [16:54:03<2:19:06,  9.65s/it]

 89%|████████▉ | 6825/7689 [16:54:23<3:00:20, 12.52s/it]

 89%|████████▉ | 6826/7689 [16:54:35<3:01:01, 12.59s/it]

 89%|████████▉ | 6827/7689 [16:54:54<3:27:20, 14.43s/it]

 89%|████████▉ | 6828/7689 [16:55:01<2:52:57, 12.05s/it]

 89%|████████▉ | 6829/7689 [16:55:07<2:30:08, 10.47s/it]

 89%|████████▉ | 6830/7689 [16:55:23<2:50:25, 11.90s/it]

 89%|████████▉ | 6831/7689 [16:55:31<2:36:05, 10.92s/it]

 89%|████████▉ | 6832/7689 [16:55:37<2:14:49,  9.44s/it]

 89%|████████▉ | 6833/7689 [16:55:47<2:14:33,  9.43s/it]

 89%|████████▉ | 6834/7689 [16:55:55<2:11:20,  9.22s/it]

 89%|████████▉ | 6835/7689 [16:56:00<1:53:10,  7.95s/it]

 89%|████████▉ | 6836/7689 [16:56:09<1:56:52,  8.22s/it]

 89%|████████▉ | 6837/7689 [16:56:18<1:57:05,  8.25s/it]

 89%|████████▉ | 6838/7689 [16:56:30<2:14:57,  9.52s/it]

 89%|████████▉ | 6839/7689 [16:56:39<2:13:44,  9.44s/it]
{'loss': 0.9298, 'grad_norm': 0.2035552219895893, 'learning_rate': 6.341881341409384e-06, 'epoch': 0.89}


 89%|████████▉ | 6841/7689 [16:56:55<2:02:30,  8.67s/it]

 89%|████████▉ | 6842/7689 [16:57:03<2:00:23,  8.53s/it]

 89%|████████▉ | 6843/7689 [16:57:19<2:33:35, 10.89s/it]

 89%|████████▉ | 6844/7689 [16:57:31<2:36:59, 11.15s/it]

 89%|████████▉ | 6845/7689 [16:57:36<2:12:22,  9.41s/it]

 89%|████████▉ | 6846/7689 [16:57:43<2:00:30,  8.58s/it]
{'loss': 1.0962, 'grad_norm': 0.19682534799748183, 'learning_rate': 6.23895246244851e-06, 'epoch': 0.89}


 89%|████████▉ | 6848/7689 [16:58:07<2:23:34, 10.24s/it]

 89%|████████▉ | 6849/7689 [16:58:13<2:07:20,  9.10s/it]
{'loss': 0.9863, 'grad_norm': 0.23547068076546962, 'learning_rate': 6.195089590030123e-06, 'epoch': 0.89}


 89%|████████▉ | 6851/7689 [16:58:29<1:54:49,  8.22s/it]

 89%|████████▉ | 6852/7689 [16:58:38<2:01:22,  8.70s/it]

 89%|████████▉ | 6853/7689 [16:58:56<2:37:28, 11.30s/it]
{'loss': 0.7486, 'grad_norm': 0.17974947305161298, 'learning_rate': 6.136838800442457e-06, 'epoch': 0.89}


 89%|████████▉ | 6855/7689 [16:59:09<2:04:27,  8.95s/it]

 89%|████████▉ | 6856/7689 [16:59:15<1:50:49,  7.98s/it]

 89%|████████▉ | 6857/7689 [16:59:20<1:41:02,  7.29s/it]
{'loss': 1.0254, 'grad_norm': 0.20807420431530357, 'learning_rate': 6.0788544941830724e-06, 'epoch': 0.89}


 89%|████████▉ | 6859/7689 [16:59:35<1:42:13,  7.39s/it]

 89%|████████▉ | 6860/7689 [16:59:42<1:42:47,  7.44s/it]

 89%|████████▉ | 6861/7689 [16:59:47<1:30:40,  6.57s/it]

 89%|████████▉ | 6862/7689 [16:59:55<1:37:19,  7.06s/it]

 89%|████████▉ | 6863/7689 [17:00:00<1:29:49,  6.52s/it]

 89%|████████▉ | 6864/7689 [17:00:08<1:32:52,  6.75s/it]

 89%|████████▉ | 6865/7689 [17:00:17<1:43:03,  7.50s/it]

 89%|████████▉ | 6866/7689 [17:00:25<1:46:30,  7.76s/it]

 89%|████████▉ | 6867/7689 [17:00:33<1:45:10,  7.68s/it]

 89%|████████▉ | 6868/7689 [17:00:46<2:08:58,  9.43s/it]

 89%|████████▉ | 6869/7689 [17:00:57<2:13:06,  9.74s/it]

 89%|████████▉ | 6870/7689 [17:01:02<1:52:59,  8.28s/it]
{'loss': 1.1002, 'grad_norm': 0.2133797110480743, 'learning_rate': 5.892247883778191e-06, 'epoch': 0.89}


 89%|████████▉ | 6872/7689 [17:01:21<2:02:50,  9.02s/it]

 89%|████████▉ | 6873/7689 [17:01:27<1:49:34,  8.06s/it]

 89%|████████▉ | 6874/7689 [17:01:36<1:54:25,  8.42s/it]
{'loss': 0.981, 'grad_norm': 0.18901799020505225, 'learning_rate': 5.835397959064837e-06, 'epoch': 0.89}


 89%|████████▉ | 6876/7689 [17:01:57<2:03:36,  9.12s/it]
{'loss': 0.9963, 'grad_norm': 0.18524488021597105, 'learning_rate': 5.8070732388268324e-06, 'epoch': 0.89}

 89%|████████▉ | 6877/7689 [17:02:04<1:54:35,  8.47s/it]

 89%|████████▉ | 6878/7689 [17:02:10<1:44:35,  7.74s/it]


 89%|████████▉ | 6880/7689 [17:02:27<1:50:56,  8.23s/it]
{'loss': 1.157, 'grad_norm': 0.19872917722456623, 'learning_rate': 5.750624383107728e-06, 'epoch': 0.89}


 90%|████████▉ | 6882/7689 [17:02:44<1:52:39,  8.38s/it]

 90%|████████▉ | 6883/7689 [17:02:52<1:50:02,  8.19s/it]

 90%|████████▉ | 6884/7689 [17:02:59<1:45:52,  7.89s/it]

 90%|████████▉ | 6885/7689 [17:03:07<1:48:18,  8.08s/it]

 90%|████████▉ | 6886/7689 [17:03:14<1:41:44,  7.60s/it]

 90%|████████▉ | 6887/7689 [17:03:31<2:19:41, 10.45s/it]

 90%|████████▉ | 6888/7689 [17:03:40<2:12:07,  9.90s/it]

 90%|████████▉ | 6889/7689 [17:03:47<2:02:33,  9.19s/it]

 90%|████████▉ | 6890/7689 [17:03:59<2:13:02,  9.99s/it]

 90%|████████▉ | 6891/7689 [17:04:14<2:32:12, 11.44s/it]

 90%|████████▉ | 6892/7689 [17:04:23<2:23:54, 10.83s/it]

 90%|████████▉ | 6893/7689 [17:04:29<2:04:54,  9.42s/it]

 90%|████████▉ | 6894/7689 [17:04:35<1:49:02,  8.23s/it]

 90%|████████▉ | 6895/7689 [17:04:40<1:35:17,  7.20s/it]

 90%|████████▉ | 6896/7689 [17:04:46<1:30:43,  6.86s/it]

 90%|████████▉ | 6897/7689 [17:04:52<1:27:39,  6.64s/it]

 90%|████████▉ | 6898/7689 [17:05:01<1:39:20,  7.54s/it]

 90%|████████▉ | 6899/7689 [17:05:09<1:40:30,  7.63s/it]

 90%|████████▉ | 6900/7689 [17:05:15<1:33:25,  7.10s/it]
 90%|████████▉ | 6900/7689 [17:05:15<1:33:25,  7.10s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 90%|████████▉ | 6901/7689 [17:05:56<3:46:29, 17.25s/it]

 90%|████████▉ | 6902/7689 [17:06:04<3:07:54, 14.33s/it]

 90%|████████▉ | 6903/7689 [17:06:14<2:50:27, 13.01s/it]

 90%|████████▉ | 6904/7689 [17:06:22<2:31:07, 11.55s/it]

 90%|████████▉ | 6905/7689 [17:06:29<2:12:57, 10.18s/it]

 90%|████████▉ | 6906/7689 [17:06:37<2:07:00,  9.73s/it]

 90%|████████▉ | 6907/7689 [17:06:45<1:58:35,  9.10s/it]

 90%|████████▉ | 6908/7689 [17:06:50<1:41:54,  7.83s/it]

 90%|████████▉ | 6909/7689 [17:06:55<1:30:25,  6.96s/it]

 90%|████████▉ | 6910/7689 [17:07:03<1:35:48,  7.38s/it]
{'loss': 0.9067, 'grad_norm': 0.1984199146154917, 'learning_rate': 5.335798015209581e-06, 'epoch': 0.9}


 90%|████████▉ | 6912/7689 [17:07:16<1:27:58,  6.79s/it]

 90%|████████▉ | 6913/7689 [17:07:28<1:46:32,  8.24s/it]

 90%|████████▉ | 6914/7689 [17:07:35<1:43:35,  8.02s/it]

 90%|████████▉ | 6915/7689 [17:07:42<1:40:15,  7.77s/it]

 90%|████████▉ | 6916/7689 [17:07:52<1:48:49,  8.45s/it]

 90%|████████▉ | 6917/7689 [17:08:04<2:01:00,  9.40s/it]

 90%|████████▉ | 6918/7689 [17:08:12<1:55:03,  8.95s/it]
{'loss': 1.0079, 'grad_norm': 0.22002501849499556, 'learning_rate': 5.227728135130372e-06, 'epoch': 0.9}


 90%|████████▉ | 6920/7689 [17:08:27<1:47:19,  8.37s/it]

 90%|█████████ | 6921/7689 [17:08:32<1:35:28,  7.46s/it]

 90%|█████████ | 6922/7689 [17:08:52<2:21:52, 11.10s/it]

 90%|█████████ | 6923/7689 [17:08:57<1:59:22,  9.35s/it]

 90%|█████████ | 6924/7689 [17:09:10<2:11:40, 10.33s/it]

 90%|█████████ | 6925/7689 [17:09:15<1:51:32,  8.76s/it]

 90%|█████████ | 6926/7689 [17:09:21<1:40:40,  7.92s/it]

 90%|█████████ | 6927/7689 [17:09:28<1:36:55,  7.63s/it]

 90%|█████████ | 6928/7689 [17:09:34<1:30:42,  7.15s/it]

 90%|█████████ | 6929/7689 [17:09:46<1:51:01,  8.77s/it]

 90%|█████████ | 6930/7689 [17:09:54<1:47:04,  8.46s/it]

 90%|█████████ | 6931/7689 [17:10:06<1:59:38,  9.47s/it]

 90%|█████████ | 6932/7689 [17:10:15<1:58:55,  9.43s/it]

 90%|█████████ | 6933/7689 [17:10:22<1:48:17,  8.59s/it]

 90%|█████████ | 6934/7689 [17:10:30<1:45:16,  8.37s/it]

 90%|█████████ | 6935/7689 [17:10:39<1:49:30,  8.71s/it]

 90%|█████████ | 6936/7689 [17:10:53<2:09:33, 10.32s/it]

 90%|█████████ | 6937/7689 [17:11:04<2:09:22, 10.32s/it]

 90%|█████████ | 6938/7689 [17:11:12<2:01:42,  9.72s/it]

 90%|█████████ | 6939/7689 [17:11:19<1:51:11,  8.89s/it]
{'loss': 1.0374, 'grad_norm': 0.22549704028687229, 'learning_rate': 4.949168453619524e-06, 'epoch': 0.9}


 90%|█████████ | 6941/7689 [17:11:39<2:02:15,  9.81s/it]

 90%|█████████ | 6942/7689 [17:11:47<1:56:08,  9.33s/it]

 90%|█████████ | 6943/7689 [17:11:57<1:59:11,  9.59s/it]

 90%|█████████ | 6944/7689 [17:12:06<1:55:51,  9.33s/it]

 90%|█████████ | 6945/7689 [17:12:18<2:03:54,  9.99s/it]

 90%|█████████ | 6946/7689 [17:12:24<1:48:56,  8.80s/it]

 90%|█████████ | 6947/7689 [17:12:31<1:43:39,  8.38s/it]

 90%|█████████ | 6948/7689 [17:12:39<1:40:15,  8.12s/it]

 90%|█████████ | 6949/7689 [17:12:47<1:39:58,  8.11s/it]

 90%|█████████ | 6950/7689 [17:12:54<1:38:04,  7.96s/it]

 90%|█████████ | 6951/7689 [17:13:02<1:36:56,  7.88s/it]

 90%|█████████ | 6952/7689 [17:13:12<1:42:56,  8.38s/it]

 90%|█████████ | 6953/7689 [17:13:18<1:34:23,  7.70s/it]

 90%|█████████ | 6954/7689 [17:13:28<1:43:03,  8.41s/it]

 90%|█████████ | 6955/7689 [17:13:33<1:32:21,  7.55s/it]

 90%|█████████ | 6956/7689 [17:13:41<1:33:47,  7.68s/it]
{'loss': 0.8991, 'grad_norm': 0.23163480139427908, 'learning_rate': 4.72911443197368e-06, 'epoch': 0.9}


 90%|█████████ | 6958/7689 [17:14:00<1:45:33,  8.66s/it]

 91%|█████████ | 6959/7689 [17:14:12<1:57:16,  9.64s/it]

 91%|█████████ | 6960/7689 [17:14:19<1:47:11,  8.82s/it]

 91%|█████████ | 6961/7689 [17:14:30<1:57:34,  9.69s/it]

 91%|█████████ | 6962/7689 [17:14:36<1:42:23,  8.45s/it]

 91%|█████████ | 6963/7689 [17:14:46<1:46:16,  8.78s/it]

 91%|█████████ | 6964/7689 [17:14:58<1:59:43,  9.91s/it]

 91%|█████████ | 6965/7689 [17:15:04<1:46:13,  8.80s/it]

 91%|█████████ | 6966/7689 [17:15:12<1:43:38,  8.60s/it]

 91%|█████████ | 6967/7689 [17:15:20<1:39:19,  8.25s/it]

 91%|█████████ | 6968/7689 [17:15:29<1:41:19,  8.43s/it]

 91%|█████████ | 6969/7689 [17:15:38<1:42:33,  8.55s/it]

 91%|█████████ | 6970/7689 [17:15:51<1:59:01,  9.93s/it]

 91%|█████████ | 6971/7689 [17:15:58<1:50:00,  9.19s/it]

 91%|█████████ | 6972/7689 [17:16:09<1:54:52,  9.61s/it]

 91%|█████████ | 6973/7689 [17:16:15<1:41:17,  8.49s/it]

 91%|█████████ | 6974/7689 [17:16:25<1:46:16,  8.92s/it]
{'loss': 1.0167, 'grad_norm': 0.198919545921957, 'learning_rate': 4.501441345610347e-06, 'epoch': 0.91}


 91%|█████████ | 6976/7689 [17:16:37<1:28:01,  7.41s/it]

 91%|█████████ | 6977/7689 [17:16:41<1:18:31,  6.62s/it]

 91%|█████████ | 6978/7689 [17:16:50<1:23:35,  7.05s/it]

 91%|█████████ | 6979/7689 [17:16:54<1:15:21,  6.37s/it]

 91%|█████████ | 6980/7689 [17:17:00<1:13:12,  6.20s/it]

 91%|█████████ | 6981/7689 [17:17:13<1:35:15,  8.07s/it]

 91%|█████████ | 6982/7689 [17:17:26<1:55:09,  9.77s/it]

 91%|█████████ | 6983/7689 [17:17:34<1:46:20,  9.04s/it]

 91%|█████████ | 6984/7689 [17:17:43<1:46:14,  9.04s/it]

 91%|█████████ | 6985/7689 [17:17:55<1:57:10,  9.99s/it]

 91%|█████████ | 6986/7689 [17:18:04<1:52:51,  9.63s/it]

 91%|█████████ | 6987/7689 [17:18:15<1:58:48, 10.15s/it]
{'loss': 0.9924, 'grad_norm': 0.2114809100673316, 'learning_rate': 4.340424523447872e-06, 'epoch': 0.91}


 91%|█████████ | 6989/7689 [17:18:32<1:48:10,  9.27s/it]

 91%|█████████ | 6990/7689 [17:18:40<1:41:50,  8.74s/it]
{'loss': 0.8735, 'grad_norm': 0.22238309525454805, 'learning_rate': 4.3036739954488205e-06, 'epoch': 0.91}


 91%|█████████ | 6992/7689 [17:18:57<1:41:35,  8.74s/it]

 91%|█████████ | 6993/7689 [17:19:05<1:38:39,  8.50s/it]

 91%|█████████ | 6994/7689 [17:19:18<1:52:45,  9.73s/it]

 91%|█████████ | 6995/7689 [17:19:23<1:37:12,  8.40s/it]

 91%|█████████ | 6996/7689 [17:19:37<1:55:30, 10.00s/it]

 91%|█████████ | 6997/7689 [17:19:46<1:53:17,  9.82s/it]

 91%|█████████ | 6998/7689 [17:19:54<1:46:13,  9.22s/it]

 91%|█████████ | 6999/7689 [17:20:04<1:49:00,  9.48s/it]

 91%|█████████ | 7000/7689 [17:20:17<2:00:01, 10.45s/it]

 91%|█████████ | 7001/7689 [17:20:25<1:52:58,  9.85s/it]

 91%|█████████ | 7002/7689 [17:20:35<1:51:23,  9.73s/it]

 91%|█████████ | 7003/7689 [17:20:48<2:03:48, 10.83s/it]

 91%|█████████ | 7004/7689 [17:20:55<1:51:08,  9.74s/it]

 91%|█████████ | 7005/7689 [17:21:02<1:42:02,  8.95s/it]

 91%|█████████ | 7006/7689 [17:21:10<1:36:45,  8.50s/it]

 91%|█████████ | 7007/7689 [17:21:16<1:30:03,  7.92s/it]

 91%|█████████ | 7008/7689 [17:21:23<1:25:53,  7.57s/it]

 91%|█████████ | 7009/7689 [17:21:28<1:16:42,  6.77s/it]

 91%|█████████ | 7010/7689 [17:21:41<1:36:58,  8.57s/it]

 91%|█████████ | 7011/7689 [17:21:49<1:35:25,  8.44s/it]
{'loss': 0.8888, 'grad_norm': 0.1831618331736546, 'learning_rate': 4.050702638550275e-06, 'epoch': 0.91}


 91%|█████████ | 7013/7689 [17:22:16<2:11:07, 11.64s/it]

 91%|█████████ | 7014/7689 [17:22:23<1:52:56, 10.04s/it]

 91%|█████████ | 7015/7689 [17:22:29<1:39:35,  8.87s/it]

 91%|█████████ | 7016/7689 [17:22:38<1:41:41,  9.07s/it]

 91%|█████████▏| 7017/7689 [17:22:44<1:29:19,  7.98s/it]

 91%|█████████▏| 7018/7689 [17:22:49<1:19:48,  7.14s/it]

 91%|█████████▏| 7019/7689 [17:23:01<1:36:37,  8.65s/it]
{'loss': 1.041, 'grad_norm': 0.20412873150468147, 'learning_rate': 3.956306473422122e-06, 'epoch': 0.91}


 91%|█████████▏| 7021/7689 [17:23:18<1:37:14,  8.73s/it]

 91%|█████████▏| 7022/7689 [17:23:37<2:10:00, 11.69s/it]

 91%|█████████▏| 7023/7689 [17:23:51<2:18:21, 12.46s/it]

 91%|█████████▏| 7024/7689 [17:23:58<2:00:46, 10.90s/it]
{'loss': 1.0253, 'grad_norm': 0.1994906137553503, 'learning_rate': 3.897862671345342e-06, 'epoch': 0.91}


 91%|█████████▏| 7026/7689 [17:24:16<1:51:38, 10.10s/it]

 91%|█████████▏| 7027/7689 [17:24:21<1:34:26,  8.56s/it]

 91%|█████████▏| 7028/7689 [17:24:33<1:47:00,  9.71s/it]

 91%|█████████▏| 7029/7689 [17:24:41<1:39:39,  9.06s/it]

 91%|█████████▏| 7030/7689 [17:24:49<1:38:12,  8.94s/it]

 91%|█████████▏| 7031/7689 [17:24:56<1:31:21,  8.33s/it]

 91%|█████████▏| 7032/7689 [17:25:08<1:42:53,  9.40s/it]

 91%|█████████▏| 7033/7689 [17:25:19<1:48:48,  9.95s/it]

 91%|█████████▏| 7034/7689 [17:25:26<1:39:10,  9.08s/it]
{'loss': 0.9708, 'grad_norm': 0.20834887225174997, 'learning_rate': 3.7822542615149505e-06, 'epoch': 0.91}


 92%|█████████▏| 7036/7689 [17:25:45<1:40:07,  9.20s/it]

 92%|█████████▏| 7037/7689 [17:25:52<1:33:44,  8.63s/it]

 92%|█████████▏| 7038/7689 [17:25:57<1:20:23,  7.41s/it]

 92%|█████████▏| 7039/7689 [17:26:05<1:23:42,  7.73s/it]

 92%|█████████▏| 7040/7689 [17:26:13<1:22:20,  7.61s/it]

 92%|█████████▏| 7041/7689 [17:26:25<1:36:55,  8.97s/it]

 92%|█████████▏| 7042/7689 [17:26:32<1:29:10,  8.27s/it]
{'loss': 1.2086, 'grad_norm': 0.21273482163029545, 'learning_rate': 3.6909966913039606e-06, 'epoch': 0.92}


 92%|█████████▏| 7044/7689 [17:26:45<1:19:17,  7.38s/it]

 92%|█████████▏| 7045/7689 [17:26:50<1:11:35,  6.67s/it]
{'loss': 0.9727, 'grad_norm': 0.20768703660588078, 'learning_rate': 3.6570570188340647e-06, 'epoch': 0.92}


 92%|█████████▏| 7047/7689 [17:27:09<1:27:28,  8.18s/it]

 92%|█████████▏| 7048/7689 [17:27:21<1:39:42,  9.33s/it]

 92%|█████████▏| 7049/7689 [17:27:28<1:31:53,  8.61s/it]

 92%|█████████▏| 7050/7689 [17:27:41<1:45:32,  9.91s/it]

 92%|█████████▏| 7051/7689 [17:27:48<1:34:00,  8.84s/it]

 92%|█████████▏| 7052/7689 [17:27:53<1:23:47,  7.89s/it]

 92%|█████████▏| 7053/7689 [17:27:58<1:14:24,  7.02s/it]

 92%|█████████▏| 7054/7689 [17:28:09<1:25:54,  8.12s/it]

 92%|█████████▏| 7055/7689 [17:28:17<1:26:14,  8.16s/it]
{'loss': 1.0025, 'grad_norm': 0.21550291221735754, 'learning_rate': 3.5450362695563788e-06, 'epoch': 0.92}

 92%|█████████▏| 7056/7689 [17:28:24<1:21:48,  7.75s/it]


 92%|█████████▏| 7058/7689 [17:28:41<1:23:47,  7.97s/it]

 92%|█████████▏| 7059/7689 [17:28:52<1:34:06,  8.96s/it]

 92%|█████████▏| 7060/7689 [17:29:03<1:38:54,  9.43s/it]

 92%|█████████▏| 7061/7689 [17:29:08<1:26:55,  8.30s/it]

 92%|█████████▏| 7062/7689 [17:29:15<1:20:33,  7.71s/it]

 92%|█████████▏| 7063/7689 [17:29:23<1:20:54,  7.76s/it]

 92%|█████████▏| 7064/7689 [17:29:31<1:24:13,  8.08s/it]

 92%|█████████▏| 7065/7689 [17:29:37<1:16:38,  7.37s/it]

 92%|█████████▏| 7066/7689 [17:29:43<1:11:56,  6.93s/it]
{'loss': 1.0489, 'grad_norm': 0.21561846332000734, 'learning_rate': 3.423790312683317e-06, 'epoch': 0.92}


 92%|█████████▏| 7068/7689 [17:29:59<1:16:53,  7.43s/it]

 92%|█████████▏| 7069/7689 [17:30:05<1:12:10,  6.98s/it]

 92%|█████████▏| 7070/7689 [17:30:12<1:13:03,  7.08s/it]
{'loss': 1.0379, 'grad_norm': 0.19703798868195413, 'learning_rate': 3.3802148359496023e-06, 'epoch': 0.92}


 92%|█████████▏| 7072/7689 [17:30:28<1:18:42,  7.65s/it]
{'loss': 1.101, 'grad_norm': 0.18233997471109062, 'learning_rate': 3.358529955963985e-06, 'epoch': 0.92}


 92%|█████████▏| 7074/7689 [17:30:43<1:17:33,  7.57s/it]

 92%|█████████▏| 7075/7689 [17:30:54<1:26:15,  8.43s/it]

 92%|█████████▏| 7076/7689 [17:31:05<1:33:48,  9.18s/it]

 92%|█████████▏| 7077/7689 [17:31:11<1:24:49,  8.32s/it]

 92%|█████████▏| 7078/7689 [17:31:18<1:21:30,  8.00s/it]
{'loss': 0.7202, 'grad_norm': 0.18852515704839343, 'learning_rate': 3.2938869340802146e-06, 'epoch': 0.92}


 92%|█████████▏| 7080/7689 [17:31:32<1:13:02,  7.20s/it]

 92%|█████████▏| 7081/7689 [17:31:41<1:18:18,  7.73s/it]

 92%|█████████▏| 7082/7689 [17:31:47<1:14:15,  7.34s/it]

 92%|█████████▏| 7083/7689 [17:32:03<1:40:30,  9.95s/it]

 92%|█████████▏| 7084/7689 [17:32:11<1:35:16,  9.45s/it]
{'loss': 0.8405, 'grad_norm': 0.19925496987300959, 'learning_rate': 3.2298616599643285e-06, 'epoch': 0.92}

 92%|█████████▏| 7085/7689 [17:32:18<1:27:01,  8.65s/it]


 92%|█████████▏| 7087/7689 [17:32:32<1:17:19,  7.71s/it]

 92%|█████████▏| 7088/7689 [17:32:42<1:25:29,  8.53s/it]
{'loss': 1.096, 'grad_norm': 0.1840114355361781, 'learning_rate': 3.187521539222471e-06, 'epoch': 0.92}


 92%|█████████▏| 7090/7689 [17:32:53<1:08:57,  6.91s/it]

 92%|█████████▏| 7091/7689 [17:33:06<1:26:34,  8.69s/it]

 92%|█████████▏| 7092/7689 [17:33:18<1:36:41,  9.72s/it]

 92%|█████████▏| 7093/7689 [17:33:28<1:39:02,  9.97s/it]

 92%|█████████▏| 7094/7689 [17:33:35<1:28:31,  8.93s/it]

 92%|█████████▏| 7095/7689 [17:33:43<1:26:47,  8.77s/it]
{'loss': 1.0074, 'grad_norm': 0.19154303008882448, 'learning_rate': 3.1140877735439387e-06, 'epoch': 0.92}


 92%|█████████▏| 7097/7689 [17:33:57<1:17:51,  7.89s/it]
{'loss': 0.9834, 'grad_norm': 0.1923100425503452, 'learning_rate': 3.093261393943381e-06, 'epoch': 0.92}


 92%|█████████▏| 7099/7689 [17:34:10<1:11:07,  7.23s/it]

 92%|█████████▏| 7100/7689 [17:34:18<1:16:11,  7.76s/it]

 92%|█████████▏| 7101/7689 [17:34:25<1:11:22,  7.28s/it]
{'loss': 1.2211, 'grad_norm': 0.22955389474694454, 'learning_rate': 3.0518149926867077e-06, 'epoch': 0.92}

 92%|█████████▏| 7102/7689 [17:34:32<1:12:02,  7.36s/it]

 92%|█████████▏| 7103/7689 [17:34:38<1:07:03,  6.87s/it]

 92%|█████████▏| 7104/7689 [17:34:43<1:02:59,  6.46s/it]


 92%|█████████▏| 7106/7689 [17:34:57<1:04:17,  6.62s/it]

 92%|█████████▏| 7107/7689 [17:35:03<1:01:17,  6.32s/it]

 92%|█████████▏| 7108/7689 [17:35:11<1:07:22,  6.96s/it]

 92%|█████████▏| 7109/7689 [17:35:19<1:08:46,  7.11s/it]

 92%|█████████▏| 7110/7689 [17:35:24<1:01:37,  6.39s/it]

 92%|█████████▏| 7111/7689 [17:35:29<59:29,  6.18s/it]

 92%|█████████▏| 7112/7689 [17:35:39<1:10:00,  7.28s/it]
{'loss': 1.0022, 'grad_norm': 0.20938855179839072, 'learning_rate': 2.9392569564224425e-06, 'epoch': 0.92}


 93%|█████████▎| 7114/7689 [17:35:55<1:14:16,  7.75s/it]

 93%|█████████▎| 7115/7689 [17:36:04<1:15:47,  7.92s/it]
{'loss': 1.0365, 'grad_norm': 0.20022975118598735, 'learning_rate': 2.9089208834162107e-06, 'epoch': 0.93}


 93%|█████████▎| 7117/7689 [17:36:21<1:22:20,  8.64s/it]

 93%|█████████▎| 7118/7689 [17:36:31<1:25:49,  9.02s/it]

 93%|█████████▎| 7119/7689 [17:36:36<1:14:16,  7.82s/it]
{'loss': 0.9301, 'grad_norm': 0.20553498642008322, 'learning_rate': 2.8687139860759995e-06, 'epoch': 0.93}

 93%|█████████▎| 7120/7689 [17:36:53<1:39:50, 10.53s/it]


 93%|█████████▎| 7122/7689 [17:37:05<1:17:40,  8.22s/it]
{'loss': 0.9383, 'grad_norm': 0.1937453254603091, 'learning_rate': 2.838739775497623e-06, 'epoch': 0.93}

 93%|█████████▎| 7123/7689 [17:37:13<1:15:49,  8.04s/it]


 93%|█████████▎| 7125/7689 [17:37:37<1:35:54, 10.20s/it]

 93%|█████████▎| 7126/7689 [17:37:43<1:25:16,  9.09s/it]

 93%|█████████▎| 7127/7689 [17:37:59<1:43:05, 11.01s/it]
{'loss': 0.8851, 'grad_norm': 0.18767657029394574, 'learning_rate': 2.789127589973661e-06, 'epoch': 0.93}


 93%|█████████▎| 7129/7689 [17:38:21<1:42:09, 10.95s/it]
{'loss': 0.8592, 'grad_norm': 0.18998092371524938, 'learning_rate': 2.7694034481010933e-06, 'epoch': 0.93}


 93%|█████████▎| 7131/7689 [17:38:40<1:35:18, 10.25s/it]

 93%|█████████▎| 7132/7689 [17:38:46<1:24:24,  9.09s/it]
{'loss': 1.0905, 'grad_norm': 0.2135959300482228, 'learning_rate': 2.7399466351544002e-06, 'epoch': 0.93}


 93%|█████████▎| 7134/7689 [17:38:57<1:07:02,  7.25s/it]

 93%|█████████▎| 7135/7689 [17:39:06<1:10:33,  7.64s/it]
{'loss': 1.015, 'grad_norm': 0.20774825745338765, 'learning_rate': 2.7106451438387637e-06, 'epoch': 0.93}

 93%|█████████▎| 7136/7689 [17:39:17<1:19:37,  8.64s/it]


 93%|█████████▎| 7138/7689 [17:39:38<1:27:05,  9.48s/it]

 93%|█████████▎| 7139/7689 [17:39:55<1:48:29, 11.84s/it]
{'loss': 0.978, 'grad_norm': 0.19689492349018056, 'learning_rate': 2.6718181810195696e-06, 'epoch': 0.93}


 93%|█████████▎| 7141/7689 [17:40:28<2:12:07, 14.47s/it]

 93%|█████████▎| 7142/7689 [17:40:40<2:04:40, 13.68s/it]
{'loss': 0.8374, 'grad_norm': 0.1948490090320454, 'learning_rate': 2.6428792883896216e-06, 'epoch': 0.93}


 93%|█████████▎| 7144/7689 [17:41:05<1:59:10, 13.12s/it]

 93%|█████████▎| 7145/7689 [17:41:11<1:39:32, 10.98s/it]
{'loss': 0.9917, 'grad_norm': 0.20965459495717254, 'learning_rate': 2.6140958724046094e-06, 'epoch': 0.93}


 93%|█████████▎| 7147/7689 [17:41:29<1:31:25, 10.12s/it]

 93%|█████████▎| 7148/7689 [17:41:35<1:20:35,  8.94s/it]

 93%|█████████▎| 7149/7689 [17:41:43<1:17:40,  8.63s/it]
{'loss': 1.1963, 'grad_norm': 0.19650265019786886, 'learning_rate': 2.5759599163956095e-06, 'epoch': 0.93}

 93%|█████████▎| 7150/7689 [17:41:53<1:19:41,  8.87s/it]

 93%|█████████▎| 7151/7689 [17:42:04<1:27:07,  9.72s/it]


 93%|█████████▎| 7153/7689 [17:42:25<1:31:57, 10.29s/it]

 93%|█████████▎| 7154/7689 [17:42:31<1:20:50,  9.07s/it]

 93%|█████████▎| 7155/7689 [17:42:41<1:23:22,  9.37s/it]
{'loss': 0.9711, 'grad_norm': 0.18768240971652853, 'learning_rate': 2.5192746276593293e-06, 'epoch': 0.93}


 93%|█████████▎| 7157/7689 [17:42:55<1:11:53,  8.11s/it]
{'loss': 0.9056, 'grad_norm': 0.18896775267790353, 'learning_rate': 2.500517890514609e-06, 'epoch': 0.93}

 93%|█████████▎| 7158/7689 [17:43:05<1:15:19,  8.51s/it]

 93%|█████████▎| 7159/7689 [17:43:12<1:13:14,  8.29s/it]


 93%|█████████▎| 7161/7689 [17:43:26<1:05:26,  7.44s/it]

 93%|█████████▎| 7162/7689 [17:43:38<1:16:34,  8.72s/it]

 93%|█████████▎| 7163/7689 [17:43:47<1:18:15,  8.93s/it]
{'loss': 0.8472, 'grad_norm': 0.20165147340700773, 'learning_rate': 2.444662942762921e-06, 'epoch': 0.93}


 93%|█████████▎| 7165/7689 [17:44:04<1:14:56,  8.58s/it]

 93%|█████████▎| 7166/7689 [17:44:12<1:11:30,  8.20s/it]

 93%|█████████▎| 7167/7689 [17:44:18<1:06:28,  7.64s/it]

 93%|█████████▎| 7168/7689 [17:44:24<1:02:05,  7.15s/it]
{'loss': 0.8835, 'grad_norm': 0.20129554263989854, 'learning_rate': 2.3985931725079458e-06, 'epoch': 0.93}


 93%|█████████▎| 7170/7689 [17:44:43<1:14:01,  8.56s/it]

 93%|█████████▎| 7171/7689 [17:44:51<1:12:44,  8.43s/it]
{'loss': 1.031, 'grad_norm': 0.18456184398783626, 'learning_rate': 2.3711591196570514e-06, 'epoch': 0.93}

 93%|█████████▎| 7172/7689 [17:45:01<1:14:17,  8.62s/it]


 93%|█████████▎| 7174/7689 [17:45:16<1:08:58,  8.04s/it]

 93%|█████████▎| 7175/7689 [17:45:27<1:17:59,  9.10s/it]
{'loss': 0.9801, 'grad_norm': 0.17415725743923016, 'learning_rate': 2.3348229176194946e-06, 'epoch': 0.93}

 93%|█████████▎| 7176/7689 [17:45:37<1:18:24,  9.17s/it]


 93%|█████████▎| 7178/7689 [17:45:51<1:11:01,  8.34s/it]

 93%|█████████▎| 7179/7689 [17:46:02<1:16:34,  9.01s/it]

 93%|█████████▎| 7180/7689 [17:46:14<1:23:26,  9.84s/it]

 93%|█████████▎| 7181/7689 [17:46:20<1:15:32,  8.92s/it]

 93%|█████████▎| 7182/7689 [17:46:29<1:15:21,  8.92s/it]

 93%|█████████▎| 7183/7689 [17:46:36<1:08:57,  8.18s/it]
{'loss': 0.9439, 'grad_norm': 0.19733062982592367, 'learning_rate': 2.2629824483784366e-06, 'epoch': 0.93}


 93%|█████████▎| 7185/7689 [17:46:53<1:10:00,  8.33s/it]

 93%|█████████▎| 7186/7689 [17:47:00<1:04:34,  7.70s/it]
{'loss': 0.9265, 'grad_norm': 0.21473562568248478, 'learning_rate': 2.236328381534014e-06, 'epoch': 0.93}


 93%|█████████▎| 7188/7689 [17:47:17<1:08:55,  8.26s/it]
{'loss': 0.8777, 'grad_norm': 0.18721153551906397, 'learning_rate': 2.2186457376291215e-06, 'epoch': 0.93}

 93%|█████████▎| 7189/7689 [17:47:25<1:06:17,  7.96s/it]

 94%|█████████▎| 7190/7689 [17:47:31<1:02:23,  7.50s/it]


 94%|█████████▎| 7192/7689 [17:47:48<1:06:50,  8.07s/it]
{'loss': 1.0093, 'grad_norm': 0.19611128823747742, 'learning_rate': 2.1834886678483323e-06, 'epoch': 0.94}


 94%|█████████▎| 7194/7689 [17:48:02<59:25,  7.20s/it]
{'loss': 1.0272, 'grad_norm': 0.2500456278098623, 'learning_rate': 2.166014266925731e-06, 'epoch': 0.94}

 94%|█████████▎| 7195/7689 [17:48:09<59:36,  7.24s/it]


 94%|█████████▎| 7197/7689 [17:48:22<58:16,  7.11s/it]

 94%|█████████▎| 7198/7689 [17:48:30<59:41,  7.30s/it]
{'loss': 0.7242, 'grad_norm': 0.1984912176219257, 'learning_rate': 2.13127379502972e-06, 'epoch': 0.94}

 94%|█████████▎| 7199/7689 [17:48:37<58:52,  7.21s/it]

 94%|█████████▎| 7200/7689 [17:48:43<54:31,  6.69s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.9067, 'grad_norm': 0.1855209258554727, 'learning_rate': 2.105400778333655e-06, 'epoch': 0.94}

 94%|█████████▎| 7202/7689 [17:49:27<1:46:01, 13.06s/it]

 94%|█████████▎| 7203/7689 [17:49:42<1:50:08, 13.60s/it]

 94%|█████████▎| 7204/7689 [17:49:49<1:33:45, 11.60s/it]

 94%|█████████▎| 7205/7689 [17:49:55<1:19:49,  9.90s/it]

 94%|█████████▎| 7206/7689 [17:50:01<1:11:12,  8.85s/it]
{'loss': 1.0257, 'grad_norm': 0.191359420598516, 'learning_rate': 2.0626265152054903e-06, 'epoch': 0.94}

 94%|█████████▎| 7207/7689 [17:50:07<1:04:49,  8.07s/it]


 94%|█████████▍| 7209/7689 [17:50:22<1:01:18,  7.66s/it]

 94%|█████████▍| 7210/7689 [17:50:32<1:06:32,  8.33s/it]

 94%|█████████▍| 7211/7689 [17:50:40<1:05:18,  8.20s/it]

 94%|█████████▍| 7212/7689 [17:50:50<1:09:49,  8.78s/it]

 94%|█████████▍| 7213/7689 [17:51:00<1:12:56,  9.19s/it]

 94%|█████████▍| 7214/7689 [17:51:14<1:24:30, 10.68s/it]

 94%|█████████▍| 7215/7689 [17:51:21<1:13:56,  9.36s/it]

 94%|█████████▍| 7216/7689 [17:51:30<1:15:10,  9.54s/it]
{'loss': 0.8092, 'grad_norm': 0.199519862722431, 'learning_rate': 1.9783815372338423e-06, 'epoch': 0.94}


 94%|█████████▍| 7218/7689 [17:51:46<1:07:59,  8.66s/it]
{'loss': 1.1246, 'grad_norm': 0.200093744703841, 'learning_rate': 1.9617412111725075e-06, 'epoch': 0.94}


 94%|█████████▍| 7220/7689 [17:52:08<1:18:04,  9.99s/it]
{'loss': 0.8325, 'grad_norm': 0.19275618027445857, 'learning_rate': 1.9451704692957652e-06, 'epoch': 0.94}


 94%|█████████▍| 7222/7689 [17:52:27<1:14:09,  9.53s/it]

 94%|█████████▍| 7223/7689 [17:52:32<1:04:56,  8.36s/it]

 94%|█████████▍| 7224/7689 [17:52:40<1:03:15,  8.16s/it]
{'loss': 0.8737, 'grad_norm': 0.1981559313513334, 'learning_rate': 1.9122377850919414e-06, 'epoch': 0.94}


 94%|█████████▍| 7226/7689 [17:52:53<55:49,  7.23s/it]

 94%|█████████▍| 7227/7689 [17:53:00<55:18,  7.18s/it]

 94%|█████████▍| 7228/7689 [17:53:06<53:12,  6.93s/it]

 94%|█████████▍| 7229/7689 [17:53:14<55:09,  7.19s/it]

 94%|█████████▍| 7230/7689 [17:53:27<1:08:04,  8.90s/it]

 94%|█████████▍| 7231/7689 [17:53:38<1:12:18,  9.47s/it]

 94%|█████████▍| 7232/7689 [17:53:46<1:08:44,  9.03s/it]

 94%|█████████▍| 7233/7689 [17:53:52<1:01:55,  8.15s/it]

 94%|█████████▍| 7234/7689 [17:54:02<1:06:26,  8.76s/it]

 94%|█████████▍| 7235/7689 [17:54:12<1:08:25,  9.04s/it]

 94%|█████████▍| 7236/7689 [17:54:18<1:01:22,  8.13s/it]

 94%|█████████▍| 7237/7689 [17:54:23<54:35,  7.25s/it]

 94%|█████████▍| 7238/7689 [17:54:30<54:31,  7.25s/it]
{'loss': 0.732, 'grad_norm': 0.22823836572756404, 'learning_rate': 1.7991670050277354e-06, 'epoch': 0.94}


 94%|█████████▍| 7240/7689 [17:54:45<52:31,  7.02s/it]

 94%|█████████▍| 7241/7689 [17:54:58<1:07:16,  9.01s/it]

 94%|█████████▍| 7242/7689 [17:55:04<59:53,  8.04s/it]

 94%|█████████▍| 7243/7689 [17:55:09<53:35,  7.21s/it]

 94%|█████████▍| 7244/7689 [17:55:15<49:39,  6.70s/it]

 94%|█████████▍| 7245/7689 [17:55:20<46:11,  6.24s/it]
{'loss': 0.8947, 'grad_norm': 0.2245190165644979, 'learning_rate': 1.7439120998964943e-06, 'epoch': 0.94}

 94%|█████████▍| 7246/7689 [17:55:26<45:17,  6.13s/it]


 94%|█████████▍| 7248/7689 [17:55:38<45:30,  6.19s/it]

 94%|█████████▍| 7249/7689 [17:55:43<42:54,  5.85s/it]

 94%|█████████▍| 7250/7689 [17:55:50<44:24,  6.07s/it]
{'loss': 1.0624, 'grad_norm': 0.1857096386029982, 'learning_rate': 1.7049673233914266e-06, 'epoch': 0.94}


 94%|█████████▍| 7252/7689 [17:56:14<1:05:15,  8.96s/it]

 94%|█████████▍| 7253/7689 [17:56:21<1:01:41,  8.49s/it]

 94%|█████████▍| 7254/7689 [17:56:30<1:01:46,  8.52s/it]

 94%|█████████▍| 7255/7689 [17:56:37<58:53,  8.14s/it]

 94%|█████████▍| 7256/7689 [17:56:46<58:56,  8.17s/it]

 94%|█████████▍| 7257/7689 [17:56:53<56:53,  7.90s/it]

 94%|█████████▍| 7258/7689 [17:56:59<53:31,  7.45s/it]
{'loss': 0.7959, 'grad_norm': 0.1866396026870258, 'learning_rate': 1.6435627154859023e-06, 'epoch': 0.94}


 94%|█████████▍| 7260/7689 [17:57:13<50:47,  7.10s/it]

 94%|█████████▍| 7261/7689 [17:57:21<52:25,  7.35s/it]

 94%|█████████▍| 7262/7689 [17:57:27<51:10,  7.19s/it]
{'loss': 1.0841, 'grad_norm': 0.1821631101803692, 'learning_rate': 1.6132792281744802e-06, 'epoch': 0.94}


 94%|█████████▍| 7264/7689 [17:57:45<55:25,  7.82s/it]

 94%|█████████▍| 7265/7689 [17:57:53<56:30,  8.00s/it]

 94%|█████████▍| 7266/7689 [17:57:59<51:45,  7.34s/it]

 95%|█████████▍| 7267/7689 [17:58:05<47:51,  6.80s/it]

 95%|█████████▍| 7268/7689 [17:58:12<49:13,  7.01s/it]

 95%|█████████▍| 7269/7689 [17:58:27<1:06:11,  9.46s/it]
{'loss': 0.6426, 'grad_norm': 0.16658959901634943, 'learning_rate': 1.5609553048258508e-06, 'epoch': 0.95}


 95%|█████████▍| 7271/7689 [17:58:49<1:10:20, 10.10s/it]
{'loss': 1.083, 'grad_norm': 0.20596924797368593, 'learning_rate': 1.5461627970860814e-06, 'epoch': 0.95}

 95%|█████████▍| 7272/7689 [17:58:58<1:08:25,  9.85s/it]


 95%|█████████▍| 7274/7689 [17:59:13<1:01:11,  8.85s/it]

 95%|█████████▍| 7275/7689 [17:59:21<58:41,  8.51s/it]

 95%|█████████▍| 7276/7689 [17:59:27<54:10,  7.87s/it]

 95%|█████████▍| 7277/7689 [17:59:37<58:50,  8.57s/it]

 95%|█████████▍| 7278/7689 [17:59:42<51:15,  7.48s/it]

 95%|█████████▍| 7279/7689 [17:59:49<49:49,  7.29s/it]

 95%|█████████▍| 7280/7689 [17:59:55<47:30,  6.97s/it]

{'loss': 1.0503, 'grad_norm': 0.19986453519142597, 'learning_rate': 1.4804614171303964e-06, 'epoch': 0.95}
{'loss': 0.8687, 'grad_norm': 0.231110926535691, 'learning_rate': 1.4732486538530987e-06, 'epoch': 0.95}

 95%|█████████▍| 7282/7689 [18:00:17<57:15,  8.44s/it]

 95%|█████████▍| 7283/7689 [18:00:21<48:58,  7.24s/it]

 95%|█████████▍| 7284/7689 [18:00:28<48:03,  7.12s/it]

 95%|█████████▍| 7285/7689 [18:00:37<51:37,  7.67s/it]

 95%|█████████▍| 7286/7689 [18:00:43<49:07,  7.31s/it]

 95%|█████████▍| 7287/7689 [18:00:49<45:14,  6.75s/it]

 95%|█████████▍| 7288/7689 [18:00:57<48:02,  7.19s/it]

 95%|█████████▍| 7289/7689 [18:01:09<58:44,  8.81s/it]

 95%|█████████▍| 7290/7689 [18:01:20<1:01:44,  9.28s/it]
{'loss': 0.8751, 'grad_norm': 0.19371877763617212, 'learning_rate': 1.4091206595723383e-06, 'epoch': 0.95}


 95%|█████████▍| 7292/7689 [18:01:31<49:26,  7.47s/it]

 95%|█████████▍| 7293/7689 [18:01:41<54:18,  8.23s/it]

 95%|█████████▍| 7294/7689 [18:01:47<47:52,  7.27s/it]
{'loss': 0.8938, 'grad_norm': 0.20547275326246958, 'learning_rate': 1.3810741209168364e-06, 'epoch': 0.95}


 95%|█████████▍| 7296/7689 [18:01:59<44:19,  6.77s/it]

 95%|█████████▍| 7297/7689 [18:02:05<43:30,  6.66s/it]

 95%|█████████▍| 7298/7689 [18:02:14<46:37,  7.15s/it]

 95%|█████████▍| 7299/7689 [18:02:21<47:08,  7.25s/it]
{'loss': 0.906, 'grad_norm': 0.18973190388810243, 'learning_rate': 1.3464096860949294e-06, 'epoch': 0.95}

 95%|█████████▍| 7300/7689 [18:02:28<46:28,  7.17s/it]


 95%|█████████▍| 7302/7689 [18:02:43<45:41,  7.08s/it]

 95%|█████████▍| 7303/7689 [18:02:49<43:45,  6.80s/it]
{'loss': 0.9774, 'grad_norm': 0.21933557151970945, 'learning_rate': 1.3189932249895176e-06, 'epoch': 0.95}


 95%|█████████▌| 7305/7689 [18:03:02<41:28,  6.48s/it]

 95%|█████████▌| 7306/7689 [18:03:08<41:04,  6.43s/it]

 95%|█████████▌| 7307/7689 [18:03:14<40:58,  6.44s/it]

 95%|█████████▌| 7308/7689 [18:03:21<40:23,  6.36s/it]

 95%|█████████▌| 7309/7689 [18:03:30<45:33,  7.19s/it]

 95%|█████████▌| 7310/7689 [18:03:37<44:53,  7.11s/it]

 95%|█████████▌| 7311/7689 [18:03:47<51:40,  8.20s/it]

 95%|█████████▌| 7312/7689 [18:03:53<46:26,  7.39s/it]

 95%|█████████▌| 7313/7689 [18:03:59<44:35,  7.11s/it]

 95%|█████████▌| 7314/7689 [18:04:17<1:03:52, 10.22s/it]

 95%|█████████▌| 7315/7689 [18:04:29<1:06:38, 10.69s/it]

 95%|█████████▌| 7316/7689 [18:04:35<58:01,  9.33s/it]
{'loss': 0.988, 'grad_norm': 0.22116131672146294, 'learning_rate': 1.23182498945279e-06, 'epoch': 0.95}

 95%|█████████▌| 7317/7689 [18:04:46<1:01:38,  9.94s/it]

 95%|█████████▌| 7318/7689 [18:04:56<1:01:38,  9.97s/it]


 95%|█████████▌| 7320/7689 [18:05:12<55:17,  8.99s/it]

 95%|█████████▌| 7321/7689 [18:05:22<57:15,  9.34s/it]

 95%|█████████▌| 7322/7689 [18:05:28<50:50,  8.31s/it]

 95%|█████████▌| 7323/7689 [18:05:34<46:59,  7.70s/it]

 95%|█████████▌| 7324/7689 [18:05:43<48:35,  7.99s/it]

 95%|█████████▌| 7325/7689 [18:05:52<50:56,  8.40s/it]

 95%|█████████▌| 7326/7689 [18:06:00<49:12,  8.13s/it]

 95%|█████████▌| 7327/7689 [18:06:10<52:55,  8.77s/it]

 95%|█████████▌| 7328/7689 [18:06:19<53:10,  8.84s/it]

 95%|█████████▌| 7329/7689 [18:06:28<53:44,  8.96s/it]

 95%|█████████▌| 7330/7689 [18:06:35<49:30,  8.27s/it]

 95%|█████████▌| 7331/7689 [18:06:41<45:15,  7.58s/it]

 95%|█████████▌| 7332/7689 [18:06:46<40:35,  6.82s/it]

 95%|█████████▌| 7333/7689 [18:07:01<56:03,  9.45s/it]
{'loss': 1.1888, 'grad_norm': 0.22509166917864112, 'learning_rate': 1.1223051593183377e-06, 'epoch': 0.95}


 95%|█████████▌| 7335/7689 [18:07:20<56:31,  9.58s/it]

 95%|█████████▌| 7336/7689 [18:07:26<50:13,  8.54s/it]

 95%|█████████▌| 7337/7689 [18:07:34<49:21,  8.41s/it]

 95%|█████████▌| 7338/7689 [18:07:46<54:20,  9.29s/it]

 95%|█████████▌| 7339/7689 [18:07:52<49:07,  8.42s/it]
{'loss': 1.0234, 'grad_norm': 0.19662095030381674, 'learning_rate': 1.0848614199658214e-06, 'epoch': 0.95}

 95%|█████████▌| 7340/7689 [18:07:58<45:35,  7.84s/it]


 95%|█████████▌| 7342/7689 [18:08:25<1:02:34, 10.82s/it]

 96%|█████████▌| 7343/7689 [18:08:34<59:04, 10.25s/it]

 96%|█████████▌| 7344/7689 [18:08:45<59:36, 10.37s/it]

 96%|█████████▌| 7345/7689 [18:08:54<57:54, 10.10s/it]

 96%|█████████▌| 7346/7689 [18:09:02<53:20,  9.33s/it]

 96%|█████████▌| 7347/7689 [18:09:17<1:03:43, 11.18s/it]

 96%|█████████▌| 7348/7689 [18:09:24<56:08,  9.88s/it]

 96%|█████████▌| 7349/7689 [18:09:37<1:01:10, 10.79s/it]

 96%|█████████▌| 7350/7689 [18:09:43<53:16,  9.43s/it]

 96%|█████████▌| 7351/7689 [18:09:53<53:42,  9.53s/it]

 96%|█████████▌| 7352/7689 [18:09:58<45:31,  8.11s/it]
{'loss': 1.045, 'grad_norm': 0.22697073915451277, 'learning_rate': 1.0059012581544292e-06, 'epoch': 0.96}


 96%|█████████▌| 7354/7689 [18:10:09<38:40,  6.93s/it]

 96%|█████████▌| 7355/7689 [18:10:18<41:06,  7.38s/it]

 96%|█████████▌| 7356/7689 [18:10:26<41:53,  7.55s/it]

 96%|█████████▌| 7357/7689 [18:10:33<41:41,  7.53s/it]

 96%|█████████▌| 7358/7689 [18:10:44<47:00,  8.52s/it]

 96%|█████████▌| 7359/7689 [18:10:53<48:10,  8.76s/it]
{'loss': 1.0663, 'grad_norm': 0.18887101830384925, 'learning_rate': 9.64613693283123e-07, 'epoch': 0.96}

 96%|█████████▌| 7360/7689 [18:11:02<48:25,  8.83s/it]


 96%|█████████▌| 7362/7689 [18:11:22<52:31,  9.64s/it]

 96%|█████████▌| 7363/7689 [18:11:30<50:12,  9.24s/it]

 96%|█████████▌| 7364/7689 [18:11:37<45:49,  8.46s/it]

 96%|█████████▌| 7365/7689 [18:11:48<49:26,  9.16s/it]

 96%|█████████▌| 7366/7689 [18:11:54<45:02,  8.37s/it]

 96%|█████████▌| 7367/7689 [18:12:00<40:59,  7.64s/it]

 96%|█████████▌| 7368/7689 [18:12:13<49:51,  9.32s/it]

 96%|█████████▌| 7369/7689 [18:12:21<47:40,  8.94s/it]

 96%|█████████▌| 7370/7689 [18:12:31<47:43,  8.98s/it]

 96%|█████████▌| 7371/7689 [18:12:36<41:56,  7.91s/it]

 96%|█████████▌| 7372/7689 [18:12:42<38:23,  7.27s/it]

 96%|█████████▌| 7373/7689 [18:12:52<42:38,  8.10s/it]

 96%|█████████▌| 7374/7689 [18:13:03<48:16,  9.19s/it]

 96%|█████████▌| 7375/7689 [18:13:10<44:33,  8.52s/it]

 96%|█████████▌| 7376/7689 [18:13:18<42:47,  8.20s/it]

 96%|█████████▌| 7377/7689 [18:13:27<44:16,  8.51s/it]

 96%|█████████▌| 7378/7689 [18:13:33<40:32,  7.82s/it]

 96%|█████████▌| 7379/7689 [18:13:39<37:36,  7.28s/it]
{'loss': 1.1525, 'grad_norm': 0.19567261790089524, 'learning_rate': 8.513951165805777e-07, 'epoch': 0.96}


 96%|█████████▌| 7381/7689 [18:14:08<53:24, 10.40s/it]

 96%|█████████▌| 7382/7689 [18:14:18<51:58, 10.16s/it]

 96%|█████████▌| 7383/7689 [18:14:26<49:36,  9.73s/it]

 96%|█████████▌| 7384/7689 [18:14:40<54:58, 10.81s/it]

 96%|█████████▌| 7385/7689 [18:14:51<55:09, 10.89s/it]

 96%|█████████▌| 7386/7689 [18:14:57<48:15,  9.56s/it]

 96%|█████████▌| 7387/7689 [18:15:02<41:37,  8.27s/it]

 96%|█████████▌| 7388/7689 [18:15:14<46:07,  9.20s/it]
{'loss': 1.0747, 'grad_norm': 0.19369772141760763, 'learning_rate': 8.027421765807508e-07, 'epoch': 0.96}

 96%|█████████▌| 7389/7689 [18:15:23<46:01,  9.20s/it]


 96%|█████████▌| 7391/7689 [18:15:35<38:18,  7.71s/it]

 96%|█████████▌| 7392/7689 [18:15:43<38:20,  7.74s/it]

 96%|█████████▌| 7393/7689 [18:15:50<36:02,  7.31s/it]

 96%|█████████▌| 7394/7689 [18:16:00<40:55,  8.32s/it]

 96%|█████████▌| 7395/7689 [18:16:08<39:15,  8.01s/it]

 96%|█████████▌| 7396/7689 [18:16:13<35:09,  7.20s/it]

 96%|█████████▌| 7397/7689 [18:16:23<39:47,  8.18s/it]
{'loss': 0.9143, 'grad_norm': 0.19845100506245697, 'learning_rate': 7.555149727028865e-07, 'epoch': 0.96}

 96%|█████████▌| 7398/7689 [18:16:33<42:10,  8.69s/it]


 96%|█████████▌| 7400/7689 [18:16:51<41:39,  8.65s/it]

 96%|█████████▋| 7401/7689 [18:17:10<55:27, 11.55s/it]

 96%|█████████▋| 7402/7689 [18:17:17<48:33, 10.15s/it]

 96%|█████████▋| 7403/7689 [18:17:28<49:41, 10.43s/it]

 96%|█████████▋| 7404/7689 [18:17:34<43:17,  9.11s/it]
{'loss': 1.0325, 'grad_norm': 0.20054045206247065, 'learning_rate': 7.197688323793194e-07, 'epoch': 0.96}

 96%|█████████▋| 7405/7689 [18:17:39<38:03,  8.04s/it]

 96%|█████████▋| 7406/7689 [18:17:45<34:37,  7.34s/it]


 96%|█████████▋| 7408/7689 [18:18:08<43:36,  9.31s/it]

 96%|█████████▋| 7409/7689 [18:18:16<41:23,  8.87s/it]

 96%|█████████▋| 7410/7689 [18:18:23<38:42,  8.32s/it]

 96%|█████████▋| 7411/7689 [18:18:30<37:58,  8.20s/it]
{'loss': 0.9582, 'grad_norm': 0.1968816310584081, 'learning_rate': 6.848858962311911e-07, 'epoch': 0.96}


 96%|█████████▋| 7413/7689 [18:18:44<35:11,  7.65s/it]

 96%|█████████▋| 7414/7689 [18:18:50<33:30,  7.31s/it]

 96%|█████████▋| 7415/7689 [18:18:57<32:09,  7.04s/it]

 96%|█████████▋| 7416/7689 [18:19:09<38:37,  8.49s/it]
{'loss': 0.7887, 'grad_norm': 0.1753095372714018, 'learning_rate': 6.604981739229743e-07, 'epoch': 0.96}


 96%|█████████▋| 7418/7689 [18:19:28<42:36,  9.44s/it]

 96%|█████████▋| 7419/7689 [18:19:34<36:53,  8.20s/it]

 97%|█████████▋| 7420/7689 [18:19:44<39:36,  8.83s/it]

 97%|█████████▋| 7421/7689 [18:19:53<39:14,  8.79s/it]

 97%|█████████▋| 7422/7689 [18:20:05<43:49,  9.85s/it]
{'loss': 0.9692, 'grad_norm': 0.1992180459576151, 'learning_rate': 6.318146053927665e-07, 'epoch': 0.97}


 97%|█████████▋| 7424/7689 [18:20:23<41:19,  9.36s/it]

 97%|█████████▋| 7425/7689 [18:20:36<46:06, 10.48s/it]

 97%|█████████▋| 7426/7689 [18:20:42<40:06,  9.15s/it]

 97%|█████████▋| 7427/7689 [18:20:55<45:23, 10.40s/it]

 97%|█████████▋| 7428/7689 [18:21:14<56:12, 12.92s/it]

 97%|█████████▋| 7429/7689 [18:21:21<48:17, 11.14s/it]

 97%|█████████▋| 7430/7689 [18:21:30<45:20, 10.50s/it]

 97%|█████████▋| 7431/7689 [18:21:39<43:03, 10.02s/it]

 97%|█████████▋| 7432/7689 [18:21:46<39:36,  9.25s/it]
{'loss': 0.9589, 'grad_norm': 0.1935746249666786, 'learning_rate': 5.854193082371562e-07, 'epoch': 0.97}

 97%|█████████▋| 7433/7689 [18:22:01<47:14, 11.07s/it]


 97%|█████████▋| 7435/7689 [18:22:25<48:29, 11.45s/it]

 97%|█████████▋| 7436/7689 [18:22:38<50:43, 12.03s/it]

 97%|█████████▋| 7437/7689 [18:22:48<48:05, 11.45s/it]
{'loss': 0.9361, 'grad_norm': 0.21623736766643378, 'learning_rate': 5.62883118338886e-07, 'epoch': 0.97}

 97%|█████████▋| 7438/7689 [18:22:56<43:01, 10.29s/it]

 97%|█████████▋| 7439/7689 [18:23:08<44:57, 10.79s/it]


 97%|█████████▋| 7441/7689 [18:23:20<34:17,  8.30s/it]

 97%|█████████▋| 7442/7689 [18:23:27<32:25,  7.88s/it]

 97%|█████████▋| 7443/7689 [18:23:35<32:30,  7.93s/it]
{'loss': 0.9675, 'grad_norm': 0.19281575117913458, 'learning_rate': 5.364219595689224e-07, 'epoch': 0.97}


 97%|█████████▋| 7445/7689 [18:23:55<35:06,  8.63s/it]

 97%|█████████▋| 7446/7689 [18:24:02<33:42,  8.32s/it]

 97%|█████████▋| 7447/7689 [18:24:17<40:59, 10.16s/it]

 97%|█████████▋| 7448/7689 [18:24:27<40:41, 10.13s/it]
{'loss': 0.9517, 'grad_norm': 0.20317517714305874, 'learning_rate': 5.148563329580247e-07, 'epoch': 0.97}


 97%|█████████▋| 7450/7689 [18:24:51<43:52, 11.02s/it]

 97%|█████████▋| 7451/7689 [18:25:03<45:15, 11.41s/it]

 97%|█████████▋| 7452/7689 [18:25:10<39:44, 10.06s/it]

 97%|█████████▋| 7453/7689 [18:25:15<33:17,  8.46s/it]

 97%|█████████▋| 7454/7689 [18:25:21<31:01,  7.92s/it]

 97%|█████████▋| 7455/7689 [18:25:30<32:11,  8.25s/it]

 97%|█████████▋| 7456/7689 [18:25:35<27:52,  7.18s/it]

 97%|█████████▋| 7457/7689 [18:25:43<29:07,  7.53s/it]

 97%|█████████▋| 7458/7689 [18:25:55<34:06,  8.86s/it]

 97%|█████████▋| 7459/7689 [18:26:02<31:39,  8.26s/it]

 97%|█████████▋| 7460/7689 [18:26:17<38:57, 10.21s/it]

 97%|█████████▋| 7461/7689 [18:26:23<34:34,  9.10s/it]
{'loss': 0.9406, 'grad_norm': 0.22409900229943644, 'learning_rate': 4.6085131195168217e-07, 'epoch': 0.97}


 97%|█████████▋| 7463/7689 [18:26:38<31:22,  8.33s/it]

 97%|█████████▋| 7464/7689 [18:26:46<30:31,  8.14s/it]

 97%|█████████▋| 7465/7689 [18:26:51<26:58,  7.23s/it]
{'loss': 1.0139, 'grad_norm': 0.2109542662265992, 'learning_rate': 4.448348286050563e-07, 'epoch': 0.97}


 97%|█████████▋| 7467/7689 [18:27:09<31:05,  8.40s/it]

 97%|█████████▋| 7468/7689 [18:27:15<28:27,  7.73s/it]
{'loss': 1.0392, 'grad_norm': 0.20836958172006637, 'learning_rate': 4.330079482910865e-07, 'epoch': 0.97}


 97%|█████████▋| 7470/7689 [18:27:33<28:30,  7.81s/it]

 97%|█████████▋| 7471/7689 [18:27:40<28:33,  7.86s/it]

 97%|█████████▋| 7472/7689 [18:27:57<37:35, 10.39s/it]
{'loss': 0.7684, 'grad_norm': 0.19188826869163264, 'learning_rate': 4.17486120042776e-07, 'epoch': 0.97}

 97%|█████████▋| 7473/7689 [18:28:04<33:36,  9.33s/it]


 97%|█████████▋| 7475/7689 [18:28:19<31:09,  8.74s/it]

 97%|█████████▋| 7476/7689 [18:28:26<28:58,  8.16s/it]

 97%|█████████▋| 7477/7689 [18:28:34<28:42,  8.13s/it]

 97%|█████████▋| 7478/7689 [18:28:43<29:14,  8.31s/it]

 97%|█████████▋| 7479/7689 [18:28:52<30:14,  8.64s/it]
{'loss': 1.0783, 'grad_norm': 0.2148093391840396, 'learning_rate': 3.910032438901312e-07, 'epoch': 0.97}

 97%|█████████▋| 7480/7689 [18:29:00<28:39,  8.23s/it]


 97%|█████████▋| 7482/7689 [18:29:17<28:43,  8.32s/it]

 97%|█████████▋| 7483/7689 [18:29:23<25:44,  7.50s/it]

 97%|█████████▋| 7484/7689 [18:29:40<35:58, 10.53s/it]

 97%|█████████▋| 7485/7689 [18:29:52<37:16, 10.96s/it]

 97%|█████████▋| 7486/7689 [18:29:58<32:12,  9.52s/it]

 97%|█████████▋| 7487/7689 [18:30:04<28:34,  8.49s/it]

 97%|█████████▋| 7488/7689 [18:30:11<26:15,  7.84s/it]

 97%|█████████▋| 7489/7689 [18:30:18<25:08,  7.54s/it]

 97%|█████████▋| 7490/7689 [18:30:26<25:32,  7.70s/it]

 97%|█████████▋| 7491/7689 [18:30:31<23:09,  7.02s/it]
{'loss': 1.0597, 'grad_norm': 0.20936331731511004, 'learning_rate': 3.4761907261356976e-07, 'epoch': 0.97}


 97%|█████████▋| 7493/7689 [18:30:53<30:47,  9.43s/it]
{'loss': 0.9394, 'grad_norm': 0.1755033044400506, 'learning_rate': 3.406359023181094e-07, 'epoch': 0.97}


 97%|█████████▋| 7495/7689 [18:31:13<31:54,  9.87s/it]
{'loss': 0.7734, 'grad_norm': 0.19813111100234426, 'learning_rate': 3.337234668120304e-07, 'epoch': 0.97}


 98%|█████████▊| 7497/7689 [18:31:28<27:50,  8.70s/it]

 98%|█████████▊| 7498/7689 [18:31:33<23:42,  7.45s/it]

 98%|█████████▊| 7499/7689 [18:31:48<30:31,  9.64s/it]

 98%|█████████▊| 7500/7689 [18:31:57<30:11,  9.58s/it]
 98%|█████████▊| 7500/7689 [18:31:57<30:11,  9.58s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0376, 'grad_norm': 0.17030592821276738, 'learning_rate': 3.1341061784110336e-07, 'epoch': 0.98}

 98%|█████████▊| 7502/7689 [18:32:49<52:30, 16.85s/it]

 98%|█████████▊| 7503/7689 [18:32:55<42:22, 13.67s/it]

 98%|█████████▊| 7504/7689 [18:33:04<37:47, 12.26s/it]

 98%|█████████▊| 7505/7689 [18:33:10<31:49, 10.38s/it]

 98%|█████████▊| 7506/7689 [18:33:22<32:19, 10.60s/it]

 98%|█████████▊| 7507/7689 [18:33:32<31:56, 10.53s/it]
{'loss': 1.1724, 'grad_norm': 0.18322304268308948, 'learning_rate': 2.9373455558538985e-07, 'epoch': 0.98}

 98%|█████████▊| 7508/7689 [18:33:38<27:47,  9.21s/it]

 98%|█████████▊| 7509/7689 [18:33:44<24:45,  8.26s/it]


 98%|█████████▊| 7511/7689 [18:34:00<23:52,  8.05s/it]

 98%|█████████▊| 7512/7689 [18:34:15<29:51, 10.12s/it]

 98%|█████████▊| 7513/7689 [18:34:23<27:48,  9.48s/it]

 98%|█████████▊| 7514/7689 [18:34:29<24:07,  8.27s/it]

 98%|█████████▊| 7515/7689 [18:34:33<20:42,  7.14s/it]

 98%|█████████▊| 7516/7689 [18:34:44<23:14,  8.06s/it]

 98%|█████████▊| 7517/7689 [18:34:51<22:33,  7.87s/it]

 98%|█████████▊| 7518/7689 [18:35:08<30:08, 10.58s/it]

 98%|█████████▊| 7519/7689 [18:35:17<28:32, 10.07s/it]

 98%|█████████▊| 7520/7689 [18:35:27<28:54, 10.27s/it]

 98%|█████████▊| 7521/7689 [18:35:35<26:37,  9.51s/it]
{'loss': 1.0334, 'grad_norm': 0.1885920997001159, 'learning_rate': 2.503008348465063e-07, 'epoch': 0.98}


 98%|█████████▊| 7523/7689 [18:35:53<25:55,  9.37s/it]

 98%|█████████▊| 7524/7689 [18:35:59<23:19,  8.48s/it]

 98%|█████████▊| 7525/7689 [18:36:07<22:49,  8.35s/it]
{'loss': 1.0609, 'grad_norm': 0.20621166550576214, 'learning_rate': 2.385283256508664e-07, 'epoch': 0.98}


 98%|█████████▊| 7527/7689 [18:36:35<29:58, 11.10s/it]

 98%|█████████▊| 7528/7689 [18:36:41<26:02,  9.71s/it]

 98%|█████████▊| 7529/7689 [18:36:49<24:42,  9.26s/it]

 98%|█████████▊| 7530/7689 [18:36:59<24:59,  9.43s/it]

 98%|█████████▊| 7531/7689 [18:37:12<27:36, 10.49s/it]

 98%|█████████▊| 7532/7689 [18:37:21<25:51,  9.88s/it]

 98%|█████████▊| 7533/7689 [18:37:26<22:03,  8.48s/it]
{'loss': 1.0486, 'grad_norm': 0.23926806451396798, 'learning_rate': 2.1583302686947192e-07, 'epoch': 0.98}


 98%|█████████▊| 7535/7689 [18:37:39<19:16,  7.51s/it]
{'loss': 1.0638, 'grad_norm': 0.22747753582048083, 'learning_rate': 2.1033625065747242e-07, 'epoch': 0.98}


 98%|█████████▊| 7537/7689 [18:38:06<26:52, 10.61s/it]
{'loss': 0.8809, 'grad_norm': 0.19304688454640176, 'learning_rate': 2.0491030171707616e-07, 'epoch': 0.98}


 98%|█████████▊| 7539/7689 [18:38:21<23:44,  9.50s/it]

 98%|█████████▊| 7540/7689 [18:38:27<20:35,  8.29s/it]

 98%|█████████▊| 7541/7689 [18:38:39<23:29,  9.53s/it]

 98%|█████████▊| 7542/7689 [18:38:49<23:31,  9.60s/it]

 98%|█████████▊| 7543/7689 [18:38:57<22:11,  9.12s/it]
{'loss': 1.1067, 'grad_norm': 0.18227389978901196, 'learning_rate': 1.890574567855463e-07, 'epoch': 0.98}


 98%|█████████▊| 7545/7689 [18:39:11<19:35,  8.17s/it]

 98%|█████████▊| 7546/7689 [18:39:28<25:59, 10.90s/it]

 98%|█████████▊| 7547/7689 [18:39:35<22:53,  9.68s/it]

 98%|█████████▊| 7548/7689 [18:39:45<22:59,  9.79s/it]
{'loss': 0.9428, 'grad_norm': 0.21234392114434356, 'learning_rate': 1.7633378959535098e-07, 'epoch': 0.98}


 98%|█████████▊| 7550/7689 [18:40:06<23:19, 10.07s/it]

 98%|█████████▊| 7551/7689 [18:40:13<20:54,  9.09s/it]

 98%|█████████▊| 7552/7689 [18:40:20<19:12,  8.41s/it]
{'loss': 1.032, 'grad_norm': 0.2026291508838877, 'learning_rate': 1.664736844250947e-07, 'epoch': 0.98}


 98%|█████████▊| 7554/7689 [18:40:41<22:19,  9.92s/it]

 98%|█████████▊| 7555/7689 [18:40:49<20:40,  9.26s/it]

 98%|█████████▊| 7556/7689 [18:40:58<20:21,  9.19s/it]

 98%|█████████▊| 7557/7689 [18:41:05<18:51,  8.57s/it]
{'loss': 1.0295, 'grad_norm': 0.2161098488937711, 'learning_rate': 1.545471346164007e-07, 'epoch': 0.98}

 98%|█████████▊| 7558/7689 [18:41:13<18:06,  8.29s/it]


 98%|█████████▊| 7560/7689 [18:41:33<19:30,  9.07s/it]
{'loss': 0.951, 'grad_norm': 0.20143792486011547, 'learning_rate': 1.4760380196722213e-07, 'epoch': 0.98}


 98%|█████████▊| 7562/7689 [18:41:52<19:21,  9.14s/it]
{'loss': 0.9793, 'grad_norm': 0.20155723242873907, 'learning_rate': 1.430635026000604e-07, 'epoch': 0.98}


 98%|█████████▊| 7564/7689 [18:42:05<16:29,  7.91s/it]

 98%|█████████▊| 7565/7689 [18:42:12<15:27,  7.48s/it]

 98%|█████████▊| 7566/7689 [18:42:25<18:54,  9.23s/it]

 98%|█████████▊| 7567/7689 [18:42:34<18:12,  8.95s/it]
{'loss': 0.7774, 'grad_norm': 0.1962831948129614, 'learning_rate': 1.3202283931788018e-07, 'epoch': 0.98}


 98%|█████████▊| 7569/7689 [18:42:44<14:20,  7.17s/it]
{'loss': 1.0254, 'grad_norm': 0.20787433538781627, 'learning_rate': 1.2773061628739148e-07, 'epoch': 0.98}

 98%|█████████▊| 7570/7689 [18:42:53<14:56,  7.53s/it]


 98%|█████████▊| 7572/7689 [18:43:05<13:30,  6.93s/it]

 98%|█████████▊| 7573/7689 [18:43:21<18:12,  9.42s/it]

 99%|█████████▊| 7574/7689 [18:43:30<18:02,  9.41s/it]

 99%|█████████▊| 7575/7689 [18:43:36<15:55,  8.38s/it]

 99%|█████████▊| 7576/7689 [18:43:40<13:35,  7.22s/it]

 99%|█████████▊| 7577/7689 [18:43:48<13:27,  7.21s/it]

 99%|█████████▊| 7578/7689 [18:43:56<13:55,  7.53s/it]

 99%|█████████▊| 7579/7689 [18:44:02<13:14,  7.23s/it]

 99%|█████████▊| 7580/7689 [18:44:14<15:38,  8.61s/it]

 99%|█████████▊| 7581/7689 [18:44:20<14:09,  7.86s/it]

 99%|█████████▊| 7582/7689 [18:44:29<14:14,  7.99s/it]

 99%|█████████▊| 7583/7689 [18:44:38<14:45,  8.36s/it]
{'loss': 0.8435, 'grad_norm': 0.19926117426791126, 'learning_rate': 9.96700245991633e-08, 'epoch': 0.99}


 99%|█████████▊| 7585/7689 [18:44:54<14:56,  8.62s/it]

 99%|█████████▊| 7586/7689 [18:45:02<14:12,  8.28s/it]
{'loss': 0.8176, 'grad_norm': 0.18284554409628248, 'learning_rate': 9.41090330024319e-08, 'epoch': 0.99}


 99%|█████████▊| 7588/7689 [18:45:20<14:48,  8.80s/it]
{'loss': 0.8373, 'grad_norm': 0.18329496239832066, 'learning_rate': 9.049034193410189e-08, 'epoch': 0.99}


 99%|█████████▊| 7590/7689 [18:45:34<12:50,  7.79s/it]

 99%|█████████▊| 7591/7689 [18:45:39<11:29,  7.03s/it]

 99%|█████████▊| 7592/7689 [18:45:47<11:48,  7.30s/it]
{'loss': 0.9368, 'grad_norm': 0.2109896998395613, 'learning_rate': 8.346569931781955e-08, 'epoch': 0.99}


 99%|█████████▉| 7594/7689 [18:46:03<11:52,  7.50s/it]

 99%|█████████▉| 7595/7689 [18:46:15<13:56,  8.90s/it]

 99%|█████████▉| 7596/7689 [18:46:23<12:58,  8.38s/it]

 99%|█████████▉| 7597/7689 [18:46:34<14:16,  9.31s/it]

 99%|█████████▉| 7598/7689 [18:46:42<13:36,  8.97s/it]

 99%|█████████▉| 7599/7689 [18:46:54<14:35,  9.73s/it]

 99%|█████████▉| 7600/7689 [18:47:09<16:43, 11.28s/it]

 99%|█████████▉| 7601/7689 [18:47:19<16:05, 10.97s/it]

 99%|█████████▉| 7602/7689 [18:47:26<14:09,  9.76s/it]

 99%|█████████▉| 7603/7689 [18:47:35<13:38,  9.52s/it]

 99%|█████████▉| 7604/7689 [18:47:42<12:37,  8.92s/it]

 99%|█████████▉| 7605/7689 [18:47:51<12:35,  8.99s/it]

 99%|█████████▉| 7606/7689 [18:48:03<13:39,  9.88s/it]

 99%|█████████▉| 7607/7689 [18:48:12<13:08,  9.62s/it]

 99%|█████████▉| 7608/7689 [18:48:19<11:44,  8.69s/it]

 99%|█████████▉| 7609/7689 [18:48:38<15:54, 11.94s/it]

 99%|█████████▉| 7610/7689 [18:48:52<16:33, 12.57s/it]

 99%|█████████▉| 7611/7689 [18:49:12<19:03, 14.66s/it]

 99%|█████████▉| 7612/7689 [18:49:21<16:34, 12.92s/it]

 99%|█████████▉| 7613/7689 [18:49:26<13:27, 10.63s/it]

 99%|█████████▉| 7614/7689 [18:49:32<11:25,  9.14s/it]

 99%|█████████▉| 7615/7689 [18:49:39<10:23,  8.43s/it]

 99%|█████████▉| 7616/7689 [18:49:46<09:56,  8.18s/it]

 99%|█████████▉| 7617/7689 [18:50:04<13:23, 11.16s/it]

 99%|█████████▉| 7618/7689 [18:50:14<12:42, 10.74s/it]

 99%|█████████▉| 7619/7689 [18:50:24<12:14, 10.49s/it]

 99%|█████████▉| 7620/7689 [18:50:38<13:25, 11.67s/it]
[2024-05-25 08:21:48,786] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8508, 'grad_norm': 0.19129114325979707, 'learning_rate': 4.223695477691969e-08, 'epoch': 0.99}

 99%|█████████▉| 7621/7689 [18:50:45<11:33, 10.21s/it]


 99%|█████████▉| 7623/7689 [18:51:11<12:32, 11.41s/it]

 99%|█████████▉| 7624/7689 [18:51:20<11:39, 10.76s/it]

 99%|█████████▉| 7625/7689 [18:51:32<11:50, 11.11s/it]

 99%|█████████▉| 7626/7689 [18:51:39<10:14,  9.75s/it]

 99%|█████████▉| 7627/7689 [18:51:47<09:33,  9.25s/it]

 99%|█████████▉| 7628/7689 [18:51:53<08:34,  8.43s/it]

 99%|█████████▉| 7629/7689 [18:52:02<08:41,  8.69s/it]

 99%|█████████▉| 7630/7689 [18:52:08<07:46,  7.90s/it]
{'loss': 1.021, 'grad_norm': 0.19271445005045296, 'learning_rate': 3.088208830667183e-08, 'epoch': 0.99}


 99%|█████████▉| 7632/7689 [18:52:21<06:38,  7.00s/it]

 99%|█████████▉| 7633/7689 [18:52:30<07:01,  7.53s/it]

 99%|█████████▉| 7634/7689 [18:52:36<06:32,  7.13s/it]

 99%|█████████▉| 7635/7689 [18:52:47<07:21,  8.18s/it]

 99%|█████████▉| 7636/7689 [18:52:54<06:58,  7.90s/it]

 99%|█████████▉| 7637/7689 [18:53:05<07:37,  8.79s/it]
{'loss': 0.8851, 'grad_norm': 0.19736574239322366, 'learning_rate': 2.398911985466157e-08, 'epoch': 0.99}


 99%|█████████▉| 7639/7689 [18:53:19<06:29,  7.80s/it]
{'loss': 1.1039, 'grad_norm': 0.20222687385749272, 'learning_rate': 2.2179356710982923e-08, 'epoch': 0.99}


 99%|█████████▉| 7641/7689 [18:53:32<05:50,  7.29s/it]

 99%|█████████▉| 7642/7689 [18:53:42<06:10,  7.88s/it]

 99%|█████████▉| 7643/7689 [18:53:48<05:39,  7.38s/it]

 99%|█████████▉| 7644/7689 [18:53:56<05:45,  7.68s/it]

 99%|█████████▉| 7645/7689 [18:54:01<04:59,  6.81s/it]

 99%|█████████▉| 7646/7689 [18:54:20<07:31, 10.49s/it]
{'loss': 1.0195, 'grad_norm': 0.2064430798305365, 'learning_rate': 1.6404010131931823e-08, 'epoch': 0.99}


 99%|█████████▉| 7648/7689 [18:54:33<05:42,  8.36s/it]
{'loss': 0.997, 'grad_norm': 0.19663578251307212, 'learning_rate': 1.491358006155963e-08, 'epoch': 0.99}


 99%|█████████▉| 7650/7689 [18:54:49<05:23,  8.29s/it]

100%|█████████▉| 7651/7689 [18:54:57<05:11,  8.20s/it]

100%|█████████▉| 7652/7689 [18:55:11<06:12, 10.06s/it]
{'loss': 0.7941, 'grad_norm': 0.17243314020984202, 'learning_rate': 1.2145618855952112e-08, 'epoch': 1.0}

100%|█████████▉| 7653/7689 [18:55:24<06:24, 10.69s/it]

100%|█████████▉| 7654/7689 [18:55:36<06:34, 11.28s/it]


100%|█████████▉| 7656/7689 [18:55:50<04:56,  9.00s/it]

100%|█████████▉| 7657/7689 [18:55:57<04:29,  8.43s/it]

100%|█████████▉| 7658/7689 [18:56:05<04:14,  8.22s/it]
{'loss': 1.1054, 'grad_norm': 0.19266168375552054, 'learning_rate': 8.525938742098838e-09, 'epoch': 1.0}

100%|█████████▉| 7659/7689 [18:56:10<03:37,  7.26s/it]

100%|█████████▉| 7660/7689 [18:56:18<03:36,  7.46s/it]


100%|█████████▉| 7662/7689 [18:56:32<03:22,  7.48s/it]

100%|█████████▉| 7663/7689 [18:56:40<03:18,  7.62s/it]
{'loss': 1.217, 'grad_norm': 0.20114582263350916, 'learning_rate': 5.997459811291073e-09, 'epoch': 1.0}

100%|█████████▉| 7664/7689 [18:56:50<03:23,  8.12s/it]


100%|█████████▉| 7666/7689 [18:57:09<03:28,  9.06s/it]
{'loss': 1.0672, 'grad_norm': 0.1879471761191492, 'learning_rate': 4.69328866340435e-09, 'epoch': 1.0}


100%|█████████▉| 7668/7689 [18:57:28<03:19,  9.49s/it]

100%|█████████▉| 7669/7689 [18:57:37<03:03,  9.16s/it]
{'loss': 0.7941, 'grad_norm': 0.21145221918223325, 'learning_rate': 3.5488072709588536e-09, 'epoch': 1.0}

100%|█████████▉| 7670/7689 [18:57:44<02:44,  8.64s/it]


100%|█████████▉| 7672/7689 [18:58:11<03:04, 10.85s/it]

100%|█████████▉| 7673/7689 [18:58:23<03:00, 11.27s/it]
{'loss': 1.0364, 'grad_norm': 0.22200573184245898, 'learning_rate': 2.2712414895598166e-09, 'epoch': 1.0}


100%|█████████▉| 7675/7689 [18:58:41<02:18,  9.87s/it]

100%|█████████▉| 7676/7689 [18:58:47<01:54,  8.79s/it]

100%|█████████▉| 7677/7689 [18:58:54<01:38,  8.20s/it]
{'loss': 0.8798, 'grad_norm': 0.22204982930322706, 'learning_rate': 1.2775754536953343e-09, 'epoch': 1.0}

100%|█████████▉| 7678/7689 [18:59:00<01:23,  7.56s/it]


100%|█████████▉| 7680/7689 [18:59:29<01:39, 11.09s/it]

100%|█████████▉| 7681/7689 [18:59:38<01:22, 10.29s/it]
{'loss': 1.0014, 'grad_norm': 0.20426004499890513, 'learning_rate': 5.678119844421126e-10, 'epoch': 1.0}

100%|█████████▉| 7682/7689 [18:59:46<01:07,  9.61s/it]


100%|█████████▉| 7684/7689 [19:00:07<00:49,  9.90s/it]

100%|█████████▉| 7685/7689 [19:00:13<00:35,  8.96s/it]
{'loss': 1.0875, 'grad_norm': 0.21770359211395363, 'learning_rate': 1.419530968660432e-10, 'epoch': 1.0}


100%|█████████▉| 7687/7689 [19:00:40<00:22, 11.50s/it]
{'loss': 0.8797, 'grad_norm': 0.20099620276310204, 'learning_rate': 3.5488280514250904e-11, 'epoch': 1.0}


100%|██████████| 7689/7689 [19:00:57<00:00,  8.90s/it]
{'loss': 0.8884, 'grad_norm': 0.18549102455488672, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 68473.5767, 'train_samples_per_second': 7.186, 'train_steps_per_second': 0.112, 'train_loss': 1.0114940399233054, 'epoch': 1.0}
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(