/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3107 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3107 [00:51<44:38:40, 51.75s/it]

  0%|          | 2/3107 [00:58<21:45:22, 25.22s/it]

  0%|          | 3/3107 [01:04<14:13:27, 16.50s/it]

  0%|          | 4/3107 [01:11<11:04:32, 12.85s/it]

  0%|          | 5/3107 [01:21<9:59:00, 11.59s/it]

  0%|          | 6/3107 [01:26<8:17:21,  9.62s/it]
{'loss': 1.5254, 'grad_norm': 0.5643815697220281, 'learning_rate': 1.2765957446808511e-05, 'epoch': 0.0}


  0%|          | 8/3107 [01:42<7:28:33,  8.68s/it]

  0%|          | 9/3107 [01:50<7:23:57,  8.60s/it]

  0%|          | 10/3107 [02:03<8:24:53,  9.78s/it]

  0%|          | 11/3107 [02:10<7:50:27,  9.12s/it]

  0%|          | 12/3107 [02:16<7:02:49,  8.20s/it]

  0%|          | 13/3107 [02:23<6:36:12,  7.68s/it]
{'loss': 1.3888, 'grad_norm': 0.4234436223221927, 'learning_rate': 2.765957446808511e-05, 'epoch': 0.0}


  0%|          | 15/3107 [02:35<5:46:58,  6.73s/it]

  1%|          | 16/3107 [02:43<6:17:59,  7.34s/it]

  1%|          | 17/3107 [02:50<6:05:10,  7.09s/it]

  1%|          | 18/3107 [02:58<6:11:19,  7.21s/it]

  1%|          | 19/3107 [03:03<5:45:31,  6.71s/it]

  1%|          | 20/3107 [03:10<5:52:30,  6.85s/it]

  1%|          | 21/3107 [03:17<5:53:31,  6.87s/it]

  1%|          | 22/3107 [03:24<5:49:55,  6.81s/it]

  1%|          | 23/3107 [03:32<6:09:11,  7.18s/it]

  1%|          | 24/3107 [03:42<7:01:53,  8.21s/it]

  1%|          | 25/3107 [03:49<6:28:18,  7.56s/it]

  1%|          | 26/3107 [03:58<7:01:10,  8.20s/it]

  1%|          | 27/3107 [04:05<6:47:09,  7.93s/it]

  1%|          | 28/3107 [04:14<6:48:30,  7.96s/it]

  1%|          | 29/3107 [04:24<7:30:48,  8.79s/it]

  1%|          | 30/3107 [04:32<7:15:08,  8.48s/it]

  1%|          | 31/3107 [04:40<7:04:01,  8.27s/it]

  1%|          | 32/3107 [04:48<6:58:10,  8.16s/it]

  1%|          | 33/3107 [04:55<6:43:25,  7.87s/it]
{'loss': 1.2314, 'grad_norm': 0.17093697762031415, 'learning_rate': 7.021276595744681e-05, 'epoch': 0.01}


  1%|          | 35/3107 [05:10<6:36:42,  7.75s/it]
{'loss': 1.2197, 'grad_norm': 0.1811018893521435, 'learning_rate': 7.446808510638298e-05, 'epoch': 0.01}


  1%|          | 37/3107 [05:23<5:59:24,  7.02s/it]

  1%|          | 38/3107 [05:32<6:31:28,  7.65s/it]

  1%|▏         | 39/3107 [05:39<6:08:14,  7.20s/it]

  1%|▏         | 40/3107 [05:49<6:53:23,  8.09s/it]

  1%|▏         | 41/3107 [05:57<6:59:41,  8.21s/it]

  1%|▏         | 42/3107 [06:04<6:43:50,  7.91s/it]

  1%|▏         | 43/3107 [06:10<6:13:04,  7.31s/it]

  1%|▏         | 44/3107 [06:16<5:42:55,  6.72s/it]

  1%|▏         | 45/3107 [06:24<6:09:55,  7.25s/it]

  1%|▏         | 46/3107 [06:31<6:05:43,  7.17s/it]

  2%|▏         | 47/3107 [06:40<6:31:11,  7.67s/it]

  2%|▏         | 48/3107 [06:47<6:27:12,  7.59s/it]

  2%|▏         | 49/3107 [06:53<6:01:35,  7.09s/it]

  2%|▏         | 50/3107 [07:05<7:14:09,  8.52s/it]

  2%|▏         | 51/3107 [07:11<6:28:44,  7.63s/it]
{'loss': 1.2325, 'grad_norm': 0.15723066156206753, 'learning_rate': 0.00010851063829787234, 'epoch': 0.02}


  2%|▏         | 53/3107 [07:26<6:24:38,  7.56s/it]

  2%|▏         | 54/3107 [07:35<6:39:15,  7.85s/it]

  2%|▏         | 55/3107 [07:40<6:02:55,  7.13s/it]

  2%|▏         | 56/3107 [07:47<5:55:28,  6.99s/it]
{'loss': 1.1286, 'grad_norm': 0.1614516847199561, 'learning_rate': 0.00011914893617021277, 'epoch': 0.02}


  2%|▏         | 58/3107 [08:03<6:24:29,  7.57s/it]

  2%|▏         | 59/3107 [08:11<6:26:54,  7.62s/it]

  2%|▏         | 60/3107 [08:17<6:09:20,  7.27s/it]

  2%|▏         | 61/3107 [08:24<5:50:41,  6.91s/it]
{'loss': 1.1854, 'grad_norm': 0.1729234890233042, 'learning_rate': 0.00012978723404255318, 'epoch': 0.02}


  2%|▏         | 63/3107 [08:37<5:45:25,  6.81s/it]

  2%|▏         | 64/3107 [08:48<6:53:10,  8.15s/it]

  2%|▏         | 65/3107 [08:53<6:10:20,  7.30s/it]

  2%|▏         | 66/3107 [08:59<5:41:18,  6.73s/it]
{'loss': 1.1381, 'grad_norm': 0.17420514772528095, 'learning_rate': 0.00014042553191489363, 'epoch': 0.02}


  2%|▏         | 68/3107 [09:16<6:34:32,  7.79s/it]

  2%|▏         | 69/3107 [09:23<6:10:50,  7.32s/it]

  2%|▏         | 70/3107 [09:30<6:09:52,  7.31s/it]

  2%|▏         | 71/3107 [09:37<6:04:16,  7.20s/it]

  2%|▏         | 72/3107 [09:47<6:51:38,  8.14s/it]

  2%|▏         | 73/3107 [10:00<7:55:45,  9.41s/it]
{'loss': 1.1201, 'grad_norm': 0.15955867697982243, 'learning_rate': 0.0001553191489361702, 'epoch': 0.02}


  2%|▏         | 75/3107 [10:24<8:51:50, 10.52s/it]

  2%|▏         | 76/3107 [10:30<7:48:34,  9.28s/it]

  2%|▏         | 77/3107 [10:36<6:58:35,  8.29s/it]

  3%|▎         | 78/3107 [10:42<6:18:43,  7.50s/it]
{'loss': 1.1134, 'grad_norm': 0.18419428053894207, 'learning_rate': 0.00016595744680851065, 'epoch': 0.03}

  3%|▎         | 79/3107 [10:48<5:52:27,  6.98s/it]


  3%|▎         | 81/3107 [11:04<6:26:47,  7.67s/it]

  3%|▎         | 82/3107 [11:10<6:02:48,  7.20s/it]

  3%|▎         | 83/3107 [11:16<5:45:30,  6.86s/it]

  3%|▎         | 84/3107 [11:22<5:29:50,  6.55s/it]

  3%|▎         | 85/3107 [11:29<5:26:02,  6.47s/it]

  3%|▎         | 86/3107 [11:39<6:22:56,  7.61s/it]

  3%|▎         | 87/3107 [11:45<5:57:00,  7.09s/it]

  3%|▎         | 88/3107 [11:52<6:03:38,  7.23s/it]

  3%|▎         | 89/3107 [11:58<5:46:32,  6.89s/it]
{'loss': 1.1941, 'grad_norm': 0.18745886029173628, 'learning_rate': 0.00018936170212765957, 'epoch': 0.03}

  3%|▎         | 90/3107 [12:08<6:23:04,  7.62s/it]

  3%|▎         | 91/3107 [12:14<6:01:26,  7.19s/it]

  3%|▎         | 92/3107 [12:24<6:42:39,  8.01s/it]

  3%|▎         | 93/3107 [12:30<6:09:06,  7.35s/it]

  3%|▎         | 94/3107 [12:40<6:51:03,  8.19s/it]


  3%|▎         | 96/3107 [12:53<6:09:21,  7.36s/it]

  3%|▎         | 97/3107 [12:59<5:45:07,  6.88s/it]

  3%|▎         | 98/3107 [13:07<6:06:26,  7.31s/it]

  3%|▎         | 99/3107 [13:19<7:14:42,  8.67s/it]
{'loss': 1.0537, 'grad_norm': 0.1711567037176047, 'learning_rate': 0.00019999864102799163, 'epoch': 0.03}

  3%|▎         | 100/3107 [13:28<7:10:07,  8.58s/it]

  3%|▎         | 101/3107 [13:34<6:35:53,  7.90s/it]


  3%|▎         | 103/3107 [13:47<5:57:37,  7.14s/it]

  3%|▎         | 104/3107 [13:53<5:44:12,  6.88s/it]
{'loss': 1.0664, 'grad_norm': 0.14862599477817412, 'learning_rate': 0.0001999945641489025, 'epoch': 0.03}


  3%|▎         | 106/3107 [14:07<5:47:13,  6.94s/it]
{'loss': 1.1549, 'grad_norm': 0.17282617287945845, 'learning_rate': 0.00019999217240562308, 'epoch': 0.03}

  3%|▎         | 107/3107 [14:14<5:45:56,  6.92s/it]


  4%|▎         | 109/3107 [14:33<7:04:44,  8.50s/it]

  4%|▎         | 110/3107 [14:40<6:40:53,  8.03s/it]

  4%|▎         | 111/3107 [14:47<6:20:34,  7.62s/it]
{'loss': 1.0457, 'grad_norm': 0.15582399093462568, 'learning_rate': 0.00019998429065932381, 'epoch': 0.04}

  4%|▎         | 112/3107 [14:54<6:13:26,  7.48s/it]

  4%|▎         | 113/3107 [15:02<6:15:15,  7.52s/it]


  4%|▎         | 115/3107 [15:15<5:50:09,  7.02s/it]

  4%|▎         | 116/3107 [15:21<5:42:41,  6.87s/it]

  4%|▍         | 117/3107 [15:30<6:18:28,  7.59s/it]

  4%|▍         | 118/3107 [15:39<6:27:54,  7.79s/it]
{'loss': 1.0531, 'grad_norm': 0.1852603981725985, 'learning_rate': 0.00019996869084791684, 'epoch': 0.04}

  4%|▍         | 119/3107 [15:46<6:25:25,  7.74s/it]

  4%|▍         | 120/3107 [15:52<5:58:31,  7.20s/it]


  4%|▍         | 122/3107 [16:05<5:34:49,  6.73s/it]

  4%|▍         | 123/3107 [16:13<5:45:12,  6.94s/it]
{'loss': 0.9741, 'grad_norm': 0.15312243516640986, 'learning_rate': 0.00019995428756122902, 'epoch': 0.04}


  4%|▍         | 125/3107 [16:28<5:51:30,  7.07s/it]

  4%|▍         | 126/3107 [16:35<5:53:06,  7.11s/it]

  4%|▍         | 127/3107 [16:44<6:19:14,  7.64s/it]

  4%|▍         | 128/3107 [16:53<6:43:29,  8.13s/it]

  4%|▍         | 129/3107 [16:59<6:14:14,  7.54s/it]

  4%|▍         | 130/3107 [17:05<5:48:56,  7.03s/it]
{'loss': 1.0979, 'grad_norm': 0.18063670260447737, 'learning_rate': 0.0001999295590029636, 'epoch': 0.04}


  4%|▍         | 132/3107 [17:17<5:24:47,  6.55s/it]
{'loss': 1.1921, 'grad_norm': 0.16729482141255278, 'learning_rate': 0.00019992151586742236, 'epoch': 0.04}


  4%|▍         | 134/3107 [17:31<5:36:32,  6.79s/it]
{'loss': 1.1337, 'grad_norm': 0.16569368803644383, 'learning_rate': 0.00019991303820131645, 'epoch': 0.04}

  4%|▍         | 135/3107 [17:38<5:34:10,  6.75s/it]


  4%|▍         | 137/3107 [17:55<6:12:44,  7.53s/it]

  4%|▍         | 138/3107 [18:03<6:22:51,  7.74s/it]
{'loss': 1.1223, 'grad_norm': 0.17450008782942142, 'learning_rate': 0.00019989477942676818, 'epoch': 0.04}

  4%|▍         | 139/3107 [18:10<6:12:51,  7.54s/it]


  5%|▍         | 141/3107 [18:29<7:02:09,  8.54s/it]

  5%|▍         | 142/3107 [18:35<6:24:42,  7.78s/it]

  5%|▍         | 143/3107 [18:41<6:06:27,  7.42s/it]
{'loss': 1.0085, 'grad_norm': 0.17450423545557328, 'learning_rate': 0.0001998695124209257, 'epoch': 0.05}


  5%|▍         | 145/3107 [18:55<5:49:01,  7.07s/it]
{'loss': 1.1364, 'grad_norm': 0.1627690271085447, 'learning_rate': 0.00019985864554636956, 'epoch': 0.05}

  5%|▍         | 146/3107 [19:02<5:51:19,  7.12s/it]

  5%|▍         | 147/3107 [19:08<5:39:22,  6.88s/it]


  5%|▍         | 149/3107 [19:25<6:15:04,  7.61s/it]

  5%|▍         | 150/3107 [19:33<6:19:04,  7.69s/it]

  5%|▍         | 151/3107 [19:40<5:58:35,  7.28s/it]
{'loss': 1.0278, 'grad_norm': 0.1779041876406507, 'learning_rate': 0.00019982343957821526, 'epoch': 0.05}

  5%|▍         | 152/3107 [19:46<5:51:15,  7.13s/it]


  5%|▍         | 154/3107 [19:59<5:37:38,  6.86s/it]

  5%|▍         | 155/3107 [20:07<5:41:59,  6.95s/it]
{'loss': 1.0771, 'grad_norm': 0.1724812141504736, 'learning_rate': 0.0001997977983274304, 'epoch': 0.05}

  5%|▌         | 156/3107 [20:14<5:47:24,  7.06s/it]


  5%|▌         | 158/3107 [20:27<5:33:15,  6.78s/it]

  5%|▌         | 159/3107 [20:33<5:23:01,  6.57s/it]
{'loss': 1.1828, 'grad_norm': 0.1609672102607586, 'learning_rate': 0.00019977042110832537, 'epoch': 0.05}


  5%|▌         | 161/3107 [20:47<5:28:14,  6.69s/it]

  5%|▌         | 162/3107 [20:53<5:21:13,  6.54s/it]
{'loss': 0.9886, 'grad_norm': 0.1652683061152913, 'learning_rate': 0.0001997487492501292, 'epoch': 0.05}


  5%|▌         | 164/3107 [21:08<5:42:06,  6.97s/it]
{'loss': 1.1005, 'grad_norm': 0.18729841463200217, 'learning_rate': 0.00019973375910736408, 'epoch': 0.05}

  5%|▌         | 165/3107 [21:14<5:35:14,  6.84s/it]


  5%|▌         | 167/3107 [21:35<7:12:20,  8.82s/it]

  5%|▌         | 168/3107 [21:44<7:14:36,  8.87s/it]

  5%|▌         | 169/3107 [21:52<7:00:43,  8.59s/it]

  5%|▌         | 170/3107 [22:02<7:20:25,  9.00s/it]

  6%|▌         | 171/3107 [22:09<7:02:01,  8.62s/it]

  6%|▌         | 172/3107 [22:15<6:20:32,  7.78s/it]

  6%|▌         | 173/3107 [22:25<6:48:19,  8.35s/it]

  6%|▌         | 174/3107 [22:34<6:56:47,  8.53s/it]
{'loss': 1.0654, 'grad_norm': 0.15697010176961357, 'learning_rate': 0.00019965230405235443, 'epoch': 0.06}

  6%|▌         | 175/3107 [22:42<6:55:04,  8.49s/it]

  6%|▌         | 176/3107 [22:52<7:18:40,  8.98s/it]

  6%|▌         | 177/3107 [22:58<6:30:45,  8.00s/it]

  6%|▌         | 178/3107 [23:04<6:02:50,  7.43s/it]


  6%|▌         | 180/3107 [23:19<6:08:09,  7.55s/it]

  6%|▌         | 181/3107 [23:27<6:11:24,  7.62s/it]
{'loss': 1.2169, 'grad_norm': 0.1644378674251842, 'learning_rate': 0.0001995888387680993, 'epoch': 0.06}


  6%|▌         | 183/3107 [23:40<5:46:21,  7.11s/it]

  6%|▌         | 184/3107 [23:48<5:58:30,  7.36s/it]
{'loss': 0.9349, 'grad_norm': 0.16071975590300247, 'learning_rate': 0.0001995600150956177, 'epoch': 0.06}

  6%|▌         | 185/3107 [23:57<6:18:19,  7.77s/it]

  6%|▌         | 186/3107 [24:03<5:54:19,  7.28s/it]


  6%|▌         | 188/3107 [24:16<5:40:59,  7.01s/it]

  6%|▌         | 189/3107 [24:23<5:45:47,  7.11s/it]

  6%|▌         | 190/3107 [24:29<5:26:42,  6.72s/it]
{'loss': 1.0399, 'grad_norm': 0.16800392547799087, 'learning_rate': 0.00019949944557366706, 'epoch': 0.06}

  6%|▌         | 191/3107 [24:39<6:10:33,  7.62s/it]


  6%|▌         | 193/3107 [24:51<5:34:55,  6.90s/it]
{'loss': 1.0106, 'grad_norm': 0.17083808201757258, 'learning_rate': 0.00019946770031684728, 'epoch': 0.06}

  6%|▌         | 194/3107 [24:59<5:43:36,  7.08s/it]


  6%|▋         | 196/3107 [25:18<6:41:20,  8.27s/it]
{'loss': 0.9253, 'grad_norm': 0.15092137774025288, 'learning_rate': 0.00019943498180710937, 'epoch': 0.06}


  6%|▋         | 198/3107 [25:33<6:29:40,  8.04s/it]

  6%|▋         | 199/3107 [25:40<6:15:52,  7.76s/it]

  6%|▋         | 200/3107 [25:47<6:02:18,  7.48s/it]

  6%|▋         | 201/3107 [25:53<5:47:44,  7.18s/it]

  7%|▋         | 202/3107 [26:00<5:31:36,  6.85s/it]

  7%|▋         | 203/3107 [26:09<6:14:58,  7.75s/it]

  7%|▋         | 204/3107 [26:15<5:45:29,  7.14s/it]
{'loss': 1.0291, 'grad_norm': 0.1686412033590879, 'learning_rate': 0.0001993429767872239, 'epoch': 0.07}

  7%|▋         | 205/3107 [26:24<6:16:15,  7.78s/it]

  7%|▋         | 206/3107 [26:32<6:20:21,  7.87s/it]

  7%|▋         | 207/3107 [26:41<6:27:42,  8.02s/it]


  7%|▋         | 209/3107 [26:54<5:59:17,  7.44s/it]
{'loss': 1.0249, 'grad_norm': 0.1764344046680905, 'learning_rate': 0.00019928196312336285, 'epoch': 0.07}


  7%|▋         | 211/3107 [27:09<6:06:08,  7.59s/it]

  7%|▋         | 212/3107 [27:16<5:57:29,  7.41s/it]

  7%|▋         | 213/3107 [27:22<5:31:52,  6.88s/it]

  7%|▋         | 214/3107 [27:29<5:35:51,  6.97s/it]

  7%|▋         | 215/3107 [27:37<5:52:10,  7.31s/it]

  7%|▋         | 216/3107 [27:46<6:14:11,  7.77s/it]

  7%|▋         | 217/3107 [27:52<5:44:50,  7.16s/it]

  7%|▋         | 218/3107 [28:02<6:30:33,  8.11s/it]

  7%|▋         | 219/3107 [28:11<6:44:30,  8.40s/it]

  7%|▋         | 220/3107 [28:18<6:25:00,  8.00s/it]

  7%|▋         | 221/3107 [28:24<5:50:42,  7.29s/it]

  7%|▋         | 222/3107 [28:29<5:24:47,  6.75s/it]

  7%|▋         | 223/3107 [28:35<5:09:52,  6.45s/it]

  7%|▋         | 224/3107 [28:42<5:10:55,  6.47s/it]
{'loss': 1.0332, 'grad_norm': 0.18951438847571395, 'learning_rate': 0.00019908273856265152, 'epoch': 0.07}


  7%|▋         | 226/3107 [28:57<5:47:53,  7.25s/it]

  7%|▋         | 227/3107 [29:06<6:08:34,  7.68s/it]

  7%|▋         | 228/3107 [29:14<6:18:39,  7.89s/it]

  7%|▋         | 229/3107 [29:24<6:35:40,  8.25s/it]

  7%|▋         | 230/3107 [29:30<6:09:01,  7.70s/it]

  7%|▋         | 231/3107 [29:39<6:35:13,  8.25s/it]
{'loss': 0.9047, 'grad_norm': 0.1821136453091556, 'learning_rate': 0.000198981469582687, 'epoch': 0.07}

  7%|▋         | 232/3107 [29:47<6:23:43,  8.01s/it]

  7%|▋         | 233/3107 [29:53<5:50:11,  7.31s/it]

  8%|▊         | 234/3107 [29:59<5:33:01,  6.96s/it]


  8%|▊         | 236/3107 [30:14<5:40:01,  7.11s/it]

  8%|▊         | 237/3107 [30:19<5:21:38,  6.72s/it]

  8%|▊         | 238/3107 [30:28<5:54:37,  7.42s/it]
{'loss': 0.9915, 'grad_norm': 0.163990165944884, 'learning_rate': 0.00019887492770276433, 'epoch': 0.08}


  8%|▊         | 240/3107 [30:44<6:03:14,  7.60s/it]
{'loss': 0.8572, 'grad_norm': 0.1718944066523694, 'learning_rate': 0.00019884351946106073, 'epoch': 0.08}

  8%|▊         | 241/3107 [30:53<6:30:10,  8.17s/it]


  8%|▊         | 243/3107 [31:08<6:25:49,  8.08s/it]
{'loss': 0.9137, 'grad_norm': 0.1411491536755054, 'learning_rate': 0.0001987956011867091, 'epoch': 0.08}


  8%|▊         | 245/3107 [31:27<6:55:11,  8.70s/it]
{'loss': 1.0965, 'grad_norm': 0.15780594476083937, 'learning_rate': 0.0001987631185985386, 'epoch': 0.08}

  8%|▊         | 246/3107 [31:37<7:08:10,  8.98s/it]


  8%|▊         | 248/3107 [31:53<6:50:15,  8.61s/it]

  8%|▊         | 249/3107 [32:00<6:23:14,  8.05s/it]

  8%|▊         | 250/3107 [32:10<6:43:35,  8.48s/it]

  8%|▊         | 251/3107 [32:16<6:10:16,  7.78s/it]

  8%|▊         | 252/3107 [32:24<6:18:17,  7.95s/it]

  8%|▊         | 253/3107 [32:32<6:13:14,  7.85s/it]

  8%|▊         | 254/3107 [32:40<6:13:44,  7.86s/it]
{'loss': 1.0804, 'grad_norm': 0.14898061648801159, 'learning_rate': 0.00019861163405885787, 'epoch': 0.08}


  8%|▊         | 256/3107 [32:58<6:57:50,  8.79s/it]

  8%|▊         | 257/3107 [33:07<6:53:43,  8.71s/it]

  8%|▊         | 258/3107 [33:15<6:53:26,  8.71s/it]
{'loss': 1.0933, 'grad_norm': 0.17008573833286994, 'learning_rate': 0.00019854151937280843, 'epoch': 0.08}


  8%|▊         | 260/3107 [33:31<6:18:55,  7.99s/it]

  8%|▊         | 261/3107 [33:36<5:43:33,  7.24s/it]

  8%|▊         | 262/3107 [33:42<5:21:54,  6.79s/it]

  8%|▊         | 263/3107 [33:52<6:11:18,  7.83s/it]

  8%|▊         | 264/3107 [33:58<5:48:02,  7.35s/it]
{'loss': 1.0754, 'grad_norm': 0.16883319768330798, 'learning_rate': 0.00019843313376640732, 'epoch': 0.08}

  9%|▊         | 265/3107 [34:05<5:35:30,  7.08s/it]


  9%|▊         | 267/3107 [34:24<6:30:20,  8.25s/it]
{'loss': 1.0069, 'grad_norm': 0.15719826276106977, 'learning_rate': 0.00019837749600059386, 'epoch': 0.09}


  9%|▊         | 269/3107 [34:38<5:56:16,  7.53s/it]

  9%|▊         | 270/3107 [34:44<5:44:36,  7.29s/it]
{'loss': 0.9613, 'grad_norm': 0.14826169084954832, 'learning_rate': 0.00019832089564908914, 'epoch': 0.09}


  9%|▉         | 272/3107 [34:58<5:31:15,  7.01s/it]

  9%|▉         | 273/3107 [35:08<6:17:54,  8.00s/it]

  9%|▉         | 274/3107 [35:22<7:40:04,  9.74s/it]

  9%|▉         | 275/3107 [35:30<7:18:46,  9.30s/it]
{'loss': 1.0332, 'grad_norm': 0.1612165430737275, 'learning_rate': 0.00019822442415904406, 'epoch': 0.09}


  9%|▉         | 277/3107 [35:46<6:47:05,  8.63s/it]

  9%|▉         | 278/3107 [35:52<6:05:15,  7.75s/it]

  9%|▉         | 279/3107 [36:01<6:18:29,  8.03s/it]

  9%|▉         | 280/3107 [36:08<6:07:17,  7.80s/it]

  9%|▉         | 281/3107 [36:16<6:03:43,  7.72s/it]

  9%|▉         | 282/3107 [36:24<6:15:35,  7.98s/it]

  9%|▉         | 283/3107 [36:31<5:58:51,  7.62s/it]
{'loss': 1.1905, 'grad_norm': 0.17251795225783098, 'learning_rate': 0.00019806451794840215, 'epoch': 0.09}


  9%|▉         | 285/3107 [36:46<6:00:42,  7.67s/it]

  9%|▉         | 286/3107 [36:52<5:32:18,  7.07s/it]

  9%|▉         | 287/3107 [36:59<5:29:25,  7.01s/it]
{'loss': 1.1983, 'grad_norm': 0.16452682610498018, 'learning_rate': 0.000197982005413034, 'epoch': 0.09}


  9%|▉         | 289/3107 [37:17<6:13:24,  7.95s/it]
{'loss': 0.9842, 'grad_norm': 0.15893436054074564, 'learning_rate': 0.00019794010991091164, 'epoch': 0.09}


  9%|▉         | 291/3107 [37:37<7:07:39,  9.11s/it]
{'loss': 0.8517, 'grad_norm': 0.152359535288329, 'learning_rate': 0.00019789778849480175, 'epoch': 0.09}


  9%|▉         | 293/3107 [37:58<7:30:58,  9.62s/it]

  9%|▉         | 294/3107 [38:03<6:31:24,  8.35s/it]

  9%|▉         | 295/3107 [38:13<6:46:46,  8.68s/it]

 10%|▉         | 296/3107 [38:20<6:29:11,  8.31s/it]

 10%|▉         | 297/3107 [38:27<6:09:41,  7.89s/it]

 10%|▉         | 298/3107 [38:35<6:14:19,  8.00s/it]
{'loss': 1.0947, 'grad_norm': 0.16962926470173925, 'learning_rate': 0.00019774631213960347, 'epoch': 0.1}


 10%|▉         | 300/3107 [38:48<5:34:53,  7.16s/it]
 10%|▉         | 300/3107 [38:48<5:34:53,  7.16s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1384, 'grad_norm': 0.15319825449348462, 'learning_rate': 0.00019767979921075866, 'epoch': 0.1}
 10%|▉         | 301/3107 [39:17<10:49:57, 13.90s/it]


 10%|▉         | 303/3107 [39:35<8:42:16, 11.18s/it]
{'loss': 1.1064, 'grad_norm': 0.15982735319534602, 'learning_rate': 0.00019763492624041615, 'epoch': 0.1}


 10%|▉         | 305/3107 [39:53<7:39:47,  9.85s/it]

 10%|▉         | 306/3107 [39:59<6:46:24,  8.71s/it]

 10%|▉         | 307/3107 [40:07<6:37:57,  8.53s/it]

 10%|▉         | 308/3107 [40:14<6:22:03,  8.19s/it]
{'loss': 1.0352, 'grad_norm': 0.17040070931091375, 'learning_rate': 0.0001975208866785928, 'epoch': 0.1}


 10%|▉         | 310/3107 [40:31<6:31:00,  8.39s/it]

 10%|█         | 311/3107 [40:39<6:21:28,  8.19s/it]

 10%|█         | 312/3107 [40:44<5:44:32,  7.40s/it]
{'loss': 1.0701, 'grad_norm': 0.16192766819911567, 'learning_rate': 0.00019742774647250002, 'epoch': 0.1}

 10%|█         | 313/3107 [40:51<5:42:02,  7.35s/it]


 10%|█         | 315/3107 [41:05<5:34:35,  7.19s/it]

 10%|█         | 316/3107 [41:13<5:41:05,  7.33s/it]

 10%|█         | 317/3107 [41:20<5:39:56,  7.31s/it]

 10%|█         | 318/3107 [41:26<5:23:48,  6.97s/it]

 10%|█         | 319/3107 [41:33<5:17:35,  6.83s/it]
{'loss': 0.9567, 'grad_norm': 0.15292668008788995, 'learning_rate': 0.00019726067413109347, 'epoch': 0.1}


 10%|█         | 321/3107 [41:47<5:27:44,  7.06s/it]

 10%|█         | 322/3107 [41:55<5:38:43,  7.30s/it]

 10%|█         | 323/3107 [42:03<5:41:25,  7.36s/it]

 10%|█         | 324/3107 [42:09<5:22:48,  6.96s/it]
{'loss': 1.1916, 'grad_norm': 0.16649645080228465, 'learning_rate': 0.0001971381640325756, 'epoch': 0.1}

 10%|█         | 325/3107 [42:16<5:25:23,  7.02s/it]


 11%|█         | 327/3107 [42:34<6:07:44,  7.94s/it]

 11%|█         | 328/3107 [42:40<5:40:47,  7.36s/it]

 11%|█         | 329/3107 [42:46<5:17:11,  6.85s/it]

 11%|█         | 330/3107 [42:53<5:23:36,  6.99s/it]

 11%|█         | 331/3107 [43:01<5:38:09,  7.31s/it]

 11%|█         | 332/3107 [43:10<5:57:33,  7.73s/it]
{'loss': 0.9438, 'grad_norm': 0.16343932586279694, 'learning_rate': 0.00019693665775228635, 'epoch': 0.11}


 11%|█         | 334/3107 [43:23<5:27:27,  7.09s/it]

 11%|█         | 335/3107 [43:30<5:28:13,  7.10s/it]

 11%|█         | 336/3107 [43:38<5:36:54,  7.30s/it]
{'loss': 1.0775, 'grad_norm': 0.1634975788572058, 'learning_rate': 0.0001968333744297198, 'epoch': 0.11}

 11%|█         | 337/3107 [43:46<5:39:13,  7.35s/it]


 11%|█         | 339/3107 [43:59<5:23:12,  7.01s/it]

 11%|█         | 340/3107 [44:06<5:30:42,  7.17s/it]

 11%|█         | 341/3107 [44:13<5:17:04,  6.88s/it]
{'loss': 0.9913, 'grad_norm': 0.16099914547266028, 'learning_rate': 0.00019670190179869345, 'epoch': 0.11}

 11%|█         | 342/3107 [44:22<5:51:46,  7.63s/it]

 11%|█         | 343/3107 [44:32<6:25:13,  8.36s/it]


 11%|█         | 345/3107 [44:47<6:06:48,  7.97s/it]

 11%|█         | 346/3107 [44:53<5:42:32,  7.44s/it]

 11%|█         | 347/3107 [45:00<5:42:44,  7.45s/it]

 11%|█         | 348/3107 [45:09<6:00:06,  7.83s/it]
{'loss': 0.908, 'grad_norm': 0.1573990749664681, 'learning_rate': 0.0001965134253793203, 'epoch': 0.11}


 11%|█▏        | 350/3107 [45:27<6:25:36,  8.39s/it]

 11%|█▏        | 351/3107 [45:34<6:06:48,  7.99s/it]

 11%|█▏        | 352/3107 [45:42<5:53:43,  7.70s/it]

 11%|█▏        | 353/3107 [45:50<5:57:48,  7.80s/it]

 11%|█▏        | 354/3107 [45:57<5:51:05,  7.65s/it]

 11%|█▏        | 355/3107 [46:03<5:36:29,  7.34s/it]
{'loss': 1.0853, 'grad_norm': 0.17341732838199436, 'learning_rate': 0.0001963198075366202, 'epoch': 0.11}

 11%|█▏        | 356/3107 [46:10<5:23:47,  7.06s/it]


 12%|█▏        | 358/3107 [46:27<5:56:49,  7.79s/it]

 12%|█▏        | 359/3107 [46:33<5:28:59,  7.18s/it]

 12%|█▏        | 360/3107 [46:39<5:10:18,  6.78s/it]

 12%|█▏        | 361/3107 [46:49<5:52:16,  7.70s/it]
{'loss': 0.9117, 'grad_norm': 0.14766079150960487, 'learning_rate': 0.00019614976504351322, 'epoch': 0.12}

 12%|█▏        | 362/3107 [47:02<7:03:50,  9.26s/it]

 12%|█▏        | 363/3107 [47:12<7:15:05,  9.51s/it]


 12%|█▏        | 365/3107 [47:25<6:02:58,  7.94s/it]

 12%|█▏        | 366/3107 [47:35<6:28:18,  8.50s/it]

 12%|█▏        | 367/3107 [47:40<5:46:18,  7.58s/it]

 12%|█▏        | 368/3107 [47:47<5:35:18,  7.35s/it]
{'loss': 0.9103, 'grad_norm': 0.16270714273467685, 'learning_rate': 0.0001959466264149678, 'epoch': 0.12}


 12%|█▏        | 370/3107 [48:01<5:31:09,  7.26s/it]

 12%|█▏        | 371/3107 [48:14<6:39:55,  8.77s/it]
{'loss': 0.937, 'grad_norm': 0.16860077200136245, 'learning_rate': 0.00019585800170014288, 'epoch': 0.12}


 12%|█▏        | 373/3107 [48:32<6:37:44,  8.73s/it]

 12%|█▏        | 374/3107 [48:39<6:24:44,  8.45s/it]

 12%|█▏        | 375/3107 [48:45<5:46:00,  7.60s/it]

 12%|█▏        | 376/3107 [48:51<5:17:57,  6.99s/it]

 12%|█▏        | 377/3107 [49:01<6:06:00,  8.04s/it]
{'loss': 1.0204, 'grad_norm': 0.163392545011226, 'learning_rate': 0.00019567793934658248, 'epoch': 0.12}


 12%|█▏        | 379/3107 [49:20<6:40:23,  8.81s/it]
{'loss': 0.8706, 'grad_norm': 0.1654247627214254, 'learning_rate': 0.00019561708605934515, 'epoch': 0.12}

 12%|█▏        | 380/3107 [49:26<6:07:16,  8.08s/it]


 12%|█▏        | 382/3107 [49:43<6:21:53,  8.41s/it]

 12%|█▏        | 383/3107 [49:49<5:51:58,  7.75s/it]

 12%|█▏        | 384/3107 [49:59<6:18:45,  8.35s/it]

 12%|█▏        | 385/3107 [50:07<6:10:10,  8.16s/it]

 12%|█▏        | 386/3107 [50:13<5:38:58,  7.47s/it]
{'loss': 1.0601, 'grad_norm': 0.176793996393379, 'learning_rate': 0.00019540082678898177, 'epoch': 0.12}


 12%|█▏        | 388/3107 [50:26<5:11:59,  6.88s/it]

 13%|█▎        | 389/3107 [50:32<5:01:11,  6.65s/it]

 13%|█▎        | 390/3107 [50:37<4:48:25,  6.37s/it]

 13%|█▎        | 391/3107 [50:45<5:02:50,  6.69s/it]

 13%|█▎        | 392/3107 [50:55<5:46:37,  7.66s/it]
{'loss': 0.8961, 'grad_norm': 0.15461973533860957, 'learning_rate': 0.0001952114162768655, 'epoch': 0.13}


 13%|█▎        | 394/3107 [51:11<6:02:09,  8.01s/it]
{'loss': 0.9282, 'grad_norm': 0.151343969216954, 'learning_rate': 0.0001951474509762724, 'epoch': 0.13}


 13%|█▎        | 396/3107 [51:24<5:18:23,  7.05s/it]
{'loss': 1.0023, 'grad_norm': 0.16566149014344642, 'learning_rate': 0.00019508307190617985, 'epoch': 0.13}


 13%|█▎        | 398/3107 [51:39<5:35:10,  7.42s/it]

 13%|█▎        | 399/3107 [51:48<5:47:35,  7.70s/it]

 13%|█▎        | 400/3107 [51:54<5:21:35,  7.13s/it]

 13%|█▎        | 401/3107 [52:02<5:33:23,  7.39s/it]

 13%|█▎        | 402/3107 [52:09<5:27:49,  7.27s/it]
{'loss': 1.0188, 'grad_norm': 0.15731435815269929, 'learning_rate': 0.00019488745488756024, 'epoch': 0.13}

 13%|█▎        | 403/3107 [52:14<5:03:22,  6.73s/it]


 13%|█▎        | 405/3107 [52:31<5:41:36,  7.59s/it]

 13%|█▎        | 406/3107 [52:38<5:26:26,  7.25s/it]

 13%|█▎        | 407/3107 [52:45<5:24:09,  7.20s/it]
{'loss': 1.0451, 'grad_norm': 0.17345564605680192, 'learning_rate': 0.00019472160349222702, 'epoch': 0.13}


 13%|█▎        | 409/3107 [53:01<5:42:32,  7.62s/it]

 13%|█▎        | 410/3107 [53:07<5:11:05,  6.92s/it]

 13%|█▎        | 411/3107 [53:15<5:29:08,  7.33s/it]

 13%|█▎        | 412/3107 [53:23<5:36:31,  7.49s/it]

 13%|█▎        | 413/3107 [53:29<5:17:29,  7.07s/it]

 13%|█▎        | 414/3107 [53:35<5:06:15,  6.82s/it]

 13%|█▎        | 415/3107 [53:41<4:53:28,  6.54s/it]

 13%|█▎        | 416/3107 [53:50<5:22:22,  7.19s/it]
{'loss': 0.9686, 'grad_norm': 0.16451080166515664, 'learning_rate': 0.00019441658636451794, 'epoch': 0.13}


 13%|█▎        | 418/3107 [54:04<5:19:09,  7.12s/it]

 13%|█▎        | 419/3107 [54:12<5:32:33,  7.42s/it]

 14%|█▎        | 420/3107 [54:22<6:01:49,  8.08s/it]
{'loss': 1.0079, 'grad_norm': 0.16049428735470814, 'learning_rate': 0.0001942783527493832, 'epoch': 0.14}

 14%|█▎        | 421/3107 [54:28<5:42:51,  7.66s/it]

 14%|█▎        | 422/3107 [54:34<5:21:45,  7.19s/it]


 14%|█▎        | 424/3107 [54:48<5:06:31,  6.85s/it]
{'loss': 1.13, 'grad_norm': 0.16715797947466482, 'learning_rate': 0.00019413847917588878, 'epoch': 0.14}


 14%|█▎        | 426/3107 [55:03<5:22:20,  7.21s/it]
{'loss': 1.0838, 'grad_norm': 0.1648925929228803, 'learning_rate': 0.0001940679281639812, 'epoch': 0.14}

 14%|█▎        | 427/3107 [55:11<5:23:50,  7.25s/it]


 14%|█▍        | 429/3107 [55:30<6:14:41,  8.39s/it]
{'loss': 1.0086, 'grad_norm': 0.15340436705660684, 'learning_rate': 0.00019396133472693642, 'epoch': 0.14}


 14%|█▍        | 431/3107 [55:46<6:08:25,  8.26s/it]

 14%|█▍        | 432/3107 [55:51<5:36:14,  7.54s/it]
{'loss': 1.1422, 'grad_norm': 0.18009364377549344, 'learning_rate': 0.00019385382191462784, 'epoch': 0.14}


 14%|█▍        | 434/3107 [56:07<5:41:57,  7.68s/it]

 14%|█▍        | 435/3107 [56:15<5:52:23,  7.91s/it]

 14%|█▍        | 436/3107 [56:22<5:32:22,  7.47s/it]

 14%|█▍        | 437/3107 [56:32<6:10:21,  8.32s/it]

 14%|█▍        | 438/3107 [56:39<5:52:21,  7.92s/it]

 14%|█▍        | 439/3107 [56:48<6:04:15,  8.19s/it]
{'loss': 1.1065, 'grad_norm': 0.16732649182385648, 'learning_rate': 0.000193599389263704, 'epoch': 0.14}


 14%|█▍        | 441/3107 [57:01<5:27:51,  7.38s/it]

 14%|█▍        | 442/3107 [57:07<5:07:07,  6.91s/it]
{'loss': 1.0262, 'grad_norm': 0.1646010684139568, 'learning_rate': 0.00019348881951520693, 'epoch': 0.14}


 14%|█▍        | 444/3107 [57:22<5:23:47,  7.30s/it]
{'loss': 1.0992, 'grad_norm': 0.15366969383529352, 'learning_rate': 0.00019341459808689898, 'epoch': 0.14}


 14%|█▍        | 446/3107 [57:36<5:10:23,  7.00s/it]

 14%|█▍        | 447/3107 [57:44<5:31:40,  7.48s/it]
{'loss': 0.9413, 'grad_norm': 0.16227946460677967, 'learning_rate': 0.00019330250435739215, 'epoch': 0.14}


 14%|█▍        | 449/3107 [58:02<6:18:33,  8.55s/it]
{'loss': 1.0606, 'grad_norm': 0.1463958304781449, 'learning_rate': 0.00019322726795361443, 'epoch': 0.14}


 15%|█▍        | 451/3107 [58:18<6:05:15,  8.25s/it]

 15%|█▍        | 452/3107 [58:24<5:32:46,  7.52s/it]

 15%|█▍        | 453/3107 [58:33<5:51:43,  7.95s/it]
{'loss': 0.9589, 'grad_norm': 0.15931449398380196, 'learning_rate': 0.00019307557921751333, 'epoch': 0.15}

 15%|█▍        | 454/3107 [58:45<6:36:48,  8.97s/it]

 15%|█▍        | 455/3107 [58:51<5:57:44,  8.09s/it]


 15%|█▍        | 457/3107 [59:04<5:29:32,  7.46s/it]

 15%|█▍        | 458/3107 [59:10<5:09:29,  7.01s/it]

 15%|█▍        | 459/3107 [59:16<4:55:39,  6.70s/it]

 15%|█▍        | 460/3107 [59:22<4:49:18,  6.56s/it]

 15%|█▍        | 461/3107 [59:28<4:42:06,  6.40s/it]
{'loss': 1.1024, 'grad_norm': 0.17520295572113057, 'learning_rate': 0.0001927673473032153, 'epoch': 0.15}

 15%|█▍        | 462/3107 [59:35<4:42:16,  6.40s/it]


 15%|█▍        | 464/3107 [59:52<5:36:11,  7.63s/it]

 15%|█▍        | 465/3107 [59:58<5:23:13,  7.34s/it]

 15%|█▍        | 466/3107 [1:00:04<5:03:53,  6.90s/it]

 15%|█▌        | 467/3107 [1:00:15<6:01:46,  8.22s/it]

 15%|█▌        | 468/3107 [1:00:24<6:03:20,  8.26s/it]

 15%|█▌        | 469/3107 [1:00:30<5:39:53,  7.73s/it]
{'loss': 1.0768, 'grad_norm': 0.16136988098034719, 'learning_rate': 0.00019245266071843596, 'epoch': 0.15}


 15%|█▌        | 471/3107 [1:00:45<5:27:33,  7.46s/it]

 15%|█▌        | 472/3107 [1:00:54<5:48:04,  7.93s/it]

 15%|█▌        | 473/3107 [1:01:02<5:50:45,  7.99s/it]
{'loss': 1.0957, 'grad_norm': 0.16898435746817245, 'learning_rate': 0.00019229290374949386, 'epoch': 0.15}


 15%|█▌        | 475/3107 [1:01:19<6:04:03,  8.30s/it]

 15%|█▌        | 476/3107 [1:01:26<5:50:15,  7.99s/it]

 15%|█▌        | 477/3107 [1:01:32<5:24:03,  7.39s/it]

 15%|█▌        | 478/3107 [1:01:39<5:15:10,  7.19s/it]
{'loss': 0.9225, 'grad_norm': 0.14663850793193972, 'learning_rate': 0.0001920909502422833, 'epoch': 0.15}


 15%|█▌        | 480/3107 [1:01:58<6:00:51,  8.24s/it]

 15%|█▌        | 481/3107 [1:02:04<5:26:29,  7.46s/it]

 16%|█▌        | 482/3107 [1:02:11<5:32:18,  7.60s/it]
{'loss': 1.1483, 'grad_norm': 0.14924892644467047, 'learning_rate': 0.00019192758502481678, 'epoch': 0.16}

 16%|█▌        | 483/3107 [1:02:19<5:27:35,  7.49s/it]


 16%|█▌        | 485/3107 [1:02:35<5:53:58,  8.10s/it]
{'loss': 0.9666, 'grad_norm': 0.15817602166526382, 'learning_rate': 0.00019180401156748396, 'epoch': 0.16}


 16%|█▌        | 487/3107 [1:02:54<6:26:12,  8.84s/it]

 16%|█▌        | 488/3107 [1:03:00<5:55:30,  8.14s/it]

 16%|█▌        | 489/3107 [1:03:08<5:46:48,  7.95s/it]

 16%|█▌        | 490/3107 [1:03:15<5:31:52,  7.61s/it]

 16%|█▌        | 491/3107 [1:03:24<5:55:19,  8.15s/it]
{'loss': 0.9409, 'grad_norm': 0.1529916798184309, 'learning_rate': 0.0001915541710706488, 'epoch': 0.16}

 16%|█▌        | 492/3107 [1:03:31<5:37:13,  7.74s/it]


 16%|█▌        | 494/3107 [1:03:46<5:42:18,  7.86s/it]

 16%|█▌        | 495/3107 [1:03:58<6:37:21,  9.13s/it]
{'loss': 0.9878, 'grad_norm': 0.16827078928013775, 'learning_rate': 0.0001913856194191403, 'epoch': 0.16}


 16%|█▌        | 497/3107 [1:04:16<6:18:30,  8.70s/it]

 16%|█▌        | 498/3107 [1:04:23<5:58:11,  8.24s/it]

 16%|█▌        | 499/3107 [1:04:29<5:28:01,  7.55s/it]

 16%|█▌        | 500/3107 [1:04:37<5:31:48,  7.64s/it]

 16%|█▌        | 501/3107 [1:04:43<5:09:09,  7.12s/it]

 16%|█▌        | 502/3107 [1:04:50<5:11:29,  7.17s/it]

 16%|█▌        | 503/3107 [1:04:57<5:08:01,  7.10s/it]
{'loss': 1.2348, 'grad_norm': 0.16383004510006832, 'learning_rate': 0.00019104375015666563, 'epoch': 0.16}

 16%|█▌        | 504/3107 [1:05:03<4:59:14,  6.90s/it]


 16%|█▋        | 506/3107 [1:05:16<4:54:52,  6.80s/it]

 16%|█▋        | 507/3107 [1:05:24<5:03:00,  6.99s/it]
{'loss': 0.9529, 'grad_norm': 0.16449482744562902, 'learning_rate': 0.00019087043849246596, 'epoch': 0.16}


 16%|█▋        | 509/3107 [1:05:42<5:54:07,  8.18s/it]

 16%|█▋        | 510/3107 [1:05:48<5:26:21,  7.54s/it]
{'loss': 0.9324, 'grad_norm': 0.15366419670921452, 'learning_rate': 0.00019073941725835666, 'epoch': 0.16}

 16%|█▋        | 511/3107 [1:05:55<5:26:25,  7.54s/it]


 17%|█▋        | 513/3107 [1:06:09<5:06:47,  7.10s/it]
{'loss': 1.0498, 'grad_norm': 0.17440397760719953, 'learning_rate': 0.00019060750817419798, 'epoch': 0.17}


 17%|█▋        | 515/3107 [1:06:26<5:44:30,  7.97s/it]

 17%|█▋        | 516/3107 [1:06:36<6:20:18,  8.81s/it]
{'loss': 0.9232, 'grad_norm': 0.16383211299300618, 'learning_rate': 0.00019047471253066914, 'epoch': 0.17}

 17%|█▋        | 517/3107 [1:06:49<7:11:32, 10.00s/it]


 17%|█▋        | 519/3107 [1:07:04<6:17:29,  8.75s/it]
{'loss': 1.0329, 'grad_norm': 0.1933926512709494, 'learning_rate': 0.00019034103162712408, 'epoch': 0.17}


 17%|█▋        | 521/3107 [1:07:27<7:09:20,  9.96s/it]

 17%|█▋        | 522/3107 [1:07:34<6:38:14,  9.24s/it]

 17%|█▋        | 523/3107 [1:07:41<6:03:41,  8.44s/it]

 17%|█▋        | 524/3107 [1:07:50<6:10:20,  8.60s/it]

 17%|█▋        | 525/3107 [1:07:56<5:37:22,  7.84s/it]

 17%|█▋        | 526/3107 [1:08:03<5:29:53,  7.67s/it]
{'loss': 0.9446, 'grad_norm': 0.17208678688324897, 'learning_rate': 0.0001900256742047014, 'epoch': 0.17}

 17%|█▋        | 527/3107 [1:08:11<5:37:54,  7.86s/it]


 17%|█▋        | 529/3107 [1:08:28<5:57:22,  8.32s/it]
{'loss': 0.9295, 'grad_norm': 0.15902185473294383, 'learning_rate': 0.0001898890519297774, 'epoch': 0.17}


 17%|█▋        | 531/3107 [1:08:48<6:20:53,  8.87s/it]
{'loss': 1.0337, 'grad_norm': 0.16789336888402695, 'learning_rate': 0.0001897974817027588, 'epoch': 0.17}


 17%|█▋        | 533/3107 [1:09:04<5:59:05,  8.37s/it]

 17%|█▋        | 534/3107 [1:09:10<5:28:56,  7.67s/it]

 17%|█▋        | 535/3107 [1:09:17<5:17:49,  7.41s/it]
{'loss': 0.8597, 'grad_norm': 0.14165415509659404, 'learning_rate': 0.0001896131701366691, 'epoch': 0.17}


 17%|█▋        | 537/3107 [1:09:32<5:16:19,  7.39s/it]
{'loss': 0.9857, 'grad_norm': 0.16273461905994185, 'learning_rate': 0.00018952042959911715, 'epoch': 0.17}


 17%|█▋        | 539/3107 [1:09:47<5:17:15,  7.41s/it]

 17%|█▋        | 540/3107 [1:09:58<6:07:13,  8.58s/it]

 17%|█▋        | 541/3107 [1:10:05<5:44:47,  8.06s/it]

 17%|█▋        | 542/3107 [1:10:16<6:19:55,  8.89s/it]

 17%|█▋        | 543/3107 [1:10:23<5:53:54,  8.28s/it]

 18%|█▊        | 544/3107 [1:10:29<5:27:55,  7.68s/it]

 18%|█▊        | 545/3107 [1:10:37<5:24:34,  7.60s/it]

 18%|█▊        | 546/3107 [1:10:47<6:04:25,  8.54s/it]

 18%|█▊        | 547/3107 [1:10:57<6:21:14,  8.94s/it]

 18%|█▊        | 548/3107 [1:11:03<5:37:02,  7.90s/it]

 18%|█▊        | 549/3107 [1:11:09<5:16:45,  7.43s/it]
{'loss': 0.8673, 'grad_norm': 0.15535455618587368, 'learning_rate': 0.0001889558253253092, 'epoch': 0.18}


 18%|█▊        | 551/3107 [1:11:25<5:34:57,  7.86s/it]
{'loss': 1.008, 'grad_norm': 0.16432307883961778, 'learning_rate': 0.0001888603682576276, 'epoch': 0.18}

 18%|█▊        | 552/3107 [1:11:34<5:43:49,  8.07s/it]


 18%|█▊        | 554/3107 [1:11:55<6:32:54,  9.23s/it]
{'loss': 1.0208, 'grad_norm': 0.16340981672801713, 'learning_rate': 0.00018871645823239128, 'epoch': 0.18}


 18%|█▊        | 556/3107 [1:12:09<5:41:14,  8.03s/it]
{'loss': 0.9423, 'grad_norm': 0.15813458335911684, 'learning_rate': 0.00018862003587486465, 'epoch': 0.18}


 18%|█▊        | 558/3107 [1:12:26<5:54:54,  8.35s/it]

 18%|█▊        | 559/3107 [1:12:32<5:24:49,  7.65s/it]

 18%|█▊        | 560/3107 [1:12:40<5:32:45,  7.84s/it]

 18%|█▊        | 561/3107 [1:12:48<5:31:01,  7.80s/it]

 18%|█▊        | 562/3107 [1:12:54<5:05:31,  7.20s/it]

 18%|█▊        | 563/3107 [1:13:01<5:07:15,  7.25s/it]
{'loss': 0.9919, 'grad_norm': 0.15650940664286353, 'learning_rate': 0.00018827952549447814, 'epoch': 0.18}


 18%|█▊        | 565/3107 [1:13:19<5:30:55,  7.81s/it]

 18%|█▊        | 566/3107 [1:13:25<5:10:31,  7.33s/it]

 18%|█▊        | 567/3107 [1:13:33<5:21:01,  7.58s/it]

 18%|█▊        | 568/3107 [1:13:42<5:39:07,  8.01s/it]

 18%|█▊        | 569/3107 [1:13:48<5:13:35,  7.41s/it]

 18%|█▊        | 570/3107 [1:13:55<5:07:38,  7.28s/it]

 18%|█▊        | 571/3107 [1:14:03<5:17:17,  7.51s/it]

 18%|█▊        | 572/3107 [1:14:09<5:02:30,  7.16s/it]
{'loss': 0.9886, 'grad_norm': 0.16700128568905864, 'learning_rate': 0.00018783481877862322, 'epoch': 0.18}

 18%|█▊        | 573/3107 [1:14:16<4:48:58,  6.84s/it]


 19%|█▊        | 575/3107 [1:14:33<5:30:29,  7.83s/it]

 19%|█▊        | 576/3107 [1:14:41<5:27:21,  7.76s/it]
{'loss': 0.9846, 'grad_norm': 0.15820917299586804, 'learning_rate': 0.0001876346862160873, 'epoch': 0.19}


 19%|█▊        | 578/3107 [1:14:56<5:33:40,  7.92s/it]
{'loss': 1.0448, 'grad_norm': 0.1585545622525372, 'learning_rate': 0.0001875340480689263, 'epoch': 0.19}

 19%|█▊        | 579/3107 [1:15:04<5:24:54,  7.71s/it]


 19%|█▊        | 581/3107 [1:15:16<4:55:15,  7.01s/it]
{'loss': 1.0331, 'grad_norm': 0.1592088170721303, 'learning_rate': 0.00018738237724611766, 'epoch': 0.19}

 19%|█▊        | 582/3107 [1:15:26<5:28:03,  7.80s/it]

 19%|█▉        | 583/3107 [1:15:32<5:02:59,  7.20s/it]


 19%|█▉        | 585/3107 [1:15:45<4:47:37,  6.84s/it]
{'loss': 1.1766, 'grad_norm': 0.1837138924932133, 'learning_rate': 0.00018717881973585816, 'epoch': 0.19}


 19%|█▉        | 587/3107 [1:16:00<5:05:59,  7.29s/it]
{'loss': 1.0658, 'grad_norm': 0.15766823758194873, 'learning_rate': 0.00018707647208476667, 'epoch': 0.19}

 19%|█▉        | 588/3107 [1:16:08<5:11:09,  7.41s/it]


 19%|█▉        | 590/3107 [1:16:23<5:07:16,  7.32s/it]

 19%|█▉        | 591/3107 [1:16:30<5:11:41,  7.43s/it]

 19%|█▉        | 592/3107 [1:16:37<5:06:57,  7.32s/it]
{'loss': 1.1999, 'grad_norm': 0.16258153585672908, 'learning_rate': 0.00018681894724871883, 'epoch': 0.19}

 19%|█▉        | 593/3107 [1:16:46<5:22:31,  7.70s/it]

 19%|█▉        | 594/3107 [1:16:56<5:47:23,  8.29s/it]


 19%|█▉        | 596/3107 [1:17:07<4:53:17,  7.01s/it]

 19%|█▉        | 597/3107 [1:17:13<4:41:11,  6.72s/it]
{'loss': 0.9766, 'grad_norm': 0.1612401281718285, 'learning_rate': 0.00018655906272228878, 'epoch': 0.19}

 19%|█▉        | 598/3107 [1:17:20<4:36:58,  6.62s/it]


 19%|█▉        | 600/3107 [1:17:33<4:37:04,  6.63s/it]
 19%|█▉        | 600/3107 [1:17:33<4:37:04,  6.63s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 19%|█▉        | 601/3107 [1:17:59<8:39:30, 12.44s/it]

 19%|█▉        | 602/3107 [1:18:05<7:23:49, 10.63s/it]

 19%|█▉        | 603/3107 [1:18:16<7:19:00, 10.52s/it]

 19%|█▉        | 604/3107 [1:18:26<7:11:34, 10.35s/it]

 19%|█▉        | 605/3107 [1:18:33<6:37:02,  9.52s/it]

 20%|█▉        | 606/3107 [1:18:39<5:52:15,  8.45s/it]

 20%|█▉        | 607/3107 [1:18:47<5:47:44,  8.35s/it]

 20%|█▉        | 608/3107 [1:18:57<6:06:51,  8.81s/it]

 20%|█▉        | 609/3107 [1:19:04<5:38:25,  8.13s/it]
{'loss': 1.0922, 'grad_norm': 0.1627663399459745, 'learning_rate': 0.0001859257547240666, 'epoch': 0.2}


 20%|█▉        | 611/3107 [1:19:21<5:47:55,  8.36s/it]
{'loss': 1.0665, 'grad_norm': 0.15376145477273964, 'learning_rate': 0.00018581889286490785, 'epoch': 0.2}

 20%|█▉        | 612/3107 [1:19:28<5:38:22,  8.14s/it]


 20%|█▉        | 614/3107 [1:19:44<5:32:40,  8.01s/it]

 20%|█▉        | 615/3107 [1:19:50<5:07:33,  7.41s/it]
{'loss': 0.9605, 'grad_norm': 0.17075563311501848, 'learning_rate': 0.00018560405000625269, 'epoch': 0.2}

 20%|█▉        | 616/3107 [1:19:56<5:00:49,  7.25s/it]


 20%|█▉        | 618/3107 [1:20:11<5:00:36,  7.25s/it]
{'loss': 0.9275, 'grad_norm': 0.1528593054499563, 'learning_rate': 0.00018544194045464886, 'epoch': 0.2}


 20%|█▉        | 620/3107 [1:20:26<5:02:38,  7.30s/it]
{'loss': 1.0986, 'grad_norm': 0.16878944753046687, 'learning_rate': 0.00018533340286850227, 'epoch': 0.2}


 20%|██        | 622/3107 [1:20:41<5:11:27,  7.52s/it]
{'loss': 0.9484, 'grad_norm': 0.15898877840430556, 'learning_rate': 0.00018522449419139083, 'epoch': 0.2}

 20%|██        | 623/3107 [1:20:48<5:11:38,  7.53s/it]

 20%|██        | 624/3107 [1:20:56<5:18:29,  7.70s/it]


 20%|██        | 626/3107 [1:21:11<5:11:06,  7.52s/it]
{'loss': 1.0347, 'grad_norm': 0.1680224302491918, 'learning_rate': 0.00018500556546033794, 'epoch': 0.2}

 20%|██        | 627/3107 [1:21:20<5:25:53,  7.88s/it]

 20%|██        | 628/3107 [1:21:32<6:20:35,  9.21s/it]


 20%|██        | 630/3107 [1:21:51<6:20:27,  9.22s/it]

 20%|██        | 631/3107 [1:21:59<6:05:29,  8.86s/it]

 20%|██        | 632/3107 [1:22:05<5:31:25,  8.03s/it]

 20%|██        | 633/3107 [1:22:13<5:33:17,  8.08s/it]
{'loss': 0.8201, 'grad_norm': 0.1572662311854938, 'learning_rate': 0.00018461888446182472, 'epoch': 0.2}

 20%|██        | 634/3107 [1:22:22<5:46:03,  8.40s/it]


 20%|██        | 636/3107 [1:22:37<5:15:27,  7.66s/it]

 21%|██        | 637/3107 [1:22:44<5:09:59,  7.53s/it]
{'loss': 0.9273, 'grad_norm': 0.15837785864455176, 'learning_rate': 0.00018439589865786767, 'epoch': 0.21}


 21%|██        | 639/3107 [1:22:56<4:36:14,  6.72s/it]
{'loss': 1.0438, 'grad_norm': 0.17088411970659143, 'learning_rate': 0.00018428385499202988, 'epoch': 0.21}


 21%|██        | 641/3107 [1:23:11<4:49:07,  7.03s/it]
{'loss': 1.0356, 'grad_norm': 0.15604028135879777, 'learning_rate': 0.00018417144479941573, 'epoch': 0.21}


 21%|██        | 643/3107 [1:23:23<4:32:19,  6.63s/it]
{'loss': 0.9082, 'grad_norm': 0.148598228602623, 'learning_rate': 0.0001840586685688655, 'epoch': 0.21}

 21%|██        | 644/3107 [1:23:29<4:17:19,  6.27s/it]

 21%|██        | 645/3107 [1:23:34<4:09:15,  6.07s/it]


 21%|██        | 647/3107 [1:23:47<4:20:35,  6.36s/it]

 21%|██        | 648/3107 [1:23:55<4:34:40,  6.70s/it]

 21%|██        | 649/3107 [1:24:03<4:53:43,  7.17s/it]
{'loss': 0.8723, 'grad_norm': 0.16048882126851038, 'learning_rate': 0.00018371814856186572, 'epoch': 0.21}


 21%|██        | 651/3107 [1:24:28<6:39:58,  9.77s/it]
{'loss': 0.8959, 'grad_norm': 0.16686763193733073, 'learning_rate': 0.0001836039130997782, 'epoch': 0.21}

 21%|██        | 652/3107 [1:24:35<6:02:20,  8.86s/it]


 21%|██        | 654/3107 [1:24:51<5:54:51,  8.68s/it]
{'loss': 0.9988, 'grad_norm': 0.1602845770147189, 'learning_rate': 0.00018343187836875928, 'epoch': 0.21}

 21%|██        | 655/3107 [1:24:58<5:41:24,  8.35s/it]


 21%|██        | 657/3107 [1:25:16<5:54:27,  8.68s/it]
{'loss': 1.1652, 'grad_norm': 0.17833598952726618, 'learning_rate': 0.00018325902728912836, 'epoch': 0.21}


 21%|██        | 659/3107 [1:25:29<5:06:27,  7.51s/it]
{'loss': 1.0306, 'grad_norm': 0.17305996507001545, 'learning_rate': 0.00018314334054390664, 'epoch': 0.21}


 21%|██▏       | 661/3107 [1:25:42<4:43:23,  6.95s/it]

 21%|██▏       | 662/3107 [1:25:53<5:36:52,  8.27s/it]

 21%|██▏       | 663/3107 [1:26:01<5:28:22,  8.06s/it]

 21%|██▏       | 664/3107 [1:26:10<5:36:00,  8.25s/it]

 21%|██▏       | 665/3107 [1:26:18<5:38:39,  8.32s/it]

 21%|██▏       | 666/3107 [1:26:26<5:28:52,  8.08s/it]

 21%|██▏       | 667/3107 [1:26:33<5:23:46,  7.96s/it]

 21%|██▏       | 668/3107 [1:26:40<5:06:38,  7.54s/it]
{'loss': 0.9655, 'grad_norm': 0.16388582380510405, 'learning_rate': 0.0001826182830989978, 'epoch': 0.21}


 22%|██▏       | 670/3107 [1:26:58<5:29:12,  8.11s/it]

 22%|██▏       | 671/3107 [1:27:08<5:54:55,  8.74s/it]

 22%|██▏       | 672/3107 [1:27:16<5:44:55,  8.50s/it]

 22%|██▏       | 673/3107 [1:27:24<5:34:00,  8.23s/it]

 22%|██▏       | 674/3107 [1:27:33<5:51:42,  8.67s/it]
{'loss': 0.8669, 'grad_norm': 0.1542606269470425, 'learning_rate': 0.000182264200008721, 'epoch': 0.22}


 22%|██▏       | 676/3107 [1:27:48<5:32:13,  8.20s/it]
{'loss': 0.9039, 'grad_norm': 0.15763219193954328, 'learning_rate': 0.00018214545613831414, 'epoch': 0.22}


 22%|██▏       | 678/3107 [1:28:00<4:46:01,  7.07s/it]

 22%|██▏       | 679/3107 [1:28:08<4:57:24,  7.35s/it]
{'loss': 0.9324, 'grad_norm': 0.1698211139782531, 'learning_rate': 0.00018196667069297123, 'epoch': 0.22}

 22%|██▏       | 680/3107 [1:28:17<5:13:54,  7.76s/it]

 22%|██▏       | 681/3107 [1:28:24<5:11:25,  7.70s/it]


 22%|██▏       | 683/3107 [1:28:37<4:47:33,  7.12s/it]

 22%|██▏       | 684/3107 [1:28:44<4:41:40,  6.98s/it]

 22%|██▏       | 685/3107 [1:28:51<4:44:10,  7.04s/it]

 22%|██▏       | 686/3107 [1:28:58<4:35:29,  6.83s/it]

 22%|██▏       | 687/3107 [1:29:05<4:42:34,  7.01s/it]

 22%|██▏       | 688/3107 [1:29:12<4:45:17,  7.08s/it]
{'loss': 1.0581, 'grad_norm': 0.1683856325239594, 'learning_rate': 0.00018142550932081277, 'epoch': 0.22}


 22%|██▏       | 690/3107 [1:29:31<5:39:12,  8.42s/it]

 22%|██▏       | 691/3107 [1:29:42<6:06:07,  9.09s/it]

 22%|██▏       | 692/3107 [1:29:49<5:42:25,  8.51s/it]
{'loss': 1.0048, 'grad_norm': 0.15992897443234702, 'learning_rate': 0.00018118268868762546, 'epoch': 0.22}


 22%|██▏       | 694/3107 [1:30:02<4:59:20,  7.44s/it]

 22%|██▏       | 695/3107 [1:30:07<4:36:52,  6.89s/it]

 22%|██▏       | 696/3107 [1:30:14<4:27:38,  6.66s/it]
{'loss': 1.0612, 'grad_norm': 0.16874654897990593, 'learning_rate': 0.00018093845589326756, 'epoch': 0.22}

 22%|██▏       | 697/3107 [1:30:21<4:34:23,  6.83s/it]


 22%|██▏       | 699/3107 [1:30:32<4:10:26,  6.24s/it]

 23%|██▎       | 700/3107 [1:30:38<4:00:18,  5.99s/it]

 23%|██▎       | 701/3107 [1:30:44<4:00:36,  6.00s/it]

 23%|██▎       | 702/3107 [1:30:52<4:26:26,  6.65s/it]

 23%|██▎       | 703/3107 [1:30:58<4:15:39,  6.38s/it]

 23%|██▎       | 704/3107 [1:31:09<5:21:10,  8.02s/it]

 23%|██▎       | 705/3107 [1:31:18<5:28:15,  8.20s/it]

 23%|██▎       | 706/3107 [1:31:26<5:23:37,  8.09s/it]

 23%|██▎       | 707/3107 [1:31:33<5:06:58,  7.67s/it]

 23%|██▎       | 708/3107 [1:31:39<4:58:19,  7.46s/it]

 23%|██▎       | 709/3107 [1:31:48<5:16:30,  7.92s/it]

 23%|██▎       | 710/3107 [1:31:58<5:32:35,  8.33s/it]

 23%|██▎       | 711/3107 [1:32:04<5:09:43,  7.76s/it]
{'loss': 0.9548, 'grad_norm': 0.16366752157155162, 'learning_rate': 0.00018001007866252813, 'epoch': 0.23}


 23%|██▎       | 713/3107 [1:32:20<5:13:39,  7.86s/it]
{'loss': 1.0235, 'grad_norm': 0.17081401331144452, 'learning_rate': 0.00017988481129801523, 'epoch': 0.23}


 23%|██▎       | 715/3107 [1:32:36<5:11:34,  7.82s/it]

 23%|██▎       | 716/3107 [1:32:42<4:52:31,  7.34s/it]

 23%|██▎       | 717/3107 [1:32:52<5:17:47,  7.98s/it]

 23%|██▎       | 718/3107 [1:32:59<5:03:25,  7.62s/it]

 23%|██▎       | 719/3107 [1:33:07<5:17:36,  7.98s/it]
{'loss': 0.9878, 'grad_norm': 0.1580965150054393, 'learning_rate': 0.0001795069270116013, 'epoch': 0.23}


 23%|██▎       | 721/3107 [1:33:29<6:08:48,  9.27s/it]

 23%|██▎       | 722/3107 [1:33:39<6:25:32,  9.70s/it]
{'loss': 0.9726, 'grad_norm': 0.1689431414274423, 'learning_rate': 0.000179316817025399, 'epoch': 0.23}


 23%|██▎       | 724/3107 [1:33:54<5:33:15,  8.39s/it]

 23%|██▎       | 725/3107 [1:34:01<5:13:33,  7.90s/it]

 23%|██▎       | 726/3107 [1:34:10<5:27:17,  8.25s/it]

 23%|██▎       | 727/3107 [1:34:22<6:15:30,  9.47s/it]

 23%|██▎       | 728/3107 [1:34:28<5:37:35,  8.51s/it]

 23%|██▎       | 729/3107 [1:34:36<5:27:22,  8.26s/it]
{'loss': 0.9201, 'grad_norm': 0.1626702135115673, 'learning_rate': 0.00017887021218115782, 'epoch': 0.23}


 24%|██▎       | 731/3107 [1:34:55<5:49:03,  8.81s/it]

 24%|██▎       | 732/3107 [1:35:02<5:28:56,  8.31s/it]

 24%|██▎       | 733/3107 [1:35:08<5:00:22,  7.59s/it]

 24%|██▎       | 734/3107 [1:35:14<4:43:56,  7.18s/it]
{'loss': 0.9489, 'grad_norm': 0.16621179508343314, 'learning_rate': 0.00017854863495260354, 'epoch': 0.24}


 24%|██▎       | 736/3107 [1:35:31<5:12:06,  7.90s/it]

 24%|██▎       | 737/3107 [1:35:39<5:14:22,  7.96s/it]
{'loss': 0.9555, 'grad_norm': 0.14613427299826834, 'learning_rate': 0.00017835466329787236, 'epoch': 0.24}

 24%|██▍       | 738/3107 [1:35:47<5:19:35,  8.09s/it]

 24%|██▍       | 739/3107 [1:35:55<5:19:07,  8.09s/it]


 24%|██▍       | 741/3107 [1:36:08<4:48:20,  7.31s/it]

 24%|██▍       | 742/3107 [1:36:19<5:27:29,  8.31s/it]
{'loss': 0.9286, 'grad_norm': 0.15389284302669604, 'learning_rate': 0.00017802967443612708, 'epoch': 0.24}


 24%|██▍       | 744/3107 [1:36:32<4:52:04,  7.42s/it]
{'loss': 1.1019, 'grad_norm': 0.16005977551237124, 'learning_rate': 0.00017789908457003777, 'epoch': 0.24}


 24%|██▍       | 746/3107 [1:36:48<4:55:56,  7.52s/it]
{'loss': 0.9518, 'grad_norm': 0.1621902277669061, 'learning_rate': 0.00017776815594274266, 'epoch': 0.24}


 24%|██▍       | 748/3107 [1:37:00<4:25:32,  6.75s/it]
{'loss': 1.0319, 'grad_norm': 0.17361498197452002, 'learning_rate': 0.00017763688912361344, 'epoch': 0.24}


 24%|██▍       | 750/3107 [1:37:16<4:56:11,  7.54s/it]

 24%|██▍       | 751/3107 [1:37:22<4:38:33,  7.09s/it]

 24%|██▍       | 752/3107 [1:37:28<4:22:09,  6.68s/it]

 24%|██▍       | 753/3107 [1:37:36<4:35:46,  7.03s/it]

 24%|██▍       | 754/3107 [1:37:45<5:02:24,  7.71s/it]

 24%|██▍       | 755/3107 [1:37:50<4:34:41,  7.01s/it]

 24%|██▍       | 756/3107 [1:37:57<4:27:41,  6.83s/it]

 24%|██▍       | 757/3107 [1:38:03<4:18:45,  6.61s/it]
{'loss': 0.9232, 'grad_norm': 0.18109554559525598, 'learning_rate': 0.00017704201865371597, 'epoch': 0.24}

 24%|██▍       | 758/3107 [1:38:13<5:03:09,  7.74s/it]

 24%|██▍       | 759/3107 [1:38:21<5:05:38,  7.81s/it]

 24%|██▍       | 760/3107 [1:38:29<5:09:57,  7.92s/it]


 25%|██▍       | 762/3107 [1:38:47<5:21:01,  8.21s/it]

 25%|██▍       | 763/3107 [1:38:56<5:35:30,  8.59s/it]

 25%|██▍       | 764/3107 [1:39:04<5:22:57,  8.27s/it]

 25%|██▍       | 765/3107 [1:39:10<4:56:05,  7.59s/it]
{'loss': 1.123, 'grad_norm': 0.1602465665876297, 'learning_rate': 0.00017650754772105325, 'epoch': 0.25}


 25%|██▍       | 767/3107 [1:39:24<4:47:40,  7.38s/it]

 25%|██▍       | 768/3107 [1:39:33<5:05:33,  7.84s/it]
{'loss': 0.9158, 'grad_norm': 0.16185319843868368, 'learning_rate': 0.00017630574669158318, 'epoch': 0.25}


 25%|██▍       | 770/3107 [1:39:53<5:35:59,  8.63s/it]

 25%|██▍       | 771/3107 [1:40:03<5:55:14,  9.12s/it]
{'loss': 0.9808, 'grad_norm': 0.16174889616081123, 'learning_rate': 0.00017610319903993923, 'epoch': 0.25}


 25%|██▍       | 773/3107 [1:40:16<5:03:07,  7.79s/it]

 25%|██▍       | 774/3107 [1:40:22<4:39:20,  7.18s/it]

 25%|██▍       | 775/3107 [1:40:29<4:38:01,  7.15s/it]

 25%|██▍       | 776/3107 [1:40:35<4:24:17,  6.80s/it]
{'loss': 1.0853, 'grad_norm': 0.1659952901135509, 'learning_rate': 0.00017576396584624833, 'epoch': 0.25}


 25%|██▌       | 778/3107 [1:40:50<4:32:35,  7.02s/it]

 25%|██▌       | 779/3107 [1:40:57<4:34:44,  7.08s/it]

 25%|██▌       | 780/3107 [1:41:02<4:15:19,  6.58s/it]

 25%|██▌       | 781/3107 [1:41:08<4:03:34,  6.28s/it]

 25%|██▌       | 782/3107 [1:41:15<4:08:57,  6.42s/it]

 25%|██▌       | 783/3107 [1:41:24<4:42:56,  7.31s/it]

 25%|██▌       | 784/3107 [1:41:30<4:25:18,  6.85s/it]

 25%|██▌       | 785/3107 [1:41:38<4:44:09,  7.34s/it]

 25%|██▌       | 786/3107 [1:41:45<4:32:18,  7.04s/it]

 25%|██▌       | 787/3107 [1:41:54<4:58:37,  7.72s/it]
{'loss': 1.0177, 'grad_norm': 0.1594877581108248, 'learning_rate': 0.00017501041742279259, 'epoch': 0.25}

 25%|██▌       | 788/3107 [1:42:02<4:58:22,  7.72s/it]


 25%|██▌       | 790/3107 [1:42:23<5:50:59,  9.09s/it]

 25%|██▌       | 791/3107 [1:42:29<5:13:02,  8.11s/it]

 25%|██▌       | 792/3107 [1:42:38<5:30:08,  8.56s/it]
{'loss': 0.9629, 'grad_norm': 0.16367405744740474, 'learning_rate': 0.0001746646274485689, 'epoch': 0.25}


 26%|██▌       | 794/3107 [1:42:54<5:14:00,  8.15s/it]

 26%|██▌       | 795/3107 [1:43:01<4:52:54,  7.60s/it]

 26%|██▌       | 796/3107 [1:43:08<4:51:09,  7.56s/it]

 26%|██▌       | 797/3107 [1:43:14<4:38:08,  7.22s/it]

 26%|██▌       | 798/3107 [1:43:25<5:10:43,  8.07s/it]

 26%|██▌       | 799/3107 [1:43:31<4:54:52,  7.67s/it]

 26%|██▌       | 800/3107 [1:43:37<4:35:03,  7.15s/it]
{'loss': 0.8706, 'grad_norm': 0.16122436970371382, 'learning_rate': 0.0001741071463872236, 'epoch': 0.26}

 26%|██▌       | 801/3107 [1:43:44<4:28:42,  6.99s/it]


 26%|██▌       | 803/3107 [1:43:57<4:25:54,  6.92s/it]
{'loss': 1.0854, 'grad_norm': 0.16226379706692656, 'learning_rate': 0.00017389675953315495, 'epoch': 0.26}


 26%|██▌       | 805/3107 [1:44:13<4:53:10,  7.64s/it]

 26%|██▌       | 806/3107 [1:44:20<4:46:26,  7.47s/it]

 26%|██▌       | 807/3107 [1:44:26<4:30:14,  7.05s/it]
{'loss': 0.969, 'grad_norm': 0.1570585535394807, 'learning_rate': 0.00017361511933828801, 'epoch': 0.26}

 26%|██▌       | 808/3107 [1:44:33<4:29:36,  7.04s/it]


 26%|██▌       | 810/3107 [1:44:49<4:39:38,  7.30s/it]

 26%|██▌       | 811/3107 [1:44:57<4:38:50,  7.29s/it]

 26%|██▌       | 812/3107 [1:45:06<5:07:21,  8.04s/it]

 26%|██▌       | 813/3107 [1:45:15<5:12:11,  8.17s/it]

 26%|██▌       | 814/3107 [1:45:21<4:49:06,  7.56s/it]

 26%|██▌       | 815/3107 [1:45:31<5:20:14,  8.38s/it]

 26%|██▋       | 816/3107 [1:45:42<5:41:17,  8.94s/it]
{'loss': 0.9535, 'grad_norm': 0.16147061246469996, 'learning_rate': 0.0001729767544820342, 'epoch': 0.26}


 26%|██▋       | 818/3107 [1:45:59<5:27:44,  8.59s/it]

 26%|██▋       | 819/3107 [1:46:08<5:40:14,  8.92s/it]

 26%|██▋       | 820/3107 [1:46:14<5:06:05,  8.03s/it]
{'loss': 1.1315, 'grad_norm': 0.158380783763371, 'learning_rate': 0.00017269097061042886, 'epoch': 0.26}


 26%|██▋       | 822/3107 [1:46:31<5:13:46,  8.24s/it]
{'loss': 1.1063, 'grad_norm': 0.16189372298182966, 'learning_rate': 0.00017254760419470853, 'epoch': 0.26}

 26%|██▋       | 823/3107 [1:46:38<4:52:35,  7.69s/it]

 27%|██▋       | 824/3107 [1:46:46<4:55:09,  7.76s/it]


 27%|██▋       | 826/3107 [1:46:59<4:36:44,  7.28s/it]

 27%|██▋       | 827/3107 [1:47:07<4:43:47,  7.47s/it]

 27%|██▋       | 828/3107 [1:47:15<4:46:42,  7.55s/it]

 27%|██▋       | 829/3107 [1:47:21<4:34:09,  7.22s/it]

 27%|██▋       | 830/3107 [1:47:31<5:01:21,  7.94s/it]

 27%|██▋       | 831/3107 [1:47:37<4:42:28,  7.45s/it]

 27%|██▋       | 832/3107 [1:47:43<4:27:41,  7.06s/it]

 27%|██▋       | 833/3107 [1:47:52<4:48:41,  7.62s/it]
{'loss': 0.8254, 'grad_norm': 0.15205763562636157, 'learning_rate': 0.0001717534663255774, 'epoch': 0.27}


 27%|██▋       | 835/3107 [1:48:07<4:40:28,  7.41s/it]
{'loss': 1.1097, 'grad_norm': 0.154218511661659, 'learning_rate': 0.00017160806043608183, 'epoch': 0.27}


 27%|██▋       | 837/3107 [1:48:22<4:42:11,  7.46s/it]

 27%|██▋       | 838/3107 [1:48:29<4:36:08,  7.30s/it]

 27%|██▋       | 839/3107 [1:48:36<4:22:58,  6.96s/it]

 27%|██▋       | 840/3107 [1:48:42<4:14:19,  6.73s/it]

 27%|██▋       | 841/3107 [1:48:51<4:37:51,  7.36s/it]

 27%|██▋       | 842/3107 [1:48:59<4:50:28,  7.69s/it]
{'loss': 0.9569, 'grad_norm': 0.16181529952248513, 'learning_rate': 0.00017109669168364225, 'epoch': 0.27}


 27%|██▋       | 844/3107 [1:49:15<4:54:22,  7.81s/it]
{'loss': 1.0742, 'grad_norm': 0.14466377694099272, 'learning_rate': 0.0001709498894783646, 'epoch': 0.27}


 27%|██▋       | 846/3107 [1:49:31<4:56:00,  7.85s/it]
{'loss': 1.0376, 'grad_norm': 0.15763722389880028, 'learning_rate': 0.0001708027787319757, 'epoch': 0.27}


 27%|██▋       | 848/3107 [1:49:51<5:26:41,  8.68s/it]

 27%|██▋       | 849/3107 [1:50:05<6:27:07, 10.29s/it]
[2024-05-28 21:01:29,030] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 850/3107 [1:50:11<5:35:36,  8.92s/it]

 27%|██▋       | 851/3107 [1:50:19<5:27:48,  8.72s/it]

 27%|██▋       | 852/3107 [1:50:25<4:58:51,  7.95s/it]
{'loss': 1.0663, 'grad_norm': 0.16495756083043164, 'learning_rate': 0.00017035960165026666, 'epoch': 0.27}


 27%|██▋       | 854/3107 [1:50:47<6:08:14,  9.81s/it]
{'loss': 0.9247, 'grad_norm': 0.15067768106029317, 'learning_rate': 0.00017021126315024145, 'epoch': 0.27}

 28%|██▊       | 855/3107 [1:50:54<5:31:17,  8.83s/it]


 28%|██▊       | 857/3107 [1:51:09<5:05:58,  8.16s/it]

 28%|██▊       | 858/3107 [1:51:17<5:05:41,  8.16s/it]
{'loss': 0.8872, 'grad_norm': 0.14574937916167371, 'learning_rate': 0.00016991367080950202, 'epoch': 0.28}

 28%|██▊       | 859/3107 [1:51:26<5:15:08,  8.41s/it]


 28%|██▊       | 861/3107 [1:51:45<5:27:04,  8.74s/it]

 28%|██▊       | 862/3107 [1:51:53<5:25:28,  8.70s/it]
{'loss': 1.1184, 'grad_norm': 0.16497651559649193, 'learning_rate': 0.00016961486233053403, 'epoch': 0.28}


 28%|██▊       | 864/3107 [1:52:09<5:11:26,  8.33s/it]

 28%|██▊       | 865/3107 [1:52:15<4:45:36,  7.64s/it]

 28%|██▊       | 866/3107 [1:52:21<4:23:57,  7.07s/it]

 28%|██▊       | 867/3107 [1:52:27<4:13:23,  6.79s/it]

 28%|██▊       | 868/3107 [1:52:33<4:06:47,  6.61s/it]

 28%|██▊       | 869/3107 [1:52:40<4:06:38,  6.61s/it]
{'loss': 0.9878, 'grad_norm': 0.1548756022452244, 'learning_rate': 0.0001690890368054082, 'epoch': 0.28}


 28%|██▊       | 871/3107 [1:53:03<5:44:32,  9.25s/it]

 28%|██▊       | 872/3107 [1:53:12<5:38:37,  9.09s/it]

 28%|██▊       | 873/3107 [1:53:19<5:22:02,  8.65s/it]

 28%|██▊       | 874/3107 [1:53:33<6:24:22, 10.33s/it]

 28%|██▊       | 875/3107 [1:53:40<5:38:17,  9.09s/it]

 28%|██▊       | 876/3107 [1:53:48<5:25:19,  8.75s/it]
{'loss': 0.9873, 'grad_norm': 0.1696013312924413, 'learning_rate': 0.00016855953079765448, 'epoch': 0.28}

 28%|██▊       | 877/3107 [1:53:54<5:02:22,  8.14s/it]


 28%|██▊       | 879/3107 [1:54:10<5:01:54,  8.13s/it]

 28%|██▊       | 880/3107 [1:54:16<4:33:58,  7.38s/it]

 28%|██▊       | 881/3107 [1:54:24<4:42:56,  7.63s/it]
{'loss': 1.0235, 'grad_norm': 0.15797603968996682, 'learning_rate': 0.00016817907448147819, 'epoch': 0.28}

 28%|██▊       | 882/3107 [1:54:30<4:30:41,  7.30s/it]

 28%|██▊       | 883/3107 [1:54:36<4:16:08,  6.91s/it]


 28%|██▊       | 885/3107 [1:54:55<4:59:22,  8.08s/it]

 29%|██▊       | 886/3107 [1:55:03<4:59:56,  8.10s/it]

 29%|██▊       | 887/3107 [1:55:13<5:16:33,  8.56s/it]

 29%|██▊       | 888/3107 [1:55:23<5:34:05,  9.03s/it]

 29%|██▊       | 889/3107 [1:55:29<5:06:15,  8.28s/it]

 29%|██▊       | 890/3107 [1:55:35<4:34:44,  7.44s/it]

 29%|██▊       | 891/3107 [1:55:41<4:22:15,  7.10s/it]

 29%|██▊       | 892/3107 [1:55:47<4:08:11,  6.72s/it]

 29%|██▊       | 893/3107 [1:55:54<4:07:31,  6.71s/it]

 29%|██▉       | 894/3107 [1:56:01<4:19:11,  7.03s/it]
{'loss': 0.997, 'grad_norm': 0.16823925120841454, 'learning_rate': 0.00016718124165072953, 'epoch': 0.29}


 29%|██▉       | 896/3107 [1:56:17<4:31:49,  7.38s/it]
{'loss': 0.9198, 'grad_norm': 0.15690499755396048, 'learning_rate': 0.00016702662873239907, 'epoch': 0.29}

 29%|██▉       | 897/3107 [1:56:27<4:53:27,  7.97s/it]


 29%|██▉       | 899/3107 [1:56:41<4:40:49,  7.63s/it]

 29%|██▉       | 900/3107 [1:56:52<5:08:12,  8.38s/it]
 29%|██▉       | 900/3107 [1:56:52<5:08:12,  8.38s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.9301, 'grad_norm': 0.16500090395810257, 'learning_rate': 0.0001666388226856989, 'epoch': 0.29}

 29%|██▉       | 902/3107 [1:57:23<6:59:49, 11.42s/it]
{'loss': 0.9276, 'grad_norm': 0.17928945623570353, 'learning_rate': 0.00016656104379349213, 'epoch': 0.29}


 29%|██▉       | 904/3107 [1:57:39<5:52:33,  9.60s/it]

 29%|██▉       | 905/3107 [1:57:45<5:11:17,  8.48s/it]

 29%|██▉       | 906/3107 [1:57:52<4:52:32,  7.97s/it]
{'loss': 0.8264, 'grad_norm': 0.16984135239197798, 'learning_rate': 0.00016624920543310468, 'epoch': 0.29}


 29%|██▉       | 908/3107 [1:58:08<4:51:23,  7.95s/it]

 29%|██▉       | 909/3107 [1:58:14<4:29:43,  7.36s/it]

 29%|██▉       | 910/3107 [1:58:21<4:27:07,  7.30s/it]

 29%|██▉       | 911/3107 [1:58:30<4:42:45,  7.73s/it]

 29%|██▉       | 912/3107 [1:58:36<4:20:34,  7.12s/it]

 29%|██▉       | 913/3107 [1:58:41<4:04:42,  6.69s/it]

 29%|██▉       | 914/3107 [1:58:48<4:05:46,  6.72s/it]

 29%|██▉       | 915/3107 [1:58:59<4:48:42,  7.90s/it]

 29%|██▉       | 916/3107 [1:59:07<4:50:15,  7.95s/it]

 30%|██▉       | 917/3107 [1:59:15<4:53:16,  8.04s/it]

 30%|██▉       | 918/3107 [1:59:26<5:23:01,  8.85s/it]

 30%|██▉       | 919/3107 [1:59:35<5:28:18,  9.00s/it]

 30%|██▉       | 920/3107 [1:59:41<4:51:15,  7.99s/it]
{'loss': 0.97, 'grad_norm': 0.1582983119812653, 'learning_rate': 0.00016514873183626078, 'epoch': 0.3}


 30%|██▉       | 922/3107 [1:59:55<4:33:42,  7.52s/it]

 30%|██▉       | 923/3107 [2:00:01<4:18:35,  7.10s/it]

 30%|██▉       | 924/3107 [2:00:09<4:18:05,  7.09s/it]

 30%|██▉       | 925/3107 [2:00:16<4:22:58,  7.23s/it]

 30%|██▉       | 926/3107 [2:00:22<4:12:47,  6.95s/it]
{'loss': 0.9483, 'grad_norm': 0.15126635936558847, 'learning_rate': 0.00016467283688772311, 'epoch': 0.3}

 30%|██▉       | 927/3107 [2:00:29<4:07:45,  6.82s/it]


 30%|██▉       | 929/3107 [2:00:41<3:48:52,  6.30s/it]
{'loss': 0.9856, 'grad_norm': 0.17892798029820073, 'learning_rate': 0.00016443393904980242, 'epoch': 0.3}

 30%|██▉       | 930/3107 [2:00:49<4:07:56,  6.83s/it]


 30%|██▉       | 932/3107 [2:01:01<3:57:45,  6.56s/it]

 30%|███       | 933/3107 [2:01:08<3:56:50,  6.54s/it]

 30%|███       | 934/3107 [2:01:13<3:47:30,  6.28s/it]

 30%|███       | 935/3107 [2:01:19<3:43:40,  6.18s/it]

 30%|███       | 936/3107 [2:01:27<4:01:06,  6.66s/it]

 30%|███       | 937/3107 [2:01:34<4:08:03,  6.86s/it]

 30%|███       | 938/3107 [2:01:42<4:16:35,  7.10s/it]

 30%|███       | 939/3107 [2:01:49<4:10:13,  6.93s/it]

 30%|███       | 940/3107 [2:01:56<4:14:01,  7.03s/it]

 30%|███       | 941/3107 [2:02:02<4:08:04,  6.87s/it]

 30%|███       | 942/3107 [2:02:09<4:00:01,  6.65s/it]
{'loss': 0.889, 'grad_norm': 0.17250215498619054, 'learning_rate': 0.00016339145989463947, 'epoch': 0.3}


 30%|███       | 944/3107 [2:02:22<4:07:03,  6.85s/it]

 30%|███       | 945/3107 [2:02:30<4:11:27,  6.98s/it]
{'loss': 0.9596, 'grad_norm': 0.1652194864429158, 'learning_rate': 0.0001631492267587301, 'epoch': 0.3}


 30%|███       | 947/3107 [2:02:48<4:47:37,  7.99s/it]

 31%|███       | 948/3107 [2:02:54<4:31:28,  7.54s/it]
{'loss': 1.0198, 'grad_norm': 0.15735717246364114, 'learning_rate': 0.00016290637573209815, 'epoch': 0.31}

 31%|███       | 949/3107 [2:03:03<4:46:22,  7.96s/it]


 31%|███       | 951/3107 [2:03:21<5:02:27,  8.42s/it]

 31%|███       | 952/3107 [2:03:26<4:34:41,  7.65s/it]
{'loss': 0.9119, 'grad_norm': 0.16757030849694, 'learning_rate': 0.00016258161730779507, 'epoch': 0.31}


 31%|███       | 954/3107 [2:03:42<4:36:46,  7.71s/it]

 31%|███       | 955/3107 [2:03:48<4:27:11,  7.45s/it]
{'loss': 0.9846, 'grad_norm': 0.16015954111285421, 'learning_rate': 0.00016233733378734005, 'epoch': 0.31}

 31%|███       | 956/3107 [2:03:57<4:39:30,  7.80s/it]


 31%|███       | 958/3107 [2:04:10<4:14:56,  7.12s/it]

 31%|███       | 959/3107 [2:04:18<4:29:57,  7.54s/it]

 31%|███       | 960/3107 [2:04:27<4:39:41,  7.82s/it]

 31%|███       | 961/3107 [2:04:35<4:49:05,  8.08s/it]

 31%|███       | 962/3107 [2:04:45<5:09:34,  8.66s/it]
{'loss': 0.9132, 'grad_norm': 0.17264715569436548, 'learning_rate': 0.00016176497103388818, 'epoch': 0.31}


 31%|███       | 964/3107 [2:04:59<4:34:23,  7.68s/it]

 31%|███       | 965/3107 [2:05:08<4:51:34,  8.17s/it]
{'loss': 0.9622, 'grad_norm': 0.15481025914774785, 'learning_rate': 0.0001615186636866643, 'epoch': 0.31}


 31%|███       | 967/3107 [2:05:22<4:31:20,  7.61s/it]

 31%|███       | 968/3107 [2:05:28<4:14:28,  7.14s/it]

 31%|███       | 969/3107 [2:05:37<4:31:34,  7.62s/it]

 31%|███       | 970/3107 [2:05:46<4:51:33,  8.19s/it]

 31%|███▏      | 971/3107 [2:05:54<4:50:30,  8.16s/it]

 31%|███▏      | 972/3107 [2:06:02<4:43:00,  7.95s/it]

 31%|███▏      | 973/3107 [2:06:08<4:27:13,  7.51s/it]

 31%|███▏      | 974/3107 [2:06:14<4:05:53,  6.92s/it]

 31%|███▏      | 975/3107 [2:06:22<4:14:35,  7.16s/it]

 31%|███▏      | 976/3107 [2:06:28<4:02:54,  6.84s/it]

 31%|███▏      | 977/3107 [2:06:36<4:19:44,  7.32s/it]

 31%|███▏      | 978/3107 [2:06:45<4:35:18,  7.76s/it]

 32%|███▏      | 979/3107 [2:06:52<4:32:53,  7.69s/it]

 32%|███▏      | 980/3107 [2:07:01<4:40:06,  7.90s/it]

 32%|███▏      | 981/3107 [2:07:08<4:31:14,  7.65s/it]

 32%|███▏      | 982/3107 [2:07:14<4:11:28,  7.10s/it]

 32%|███▏      | 983/3107 [2:07:20<4:02:40,  6.86s/it]

 32%|███▏      | 984/3107 [2:07:28<4:15:08,  7.21s/it]

 32%|███▏      | 985/3107 [2:07:34<3:56:27,  6.69s/it]

 32%|███▏      | 986/3107 [2:07:44<4:35:59,  7.81s/it]

 32%|███▏      | 987/3107 [2:07:50<4:21:33,  7.40s/it]

 32%|███▏      | 988/3107 [2:07:56<4:07:04,  7.00s/it]

 32%|███▏      | 989/3107 [2:08:02<3:56:38,  6.70s/it]

 32%|███▏      | 990/3107 [2:08:10<4:04:01,  6.92s/it]

 32%|███▏      | 991/3107 [2:08:21<4:44:48,  8.08s/it]

 32%|███▏      | 992/3107 [2:08:27<4:21:03,  7.41s/it]
{'loss': 0.9647, 'grad_norm': 0.1652062577421615, 'learning_rate': 0.00015927510156369095, 'epoch': 0.32}


 32%|███▏      | 994/3107 [2:08:44<4:34:08,  7.78s/it]
{'loss': 0.9436, 'grad_norm': 0.15605346326511255, 'learning_rate': 0.0001591070209906579, 'epoch': 0.32}


 32%|███▏      | 996/3107 [2:09:01<4:50:09,  8.25s/it]

 32%|███▏      | 997/3107 [2:09:07<4:27:47,  7.61s/it]

 32%|███▏      | 998/3107 [2:09:16<4:42:15,  8.03s/it]
{'loss': 0.9907, 'grad_norm': 0.15660233059935905, 'learning_rate': 0.00015877008945722215, 'epoch': 0.32}


 32%|███▏      | 1000/3107 [2:09:32<4:38:35,  7.93s/it]

 32%|███▏      | 1001/3107 [2:09:38<4:14:30,  7.25s/it]

 32%|███▏      | 1002/3107 [2:09:44<4:05:04,  6.99s/it]
{'loss': 0.8974, 'grad_norm': 0.16776875931251298, 'learning_rate': 0.00015843213562654959, 'epoch': 0.32}


 32%|███▏      | 1004/3107 [2:10:00<4:22:14,  7.48s/it]

 32%|███▏      | 1005/3107 [2:10:07<4:18:10,  7.37s/it]

 32%|███▏      | 1006/3107 [2:10:13<4:07:05,  7.06s/it]

 32%|███▏      | 1007/3107 [2:10:22<4:30:41,  7.73s/it]

 32%|███▏      | 1008/3107 [2:10:28<4:11:58,  7.20s/it]

 32%|███▏      | 1009/3107 [2:10:34<3:57:49,  6.80s/it]

 33%|███▎      | 1010/3107 [2:10:40<3:45:36,  6.46s/it]

 33%|███▎      | 1011/3107 [2:10:46<3:44:28,  6.43s/it]

 33%|███▎      | 1012/3107 [2:10:53<3:49:08,  6.56s/it]
{'loss': 0.9485, 'grad_norm': 0.1720150089595613, 'learning_rate': 0.0001575828171216367, 'epoch': 0.33}


 33%|███▎      | 1014/3107 [2:11:11<4:28:49,  7.71s/it]

 33%|███▎      | 1015/3107 [2:11:20<4:51:11,  8.35s/it]

 33%|███▎      | 1016/3107 [2:11:31<5:15:56,  9.07s/it]

 33%|███▎      | 1017/3107 [2:11:41<5:24:57,  9.33s/it]

 33%|███▎      | 1018/3107 [2:11:47<4:45:56,  8.21s/it]
{'loss': 1.006, 'grad_norm': 0.17324668538123594, 'learning_rate': 0.00015707021516924593, 'epoch': 0.33}

 33%|███▎      | 1019/3107 [2:11:55<4:49:24,  8.32s/it]


 33%|███▎      | 1021/3107 [2:12:14<5:01:08,  8.66s/it]
{'loss': 0.9648, 'grad_norm': 0.15800572451800862, 'learning_rate': 0.00015681307532285215, 'epoch': 0.33}

 33%|███▎      | 1022/3107 [2:12:22<4:47:43,  8.28s/it]

 33%|███▎      | 1023/3107 [2:12:28<4:23:06,  7.57s/it]


 33%|███▎      | 1025/3107 [2:12:39<3:51:53,  6.68s/it]

 33%|███▎      | 1026/3107 [2:12:47<4:03:01,  7.01s/it]

 33%|███▎      | 1027/3107 [2:12:55<4:10:39,  7.23s/it]
{'loss': 0.9664, 'grad_norm': 0.1678810177007705, 'learning_rate': 0.00015629713046971214, 'epoch': 0.33}


 33%|███▎      | 1029/3107 [2:13:11<4:18:38,  7.47s/it]

 33%|███▎      | 1030/3107 [2:13:18<4:22:25,  7.58s/it]

 33%|███▎      | 1031/3107 [2:13:25<4:16:19,  7.41s/it]
{'loss': 1.0455, 'grad_norm': 0.1730045019600116, 'learning_rate': 0.00015595194188586063, 'epoch': 0.33}

 33%|███▎      | 1032/3107 [2:13:32<4:05:28,  7.10s/it]


 33%|███▎      | 1034/3107 [2:13:44<3:52:20,  6.72s/it]

 33%|███▎      | 1035/3107 [2:13:53<4:07:40,  7.17s/it]

 33%|███▎      | 1036/3107 [2:13:58<3:49:15,  6.64s/it]

 33%|███▎      | 1037/3107 [2:14:06<4:03:17,  7.05s/it]
{'loss': 0.9127, 'grad_norm': 0.15060801491240727, 'learning_rate': 0.00015543233599868742, 'epoch': 0.33}


 33%|███▎      | 1039/3107 [2:14:21<4:08:01,  7.20s/it]

 33%|███▎      | 1040/3107 [2:14:27<3:59:11,  6.94s/it]

 34%|███▎      | 1041/3107 [2:14:35<4:06:55,  7.17s/it]

 34%|███▎      | 1042/3107 [2:14:41<4:00:49,  7.00s/it]
{'loss': 0.9822, 'grad_norm': 0.16772897304231588, 'learning_rate': 0.00015499767294691093, 'epoch': 0.34}

 34%|███▎      | 1043/3107 [2:14:50<4:13:39,  7.37s/it]

 34%|███▎      | 1044/3107 [2:14:58<4:17:43,  7.50s/it]


 34%|███▎      | 1046/3107 [2:15:11<4:01:01,  7.02s/it]

 34%|███▎      | 1047/3107 [2:15:22<4:46:48,  8.35s/it]
{'loss': 0.8679, 'grad_norm': 0.164639097949459, 'learning_rate': 0.00015456151508917316, 'epoch': 0.34}


 34%|███▍      | 1049/3107 [2:15:34<4:06:53,  7.20s/it]
{'loss': 0.988, 'grad_norm': 0.16378542628480697, 'learning_rate': 0.00015438663605446507, 'epoch': 0.34}


 34%|███▍      | 1051/3107 [2:15:49<4:11:18,  7.33s/it]

 34%|███▍      | 1052/3107 [2:15:57<4:25:52,  7.76s/it]
{'loss': 0.9111, 'grad_norm': 0.16902090264632788, 'learning_rate': 0.00015412387428000054, 'epoch': 0.34}


 34%|███▍      | 1054/3107 [2:16:12<4:24:30,  7.73s/it]

 34%|███▍      | 1055/3107 [2:16:21<4:36:40,  8.09s/it]

 34%|███▍      | 1056/3107 [2:16:29<4:29:07,  7.87s/it]
{'loss': 1.0279, 'grad_norm': 0.16188783673533247, 'learning_rate': 0.00015377270189961058, 'epoch': 0.34}


 34%|███▍      | 1058/3107 [2:16:44<4:17:00,  7.53s/it]

 34%|███▍      | 1059/3107 [2:16:50<4:09:35,  7.31s/it]

 34%|███▍      | 1060/3107 [2:16:58<4:14:29,  7.46s/it]

 34%|███▍      | 1061/3107 [2:17:05<4:05:10,  7.19s/it]

 34%|███▍      | 1062/3107 [2:17:11<3:50:22,  6.76s/it]
{'loss': 0.9744, 'grad_norm': 0.16243512851487651, 'learning_rate': 0.000153244191426662, 'epoch': 0.34}


 34%|███▍      | 1064/3107 [2:17:28<4:33:52,  8.04s/it]

 34%|███▍      | 1065/3107 [2:17:36<4:25:17,  7.80s/it]

 34%|███▍      | 1066/3107 [2:17:44<4:34:43,  8.08s/it]

 34%|███▍      | 1067/3107 [2:17:51<4:22:20,  7.72s/it]

 34%|███▍      | 1068/3107 [2:17:57<4:00:51,  7.09s/it]

 34%|███▍      | 1069/3107 [2:18:03<3:49:50,  6.77s/it]

 34%|███▍      | 1070/3107 [2:18:10<3:57:24,  6.99s/it]

 34%|███▍      | 1071/3107 [2:18:20<4:27:13,  7.87s/it]

 35%|███▍      | 1072/3107 [2:18:26<4:08:25,  7.32s/it]

 35%|███▍      | 1073/3107 [2:18:32<3:53:33,  6.89s/it]

 35%|███▍      | 1074/3107 [2:18:38<3:38:33,  6.45s/it]

 35%|███▍      | 1075/3107 [2:18:44<3:33:03,  6.29s/it]

 35%|███▍      | 1076/3107 [2:18:49<3:24:01,  6.03s/it]
{'loss': 1.0398, 'grad_norm': 0.1565973623545062, 'learning_rate': 0.00015200293221074505, 'epoch': 0.35}

 35%|███▍      | 1077/3107 [2:18:56<3:31:57,  6.26s/it]


 35%|███▍      | 1079/3107 [2:19:14<4:31:05,  8.02s/it]

 35%|███▍      | 1080/3107 [2:19:22<4:32:33,  8.07s/it]

 35%|███▍      | 1081/3107 [2:19:29<4:17:31,  7.63s/it]

 35%|███▍      | 1082/3107 [2:19:35<4:03:40,  7.22s/it]

 35%|███▍      | 1083/3107 [2:19:41<3:45:49,  6.69s/it]

 35%|███▍      | 1084/3107 [2:19:47<3:38:26,  6.48s/it]

 35%|███▍      | 1085/3107 [2:19:58<4:31:26,  8.05s/it]

 35%|███▍      | 1086/3107 [2:20:12<5:21:35,  9.55s/it]
{'loss': 0.8734, 'grad_norm': 0.15110922128692703, 'learning_rate': 0.00015110951870312187, 'epoch': 0.35}

 35%|███▍      | 1087/3107 [2:20:18<4:52:41,  8.69s/it]


 35%|███▌      | 1089/3107 [2:20:35<4:47:29,  8.55s/it]

 35%|███▌      | 1090/3107 [2:20:43<4:45:41,  8.50s/it]

 35%|███▌      | 1091/3107 [2:20:53<4:53:05,  8.72s/it]

 35%|███▌      | 1092/3107 [2:21:00<4:45:21,  8.50s/it]

 35%|███▌      | 1093/3107 [2:21:06<4:19:27,  7.73s/it]

 35%|███▌      | 1094/3107 [2:21:15<4:24:53,  7.90s/it]

 35%|███▌      | 1095/3107 [2:21:21<4:07:32,  7.38s/it]

 35%|███▌      | 1096/3107 [2:21:29<4:12:26,  7.53s/it]

 35%|███▌      | 1097/3107 [2:21:35<4:03:09,  7.26s/it]

 35%|███▌      | 1098/3107 [2:21:41<3:51:22,  6.91s/it]

 35%|███▌      | 1099/3107 [2:21:47<3:40:32,  6.59s/it]

 35%|███▌      | 1100/3107 [2:21:53<3:30:47,  6.30s/it]

 35%|███▌      | 1101/3107 [2:22:07<4:43:48,  8.49s/it]

 35%|███▌      | 1102/3107 [2:22:13<4:20:12,  7.79s/it]

 36%|███▌      | 1103/3107 [2:22:19<4:09:53,  7.48s/it]
{'loss': 0.9147, 'grad_norm': 0.17771142721269792, 'learning_rate': 0.0001495780159430593, 'epoch': 0.36}


 36%|███▌      | 1105/3107 [2:22:41<5:00:52,  9.02s/it]

 36%|███▌      | 1106/3107 [2:22:47<4:28:50,  8.06s/it]

 36%|███▌      | 1107/3107 [2:22:53<4:11:23,  7.54s/it]

 36%|███▌      | 1108/3107 [2:23:06<5:03:15,  9.10s/it]
{'loss': 0.9256, 'grad_norm': 0.1582253247135312, 'learning_rate': 0.0001491245878459653, 'epoch': 0.36}


 36%|███▌      | 1110/3107 [2:23:24<4:58:49,  8.98s/it]

 36%|███▌      | 1111/3107 [2:23:32<4:50:16,  8.73s/it]

 36%|███▌      | 1112/3107 [2:23:38<4:26:18,  8.01s/it]

 36%|███▌      | 1113/3107 [2:23:44<4:06:12,  7.41s/it]

 36%|███▌      | 1114/3107 [2:23:53<4:22:11,  7.89s/it]

 36%|███▌      | 1115/3107 [2:24:01<4:21:33,  7.88s/it]
{'loss': 0.9478, 'grad_norm': 0.159976192426701, 'learning_rate': 0.0001484875481771318, 'epoch': 0.36}

 36%|███▌      | 1116/3107 [2:24:10<4:37:28,  8.36s/it]

 36%|███▌      | 1117/3107 [2:24:19<4:35:42,  8.31s/it]


 36%|███▌      | 1119/3107 [2:24:31<3:58:58,  7.21s/it]

 36%|███▌      | 1120/3107 [2:24:41<4:28:10,  8.10s/it]

 36%|███▌      | 1121/3107 [2:24:49<4:27:42,  8.09s/it]

 36%|███▌      | 1122/3107 [2:24:55<4:08:34,  7.51s/it]

 36%|███▌      | 1123/3107 [2:25:02<4:01:38,  7.31s/it]

 36%|███▌      | 1124/3107 [2:25:09<3:58:18,  7.21s/it]

 36%|███▌      | 1125/3107 [2:25:15<3:49:35,  6.95s/it]

 36%|███▌      | 1126/3107 [2:25:22<3:44:54,  6.81s/it]
{'loss': 1.0185, 'grad_norm': 0.1554783126509929, 'learning_rate': 0.00014748128021595543, 'epoch': 0.36}


 36%|███▋      | 1128/3107 [2:25:41<4:27:36,  8.11s/it]

 36%|███▋      | 1129/3107 [2:25:47<4:11:49,  7.64s/it]

 36%|███▋      | 1130/3107 [2:25:54<3:57:05,  7.20s/it]

 36%|███▋      | 1131/3107 [2:25:59<3:43:41,  6.79s/it]
{'loss': 0.8597, 'grad_norm': 0.161293613803746, 'learning_rate': 0.00014702181289598292, 'epoch': 0.36}


 36%|███▋      | 1133/3107 [2:26:15<4:07:44,  7.53s/it]

 36%|███▋      | 1134/3107 [2:26:25<4:29:01,  8.18s/it]

 37%|███▋      | 1135/3107 [2:26:34<4:32:39,  8.30s/it]

 37%|███▋      | 1136/3107 [2:26:40<4:15:27,  7.78s/it]

 37%|███▋      | 1137/3107 [2:26:46<3:57:49,  7.24s/it]

 37%|███▋      | 1138/3107 [2:26:53<3:51:16,  7.05s/it]

 37%|███▋      | 1139/3107 [2:27:01<4:04:56,  7.47s/it]

 37%|███▋      | 1140/3107 [2:27:08<3:55:54,  7.20s/it]

 37%|███▋      | 1141/3107 [2:27:14<3:45:58,  6.90s/it]

 37%|███▋      | 1142/3107 [2:27:21<3:44:31,  6.86s/it]
{'loss': 0.85, 'grad_norm': 0.15660147916496067, 'learning_rate': 0.00014600650377311522, 'epoch': 0.37}


 37%|███▋      | 1144/3107 [2:27:36<3:57:37,  7.26s/it]
{'loss': 0.9288, 'grad_norm': 0.15585121789470965, 'learning_rate': 0.00014582124796959765, 'epoch': 0.37}


 37%|███▋      | 1146/3107 [2:27:51<4:03:25,  7.45s/it]
{'loss': 0.856, 'grad_norm': 0.16578409033362876, 'learning_rate': 0.00014563579290236223, 'epoch': 0.37}


 37%|███▋      | 1148/3107 [2:28:08<4:22:56,  8.05s/it]
{'loss': 0.9886, 'grad_norm': 0.1551318415758617, 'learning_rate': 0.0001454501393779007, 'epoch': 0.37}

 37%|███▋      | 1149/3107 [2:28:17<4:28:51,  8.24s/it]


 37%|███▋      | 1151/3107 [2:28:33<4:20:14,  7.98s/it]

 37%|███▋      | 1152/3107 [2:28:42<4:33:49,  8.40s/it]

 37%|███▋      | 1153/3107 [2:28:53<4:55:53,  9.09s/it]

 37%|███▋      | 1154/3107 [2:29:01<4:48:50,  8.87s/it]

 37%|███▋      | 1155/3107 [2:29:09<4:39:36,  8.59s/it]

 37%|███▋      | 1156/3107 [2:29:16<4:21:12,  8.03s/it]

 37%|███▋      | 1157/3107 [2:29:23<4:13:21,  7.80s/it]

 37%|███▋      | 1158/3107 [2:29:32<4:20:43,  8.03s/it]

 37%|███▋      | 1159/3107 [2:29:40<4:21:27,  8.05s/it]

 37%|███▋      | 1160/3107 [2:29:46<4:04:03,  7.52s/it]

 37%|███▋      | 1161/3107 [2:29:52<3:50:46,  7.12s/it]

 37%|███▋      | 1162/3107 [2:30:00<3:51:05,  7.13s/it]

 37%|███▋      | 1163/3107 [2:30:12<4:41:24,  8.69s/it]

 37%|███▋      | 1164/3107 [2:30:18<4:11:26,  7.76s/it]

 37%|███▋      | 1165/3107 [2:30:23<3:52:55,  7.20s/it]

 38%|███▊      | 1166/3107 [2:30:34<4:27:01,  8.25s/it]
{'loss': 0.8833, 'grad_norm': 0.16297515226059409, 'learning_rate': 0.00014377046057816145, 'epoch': 0.38}


 38%|███▊      | 1168/3107 [2:30:50<4:18:48,  8.01s/it]

 38%|███▊      | 1169/3107 [2:31:00<4:28:50,  8.32s/it]

 38%|███▊      | 1170/3107 [2:31:08<4:31:01,  8.40s/it]

 38%|███▊      | 1171/3107 [2:31:14<4:07:48,  7.68s/it]
{'loss': 0.918, 'grad_norm': 0.16754790355033616, 'learning_rate': 0.00014330112158641943, 'epoch': 0.38}


 38%|███▊      | 1173/3107 [2:31:32<4:33:33,  8.49s/it]
[2024-05-28 21:42:55,991] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9828, 'grad_norm': 0.16932558514356955, 'learning_rate': 0.00014311305574221916, 'epoch': 0.38}


 38%|███▊      | 1175/3107 [2:31:51<4:48:41,  8.97s/it]

 38%|███▊      | 1176/3107 [2:31:57<4:21:18,  8.12s/it]

 38%|███▊      | 1177/3107 [2:32:06<4:25:06,  8.24s/it]
{'loss': 0.8223, 'grad_norm': 0.17597688288736882, 'learning_rate': 0.00014273636241282417, 'epoch': 0.38}


 38%|███▊      | 1179/3107 [2:32:21<4:17:36,  8.02s/it]

 38%|███▊      | 1180/3107 [2:32:30<4:26:57,  8.31s/it]
{'loss': 0.8277, 'grad_norm': 0.15808378293481132, 'learning_rate': 0.00014245335420538146, 'epoch': 0.38}


 38%|███▊      | 1182/3107 [2:32:48<4:38:30,  8.68s/it]

 38%|███▊      | 1183/3107 [2:32:57<4:35:37,  8.60s/it]

 38%|███▊      | 1184/3107 [2:33:03<4:10:47,  7.83s/it]
{'loss': 1.0246, 'grad_norm': 0.15986992149781856, 'learning_rate': 0.0001420753642463504, 'epoch': 0.38}


 38%|███▊      | 1186/3107 [2:33:17<3:59:49,  7.49s/it]

 38%|███▊      | 1187/3107 [2:33:25<3:57:14,  7.41s/it]

 38%|███▊      | 1188/3107 [2:33:30<3:41:49,  6.94s/it]
{'loss': 0.8342, 'grad_norm': 0.15326811143037192, 'learning_rate': 0.00014169664239242175, 'epoch': 0.38}


 38%|███▊      | 1190/3107 [2:33:46<3:53:26,  7.31s/it]
{'loss': 0.9377, 'grad_norm': 0.18191106543637028, 'learning_rate': 0.0001415070090630679, 'epoch': 0.38}


 38%|███▊      | 1192/3107 [2:34:07<4:41:50,  8.83s/it]

 38%|███▊      | 1193/3107 [2:34:16<4:42:31,  8.86s/it]
{'loss': 0.8283, 'grad_norm': 0.16105262383871682, 'learning_rate': 0.000141222220885134, 'epoch': 0.38}


 38%|███▊      | 1195/3107 [2:34:31<4:10:39,  7.87s/it]

 38%|███▊      | 1196/3107 [2:34:38<4:02:42,  7.62s/it]

 39%|███▊      | 1197/3107 [2:34:46<4:13:13,  7.95s/it]
{'loss': 0.9448, 'grad_norm': 0.16061087745189748, 'learning_rate': 0.00014084187637358324, 'epoch': 0.39}

 39%|███▊      | 1198/3107 [2:34:53<3:59:15,  7.52s/it]

 39%|███▊      | 1199/3107 [2:35:03<4:23:35,  8.29s/it]

 39%|███▊      | 1200/3107 [2:35:09<4:02:17,  7.62s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.8526, 'grad_norm': 0.1525328059689823, 'learning_rate': 0.00014046082142347863, 'epoch': 0.39}

 39%|███▊      | 1202/3107 [2:35:42<5:56:14, 11.22s/it]
{'loss': 0.9342, 'grad_norm': 0.15245754671544712, 'learning_rate': 0.00014036544745642374, 'epoch': 0.39}

 39%|███▊      | 1203/3107 [2:35:49<5:15:19,  9.94s/it]


 39%|███▉      | 1205/3107 [2:36:05<4:41:01,  8.87s/it]
{'loss': 1.0279, 'grad_norm': 0.16898293555421828, 'learning_rate': 0.00014007906266321615, 'epoch': 0.39}

 39%|███▉      | 1206/3107 [2:36:11<4:17:53,  8.14s/it]


 39%|███▉      | 1208/3107 [2:36:30<4:40:44,  8.87s/it]

 39%|███▉      | 1209/3107 [2:36:37<4:16:10,  8.10s/it]

 39%|███▉      | 1210/3107 [2:36:42<3:49:27,  7.26s/it]

 39%|███▉      | 1211/3107 [2:36:49<3:44:28,  7.10s/it]

 39%|███▉      | 1212/3107 [2:36:55<3:35:19,  6.82s/it]

 39%|███▉      | 1213/3107 [2:37:01<3:24:04,  6.47s/it]

 39%|███▉      | 1214/3107 [2:37:09<3:38:38,  6.93s/it]

 39%|███▉      | 1215/3107 [2:37:15<3:31:46,  6.72s/it]

 39%|███▉      | 1216/3107 [2:37:22<3:37:53,  6.91s/it]
{'loss': 0.8443, 'grad_norm': 0.17235478654460168, 'learning_rate': 0.00013902565130920988, 'epoch': 0.39}

 39%|███▉      | 1217/3107 [2:37:29<3:40:38,  7.00s/it]


 39%|███▉      | 1219/3107 [2:37:46<4:10:44,  7.97s/it]

 39%|███▉      | 1220/3107 [2:37:54<4:09:40,  7.94s/it]
{'loss': 0.9899, 'grad_norm': 0.16832978218750172, 'learning_rate': 0.00013864131250263646, 'epoch': 0.39}

 39%|███▉      | 1221/3107 [2:38:04<4:26:32,  8.48s/it]


 39%|███▉      | 1223/3107 [2:38:19<4:10:37,  7.98s/it]
{'loss': 0.916, 'grad_norm': 0.17618850866417715, 'learning_rate': 0.00013835261692661044, 'epoch': 0.39}

 39%|███▉      | 1224/3107 [2:38:26<3:59:28,  7.63s/it]

 39%|███▉      | 1225/3107 [2:38:32<3:45:43,  7.20s/it]


 39%|███▉      | 1227/3107 [2:38:46<3:40:47,  7.05s/it]
{'loss': 0.9231, 'grad_norm': 0.16269121461640304, 'learning_rate': 0.00013796710623433696, 'epoch': 0.39}

 40%|███▉      | 1228/3107 [2:38:55<4:02:10,  7.73s/it]

 40%|███▉      | 1229/3107 [2:39:04<4:09:52,  7.98s/it]


 40%|███▉      | 1231/3107 [2:39:19<3:55:23,  7.53s/it]

 40%|███▉      | 1232/3107 [2:39:24<3:38:04,  6.98s/it]

 40%|███▉      | 1233/3107 [2:39:35<4:08:49,  7.97s/it]

 40%|███▉      | 1234/3107 [2:39:43<4:09:08,  7.98s/it]

 40%|███▉      | 1235/3107 [2:39:49<3:53:11,  7.47s/it]

 40%|███▉      | 1236/3107 [2:39:55<3:38:02,  6.99s/it]
{'loss': 0.9375, 'grad_norm': 0.16968592175439098, 'learning_rate': 0.0001370973027055827, 'epoch': 0.4}

 40%|███▉      | 1237/3107 [2:40:01<3:31:02,  6.77s/it]


 40%|███▉      | 1239/3107 [2:40:14<3:28:19,  6.69s/it]
{'loss': 0.9173, 'grad_norm': 0.16540916666003047, 'learning_rate': 0.0001368066384444028, 'epoch': 0.4}

 40%|███▉      | 1240/3107 [2:40:21<3:29:42,  6.74s/it]

 40%|███▉      | 1241/3107 [2:40:28<3:25:46,  6.62s/it]

 40%|███▉      | 1242/3107 [2:40:34<3:24:02,  6.56s/it]

 40%|████      | 1243/3107 [2:40:42<3:36:29,  6.97s/it]


 40%|████      | 1245/3107 [2:40:54<3:25:25,  6.62s/it]

 40%|████      | 1246/3107 [2:41:03<3:41:40,  7.15s/it]

 40%|████      | 1247/3107 [2:41:09<3:32:09,  6.84s/it]

 40%|████      | 1248/3107 [2:41:17<3:40:41,  7.12s/it]

 40%|████      | 1249/3107 [2:41:25<3:48:08,  7.37s/it]
{'loss': 1.0973, 'grad_norm': 0.16963513244687173, 'learning_rate': 0.00013583517257418278, 'epoch': 0.4}

 40%|████      | 1250/3107 [2:41:31<3:41:52,  7.17s/it]


 40%|████      | 1252/3107 [2:41:45<3:37:39,  7.04s/it]

 40%|████      | 1253/3107 [2:41:53<3:41:52,  7.18s/it]

 40%|████      | 1254/3107 [2:41:59<3:34:16,  6.94s/it]
{'loss': 1.0418, 'grad_norm': 0.17718683986841738, 'learning_rate': 0.000135347972061497, 'epoch': 0.4}

 40%|████      | 1255/3107 [2:42:07<3:45:43,  7.31s/it]

 40%|████      | 1256/3107 [2:42:14<3:41:45,  7.19s/it]

 40%|████      | 1257/3107 [2:42:22<3:45:33,  7.32s/it]

 40%|████      | 1258/3107 [2:42:27<3:30:36,  6.83s/it]


 41%|████      | 1260/3107 [2:42:40<3:23:30,  6.61s/it]
{'loss': 1.1375, 'grad_norm': 0.1692675830906511, 'learning_rate': 0.000134762064439117, 'epoch': 0.41}


 41%|████      | 1262/3107 [2:42:52<3:15:02,  6.34s/it]

 41%|████      | 1263/3107 [2:43:01<3:38:14,  7.10s/it]

 41%|████      | 1264/3107 [2:43:12<4:15:04,  8.30s/it]

 41%|████      | 1265/3107 [2:43:21<4:14:52,  8.30s/it]

 41%|████      | 1266/3107 [2:43:27<4:00:31,  7.84s/it]

 41%|████      | 1267/3107 [2:43:33<3:41:06,  7.21s/it]
{'loss': 1.1127, 'grad_norm': 0.1769410989974355, 'learning_rate': 0.00013407678760069891, 'epoch': 0.41}

 41%|████      | 1268/3107 [2:43:44<4:10:22,  8.17s/it]


 41%|████      | 1270/3107 [2:43:57<3:50:25,  7.53s/it]
{'loss': 0.9791, 'grad_norm': 0.1830417245410226, 'learning_rate': 0.0001337825396834312, 'epoch': 0.41}


 41%|████      | 1272/3107 [2:44:13<3:58:23,  7.79s/it]
{'loss': 0.9788, 'grad_norm': 0.16339091422155053, 'learning_rate': 0.00013358619058902762, 'epoch': 0.41}


 41%|████      | 1274/3107 [2:44:27<3:46:32,  7.42s/it]
{'loss': 0.9646, 'grad_norm': 0.1737422640380935, 'learning_rate': 0.00013338969543772892, 'epoch': 0.41}


 41%|████      | 1276/3107 [2:44:41<3:39:59,  7.21s/it]
{'loss': 1.0385, 'grad_norm': 0.1738374977285869, 'learning_rate': 0.00013319305508403728, 'epoch': 0.41}


 41%|████      | 1278/3107 [2:44:57<3:49:00,  7.51s/it]

 41%|████      | 1279/3107 [2:45:05<3:50:25,  7.56s/it]

 41%|████      | 1280/3107 [2:45:14<4:09:43,  8.20s/it]

 41%|████      | 1281/3107 [2:45:25<4:28:12,  8.81s/it]
{'loss': 0.879, 'grad_norm': 0.16527105033371361, 'learning_rate': 0.00013270082455269695, 'epoch': 0.41}

 41%|████▏     | 1282/3107 [2:45:30<3:58:19,  7.84s/it]

 41%|████▏     | 1283/3107 [2:45:36<3:42:54,  7.33s/it]


 41%|████▏     | 1285/3107 [2:45:51<3:37:34,  7.17s/it]

 41%|████▏     | 1286/3107 [2:46:05<4:37:07,  9.13s/it]

 41%|████▏     | 1287/3107 [2:46:12<4:15:34,  8.43s/it]
{'loss': 0.923, 'grad_norm': 0.16139108244623812, 'learning_rate': 0.000132108975891168, 'epoch': 0.41}

 41%|████▏     | 1288/3107 [2:46:20<4:17:46,  8.50s/it]

 41%|████▏     | 1289/3107 [2:46:26<3:49:16,  7.57s/it]


 42%|████▏     | 1291/3107 [2:46:39<3:39:14,  7.24s/it]

 42%|████▏     | 1292/3107 [2:46:51<4:18:06,  8.53s/it]
{'loss': 0.866, 'grad_norm': 0.1497190180321996, 'learning_rate': 0.00013161480771527157, 'epoch': 0.42}


 42%|████▏     | 1294/3107 [2:47:05<3:59:52,  7.94s/it]
{'loss': 0.9458, 'grad_norm': 0.18220975379818533, 'learning_rate': 0.00013141689909559943, 'epoch': 0.42}

 42%|████▏     | 1295/3107 [2:47:12<3:46:59,  7.52s/it]


 42%|████▏     | 1297/3107 [2:47:28<3:56:09,  7.83s/it]

 42%|████▏     | 1298/3107 [2:47:35<3:52:27,  7.71s/it]

 42%|████▏     | 1299/3107 [2:47:41<3:32:44,  7.06s/it]

 42%|████▏     | 1300/3107 [2:47:49<3:41:28,  7.35s/it]
{'loss': 1.0505, 'grad_norm': 0.16309236290268392, 'learning_rate': 0.00013082235694260595, 'epoch': 0.42}

 42%|████▏     | 1301/3107 [2:47:57<3:46:17,  7.52s/it]

 42%|████▏     | 1302/3107 [2:48:04<3:49:42,  7.64s/it]

 42%|████▏     | 1303/3107 [2:48:12<3:50:41,  7.67s/it]

 42%|████▏     | 1304/3107 [2:48:20<3:51:37,  7.71s/it]

 42%|████▏     | 1305/3107 [2:48:28<3:53:44,  7.78s/it]

 42%|████▏     | 1306/3107 [2:48:36<3:53:44,  7.79s/it]


 42%|████▏     | 1308/3107 [2:48:57<4:36:35,  9.22s/it]
{'loss': 0.9382, 'grad_norm': 0.16549733577245102, 'learning_rate': 0.00013002776157743762, 'epoch': 0.42}

 42%|████▏     | 1309/3107 [2:49:05<4:23:16,  8.79s/it]


 42%|████▏     | 1311/3107 [2:49:22<4:20:25,  8.70s/it]

 42%|████▏     | 1312/3107 [2:49:30<4:16:17,  8.57s/it]

 42%|████▏     | 1313/3107 [2:49:37<4:01:40,  8.08s/it]
{'loss': 1.0523, 'grad_norm': 0.1742289739815875, 'learning_rate': 0.00012953007498333808, 'epoch': 0.42}


 42%|████▏     | 1315/3107 [2:49:47<3:19:47,  6.69s/it]

 42%|████▏     | 1316/3107 [2:49:55<3:30:37,  7.06s/it]

 42%|████▏     | 1317/3107 [2:50:04<3:42:08,  7.45s/it]

 42%|████▏     | 1318/3107 [2:50:11<3:44:03,  7.51s/it]

 42%|████▏     | 1319/3107 [2:50:18<3:31:28,  7.10s/it]

 42%|████▏     | 1320/3107 [2:50:23<3:17:11,  6.62s/it]

 43%|████▎     | 1321/3107 [2:50:32<3:36:38,  7.28s/it]

 43%|████▎     | 1322/3107 [2:50:37<3:20:19,  6.73s/it]
{'loss': 1.0665, 'grad_norm': 0.18398510147573885, 'learning_rate': 0.0001286322256384496, 'epoch': 0.43}


 43%|████▎     | 1324/3107 [2:50:53<3:39:42,  7.39s/it]

 43%|████▎     | 1325/3107 [2:50:59<3:26:24,  6.95s/it]
{'loss': 0.9582, 'grad_norm': 0.16637560643381213, 'learning_rate': 0.00012833237830573766, 'epoch': 0.43}

 43%|████▎     | 1326/3107 [2:51:06<3:31:21,  7.12s/it]


 43%|████▎     | 1328/3107 [2:51:19<3:18:43,  6.70s/it]

 43%|████▎     | 1329/3107 [2:51:25<3:16:45,  6.64s/it]
{'loss': 0.9321, 'grad_norm': 0.16516654676951636, 'learning_rate': 0.00012793215113639862, 'epoch': 0.43}


 43%|████▎     | 1331/3107 [2:51:40<3:21:57,  6.82s/it]
{'loss': 1.0283, 'grad_norm': 0.17026243086843532, 'learning_rate': 0.00012773185491287532, 'epoch': 0.43}

 43%|████▎     | 1332/3107 [2:51:48<3:35:56,  7.30s/it]

 43%|████▎     | 1333/3107 [2:51:57<3:44:48,  7.60s/it]

 43%|████▎     | 1334/3107 [2:52:07<4:06:59,  8.36s/it]


 43%|████▎     | 1336/3107 [2:52:24<4:02:47,  8.23s/it]

 43%|████▎     | 1337/3107 [2:52:29<3:37:43,  7.38s/it]

 43%|████▎     | 1338/3107 [2:52:36<3:34:06,  7.26s/it]

 43%|████▎     | 1339/3107 [2:52:43<3:34:11,  7.27s/it]

 43%|████▎     | 1340/3107 [2:52:51<3:39:48,  7.46s/it]

 43%|████▎     | 1341/3107 [2:53:00<3:48:52,  7.78s/it]

 43%|████▎     | 1342/3107 [2:53:09<4:05:20,  8.34s/it]

 43%|████▎     | 1343/3107 [2:53:18<4:08:27,  8.45s/it]

 43%|████▎     | 1344/3107 [2:53:26<4:03:59,  8.30s/it]

 43%|████▎     | 1345/3107 [2:53:32<3:43:07,  7.60s/it]

 43%|████▎     | 1346/3107 [2:53:37<3:23:29,  6.93s/it]

 43%|████▎     | 1347/3107 [2:53:45<3:33:54,  7.29s/it]
{'loss': 0.8592, 'grad_norm': 0.1720583045689355, 'learning_rate': 0.00012612521687125085, 'epoch': 0.43}


 43%|████▎     | 1349/3107 [2:54:00<3:32:11,  7.24s/it]

 43%|████▎     | 1350/3107 [2:54:06<3:20:43,  6.85s/it]
{'loss': 0.9993, 'grad_norm': 0.1739090815987363, 'learning_rate': 0.00012582314926081245, 'epoch': 0.43}


 44%|████▎     | 1352/3107 [2:54:22<3:44:02,  7.66s/it]
{'loss': 0.9564, 'grad_norm': 0.1713062290416605, 'learning_rate': 0.00012562163029935462, 'epoch': 0.44}


 44%|████▎     | 1354/3107 [2:54:36<3:32:56,  7.29s/it]
{'loss': 1.0029, 'grad_norm': 0.16753115070347518, 'learning_rate': 0.00012541999991663388, 'epoch': 0.44}


 44%|████▎     | 1356/3107 [2:55:00<4:41:06,  9.63s/it]

 44%|████▎     | 1357/3107 [2:55:09<4:37:39,  9.52s/it]

 44%|████▎     | 1358/3107 [2:55:16<4:15:57,  8.78s/it]

 44%|████▎     | 1359/3107 [2:55:22<3:47:49,  7.82s/it]
{'loss': 0.9473, 'grad_norm': 0.1740442704541929, 'learning_rate': 0.00012491544224721136, 'epoch': 0.44}


 44%|████▍     | 1361/3107 [2:55:38<3:47:16,  7.81s/it]

 44%|████▍     | 1362/3107 [2:55:47<4:01:57,  8.32s/it]
{'loss': 0.8521, 'grad_norm': 0.15355505267929118, 'learning_rate': 0.00012461238171698725, 'epoch': 0.44}

 44%|████▍     | 1363/3107 [2:55:56<4:08:05,  8.54s/it]


 44%|████▍     | 1365/3107 [2:56:08<3:25:39,  7.08s/it]

 44%|████▍     | 1366/3107 [2:56:14<3:20:42,  6.92s/it]

 44%|████▍     | 1367/3107 [2:56:26<4:04:06,  8.42s/it]
{'loss': 0.9884, 'grad_norm': 0.15887864163554158, 'learning_rate': 0.00012410674713732764, 'epoch': 0.44}


 44%|████▍     | 1369/3107 [2:56:43<4:02:56,  8.39s/it]
{'loss': 0.9782, 'grad_norm': 0.17472729273363033, 'learning_rate': 0.00012390430907705134, 'epoch': 0.44}

 44%|████▍     | 1370/3107 [2:56:53<4:10:00,  8.64s/it]


 44%|████▍     | 1372/3107 [2:57:06<3:40:04,  7.61s/it]
{'loss': 1.0019, 'grad_norm': 0.16371485244935743, 'learning_rate': 0.00012360045734977654, 'epoch': 0.44}

 44%|████▍     | 1373/3107 [2:57:13<3:37:01,  7.51s/it]


 44%|████▍     | 1375/3107 [2:57:35<4:29:10,  9.32s/it]
{'loss': 0.8915, 'grad_norm': 0.16817979126315274, 'learning_rate': 0.0001232963747011683, 'epoch': 0.44}


 44%|████▍     | 1377/3107 [2:57:50<3:54:23,  8.13s/it]

 44%|████▍     | 1378/3107 [2:57:55<3:34:56,  7.46s/it]
{'loss': 0.938, 'grad_norm': 0.17497411298052787, 'learning_rate': 0.00012299206410655765, 'epoch': 0.44}


 44%|████▍     | 1380/3107 [2:58:10<3:35:11,  7.48s/it]
{'loss': 0.8975, 'grad_norm': 0.15003063479033119, 'learning_rate': 0.00012278906521045888, 'epoch': 0.44}


 44%|████▍     | 1382/3107 [2:58:26<3:37:29,  7.57s/it]
{'loss': 0.9516, 'grad_norm': 0.18400840791981715, 'learning_rate': 0.00012258596721112608, 'epoch': 0.44}

 45%|████▍     | 1383/3107 [2:58:32<3:24:53,  7.13s/it]

 45%|████▍     | 1384/3107 [2:58:40<3:31:37,  7.37s/it]

 45%|████▍     | 1385/3107 [2:58:47<3:25:49,  7.17s/it]

 45%|████▍     | 1386/3107 [2:58:55<3:30:20,  7.33s/it]


 45%|████▍     | 1388/3107 [2:59:10<3:41:49,  7.74s/it]

 45%|████▍     | 1389/3107 [2:59:18<3:35:47,  7.54s/it]

 45%|████▍     | 1390/3107 [2:59:24<3:28:51,  7.30s/it]

 45%|████▍     | 1391/3107 [2:59:30<3:13:42,  6.77s/it]

 45%|████▍     | 1392/3107 [2:59:37<3:19:34,  6.98s/it]

 45%|████▍     | 1393/3107 [2:59:44<3:19:10,  6.97s/it]

 45%|████▍     | 1394/3107 [2:59:50<3:11:34,  6.71s/it]

 45%|████▍     | 1395/3107 [2:59:56<3:01:46,  6.37s/it]

 45%|████▍     | 1396/3107 [3:00:06<3:32:30,  7.45s/it]

 45%|████▍     | 1397/3107 [3:00:13<3:33:43,  7.50s/it]
{'loss': 0.9358, 'grad_norm': 0.16862050943738852, 'learning_rate': 0.00012105966251953247, 'epoch': 0.45}


 45%|████▌     | 1399/3107 [3:00:30<3:48:38,  8.03s/it]

 45%|████▌     | 1400/3107 [3:00:35<3:29:16,  7.36s/it]
{'loss': 0.9109, 'grad_norm': 0.16687907990335576, 'learning_rate': 0.00012075377144142685, 'epoch': 0.45}


 45%|████▌     | 1402/3107 [3:00:50<3:25:29,  7.23s/it]
{'loss': 1.0254, 'grad_norm': 0.16830470865094704, 'learning_rate': 0.00012054973105584497, 'epoch': 0.45}

 45%|████▌     | 1403/3107 [3:00:57<3:23:22,  7.16s/it]


 45%|████▌     | 1405/3107 [3:01:16<3:56:53,  8.35s/it]
{'loss': 0.7906, 'grad_norm': 0.1541665832653108, 'learning_rate': 0.00012024350319548976, 'epoch': 0.45}

 45%|████▌     | 1406/3107 [3:01:23<3:46:50,  8.00s/it]

 45%|████▌     | 1407/3107 [3:01:29<3:29:22,  7.39s/it]

 45%|████▌     | 1408/3107 [3:01:35<3:19:03,  7.03s/it]

 45%|████▌     | 1409/3107 [3:01:43<3:27:48,  7.34s/it]

 45%|████▌     | 1410/3107 [3:01:49<3:16:38,  6.95s/it]

 45%|████▌     | 1411/3107 [3:01:55<3:07:55,  6.65s/it]


 45%|████▌     | 1413/3107 [3:02:12<3:31:17,  7.48s/it]
{'loss': 0.9454, 'grad_norm': 0.18392572410910285, 'learning_rate': 0.00011942593534628907, 'epoch': 0.45}


 46%|████▌     | 1415/3107 [3:02:28<3:33:10,  7.56s/it]

 46%|████▌     | 1416/3107 [3:02:34<3:16:27,  6.97s/it]
{'loss': 0.9203, 'grad_norm': 0.17309028452893938, 'learning_rate': 0.00011911899587485678, 'epoch': 0.46}


 46%|████▌     | 1418/3107 [3:02:51<3:30:11,  7.47s/it]

 46%|████▌     | 1419/3107 [3:02:58<3:30:05,  7.47s/it]

 46%|████▌     | 1420/3107 [3:03:04<3:18:29,  7.06s/it]
{'loss': 1.0715, 'grad_norm': 0.1784425906822328, 'learning_rate': 0.00011870945276479037, 'epoch': 0.46}

 46%|████▌     | 1421/3107 [3:03:11<3:16:36,  7.00s/it]

 46%|████▌     | 1422/3107 [3:03:17<3:10:38,  6.79s/it]


 46%|████▌     | 1424/3107 [3:03:30<3:03:19,  6.54s/it]
{'loss': 0.8539, 'grad_norm': 0.16101309860506216, 'learning_rate': 0.0001182995842064893, 'epoch': 0.46}

 46%|████▌     | 1425/3107 [3:03:37<3:08:59,  6.74s/it]

 46%|████▌     | 1426/3107 [3:03:46<3:23:33,  7.27s/it]


 46%|████▌     | 1428/3107 [3:03:58<3:06:14,  6.66s/it]

 46%|████▌     | 1429/3107 [3:04:04<3:05:42,  6.64s/it]
{'loss': 0.9864, 'grad_norm': 0.18106741686682495, 'learning_rate': 0.00011778680170906888, 'epoch': 0.46}

 46%|████▌     | 1430/3107 [3:04:17<3:55:18,  8.42s/it]

 46%|████▌     | 1431/3107 [3:04:23<3:36:06,  7.74s/it]

 46%|████▌     | 1432/3107 [3:04:32<3:43:13,  8.00s/it]

 46%|████▌     | 1433/3107 [3:04:40<3:42:38,  7.98s/it]


 46%|████▌     | 1435/3107 [3:04:53<3:19:06,  7.15s/it]

 46%|████▌     | 1436/3107 [3:05:00<3:24:40,  7.35s/it]
{'loss': 0.9522, 'grad_norm': 0.16827928748517193, 'learning_rate': 0.00011706809716578475, 'epoch': 0.46}


 46%|████▋     | 1438/3107 [3:05:19<3:48:43,  8.22s/it]
{'loss': 0.9249, 'grad_norm': 0.16938316681325766, 'learning_rate': 0.00011686258433088055, 'epoch': 0.46}

 46%|████▋     | 1439/3107 [3:05:25<3:35:20,  7.75s/it]

 46%|████▋     | 1440/3107 [3:05:31<3:22:02,  7.27s/it]


 46%|████▋     | 1442/3107 [3:05:46<3:22:36,  7.30s/it]

 46%|████▋     | 1443/3107 [3:05:52<3:11:10,  6.89s/it]
{'loss': 0.9864, 'grad_norm': 0.19857301652685613, 'learning_rate': 0.00011634848337787927, 'epoch': 0.46}


 47%|████▋     | 1445/3107 [3:06:06<3:11:41,  6.92s/it]
{'loss': 0.9181, 'grad_norm': 0.1766374793641491, 'learning_rate': 0.000116142717797847, 'epoch': 0.47}


 47%|████▋     | 1447/3107 [3:06:24<3:38:42,  7.91s/it]

 47%|████▋     | 1448/3107 [3:06:35<4:02:44,  8.78s/it]

 47%|████▋     | 1449/3107 [3:06:42<3:49:56,  8.32s/it]
{'loss': 1.0926, 'grad_norm': 0.17491700233975138, 'learning_rate': 0.00011573097693248805, 'epoch': 0.47}


 47%|████▋     | 1451/3107 [3:07:03<4:17:38,  9.34s/it]
{'loss': 0.8272, 'grad_norm': 0.15942446100561916, 'learning_rate': 0.00011552500343770658, 'epoch': 0.47}


 47%|████▋     | 1453/3107 [3:07:17<3:46:17,  8.21s/it]
{'loss': 0.9615, 'grad_norm': 0.17733727540036334, 'learning_rate': 0.00011531896242905233, 'epoch': 0.47}


 47%|████▋     | 1455/3107 [3:07:31<3:31:15,  7.67s/it]

 47%|████▋     | 1456/3107 [3:07:40<3:41:44,  8.06s/it]

 47%|████▋     | 1457/3107 [3:07:47<3:31:50,  7.70s/it]

 47%|████▋     | 1458/3107 [3:07:53<3:18:20,  7.22s/it]

 47%|████▋     | 1459/3107 [3:08:01<3:24:41,  7.45s/it]

 47%|████▋     | 1460/3107 [3:08:13<4:00:04,  8.75s/it]

 47%|████▋     | 1461/3107 [3:08:19<3:40:28,  8.04s/it]
{'loss': 1.0649, 'grad_norm': 0.17720815607916265, 'learning_rate': 0.0001144941411803175, 'epoch': 0.47}


 47%|████▋     | 1463/3107 [3:08:35<3:35:23,  7.86s/it]
{'loss': 1.0158, 'grad_norm': 0.1691116615546171, 'learning_rate': 0.00011428777604825077, 'epoch': 0.47}


 47%|████▋     | 1465/3107 [3:08:51<3:34:35,  7.84s/it]
{'loss': 1.0207, 'grad_norm': 0.17870691815464143, 'learning_rate': 0.0001140813487826651, 'epoch': 0.47}


 47%|████▋     | 1467/3107 [3:09:08<3:45:57,  8.27s/it]

 47%|████▋     | 1468/3107 [3:09:15<3:31:56,  7.76s/it]

 47%|████▋     | 1469/3107 [3:09:20<3:14:50,  7.14s/it]

 47%|████▋     | 1470/3107 [3:09:26<3:05:53,  6.81s/it]

 47%|████▋     | 1471/3107 [3:09:34<3:14:54,  7.15s/it]

 47%|████▋     | 1472/3107 [3:09:41<3:11:10,  7.02s/it]
{'loss': 0.924, 'grad_norm': 0.16556275665554485, 'learning_rate': 0.00011335837701450518, 'epoch': 0.47}

 47%|████▋     | 1473/3107 [3:09:48<3:06:29,  6.85s/it]

 47%|████▋     | 1474/3107 [3:09:54<3:00:45,  6.64s/it]

 47%|████▋     | 1475/3107 [3:09:59<2:52:18,  6.34s/it]

 48%|████▊     | 1476/3107 [3:10:05<2:48:19,  6.19s/it]

 48%|████▊     | 1477/3107 [3:10:16<3:23:43,  7.50s/it]


 48%|████▊     | 1479/3107 [3:10:30<3:20:03,  7.37s/it]

 48%|████▊     | 1480/3107 [3:10:36<3:08:28,  6.95s/it]
{'loss': 0.8863, 'grad_norm': 0.16893571071207872, 'learning_rate': 0.00011253125443883172, 'epoch': 0.48}


 48%|████▊     | 1482/3107 [3:10:51<3:11:36,  7.07s/it]

 48%|████▊     | 1483/3107 [3:10:58<3:16:17,  7.25s/it]

 48%|████▊     | 1484/3107 [3:11:07<3:24:16,  7.55s/it]

 48%|████▊     | 1485/3107 [3:11:16<3:41:06,  8.18s/it]
{'loss': 0.8716, 'grad_norm': 0.1656479777339215, 'learning_rate': 0.00011201385640469742, 'epoch': 0.48}

 48%|████▊     | 1486/3107 [3:11:24<3:35:38,  7.98s/it]

 48%|████▊     | 1487/3107 [3:11:32<3:33:50,  7.92s/it]


 48%|████▊     | 1489/3107 [3:11:43<3:04:00,  6.82s/it]

 48%|████▊     | 1490/3107 [3:11:53<3:23:42,  7.56s/it]

 48%|████▊     | 1491/3107 [3:12:01<3:29:26,  7.78s/it]

 48%|████▊     | 1492/3107 [3:12:08<3:25:50,  7.65s/it]

 48%|████▊     | 1493/3107 [3:12:16<3:30:34,  7.83s/it]
{'loss': 1.0132, 'grad_norm': 0.16662897087086964, 'learning_rate': 0.00011118534622127967, 'epoch': 0.48}

 48%|████▊     | 1494/3107 [3:12:23<3:24:22,  7.60s/it]


 48%|████▊     | 1496/3107 [3:12:37<3:11:53,  7.15s/it]

 48%|████▊     | 1497/3107 [3:12:45<3:19:47,  7.45s/it]
{'loss': 0.9278, 'grad_norm': 0.1765311512775968, 'learning_rate': 0.00011077079567451111, 'epoch': 0.48}

 48%|████▊     | 1498/3107 [3:12:56<3:47:23,  8.48s/it]


 48%|████▊     | 1500/3107 [3:13:11<3:30:51,  7.87s/it]
 48%|████▊     | 1500/3107 [3:13:11<3:30:51,  7.87s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0315, 'grad_norm': 0.1706759311547684, 'learning_rate': 0.00011035605777130397, 'epoch': 0.48}
 48%|████▊     | 1501/3107 [3:13:39<6:19:47, 14.19s/it]


 48%|████▊     | 1503/3107 [3:13:53<4:37:04, 10.36s/it]

 48%|████▊     | 1504/3107 [3:14:01<4:16:18,  9.59s/it]
{'loss': 0.8631, 'grad_norm': 0.15621801267198687, 'learning_rate': 0.0001100448857309728, 'epoch': 0.48}

 48%|████▊     | 1505/3107 [3:14:08<3:58:07,  8.92s/it]


 49%|████▊     | 1507/3107 [3:14:23<3:40:46,  8.28s/it]

 49%|████▊     | 1508/3107 [3:14:29<3:25:03,  7.69s/it]

 49%|████▊     | 1509/3107 [3:14:35<3:10:20,  7.15s/it]
{'loss': 0.9953, 'grad_norm': 0.17783496980249547, 'learning_rate': 0.00010952604875593171, 'epoch': 0.49}

 49%|████▊     | 1510/3107 [3:14:42<3:05:54,  6.98s/it]


 49%|████▊     | 1512/3107 [3:14:53<2:49:55,  6.39s/it]

 49%|████▊     | 1513/3107 [3:15:01<3:03:14,  6.90s/it]

 49%|████▊     | 1514/3107 [3:15:13<3:38:06,  8.22s/it]
{'loss': 0.9401, 'grad_norm': 0.16704099408166706, 'learning_rate': 0.00010900695286821843, 'epoch': 0.49}

 49%|████▉     | 1515/3107 [3:15:20<3:31:58,  7.99s/it]


 49%|████▉     | 1517/3107 [3:15:33<3:08:03,  7.10s/it]

 49%|████▉     | 1518/3107 [3:15:39<3:00:54,  6.83s/it]
{'loss': 0.9113, 'grad_norm': 0.16107089086399717, 'learning_rate': 0.0001085914992217675, 'epoch': 0.49}


 49%|████▉     | 1520/3107 [3:15:55<3:17:20,  7.46s/it]
{'loss': 0.9908, 'grad_norm': 0.17276568019121835, 'learning_rate': 0.00010838371590380765, 'epoch': 0.49}


 49%|████▉     | 1522/3107 [3:16:09<3:13:05,  7.31s/it]

 49%|████▉     | 1523/3107 [3:16:17<3:19:05,  7.54s/it]

 49%|████▉     | 1524/3107 [3:16:27<3:41:34,  8.40s/it]

 49%|████▉     | 1525/3107 [3:16:37<3:53:38,  8.86s/it]
{'loss': 0.9719, 'grad_norm': 0.20047044974880412, 'learning_rate': 0.00010786410008030625, 'epoch': 0.49}

 49%|████▉     | 1526/3107 [3:16:46<3:49:32,  8.71s/it]

 49%|████▉     | 1527/3107 [3:16:52<3:29:53,  7.97s/it]


 49%|████▉     | 1529/3107 [3:17:09<3:41:13,  8.41s/it]

 49%|████▉     | 1530/3107 [3:17:15<3:22:38,  7.71s/it]

 49%|████▉     | 1531/3107 [3:17:22<3:13:46,  7.38s/it]
{'loss': 0.9513, 'grad_norm': 0.17539064657287226, 'learning_rate': 0.00010724028019610449, 'epoch': 0.49}

 49%|████▉     | 1532/3107 [3:17:28<3:06:03,  7.09s/it]


 49%|████▉     | 1534/3107 [3:17:43<3:09:24,  7.22s/it]

 49%|████▉     | 1535/3107 [3:17:49<2:59:28,  6.85s/it]

 49%|████▉     | 1536/3107 [3:18:00<3:28:56,  7.98s/it]
{'loss': 0.8898, 'grad_norm': 0.15355003008034035, 'learning_rate': 0.0001067202127908976, 'epoch': 0.49}


 50%|████▉     | 1538/3107 [3:18:15<3:25:49,  7.87s/it]
{'loss': 1.0178, 'grad_norm': 0.17232128790522888, 'learning_rate': 0.00010651213389462198, 'epoch': 0.5}

 50%|████▉     | 1539/3107 [3:18:22<3:22:33,  7.75s/it]


 50%|████▉     | 1541/3107 [3:18:43<3:57:43,  9.11s/it]

 50%|████▉     | 1542/3107 [3:18:49<3:32:04,  8.13s/it]
{'loss': 0.9691, 'grad_norm': 0.15845665990756833, 'learning_rate': 0.00010609589204875518, 'epoch': 0.5}


 50%|████▉     | 1544/3107 [3:19:05<3:40:29,  8.46s/it]

 50%|████▉     | 1545/3107 [3:19:13<3:33:36,  8.21s/it]

 50%|████▉     | 1546/3107 [3:19:21<3:35:05,  8.27s/it]

 50%|████▉     | 1547/3107 [3:19:28<3:20:08,  7.70s/it]

 50%|████▉     | 1548/3107 [3:19:37<3:30:56,  8.12s/it]

 50%|████▉     | 1549/3107 [3:19:49<4:01:41,  9.31s/it]
{'loss': 0.8355, 'grad_norm': 0.16031296323843527, 'learning_rate': 0.00010536721802320027, 'epoch': 0.5}

 50%|████▉     | 1550/3107 [3:19:55<3:33:42,  8.24s/it]


 50%|████▉     | 1552/3107 [3:20:09<3:21:13,  7.76s/it]

 50%|████▉     | 1553/3107 [3:20:20<3:43:07,  8.61s/it]

 50%|█████     | 1554/3107 [3:20:27<3:31:55,  8.19s/it]
{'loss': 0.9445, 'grad_norm': 0.1622931567929038, 'learning_rate': 0.00010484655925913669, 'epoch': 0.5}

 50%|█████     | 1555/3107 [3:20:32<3:10:34,  7.37s/it]

 50%|█████     | 1556/3107 [3:20:40<3:14:09,  7.51s/it]


 50%|█████     | 1558/3107 [3:20:54<3:03:40,  7.11s/it]
{'loss': 1.0074, 'grad_norm': 0.17255751454903148, 'learning_rate': 0.00010442993672523167, 'epoch': 0.5}

 50%|█████     | 1559/3107 [3:21:00<2:58:09,  6.91s/it]


 50%|█████     | 1561/3107 [3:21:16<3:08:53,  7.33s/it]

 50%|█████     | 1562/3107 [3:21:24<3:14:51,  7.57s/it]
{'loss': 0.9493, 'grad_norm': 0.17347196007925855, 'learning_rate': 0.00010401323713321569, 'epoch': 0.5}


 50%|█████     | 1564/3107 [3:21:42<3:34:45,  8.35s/it]

 50%|█████     | 1565/3107 [3:21:50<3:29:19,  8.14s/it]

 50%|█████     | 1566/3107 [3:21:57<3:23:10,  7.91s/it]
{'loss': 0.9466, 'grad_norm': 0.17720117631643054, 'learning_rate': 0.00010359646773151814, 'epoch': 0.5}


 50%|█████     | 1568/3107 [3:22:11<3:12:53,  7.52s/it]
{'loss': 0.8385, 'grad_norm': 0.17453245408992094, 'learning_rate': 0.00010338805911750842, 'epoch': 0.5}


 51%|█████     | 1570/3107 [3:22:27<3:20:19,  7.82s/it]
{'loss': 1.0283, 'grad_norm': 0.1595056197823103, 'learning_rate': 0.00010317963576978267, 'epoch': 0.51}

 51%|█████     | 1571/3107 [3:22:34<3:14:51,  7.61s/it]


 51%|█████     | 1573/3107 [3:22:49<3:14:07,  7.59s/it]
{'loss': 0.848, 'grad_norm': 0.15677653786943416, 'learning_rate': 0.00010286697510518723, 'epoch': 0.51}

 51%|█████     | 1574/3107 [3:22:55<2:57:48,  6.96s/it]


 51%|█████     | 1576/3107 [3:23:10<3:05:18,  7.26s/it]

 51%|█████     | 1577/3107 [3:23:16<2:55:08,  6.87s/it]

 51%|█████     | 1578/3107 [3:23:26<3:17:49,  7.76s/it]
{'loss': 0.8687, 'grad_norm': 0.16361329708727618, 'learning_rate': 0.00010234581317008771, 'epoch': 0.51}


 51%|█████     | 1580/3107 [3:23:38<2:55:40,  6.90s/it]

 51%|█████     | 1581/3107 [3:23:45<2:56:43,  6.95s/it]

 51%|█████     | 1582/3107 [3:23:51<2:48:58,  6.65s/it]

 51%|█████     | 1583/3107 [3:24:00<3:06:59,  7.36s/it]
{'loss': 0.885, 'grad_norm': 0.1560095033904043, 'learning_rate': 0.00010182458747709949, 'epoch': 0.51}

 51%|█████     | 1584/3107 [3:24:06<2:57:27,  6.99s/it]


 51%|█████     | 1586/3107 [3:24:18<2:42:56,  6.43s/it]

 51%|█████     | 1587/3107 [3:24:25<2:47:34,  6.61s/it]

 51%|█████     | 1588/3107 [3:24:33<2:59:12,  7.08s/it]

 51%|█████     | 1589/3107 [3:24:40<2:55:55,  6.95s/it]

 51%|█████     | 1590/3107 [3:24:47<2:59:17,  7.09s/it]
{'loss': 0.8069, 'grad_norm': 0.14456121394375698, 'learning_rate': 0.00010109479136717773, 'epoch': 0.51}


 51%|█████     | 1592/3107 [3:25:03<3:09:58,  7.52s/it]
{'loss': 0.8549, 'grad_norm': 0.1635527622324476, 'learning_rate': 0.00010088626578057063, 'epoch': 0.51}


 51%|█████▏    | 1594/3107 [3:25:18<3:09:34,  7.52s/it]

 51%|█████▏    | 1595/3107 [3:25:25<3:08:18,  7.47s/it]
{'loss': 1.0042, 'grad_norm': 0.17163303185283357, 'learning_rate': 0.00010057347045756958, 'epoch': 0.51}


 51%|█████▏    | 1597/3107 [3:25:40<3:03:42,  7.30s/it]

 51%|█████▏    | 1598/3107 [3:25:46<2:55:08,  6.96s/it]
{'loss': 1.0037, 'grad_norm': 0.16102394433940434, 'learning_rate': 0.00010026066952338222, 'epoch': 0.51}


 51%|█████▏    | 1600/3107 [3:26:02<3:11:15,  7.62s/it]

 52%|█████▏    | 1601/3107 [3:26:09<3:09:33,  7.55s/it]

 52%|█████▏    | 1602/3107 [3:26:19<3:25:45,  8.20s/it]

 52%|█████▏    | 1603/3107 [3:26:25<3:09:18,  7.55s/it]
{'loss': 0.9809, 'grad_norm': 0.17285889296319024, 'learning_rate': 9.973933047661777e-05, 'epoch': 0.52}


 52%|█████▏    | 1605/3107 [3:26:37<2:49:59,  6.79s/it]
{'loss': 0.9346, 'grad_norm': 0.16647393359147927, 'learning_rate': 9.953079604816943e-05, 'epoch': 0.52}

 52%|█████▏    | 1606/3107 [3:26:43<2:40:29,  6.42s/it]

 52%|█████▏    | 1607/3107 [3:26:51<2:53:39,  6.95s/it]

 52%|█████▏    | 1608/3107 [3:26:57<2:44:21,  6.58s/it]


 52%|█████▏    | 1610/3107 [3:27:08<2:32:36,  6.12s/it]
{'loss': 0.9877, 'grad_norm': 0.17496154683583812, 'learning_rate': 9.900947088768412e-05, 'epoch': 0.52}


 52%|█████▏    | 1612/3107 [3:27:24<2:56:21,  7.08s/it]
{'loss': 0.9524, 'grad_norm': 0.1689005556830633, 'learning_rate': 9.880094756819572e-05, 'epoch': 0.52}


 52%|█████▏    | 1614/3107 [3:27:42<3:19:03,  8.00s/it]

 52%|█████▏    | 1615/3107 [3:27:48<3:02:40,  7.35s/it]
{'loss': 0.9339, 'grad_norm': 0.17171049463901553, 'learning_rate': 9.848817264922561e-05, 'epoch': 0.52}

 52%|█████▏    | 1616/3107 [3:27:55<3:00:01,  7.24s/it]


 52%|█████▏    | 1618/3107 [3:28:14<3:25:14,  8.27s/it]

 52%|█████▏    | 1619/3107 [3:28:25<3:48:50,  9.23s/it]

 52%|█████▏    | 1620/3107 [3:28:34<3:42:50,  8.99s/it]

 52%|█████▏    | 1621/3107 [3:28:45<4:01:05,  9.73s/it]

 52%|█████▏    | 1622/3107 [3:28:54<3:50:37,  9.32s/it]

 52%|█████▏    | 1623/3107 [3:29:02<3:41:41,  8.96s/it]

 52%|█████▏    | 1624/3107 [3:29:12<3:47:56,  9.22s/it]

 52%|█████▏    | 1625/3107 [3:29:18<3:27:55,  8.42s/it]

 52%|█████▏    | 1626/3107 [3:29:24<3:08:49,  7.65s/it]
{'loss': 0.9587, 'grad_norm': 0.1766425592721273, 'learning_rate': 9.734148111130969e-05, 'epoch': 0.52}

 52%|█████▏    | 1627/3107 [3:29:33<3:19:02,  8.07s/it]


 52%|█████▏    | 1629/3107 [3:29:46<2:56:46,  7.18s/it]

 52%|█████▏    | 1630/3107 [3:29:52<2:51:03,  6.95s/it]

 52%|█████▏    | 1631/3107 [3:29:58<2:42:13,  6.59s/it]
{'loss': 0.8624, 'grad_norm': 0.17452569770992768, 'learning_rate': 9.682036423021737e-05, 'epoch': 0.52}


 53%|█████▎    | 1633/3107 [3:30:18<3:34:41,  8.74s/it]
{'loss': 0.8891, 'grad_norm': 0.17390376629158286, 'learning_rate': 9.661194088249159e-05, 'epoch': 0.53}

 53%|█████▎    | 1634/3107 [3:30:27<3:36:02,  8.80s/it]


 53%|█████▎    | 1636/3107 [3:30:38<2:54:04,  7.10s/it]

 53%|█████▎    | 1637/3107 [3:30:48<3:09:58,  7.75s/it]
{'loss': 0.9686, 'grad_norm': 0.1564279332516168, 'learning_rate': 9.619513929449867e-05, 'epoch': 0.53}


 53%|█████▎    | 1639/3107 [3:31:07<3:31:59,  8.66s/it]
{'loss': 1.0028, 'grad_norm': 0.16726825250729044, 'learning_rate': 9.598676286678434e-05, 'epoch': 0.53}
[2024-05-28 22:42:41,369] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1640/3107 [3:31:17<3:47:07,  9.29s/it]

 53%|█████▎    | 1641/3107 [3:31:25<3:37:23,  8.90s/it]


 53%|█████▎    | 1643/3107 [3:31:43<3:36:56,  8.89s/it]
{'loss': 0.9321, 'grad_norm': 0.15990193030603783, 'learning_rate': 9.557006327476837e-05, 'epoch': 0.53}


 53%|█████▎    | 1645/3107 [3:32:02<3:50:06,  9.44s/it]
{'loss': 0.9431, 'grad_norm': 0.17501290909117387, 'learning_rate': 9.536174192257603e-05, 'epoch': 0.53}


 53%|█████▎    | 1647/3107 [3:32:16<3:19:19,  8.19s/it]
{'loss': 0.9356, 'grad_norm': 0.16772086732801272, 'learning_rate': 9.515344074086332e-05, 'epoch': 0.53}


 53%|█████▎    | 1649/3107 [3:32:32<3:17:52,  8.14s/it]

 53%|█████▎    | 1650/3107 [3:32:44<3:48:54,  9.43s/it]
{'loss': 0.8572, 'grad_norm': 0.17230165463424973, 'learning_rate': 9.484102876945075e-05, 'epoch': 0.53}


 53%|█████▎    | 1652/3107 [3:32:58<3:14:52,  8.04s/it]
{'loss': 0.8446, 'grad_norm': 0.16033660225359544, 'learning_rate': 9.463278197679977e-05, 'epoch': 0.53}

 53%|█████▎    | 1653/3107 [3:33:05<3:12:41,  7.95s/it]


 53%|█████▎    | 1655/3107 [3:33:22<3:11:32,  7.91s/it]

 53%|█████▎    | 1656/3107 [3:33:28<2:56:51,  7.31s/it]

 53%|█████▎    | 1657/3107 [3:33:34<2:49:31,  7.01s/it]

 53%|█████▎    | 1658/3107 [3:33:40<2:43:52,  6.79s/it]
{'loss': 0.9941, 'grad_norm': 0.1568119144466462, 'learning_rate': 9.400818526389063e-05, 'epoch': 0.53}


 53%|█████▎    | 1660/3107 [3:33:57<3:05:04,  7.67s/it]

 53%|█████▎    | 1661/3107 [3:34:03<2:51:55,  7.13s/it]
{'loss': 0.9476, 'grad_norm': 0.16862071906636425, 'learning_rate': 9.369597332109209e-05, 'epoch': 0.53}


 54%|█████▎    | 1663/3107 [3:34:18<3:00:35,  7.50s/it]

 54%|█████▎    | 1664/3107 [3:34:25<2:53:35,  7.22s/it]

 54%|█████▎    | 1665/3107 [3:34:31<2:45:12,  6.87s/it]

 54%|█████▎    | 1666/3107 [3:34:40<3:05:57,  7.74s/it]

 54%|█████▎    | 1667/3107 [3:34:49<3:09:23,  7.89s/it]

 54%|█████▎    | 1668/3107 [3:34:54<2:53:42,  7.24s/it]
{'loss': 0.9096, 'grad_norm': 0.1857011253911205, 'learning_rate': 9.296772394301901e-05, 'epoch': 0.54}


 54%|█████▎    | 1670/3107 [3:35:08<2:46:50,  6.97s/it]

 54%|█████▍    | 1671/3107 [3:35:16<2:53:39,  7.26s/it]
{'loss': 0.9782, 'grad_norm': 0.17110393068407168, 'learning_rate': 9.265572948503286e-05, 'epoch': 0.54}


 54%|█████▍    | 1673/3107 [3:35:32<3:02:55,  7.65s/it]

 54%|█████▍    | 1674/3107 [3:35:38<2:50:38,  7.14s/it]

 54%|█████▍    | 1675/3107 [3:35:44<2:45:17,  6.93s/it]

 54%|█████▍    | 1676/3107 [3:35:51<2:39:32,  6.69s/it]
{'loss': 0.841, 'grad_norm': 0.1603166256904676, 'learning_rate': 9.213589991969378e-05, 'epoch': 0.54}

 54%|█████▍    | 1677/3107 [3:35:59<2:52:25,  7.23s/it]


 54%|█████▍    | 1679/3107 [3:36:16<3:07:53,  7.89s/it]
{'loss': 0.7903, 'grad_norm': 0.17230369766405196, 'learning_rate': 9.182410387257434e-05, 'epoch': 0.54}

 54%|█████▍    | 1680/3107 [3:36:22<2:51:28,  7.21s/it]
[2024-05-28 22:48:01,507] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 54%|█████▍    | 1682/3107 [3:36:45<3:33:57,  9.01s/it]

 54%|█████▍    | 1683/3107 [3:36:51<3:14:09,  8.18s/it]
{'loss': 1.0024, 'grad_norm': 0.18114062541107642, 'learning_rate': 9.140850077823252e-05, 'epoch': 0.54}


 54%|█████▍    | 1685/3107 [3:37:04<2:55:38,  7.41s/it]
{'loss': 0.8546, 'grad_norm': 0.16928006607514137, 'learning_rate': 9.120075482228604e-05, 'epoch': 0.54}


 54%|█████▍    | 1687/3107 [3:37:16<2:37:53,  6.67s/it]

 54%|█████▍    | 1688/3107 [3:37:23<2:39:33,  6.75s/it]

 54%|█████▍    | 1689/3107 [3:37:29<2:34:46,  6.55s/it]

 54%|█████▍    | 1690/3107 [3:37:35<2:28:26,  6.29s/it]

 54%|█████▍    | 1691/3107 [3:37:46<3:05:46,  7.87s/it]
[2024-05-28 22:49:10,194] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 54%|█████▍    | 1692/3107 [3:37:56<3:20:01,  8.48s/it]

 54%|█████▍    | 1693/3107 [3:38:06<3:30:53,  8.95s/it]
[2024-05-28 22:49:30,136] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 55%|█████▍    | 1694/3107 [3:38:15<3:28:49,  8.87s/it]

 55%|█████▍    | 1695/3107 [3:38:22<3:18:32,  8.44s/it]
{'loss': 1.0611, 'grad_norm': 0.1707728622152067, 'learning_rate': 9.01626170868522e-05, 'epoch': 0.55}


 55%|█████▍    | 1697/3107 [3:38:36<3:01:15,  7.71s/it]

 55%|█████▍    | 1698/3107 [3:38:44<3:01:53,  7.75s/it]

 55%|█████▍    | 1699/3107 [3:38:50<2:51:54,  7.33s/it]

 55%|█████▍    | 1700/3107 [3:38:58<2:53:07,  7.38s/it]

 55%|█████▍    | 1701/3107 [3:39:05<2:49:59,  7.25s/it]

 55%|█████▍    | 1702/3107 [3:39:13<2:55:40,  7.50s/it]

 55%|█████▍    | 1703/3107 [3:39:19<2:43:50,  7.00s/it]

 55%|█████▍    | 1704/3107 [3:39:24<2:32:35,  6.53s/it]

 55%|█████▍    | 1705/3107 [3:39:34<2:54:31,  7.47s/it]

 55%|█████▍    | 1706/3107 [3:39:40<2:44:38,  7.05s/it]

 55%|█████▍    | 1707/3107 [3:39:46<2:39:42,  6.84s/it]
{'loss': 0.9654, 'grad_norm': 0.18462660664250857, 'learning_rate': 8.891827345633534e-05, 'epoch': 0.55}

 55%|█████▍    | 1708/3107 [3:39:56<2:56:26,  7.57s/it]

 55%|█████▌    | 1709/3107 [3:40:02<2:47:33,  7.19s/it]

 55%|█████▌    | 1710/3107 [3:40:08<2:38:22,  6.80s/it]


 55%|█████▌    | 1712/3107 [3:40:20<2:31:22,  6.51s/it]

 55%|█████▌    | 1713/3107 [3:40:27<2:30:08,  6.46s/it]
{'loss': 1.001, 'grad_norm': 0.16315427895603696, 'learning_rate': 8.829674005051041e-05, 'epoch': 0.55}

 55%|█████▌    | 1714/3107 [3:40:33<2:32:23,  6.56s/it]


 55%|█████▌    | 1716/3107 [3:40:48<2:45:26,  7.14s/it]

 55%|█████▌    | 1717/3107 [3:40:57<2:56:34,  7.62s/it]

 55%|█████▌    | 1718/3107 [3:41:06<3:07:30,  8.10s/it]

 55%|█████▌    | 1719/3107 [3:41:16<3:21:38,  8.72s/it]
{'loss': 0.943, 'grad_norm': 0.1635054275117924, 'learning_rate': 8.767566469102613e-05, 'epoch': 0.55}

 55%|█████▌    | 1720/3107 [3:41:23<3:09:58,  8.22s/it]

 55%|█████▌    | 1721/3107 [3:41:30<2:57:57,  7.70s/it]


 55%|█████▌    | 1723/3107 [3:41:44<2:49:17,  7.34s/it]
{'loss': 0.7847, 'grad_norm': 0.17544119057318905, 'learning_rate': 8.7261880926211e-05, 'epoch': 0.55}


 56%|█████▌    | 1725/3107 [3:42:07<3:42:50,  9.67s/it]
{'loss': 0.8582, 'grad_norm': 0.16976326375009698, 'learning_rate': 8.705507168575028e-05, 'epoch': 0.56}


 56%|█████▌    | 1727/3107 [3:42:21<3:12:01,  8.35s/it]

 56%|█████▌    | 1728/3107 [3:42:30<3:17:07,  8.58s/it]
{'loss': 0.9096, 'grad_norm': 0.17289089106549912, 'learning_rate': 8.674496365701312e-05, 'epoch': 0.56}


 56%|█████▌    | 1730/3107 [3:42:51<3:33:19,  9.30s/it]

 56%|█████▌    | 1731/3107 [3:42:59<3:21:50,  8.80s/it]
{'loss': 0.8921, 'grad_norm': 0.1705145700602881, 'learning_rate': 8.643498532367217e-05, 'epoch': 0.56}

 56%|█████▌    | 1732/3107 [3:43:05<3:06:00,  8.12s/it]


 56%|█████▌    | 1734/3107 [3:43:20<2:57:13,  7.74s/it]

 56%|█████▌    | 1735/3107 [3:43:27<2:47:41,  7.33s/it]
{'loss': 1.0722, 'grad_norm': 0.16511270620757468, 'learning_rate': 8.602188786967835e-05, 'epoch': 0.56}


 56%|█████▌    | 1737/3107 [3:43:40<2:39:55,  7.00s/it]
{'loss': 0.9799, 'grad_norm': 0.18075528312510708, 'learning_rate': 8.581542987395181e-05, 'epoch': 0.56}


 56%|█████▌    | 1739/3107 [3:43:59<3:11:12,  8.39s/it]

 56%|█████▌    | 1740/3107 [3:44:06<3:04:46,  8.11s/it]

 56%|█████▌    | 1741/3107 [3:44:13<2:53:29,  7.62s/it]

 56%|█████▌    | 1742/3107 [3:44:20<2:51:08,  7.52s/it]

 56%|█████▌    | 1743/3107 [3:44:27<2:42:30,  7.15s/it]
{'loss': 0.9247, 'grad_norm': 0.1648457992760446, 'learning_rate': 8.51964295849703e-05, 'epoch': 0.56}


 56%|█████▌    | 1745/3107 [3:44:39<2:31:31,  6.68s/it]
{'loss': 1.0672, 'grad_norm': 0.17913945791885286, 'learning_rate': 8.499022371232975e-05, 'epoch': 0.56}


 56%|█████▌    | 1747/3107 [3:44:55<2:48:50,  7.45s/it]

 56%|█████▋    | 1748/3107 [3:45:01<2:37:50,  6.97s/it]
{'loss': 0.8705, 'grad_norm': 0.18482654396903164, 'learning_rate': 8.468103757094768e-05, 'epoch': 0.56}


 56%|█████▋    | 1750/3107 [3:45:17<2:45:59,  7.34s/it]
{'loss': 0.8839, 'grad_norm': 0.18570292744216235, 'learning_rate': 8.447499656229344e-05, 'epoch': 0.56}


 56%|█████▋    | 1752/3107 [3:45:33<2:51:04,  7.58s/it]

 56%|█████▋    | 1753/3107 [3:45:41<2:57:00,  7.84s/it]

 56%|█████▋    | 1754/3107 [3:45:49<3:00:51,  8.02s/it]
{'loss': 1.0357, 'grad_norm': 0.17997729142789343, 'learning_rate': 8.406311798232408e-05, 'epoch': 0.56}

 56%|█████▋    | 1755/3107 [3:45:58<3:04:32,  8.19s/it]


 57%|█████▋    | 1757/3107 [3:46:13<2:57:16,  7.88s/it]
{'loss': 0.9745, 'grad_norm': 0.1616213810700836, 'learning_rate': 8.37543905811863e-05, 'epoch': 0.57}


 57%|█████▋    | 1759/3107 [3:46:29<2:54:53,  7.78s/it]

 57%|█████▋    | 1760/3107 [3:46:35<2:43:21,  7.28s/it]

 57%|█████▋    | 1761/3107 [3:46:42<2:46:20,  7.41s/it]

 57%|█████▋    | 1762/3107 [3:46:49<2:41:05,  7.19s/it]

 57%|█████▋    | 1763/3107 [3:46:55<2:35:09,  6.93s/it]

 57%|█████▋    | 1764/3107 [3:47:03<2:36:49,  7.01s/it]

 57%|█████▋    | 1765/3107 [3:47:12<2:50:14,  7.61s/it]

 57%|█████▋    | 1766/3107 [3:47:19<2:45:22,  7.40s/it]
{'loss': 1.0113, 'grad_norm': 0.1817627438606256, 'learning_rate': 8.28291741950509e-05, 'epoch': 0.57}


 57%|█████▋    | 1768/3107 [3:47:30<2:28:44,  6.67s/it]
{'loss': 0.9073, 'grad_norm': 0.17513884010062156, 'learning_rate': 8.262377303172008e-05, 'epoch': 0.57}

 57%|█████▋    | 1769/3107 [3:47:36<2:21:41,  6.35s/it]


 57%|█████▋    | 1771/3107 [3:47:49<2:22:34,  6.40s/it]

 57%|█████▋    | 1772/3107 [3:47:55<2:22:47,  6.42s/it]
{'loss': 0.9775, 'grad_norm': 0.18465033238312972, 'learning_rate': 8.221319829093114e-05, 'epoch': 0.57}


 57%|█████▋    | 1774/3107 [3:48:11<2:38:28,  7.13s/it]

 57%|█████▋    | 1775/3107 [3:48:19<2:44:36,  7.41s/it]

 57%|█████▋    | 1776/3107 [3:48:27<2:52:20,  7.77s/it]

 57%|█████▋    | 1777/3107 [3:48:34<2:41:44,  7.30s/it]

 57%|█████▋    | 1778/3107 [3:48:42<2:45:58,  7.49s/it]

 57%|█████▋    | 1779/3107 [3:48:53<3:11:23,  8.65s/it]

 57%|█████▋    | 1780/3107 [3:49:00<2:58:10,  8.06s/it]

 57%|█████▋    | 1781/3107 [3:49:07<2:55:55,  7.96s/it]

 57%|█████▋    | 1782/3107 [3:49:16<2:57:53,  8.06s/it]

 57%|█████▋    | 1783/3107 [3:49:25<3:04:37,  8.37s/it]

 57%|█████▋    | 1784/3107 [3:49:34<3:07:33,  8.51s/it]
{'loss': 1.001, 'grad_norm': 0.1673221091964223, 'learning_rate': 8.098335900219929e-05, 'epoch': 0.57}


 57%|█████▋    | 1786/3107 [3:49:48<2:51:20,  7.78s/it]

 58%|█████▊    | 1787/3107 [3:49:59<3:16:05,  8.91s/it]

 58%|█████▊    | 1788/3107 [3:50:07<3:09:30,  8.62s/it]
{'loss': 0.9482, 'grad_norm': 0.16264352976782123, 'learning_rate': 8.057406465371092e-05, 'epoch': 0.58}


 58%|█████▊    | 1790/3107 [3:50:23<3:02:56,  8.33s/it]

 58%|█████▊    | 1791/3107 [3:50:31<2:58:57,  8.16s/it]
{'loss': 0.8418, 'grad_norm': 0.15921988420082234, 'learning_rate': 8.026731525753128e-05, 'epoch': 0.58}


 58%|█████▊    | 1793/3107 [3:50:49<3:11:48,  8.76s/it]
{'loss': 1.0742, 'grad_norm': 0.16350471203706513, 'learning_rate': 8.006292273970282e-05, 'epoch': 0.58}


 58%|█████▊    | 1795/3107 [3:51:04<2:54:42,  7.99s/it]

 58%|█████▊    | 1796/3107 [3:51:12<2:53:04,  7.92s/it]

 58%|█████▊    | 1797/3107 [3:51:18<2:42:20,  7.44s/it]

 58%|█████▊    | 1798/3107 [3:51:23<2:30:16,  6.89s/it]
{'loss': 0.9314, 'grad_norm': 0.16205646186098874, 'learning_rate': 7.955232270429322e-05, 'epoch': 0.58}


 58%|█████▊    | 1800/3107 [3:51:38<2:32:23,  7.00s/it]
 58%|█████▊    | 1800/3107 [3:51:38<2:32:23,  7.00s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 58%|█████▊    | 1801/3107 [3:52:07<5:01:32, 13.85s/it]

 58%|█████▊    | 1802/3107 [3:52:14<4:12:26, 11.61s/it]

 58%|█████▊    | 1803/3107 [3:52:26<4:14:44, 11.72s/it]

 58%|█████▊    | 1804/3107 [3:52:35<3:59:30, 11.03s/it]

 58%|█████▊    | 1805/3107 [3:52:41<3:26:38,  9.52s/it]
{'loss': 0.9148, 'grad_norm': 0.18481194388286357, 'learning_rate': 7.883841943127311e-05, 'epoch': 0.58}


 58%|█████▊    | 1807/3107 [3:52:53<2:47:21,  7.72s/it]
{'loss': 1.0074, 'grad_norm': 0.17256405966959265, 'learning_rate': 7.863465246300212e-05, 'epoch': 0.58}


 58%|█████▊    | 1809/3107 [3:53:07<2:39:07,  7.36s/it]
{'loss': 1.0405, 'grad_norm': 0.17296988723139542, 'learning_rate': 7.843097840661748e-05, 'epoch': 0.58}

 58%|█████▊    | 1810/3107 [3:53:14<2:36:36,  7.25s/it]


 58%|█████▊    | 1812/3107 [3:53:30<2:45:25,  7.66s/it]

 58%|█████▊    | 1813/3107 [3:53:39<2:55:16,  8.13s/it]
{'loss': 0.8704, 'grad_norm': 0.16837031943643457, 'learning_rate': 7.802391257198417e-05, 'epoch': 0.58}


 58%|█████▊    | 1815/3107 [3:53:52<2:38:08,  7.34s/it]
{'loss': 0.9076, 'grad_norm': 0.18534935418311427, 'learning_rate': 7.782052256395032e-05, 'epoch': 0.58}

 58%|█████▊    | 1816/3107 [3:54:00<2:44:50,  7.66s/it]

 58%|█████▊    | 1817/3107 [3:54:07<2:36:21,  7.27s/it]


 59%|█████▊    | 1819/3107 [3:54:22<2:38:25,  7.38s/it]

 59%|█████▊    | 1820/3107 [3:54:31<2:50:30,  7.95s/it]
{'loss': 0.957, 'grad_norm': 0.16131684323328177, 'learning_rate': 7.731247145649425e-05, 'epoch': 0.59}

 59%|█████▊    | 1821/3107 [3:54:37<2:37:45,  7.36s/it]

 59%|█████▊    | 1822/3107 [3:54:45<2:41:25,  7.54s/it]


 59%|█████▊    | 1824/3107 [3:54:57<2:28:42,  6.95s/it]

 59%|█████▊    | 1825/3107 [3:55:03<2:21:25,  6.62s/it]
{'loss': 1.007, 'grad_norm': 0.18322756214632008, 'learning_rate': 7.680503698336276e-05, 'epoch': 0.59}


 59%|█████▉    | 1827/3107 [3:55:17<2:27:59,  6.94s/it]

 59%|█████▉    | 1828/3107 [3:55:23<2:20:46,  6.60s/it]
{'loss': 1.0563, 'grad_norm': 0.1736727264851908, 'learning_rate': 7.650087802205068e-05, 'epoch': 0.59}

 59%|█████▉    | 1829/3107 [3:55:28<2:13:00,  6.24s/it]


 59%|█████▉    | 1831/3107 [3:55:42<2:18:58,  6.53s/it]

 59%|█████▉    | 1832/3107 [3:55:48<2:14:22,  6.32s/it]

 59%|█████▉    | 1833/3107 [3:55:55<2:22:39,  6.72s/it]

 59%|█████▉    | 1834/3107 [3:56:05<2:42:35,  7.66s/it]

 59%|█████▉    | 1835/3107 [3:56:13<2:43:01,  7.69s/it]

 59%|█████▉    | 1836/3107 [3:56:19<2:31:41,  7.16s/it]
{'loss': 1.02, 'grad_norm': 0.1687806488642216, 'learning_rate': 7.569091963585822e-05, 'epoch': 0.59}

 59%|█████▉    | 1837/3107 [3:56:25<2:24:17,  6.82s/it]

 59%|█████▉    | 1838/3107 [3:56:33<2:28:22,  7.02s/it]


 59%|█████▉    | 1840/3107 [3:56:48<2:40:42,  7.61s/it]
{'loss': 0.9462, 'grad_norm': 0.1729292717925711, 'learning_rate': 7.528657120171754e-05, 'epoch': 0.59}

 59%|█████▉    | 1841/3107 [3:56:55<2:32:41,  7.24s/it]

 59%|█████▉    | 1842/3107 [3:57:05<2:51:27,  8.13s/it]


 59%|█████▉    | 1844/3107 [3:57:20<2:43:32,  7.77s/it]
{'loss': 0.8884, 'grad_norm': 0.18270566175332617, 'learning_rate': 7.48826526541094e-05, 'epoch': 0.59}

 59%|█████▉    | 1845/3107 [3:57:27<2:37:41,  7.50s/it]

 59%|█████▉    | 1846/3107 [3:57:33<2:28:43,  7.08s/it]


 59%|█████▉    | 1848/3107 [3:57:46<2:22:33,  6.79s/it]

 60%|█████▉    | 1849/3107 [3:57:55<2:41:28,  7.70s/it]

 60%|█████▉    | 1850/3107 [3:58:04<2:49:08,  8.07s/it]

 60%|█████▉    | 1851/3107 [3:58:12<2:44:53,  7.88s/it]

 60%|█████▉    | 1852/3107 [3:58:21<2:55:23,  8.39s/it]

 60%|█████▉    | 1853/3107 [3:58:29<2:53:25,  8.30s/it]

 60%|█████▉    | 1854/3107 [3:58:36<2:40:01,  7.66s/it]
{'loss': 1.0139, 'grad_norm': 0.17974623244988058, 'learning_rate': 7.387478312874916e-05, 'epoch': 0.6}


 60%|█████▉    | 1856/3107 [3:58:50<2:38:32,  7.60s/it]

 60%|█████▉    | 1857/3107 [3:59:00<2:52:32,  8.28s/it]

 60%|█████▉    | 1858/3107 [3:59:07<2:46:40,  8.01s/it]

 60%|█████▉    | 1859/3107 [3:59:13<2:34:08,  7.41s/it]
{'loss': 0.8964, 'grad_norm': 0.17417149205830876, 'learning_rate': 7.337190662562174e-05, 'epoch': 0.6}


 60%|█████▉    | 1861/3107 [3:59:30<2:43:29,  7.87s/it]
{'loss': 0.9422, 'grad_norm': 0.1658518202554954, 'learning_rate': 7.317095790589299e-05, 'epoch': 0.6}

 60%|█████▉    | 1862/3107 [3:59:39<2:46:34,  8.03s/it]

 60%|█████▉    | 1863/3107 [3:59:45<2:35:05,  7.48s/it]


 60%|██████    | 1865/3107 [3:59:57<2:21:10,  6.82s/it]

 60%|██████    | 1866/3107 [4:00:03<2:15:29,  6.55s/it]

 60%|██████    | 1867/3107 [4:00:10<2:15:29,  6.56s/it]

 60%|██████    | 1868/3107 [4:00:16<2:12:36,  6.42s/it]

 60%|██████    | 1869/3107 [4:00:22<2:11:37,  6.38s/it]

 60%|██████    | 1870/3107 [4:00:30<2:20:10,  6.80s/it]

 60%|██████    | 1871/3107 [4:00:36<2:17:23,  6.67s/it]
{'loss': 0.9783, 'grad_norm': 0.1829281629808868, 'learning_rate': 7.216798184615671e-05, 'epoch': 0.6}


 60%|██████    | 1873/3107 [4:00:48<2:09:10,  6.28s/it]

 60%|██████    | 1874/3107 [4:00:55<2:14:06,  6.53s/it]
{'loss': 0.8417, 'grad_norm': 0.168610030033784, 'learning_rate': 7.186767410914601e-05, 'epoch': 0.6}

 60%|██████    | 1875/3107 [4:01:01<2:07:30,  6.21s/it]

 60%|██████    | 1876/3107 [4:01:09<2:16:58,  6.68s/it]


 60%|██████    | 1878/3107 [4:01:26<2:38:25,  7.73s/it]

 60%|██████    | 1879/3107 [4:01:31<2:24:17,  7.05s/it]
{'loss': 0.952, 'grad_norm': 0.17403881625824671, 'learning_rate': 7.136777436155041e-05, 'epoch': 0.6}


 61%|██████    | 1881/3107 [4:01:47<2:29:53,  7.34s/it]

 61%|██████    | 1882/3107 [4:01:58<2:49:36,  8.31s/it]

 61%|██████    | 1883/3107 [4:02:06<2:48:21,  8.25s/it]

 61%|██████    | 1884/3107 [4:02:17<3:01:50,  8.92s/it]

 61%|██████    | 1885/3107 [4:02:22<2:43:27,  8.03s/it]

 61%|██████    | 1886/3107 [4:02:30<2:38:07,  7.77s/it]
{'loss': 0.914, 'grad_norm': 0.1695828643292389, 'learning_rate': 7.066922514359564e-05, 'epoch': 0.61}

 61%|██████    | 1887/3107 [4:02:35<2:25:32,  7.16s/it]


 61%|██████    | 1889/3107 [4:02:52<2:33:11,  7.55s/it]

 61%|██████    | 1890/3107 [4:03:03<2:58:49,  8.82s/it]

 61%|██████    | 1891/3107 [4:03:10<2:46:35,  8.22s/it]

 61%|██████    | 1892/3107 [4:03:21<2:58:56,  8.84s/it]

 61%|██████    | 1893/3107 [4:03:28<2:50:41,  8.44s/it]
{'loss': 0.9386, 'grad_norm': 0.16437955229986428, 'learning_rate': 6.99722384225624e-05, 'epoch': 0.61}

 61%|██████    | 1894/3107 [4:03:35<2:41:54,  8.01s/it]


 61%|██████    | 1896/3107 [4:03:48<2:25:29,  7.21s/it]

 61%|██████    | 1897/3107 [4:03:54<2:16:22,  6.76s/it]
{'loss': 1.0068, 'grad_norm': 0.17376396873280345, 'learning_rate': 6.957467611791604e-05, 'epoch': 0.61}


 61%|██████    | 1899/3107 [4:04:08<2:18:34,  6.88s/it]

 61%|██████    | 1900/3107 [4:04:14<2:13:25,  6.63s/it]
{'loss': 0.9855, 'grad_norm': 0.17038775284735352, 'learning_rate': 6.927685132803937e-05, 'epoch': 0.61}


 61%|██████    | 1902/3107 [4:04:32<2:32:40,  7.60s/it]
{'loss': 0.9609, 'grad_norm': 0.17749390221819586, 'learning_rate': 6.907846829619789e-05, 'epoch': 0.61}

 61%|██████    | 1903/3107 [4:04:39<2:30:43,  7.51s/it]


 61%|██████▏   | 1905/3107 [4:04:53<2:25:32,  7.27s/it]
{'loss': 0.8899, 'grad_norm': 0.17446524063788452, 'learning_rate': 6.878114614732932e-05, 'epoch': 0.61}


 61%|██████▏   | 1907/3107 [4:05:08<2:33:17,  7.66s/it]

 61%|██████▏   | 1908/3107 [4:05:16<2:33:07,  7.66s/it]

 61%|██████▏   | 1909/3107 [4:05:22<2:23:25,  7.18s/it]

 61%|██████▏   | 1910/3107 [4:05:30<2:26:36,  7.35s/it]
{'loss': 1.0112, 'grad_norm': 0.16859783412253693, 'learning_rate': 6.82862894775897e-05, 'epoch': 0.61}

 62%|██████▏   | 1911/3107 [4:05:38<2:29:31,  7.50s/it]

 62%|██████▏   | 1912/3107 [4:05:43<2:17:05,  6.88s/it]


 62%|██████▏   | 1914/3107 [4:05:58<2:21:11,  7.10s/it]

 62%|██████▏   | 1915/3107 [4:06:06<2:25:24,  7.32s/it]

 62%|██████▏   | 1916/3107 [4:06:14<2:29:19,  7.52s/it]

 62%|██████▏   | 1917/3107 [4:06:20<2:21:06,  7.11s/it]

 62%|██████▏   | 1918/3107 [4:06:26<2:17:24,  6.93s/it]

 62%|██████▏   | 1919/3107 [4:06:32<2:09:23,  6.53s/it]

 62%|██████▏   | 1920/3107 [4:06:38<2:07:45,  6.46s/it]
{'loss': 0.9647, 'grad_norm': 0.17467889551929283, 'learning_rate': 6.729917544730304e-05, 'epoch': 0.62}

 62%|██████▏   | 1921/3107 [4:06:45<2:09:09,  6.53s/it]

 62%|██████▏   | 1922/3107 [4:06:54<2:20:46,  7.13s/it]


 62%|██████▏   | 1924/3107 [4:07:07<2:17:26,  6.97s/it]
{'loss': 0.7444, 'grad_norm': 0.16723874183255402, 'learning_rate': 6.690531927650126e-05, 'epoch': 0.62}


 62%|██████▏   | 1926/3107 [4:07:26<2:47:49,  8.53s/it]

 62%|██████▏   | 1927/3107 [4:07:34<2:42:34,  8.27s/it]
{'loss': 0.9763, 'grad_norm': 0.16904527295736385, 'learning_rate': 6.661030456227109e-05, 'epoch': 0.62}


 62%|██████▏   | 1929/3107 [4:07:49<2:33:11,  7.80s/it]
{'loss': 0.8927, 'grad_norm': 0.1614177903653196, 'learning_rate': 6.641380941097237e-05, 'epoch': 0.62}

 62%|██████▏   | 1930/3107 [4:07:55<2:24:00,  7.34s/it]

 62%|██████▏   | 1931/3107 [4:08:05<2:40:38,  8.20s/it]


 62%|██████▏   | 1933/3107 [4:08:23<2:43:41,  8.37s/it]

 62%|██████▏   | 1934/3107 [4:08:33<2:52:19,  8.81s/it]

 62%|██████▏   | 1935/3107 [4:08:39<2:34:29,  7.91s/it]
{'loss': 0.9991, 'grad_norm': 0.18065310903916387, 'learning_rate': 6.582520371327592e-05, 'epoch': 0.62}

 62%|██████▏   | 1936/3107 [4:08:46<2:29:53,  7.68s/it]

 62%|██████▏   | 1937/3107 [4:08:53<2:28:53,  7.64s/it]


 62%|██████▏   | 1939/3107 [4:09:07<2:18:03,  7.09s/it]

 62%|██████▏   | 1940/3107 [4:09:14<2:20:13,  7.21s/it]

 62%|██████▏   | 1941/3107 [4:09:22<2:25:24,  7.48s/it]
{'loss': 0.8296, 'grad_norm': 0.16042990754622882, 'learning_rate': 6.523793556088302e-05, 'epoch': 0.62}


 63%|██████▎   | 1943/3107 [4:09:35<2:11:37,  6.78s/it]

 63%|██████▎   | 1944/3107 [4:09:41<2:06:33,  6.53s/it]
{'loss': 0.9462, 'grad_norm': 0.17434676826684908, 'learning_rate': 6.49448102489666e-05, 'epoch': 0.63}

 63%|██████▎   | 1945/3107 [4:09:48<2:08:34,  6.64s/it]


 63%|██████▎   | 1947/3107 [4:10:01<2:10:13,  6.74s/it]

 63%|██████▎   | 1948/3107 [4:10:08<2:11:35,  6.81s/it]

 63%|██████▎   | 1949/3107 [4:10:14<2:07:01,  6.58s/it]

 63%|██████▎   | 1950/3107 [4:10:21<2:08:13,  6.65s/it]

 63%|██████▎   | 1951/3107 [4:10:29<2:13:29,  6.93s/it]

 63%|██████▎   | 1952/3107 [4:10:34<2:06:15,  6.56s/it]

 63%|██████▎   | 1953/3107 [4:10:44<2:26:49,  7.63s/it]
{'loss': 0.875, 'grad_norm': 0.16689201762487324, 'learning_rate': 6.406750377759639e-05, 'epoch': 0.63}


 63%|██████▎   | 1955/3107 [4:11:02<2:33:46,  8.01s/it]

 63%|██████▎   | 1956/3107 [4:11:09<2:27:48,  7.71s/it]

 63%|██████▎   | 1957/3107 [4:11:15<2:17:53,  7.19s/it]
{'loss': 1.0055, 'grad_norm': 0.19576555958873182, 'learning_rate': 6.367860089306028e-05, 'epoch': 0.63}

 63%|██████▎   | 1958/3107 [4:11:22<2:15:02,  7.05s/it]


 63%|██████▎   | 1960/3107 [4:11:37<2:23:24,  7.50s/it]
{'loss': 0.8943, 'grad_norm': 0.1656812826772998, 'learning_rate': 6.338733798247585e-05, 'epoch': 0.63}


 63%|██████▎   | 1962/3107 [4:11:53<2:28:23,  7.78s/it]

 63%|██████▎   | 1963/3107 [4:11:59<2:16:22,  7.15s/it]

 63%|██████▎   | 1964/3107 [4:12:07<2:22:35,  7.49s/it]

 63%|██████▎   | 1965/3107 [4:12:13<2:12:42,  6.97s/it]

 63%|██████▎   | 1966/3107 [4:12:21<2:17:15,  7.22s/it]

 63%|██████▎   | 1967/3107 [4:12:27<2:10:51,  6.89s/it]
{'loss': 1.0233, 'grad_norm': 0.17853202498747625, 'learning_rate': 6.270912260196155e-05, 'epoch': 0.63}


 63%|██████▎   | 1969/3107 [4:12:41<2:12:10,  6.97s/it]
{'loss': 0.9664, 'grad_norm': 0.16280585622918595, 'learning_rate': 6.25157100770417e-05, 'epoch': 0.63}

 63%|██████▎   | 1970/3107 [4:12:50<2:22:11,  7.50s/it]


 63%|██████▎   | 1972/3107 [4:13:02<2:10:12,  6.88s/it]
{'loss': 0.9994, 'grad_norm': 0.1904549637293469, 'learning_rate': 6.222589719349691e-05, 'epoch': 0.63}

 64%|██████▎   | 1973/3107 [4:13:10<2:15:47,  7.18s/it]

 64%|██████▎   | 1974/3107 [4:13:18<2:19:55,  7.41s/it]

 64%|██████▎   | 1975/3107 [4:13:24<2:11:26,  6.97s/it]


 64%|██████▎   | 1977/3107 [4:13:37<2:06:13,  6.70s/it]

 64%|██████▎   | 1978/3107 [4:13:43<2:02:40,  6.52s/it]
{'loss': 1.0886, 'grad_norm': 0.17079143905786548, 'learning_rate': 6.164738307338959e-05, 'epoch': 0.64}


 64%|██████▎   | 1980/3107 [4:14:01<2:26:01,  7.77s/it]

 64%|██████▍   | 1981/3107 [4:14:09<2:28:26,  7.91s/it]
{'loss': 0.8971, 'grad_norm': 0.1776767263202955, 'learning_rate': 6.13586874973636e-05, 'epoch': 0.64}

 64%|██████▍   | 1982/3107 [4:14:20<2:41:36,  8.62s/it]


 64%|██████▍   | 1984/3107 [4:14:39<2:58:00,  9.51s/it]

 64%|██████▍   | 1985/3107 [4:14:47<2:47:08,  8.94s/it]

 64%|██████▍   | 1986/3107 [4:14:53<2:34:36,  8.28s/it]

 64%|██████▍   | 1987/3107 [4:15:04<2:48:51,  9.05s/it]

 64%|██████▍   | 1988/3107 [4:15:11<2:34:21,  8.28s/it]

 64%|██████▍   | 1989/3107 [4:15:18<2:30:00,  8.05s/it]
{'loss': 0.9404, 'grad_norm': 0.18669581780395708, 'learning_rate': 6.0590688729797295e-05, 'epoch': 0.64}

 64%|██████▍   | 1990/3107 [4:15:24<2:15:02,  7.25s/it]


 64%|██████▍   | 1992/3107 [4:15:40<2:22:57,  7.69s/it]

 64%|██████▍   | 1993/3107 [4:15:47<2:13:55,  7.21s/it]

 64%|██████▍   | 1994/3107 [4:15:54<2:17:47,  7.43s/it]

 64%|██████▍   | 1995/3107 [4:16:03<2:21:18,  7.62s/it]

 64%|██████▍   | 1996/3107 [4:16:08<2:11:19,  7.09s/it]
{'loss': 0.9541, 'grad_norm': 0.16188862216932726, 'learning_rate': 5.99209373367839e-05, 'epoch': 0.64}


 64%|██████▍   | 1998/3107 [4:16:21<2:05:56,  6.81s/it]

 64%|██████▍   | 1999/3107 [4:16:27<1:58:44,  6.43s/it]
{'loss': 0.9166, 'grad_norm': 0.18298212031232008, 'learning_rate': 5.963455254357631e-05, 'epoch': 0.64}

 64%|██████▍   | 2000/3107 [4:16:34<2:03:06,  6.67s/it]


 64%|██████▍   | 2002/3107 [4:16:53<2:30:17,  8.16s/it]

 64%|██████▍   | 2003/3107 [4:17:05<2:49:34,  9.22s/it]
{'loss': 0.8812, 'grad_norm': 0.16831677787585378, 'learning_rate': 5.925332101903994e-05, 'epoch': 0.64}


 65%|██████▍   | 2005/3107 [4:17:21<2:34:13,  8.40s/it]

 65%|██████▍   | 2006/3107 [4:17:27<2:23:24,  7.82s/it]

 65%|██████▍   | 2007/3107 [4:17:36<2:25:58,  7.96s/it]
{'loss': 0.9653, 'grad_norm': 0.1708000227201287, 'learning_rate': 5.887279827711243e-05, 'epoch': 0.65}


 65%|██████▍   | 2009/3107 [4:17:51<2:21:01,  7.71s/it]
{'loss': 1.0063, 'grad_norm': 0.1730640282801583, 'learning_rate': 5.868280476859249e-05, 'epoch': 0.65}

 65%|██████▍   | 2010/3107 [4:17:58<2:19:44,  7.64s/it]

 65%|██████▍   | 2011/3107 [4:18:05<2:11:56,  7.22s/it]

 65%|██████▍   | 2012/3107 [4:18:11<2:06:41,  6.94s/it]

 65%|██████▍   | 2013/3107 [4:18:18<2:09:09,  7.08s/it]

 65%|██████▍   | 2014/3107 [4:18:24<2:03:24,  6.77s/it]


 65%|██████▍   | 2016/3107 [4:18:44<2:32:32,  8.39s/it]
{'loss': 1.0036, 'grad_norm': 0.17677018460686386, 'learning_rate': 5.801924785910482e-05, 'epoch': 0.65}


 65%|██████▍   | 2018/3107 [4:18:58<2:19:30,  7.69s/it]
{'loss': 0.8933, 'grad_norm': 0.17506825301711906, 'learning_rate': 5.783006939168765e-05, 'epoch': 0.65}

 65%|██████▍   | 2019/3107 [4:19:04<2:10:46,  7.21s/it]

 65%|██████▌   | 2020/3107 [4:19:10<2:07:53,  7.06s/it]

 65%|██████▌   | 2021/3107 [4:19:19<2:13:32,  7.38s/it]

 65%|██████▌   | 2022/3107 [4:19:28<2:23:20,  7.93s/it]

 65%|██████▌   | 2023/3107 [4:19:37<2:28:15,  8.21s/it]

 65%|██████▌   | 2024/3107 [4:19:45<2:26:18,  8.11s/it]


 65%|██████▌   | 2026/3107 [4:19:59<2:18:42,  7.70s/it]
{'loss': 0.8831, 'grad_norm': 0.1764307096562485, 'learning_rate': 5.707519758853288e-05, 'epoch': 0.65}

 65%|██████▌   | 2027/3107 [4:20:06<2:12:59,  7.39s/it]

 65%|██████▌   | 2028/3107 [4:20:12<2:06:01,  7.01s/it]


 65%|██████▌   | 2030/3107 [4:20:26<2:04:38,  6.94s/it]

 65%|██████▌   | 2031/3107 [4:20:32<1:58:42,  6.62s/it]
{'loss': 0.8919, 'grad_norm': 0.1862198226668687, 'learning_rate': 5.660491605454601e-05, 'epoch': 0.65}

 65%|██████▌   | 2032/3107 [4:20:37<1:52:28,  6.28s/it]


 65%|██████▌   | 2034/3107 [4:20:51<2:00:56,  6.76s/it]

 65%|██████▌   | 2035/3107 [4:20:57<1:56:33,  6.52s/it]

 66%|██████▌   | 2036/3107 [4:21:05<2:05:04,  7.01s/it]

 66%|██████▌   | 2037/3107 [4:21:13<2:09:17,  7.25s/it]

 66%|██████▌   | 2038/3107 [4:21:19<2:03:23,  6.93s/it]
{'loss': 0.9047, 'grad_norm': 0.18408179638917985, 'learning_rate': 5.5948506246901564e-05, 'epoch': 0.66}


 66%|██████▌   | 2040/3107 [4:21:34<2:03:12,  6.93s/it]
{'loss': 1.0767, 'grad_norm': 0.17272033272017429, 'learning_rate': 5.576139008671094e-05, 'epoch': 0.66}


 66%|██████▌   | 2042/3107 [4:21:47<2:01:39,  6.85s/it]
{'loss': 1.1094, 'grad_norm': 0.18218978983240014, 'learning_rate': 5.55744663077905e-05, 'epoch': 0.66}

 66%|██████▌   | 2043/3107 [4:21:55<2:05:35,  7.08s/it]


 66%|██████▌   | 2045/3107 [4:22:11<2:17:11,  7.75s/it]

 66%|██████▌   | 2046/3107 [4:22:20<2:20:45,  7.96s/it]

 66%|██████▌   | 2047/3107 [4:22:26<2:08:58,  7.30s/it]
{'loss': 1.1245, 'grad_norm': 0.19406302758748395, 'learning_rate': 5.510800386099617e-05, 'epoch': 0.66}

 66%|██████▌   | 2048/3107 [4:22:39<2:38:59,  9.01s/it]

 66%|██████▌   | 2049/3107 [4:22:45<2:23:15,  8.12s/it]

 66%|██████▌   | 2050/3107 [4:22:51<2:13:28,  7.58s/it]

 66%|██████▌   | 2051/3107 [4:23:00<2:23:01,  8.13s/it]

 66%|██████▌   | 2052/3107 [4:23:06<2:10:29,  7.42s/it]


 66%|██████▌   | 2054/3107 [4:23:21<2:09:37,  7.39s/it]
{'loss': 0.9954, 'grad_norm': 0.18031976007240164, 'learning_rate': 5.445700910315481e-05, 'epoch': 0.66}


 66%|██████▌   | 2056/3107 [4:23:39<2:24:57,  8.28s/it]
{'loss': 1.0315, 'grad_norm': 0.17253840161158485, 'learning_rate': 5.427145470644084e-05, 'epoch': 0.66}


 66%|██████▌   | 2058/3107 [4:23:55<2:20:25,  8.03s/it]
{'loss': 0.8373, 'grad_norm': 0.1694309973669355, 'learning_rate': 5.408609917030689e-05, 'epoch': 0.66}

 66%|██████▋   | 2059/3107 [4:24:02<2:14:21,  7.69s/it]


 66%|██████▋   | 2061/3107 [4:24:20<2:19:50,  8.02s/it]

 66%|██████▋   | 2062/3107 [4:24:28<2:20:14,  8.05s/it]
{'loss': 0.8302, 'grad_norm': 0.1560489563380917, 'learning_rate': 5.371598790314728e-05, 'epoch': 0.66}

 66%|██████▋   | 2063/3107 [4:24:37<2:27:36,  8.48s/it]

 66%|██████▋   | 2064/3107 [4:24:47<2:31:50,  8.74s/it]


 66%|██████▋   | 2066/3107 [4:25:02<2:23:08,  8.25s/it]
{'loss': 0.8476, 'grad_norm': 0.1680896888820812, 'learning_rate': 5.334668173970819e-05, 'epoch': 0.66}


 67%|██████▋   | 2068/3107 [4:25:17<2:19:10,  8.04s/it]

 67%|██████▋   | 2069/3107 [4:25:26<2:21:50,  8.20s/it]

 67%|██████▋   | 2070/3107 [4:25:32<2:09:45,  7.51s/it]
{'loss': 0.8307, 'grad_norm': 0.18050145926556033, 'learning_rate': 5.297818710401712e-05, 'epoch': 0.67}

 67%|██████▋   | 2071/3107 [4:25:39<2:06:48,  7.34s/it]

 67%|██████▋   | 2072/3107 [4:25:44<1:57:09,  6.79s/it]

 67%|██████▋   | 2073/3107 [4:25:51<1:56:30,  6.76s/it]
[2024-05-28 23:37:26,588] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 2074/3107 [4:26:02<2:20:24,  8.16s/it]

 67%|██████▋   | 2075/3107 [4:26:09<2:12:56,  7.73s/it]

 67%|██████▋   | 2076/3107 [4:26:15<2:03:52,  7.21s/it]


 67%|██████▋   | 2078/3107 [4:26:30<2:03:40,  7.21s/it]
{'loss': 0.97, 'grad_norm': 0.17749840956959412, 'learning_rate': 5.224365804129523e-05, 'epoch': 0.67}


 67%|██████▋   | 2080/3107 [4:26:50<2:24:09,  8.42s/it]
{'loss': 1.0006, 'grad_norm': 0.16793951691744002, 'learning_rate': 5.206054297868682e-05, 'epoch': 0.67}


 67%|██████▋   | 2082/3107 [4:27:04<2:12:48,  7.77s/it]
{'loss': 0.9652, 'grad_norm': 0.15939777050549028, 'learning_rate': 5.187763639129165e-05, 'epoch': 0.67}

 67%|██████▋   | 2083/3107 [4:27:10<2:04:45,  7.31s/it]


 67%|██████▋   | 2085/3107 [4:27:24<1:59:46,  7.03s/it]
{'loss': 0.9267, 'grad_norm': 0.1707312612869382, 'learning_rate': 5.160366914093286e-05, 'epoch': 0.67}

 67%|██████▋   | 2086/3107 [4:27:31<2:01:21,  7.13s/it]

 67%|██████▋   | 2087/3107 [4:27:37<1:55:08,  6.77s/it]

 67%|██████▋   | 2088/3107 [4:27:43<1:50:26,  6.50s/it]

 67%|██████▋   | 2089/3107 [4:27:49<1:46:29,  6.28s/it]


 67%|██████▋   | 2091/3107 [4:28:08<2:12:19,  7.81s/it]
{'loss': 0.8592, 'grad_norm': 0.1826736402431252, 'learning_rate': 5.1057157934297804e-05, 'epoch': 0.67}

 67%|██████▋   | 2092/3107 [4:28:15<2:11:36,  7.78s/it]

 67%|██████▋   | 2093/3107 [4:28:21<2:01:51,  7.21s/it]

 67%|██████▋   | 2094/3107 [4:28:27<1:55:29,  6.84s/it]

 67%|██████▋   | 2095/3107 [4:28:35<2:01:18,  7.19s/it]

 67%|██████▋   | 2096/3107 [4:28:44<2:11:14,  7.79s/it]

 67%|██████▋   | 2097/3107 [4:28:55<2:24:15,  8.57s/it]

 68%|██████▊   | 2098/3107 [4:29:04<2:24:47,  8.61s/it]


 68%|██████▊   | 2100/3107 [4:29:16<2:03:57,  7.39s/it]
 68%|██████▊   | 2100/3107 [4:29:16<2:03:57,  7.39s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.938, 'grad_norm': 0.15881883013644768, 'learning_rate': 5.015057321277761e-05, 'epoch': 0.68}
 68%|██████▊   | 2101/3107 [4:29:41<3:35:18, 12.84s/it]

 68%|██████▊   | 2102/3107 [4:29:49<3:08:33, 11.26s/it]

 68%|██████▊   | 2103/3107 [4:29:57<2:54:21, 10.42s/it]

 68%|██████▊   | 2104/3107 [4:30:09<3:01:53, 10.88s/it]

 68%|██████▊   | 2105/3107 [4:30:15<2:37:28,  9.43s/it]


 68%|██████▊   | 2107/3107 [4:30:36<2:38:47,  9.53s/it]

 68%|██████▊   | 2108/3107 [4:30:44<2:31:32,  9.10s/it]
{'loss': 0.8782, 'grad_norm': 0.15337764802178888, 'learning_rate': 4.9519182658204044e-05, 'epoch': 0.68}

 68%|██████▊   | 2109/3107 [4:30:53<2:31:57,  9.14s/it]


 68%|██████▊   | 2111/3107 [4:31:12<2:38:49,  9.57s/it]

 68%|██████▊   | 2112/3107 [4:31:22<2:39:16,  9.60s/it]

 68%|██████▊   | 2113/3107 [4:31:30<2:31:07,  9.12s/it]
{'loss': 1.0778, 'grad_norm': 0.18077115370640962, 'learning_rate': 4.9069833894197984e-05, 'epoch': 0.68}


 68%|██████▊   | 2115/3107 [4:31:44<2:12:30,  8.01s/it]
{'loss': 0.8696, 'grad_norm': 0.17875127030688778, 'learning_rate': 4.889048129687817e-05, 'epoch': 0.68}

 68%|██████▊   | 2116/3107 [4:31:53<2:16:40,  8.28s/it]

 68%|██████▊   | 2117/3107 [4:32:00<2:09:48,  7.87s/it]

 68%|██████▊   | 2118/3107 [4:32:06<2:00:17,  7.30s/it]


 68%|██████▊   | 2120/3107 [4:32:20<2:03:05,  7.48s/it]
{'loss': 0.9445, 'grad_norm': 0.1689342839362644, 'learning_rate': 4.844307389887246e-05, 'epoch': 0.68}

 68%|██████▊   | 2121/3107 [4:32:28<2:00:46,  7.35s/it]

 68%|██████▊   | 2122/3107 [4:32:34<1:55:27,  7.03s/it]

 68%|██████▊   | 2123/3107 [4:32:41<1:55:11,  7.02s/it]

 68%|██████▊   | 2124/3107 [4:32:48<1:53:54,  6.95s/it]

 68%|██████▊   | 2125/3107 [4:32:59<2:15:55,  8.31s/it]

 68%|██████▊   | 2126/3107 [4:33:07<2:15:44,  8.30s/it]

 68%|██████▊   | 2127/3107 [4:33:17<2:19:49,  8.56s/it]

 68%|██████▊   | 2128/3107 [4:33:25<2:18:12,  8.47s/it]

 69%|██████▊   | 2129/3107 [4:33:34<2:19:17,  8.55s/it]

 69%|██████▊   | 2130/3107 [4:33:42<2:18:34,  8.51s/it]

 69%|██████▊   | 2131/3107 [4:33:48<2:05:31,  7.72s/it]


 69%|██████▊   | 2133/3107 [4:33:59<1:46:19,  6.55s/it]

 69%|██████▊   | 2134/3107 [4:34:08<2:00:33,  7.43s/it]

 69%|██████▊   | 2135/3107 [4:34:16<2:04:35,  7.69s/it]
{'loss': 0.8883, 'grad_norm': 0.15976808015356725, 'learning_rate': 4.710930788555441e-05, 'epoch': 0.69}

 69%|██████▊   | 2136/3107 [4:34:23<2:01:06,  7.48s/it]


 69%|██████▉   | 2138/3107 [4:34:39<2:04:23,  7.70s/it]
{'loss': 0.9493, 'grad_norm': 0.17029180777589126, 'learning_rate': 4.6844096812405925e-05, 'epoch': 0.69}

 69%|██████▉   | 2139/3107 [4:34:51<2:28:19,  9.19s/it]

 69%|██████▉   | 2140/3107 [4:35:02<2:33:26,  9.52s/it]

 69%|██████▉   | 2141/3107 [4:35:12<2:36:07,  9.70s/it]

 69%|██████▉   | 2142/3107 [4:35:18<2:18:48,  8.63s/it]


 69%|██████▉   | 2144/3107 [4:35:33<2:07:36,  7.95s/it]
{'loss': 1.0106, 'grad_norm': 0.17875367558475175, 'learning_rate': 4.631523758577475e-05, 'epoch': 0.69}

 69%|██████▉   | 2145/3107 [4:35:41<2:11:24,  8.20s/it]

 69%|██████▉   | 2146/3107 [4:35:48<2:03:15,  7.70s/it]

 69%|██████▉   | 2147/3107 [4:35:54<1:55:48,  7.24s/it]


 69%|██████▉   | 2149/3107 [4:36:07<1:48:30,  6.80s/it]

 69%|██████▉   | 2150/3107 [4:36:13<1:44:49,  6.57s/it]
{'loss': 0.9415, 'grad_norm': 0.14939780703959132, 'learning_rate': 4.5788479492424304e-05, 'epoch': 0.69}

 69%|██████▉   | 2151/3107 [4:36:21<1:53:08,  7.10s/it]

 69%|██████▉   | 2152/3107 [4:36:28<1:53:02,  7.10s/it]

 69%|██████▉   | 2153/3107 [4:36:34<1:47:51,  6.78s/it]

 69%|██████▉   | 2154/3107 [4:36:41<1:50:22,  6.95s/it]

 69%|██████▉   | 2155/3107 [4:36:47<1:45:52,  6.67s/it]


 69%|██████▉   | 2157/3107 [4:37:03<1:52:00,  7.07s/it]

 69%|██████▉   | 2158/3107 [4:37:10<1:54:26,  7.24s/it]
{'loss': 0.9535, 'grad_norm': 0.16469716419834965, 'learning_rate': 4.508943941892157e-05, 'epoch': 0.69}

 69%|██████▉   | 2159/3107 [4:37:17<1:53:35,  7.19s/it]

 70%|██████▉   | 2160/3107 [4:37:23<1:47:47,  6.83s/it]


 70%|██████▉   | 2162/3107 [4:37:47<2:25:00,  9.21s/it]

 70%|██████▉   | 2163/3107 [4:37:55<2:20:38,  8.94s/it]
{'loss': 0.9234, 'grad_norm': 0.1572487383810631, 'learning_rate': 4.465447645953621e-05, 'epoch': 0.7}

 70%|██████▉   | 2164/3107 [4:38:04<2:19:21,  8.87s/it]

 70%|██████▉   | 2165/3107 [4:38:14<2:25:12,  9.25s/it]

 70%|██████▉   | 2166/3107 [4:38:21<2:17:58,  8.80s/it]

 70%|██████▉   | 2167/3107 [4:38:27<2:04:46,  7.96s/it]

 70%|██████▉   | 2168/3107 [4:38:34<1:57:13,  7.49s/it]


 70%|██████▉   | 2170/3107 [4:38:45<1:41:05,  6.47s/it]

 70%|██████▉   | 2171/3107 [4:38:51<1:38:13,  6.30s/it]

 70%|██████▉   | 2172/3107 [4:38:57<1:37:18,  6.24s/it]

 70%|██████▉   | 2173/3107 [4:39:04<1:43:44,  6.66s/it]

 70%|██████▉   | 2174/3107 [4:39:11<1:43:19,  6.64s/it]

 70%|███████   | 2175/3107 [4:39:17<1:38:18,  6.33s/it]
{'loss': 0.9485, 'grad_norm': 0.18376222867670672, 'learning_rate': 4.361672516272116e-05, 'epoch': 0.7}


 70%|███████   | 2177/3107 [4:39:35<2:03:16,  7.95s/it]

 70%|███████   | 2178/3107 [4:39:43<2:04:35,  8.05s/it]
{'loss': 0.9229, 'grad_norm': 0.17073022878913136, 'learning_rate': 4.3358660226907935e-05, 'epoch': 0.7}

 70%|███████   | 2179/3107 [4:39:52<2:06:29,  8.18s/it]


 70%|███████   | 2181/3107 [4:40:07<2:03:28,  8.00s/it]
{'loss': 0.8816, 'grad_norm': 0.16211882889142634, 'learning_rate': 4.310114950466405e-05, 'epoch': 0.7}

 70%|███████   | 2182/3107 [4:40:13<1:57:27,  7.62s/it]


 70%|███████   | 2184/3107 [4:40:29<1:58:47,  7.72s/it]
{'loss': 0.8555, 'grad_norm': 0.17443912825916155, 'learning_rate': 4.2844195515632166e-05, 'epoch': 0.7}

 70%|███████   | 2185/3107 [4:40:36<1:55:35,  7.52s/it]


 70%|███████   | 2187/3107 [4:40:49<1:48:37,  7.08s/it]

 70%|███████   | 2188/3107 [4:40:57<1:51:08,  7.26s/it]
{'loss': 0.9411, 'grad_norm': 0.17851070678965178, 'learning_rate': 4.250246057109881e-05, 'epoch': 0.7}


 70%|███████   | 2190/3107 [4:41:15<2:04:30,  8.15s/it]
{'loss': 0.8692, 'grad_norm': 0.16349979361603018, 'learning_rate': 4.2331967788513295e-05, 'epoch': 0.7}

 71%|███████   | 2191/3107 [4:41:26<2:20:32,  9.21s/it]


 71%|███████   | 2193/3107 [4:41:43<2:12:15,  8.68s/it]
{'loss': 0.8415, 'grad_norm': 0.1796504468192782, 'learning_rate': 4.207669906237621e-05, 'epoch': 0.71}

 71%|███████   | 2194/3107 [4:41:52<2:13:52,  8.80s/it]


 71%|███████   | 2196/3107 [4:42:07<2:03:17,  8.12s/it]
{'loss': 0.7463, 'grad_norm': 0.16076715882126344, 'learning_rate': 4.1821997093301904e-05, 'epoch': 0.71}

 71%|███████   | 2197/3107 [4:42:14<1:59:22,  7.87s/it]

 71%|███████   | 2198/3107 [4:42:22<1:57:56,  7.79s/it]

 71%|███████   | 2199/3107 [4:42:28<1:48:03,  7.14s/it]

 71%|███████   | 2200/3107 [4:42:34<1:42:45,  6.80s/it]


 71%|███████   | 2202/3107 [4:42:47<1:40:29,  6.66s/it]
{'loss': 0.8083, 'grad_norm': 0.16792416846691557, 'learning_rate': 4.1314303389412013e-05, 'epoch': 0.71}


 71%|███████   | 2204/3107 [4:42:59<1:35:40,  6.36s/it]

 71%|███████   | 2205/3107 [4:43:09<1:49:37,  7.29s/it]
{'loss': 0.8141, 'grad_norm': 0.17955292249614715, 'learning_rate': 4.106131662218269e-05, 'epoch': 0.71}


 71%|███████   | 2207/3107 [4:43:23<1:47:20,  7.16s/it]
{'loss': 1.0569, 'grad_norm': 0.1811201786002853, 'learning_rate': 4.08929790093421e-05, 'epoch': 0.71}

 71%|███████   | 2208/3107 [4:43:30<1:48:23,  7.23s/it]

 71%|███████   | 2209/3107 [4:43:36<1:41:16,  6.77s/it]


 71%|███████   | 2211/3107 [4:43:51<1:48:01,  7.23s/it]
{'loss': 0.9616, 'grad_norm': 0.17687519473996072, 'learning_rate': 4.055707563401864e-05, 'epoch': 0.71}

 71%|███████   | 2212/3107 [4:43:58<1:47:20,  7.20s/it]

 71%|███████   | 2213/3107 [4:44:04<1:41:28,  6.81s/it]

 71%|███████▏  | 2214/3107 [4:44:11<1:40:18,  6.74s/it]


 71%|███████▏  | 2216/3107 [4:44:23<1:36:57,  6.53s/it]
{'loss': 1.0722, 'grad_norm': 0.18104194614328648, 'learning_rate': 4.0138651161929906e-05, 'epoch': 0.71}


 71%|███████▏  | 2218/3107 [4:44:37<1:40:26,  6.78s/it]

 71%|███████▏  | 2219/3107 [4:44:43<1:36:51,  6.54s/it]

 71%|███████▏  | 2220/3107 [4:44:49<1:32:43,  6.27s/it]

 71%|███████▏  | 2221/3107 [4:44:55<1:31:52,  6.22s/it]
{'loss': 0.8669, 'grad_norm': 0.17824259305207849, 'learning_rate': 3.97218536877903e-05, 'epoch': 0.71}


 72%|███████▏  | 2223/3107 [4:45:09<1:37:51,  6.64s/it]

 72%|███████▏  | 2224/3107 [4:45:15<1:37:06,  6.60s/it]

 72%|███████▏  | 2225/3107 [4:45:23<1:40:56,  6.87s/it]

 72%|███████▏  | 2226/3107 [4:45:31<1:46:35,  7.26s/it]

 72%|███████▏  | 2227/3107 [4:45:39<1:51:35,  7.61s/it]
{'loss': 0.898, 'grad_norm': 0.16987485946551822, 'learning_rate': 3.922386030326163e-05, 'epoch': 0.72}

 72%|███████▏  | 2228/3107 [4:45:46<1:47:27,  7.34s/it]


 72%|███████▏  | 2230/3107 [4:46:01<1:49:15,  7.47s/it]
{'loss': 0.906, 'grad_norm': 0.17018131951804827, 'learning_rate': 3.8975754400755324e-05, 'epoch': 0.72}

 72%|███████▏  | 2231/3107 [4:46:10<1:56:05,  7.95s/it]

 72%|███████▏  | 2232/3107 [4:46:19<1:59:15,  8.18s/it]


 72%|███████▏  | 2234/3107 [4:46:35<1:57:42,  8.09s/it]
{'loss': 0.9199, 'grad_norm': 0.17496473333367382, 'learning_rate': 3.864587576954688e-05, 'epoch': 0.72}

 72%|███████▏  | 2235/3107 [4:46:44<2:02:02,  8.40s/it]

 72%|███████▏  | 2236/3107 [4:46:53<2:01:46,  8.39s/it]

 72%|███████▏  | 2237/3107 [4:47:00<1:56:18,  8.02s/it]

 72%|███████▏  | 2238/3107 [4:47:06<1:49:13,  7.54s/it]

 72%|███████▏  | 2239/3107 [4:47:12<1:42:58,  7.12s/it]

 72%|███████▏  | 2240/3107 [4:47:19<1:40:05,  6.93s/it]


 72%|███████▏  | 2242/3107 [4:47:42<2:09:29,  8.98s/it]

 72%|███████▏  | 2243/3107 [4:47:53<2:20:36,  9.76s/it]
{'loss': 0.8765, 'grad_norm': 0.16951516379402135, 'learning_rate': 3.7907559679779337e-05, 'epoch': 0.72}

 72%|███████▏  | 2244/3107 [4:48:00<2:07:04,  8.83s/it]

 72%|███████▏  | 2245/3107 [4:48:06<1:55:18,  8.03s/it]


 72%|███████▏  | 2247/3107 [4:48:21<1:50:18,  7.70s/it]
{'loss': 0.9178, 'grad_norm': 0.18791918450962547, 'learning_rate': 3.758117048249815e-05, 'epoch': 0.72}


 72%|███████▏  | 2249/3107 [4:48:39<1:56:29,  8.15s/it]

 72%|███████▏  | 2250/3107 [4:48:45<1:48:41,  7.61s/it]

 72%|███████▏  | 2251/3107 [4:48:53<1:50:51,  7.77s/it]
{'loss': 0.9886, 'grad_norm': 0.18661268056109565, 'learning_rate': 3.725586705176125e-05, 'epoch': 0.72}

 72%|███████▏  | 2252/3107 [4:49:04<2:03:34,  8.67s/it]

 73%|███████▎  | 2253/3107 [4:49:11<1:53:32,  7.98s/it]

 73%|███████▎  | 2254/3107 [4:49:18<1:50:35,  7.78s/it]

 73%|███████▎  | 2255/3107 [4:49:27<1:55:33,  8.14s/it]

 73%|███████▎  | 2256/3107 [4:49:33<1:46:16,  7.49s/it]

 73%|███████▎  | 2257/3107 [4:49:39<1:38:27,  6.95s/it]

 73%|███████▎  | 2258/3107 [4:49:46<1:40:52,  7.13s/it]


 73%|███████▎  | 2260/3107 [4:50:05<1:58:47,  8.41s/it]

 73%|███████▎  | 2261/3107 [4:50:11<1:49:02,  7.73s/it]

 73%|███████▎  | 2262/3107 [4:50:18<1:44:27,  7.42s/it]
{'loss': 0.9608, 'grad_norm': 0.17131518043283198, 'learning_rate': 3.6366927230331216e-05, 'epoch': 0.73}

 73%|███████▎  | 2263/3107 [4:50:24<1:39:00,  7.04s/it]

 73%|███████▎  | 2264/3107 [4:50:30<1:35:39,  6.81s/it]


 73%|███████▎  | 2266/3107 [4:50:48<1:49:47,  7.83s/it]
{'loss': 0.9475, 'grad_norm': 0.1857747533788107, 'learning_rate': 3.604574566575156e-05, 'epoch': 0.73}

 73%|███████▎  | 2267/3107 [4:50:56<1:51:27,  7.96s/it]


 73%|███████▎  | 2269/3107 [4:51:11<1:47:08,  7.67s/it]

 73%|███████▎  | 2270/3107 [4:51:18<1:41:38,  7.29s/it]
{'loss': 1.0299, 'grad_norm': 0.16999515957214917, 'learning_rate': 3.572567657620969e-05, 'epoch': 0.73}

 73%|███████▎  | 2271/3107 [4:51:27<1:49:45,  7.88s/it]

 73%|███████▎  | 2272/3107 [4:51:34<1:47:58,  7.76s/it]

 73%|███████▎  | 2273/3107 [4:51:41<1:43:12,  7.42s/it]

 73%|███████▎  | 2274/3107 [4:51:51<1:52:49,  8.13s/it]

 73%|███████▎  | 2275/3107 [4:51:59<1:52:10,  8.09s/it]

 73%|███████▎  | 2276/3107 [4:52:06<1:48:50,  7.86s/it]


 73%|███████▎  | 2278/3107 [4:52:24<1:54:01,  8.25s/it]

 73%|███████▎  | 2279/3107 [4:52:31<1:50:26,  8.00s/it]
{'loss': 1.0161, 'grad_norm': 0.1785094737180643, 'learning_rate': 3.500961741830821e-05, 'epoch': 0.73}

 73%|███████▎  | 2280/3107 [4:52:43<2:06:03,  9.15s/it]

 73%|███████▎  | 2281/3107 [4:52:53<2:07:34,  9.27s/it]

 73%|███████▎  | 2282/3107 [4:53:01<2:03:54,  9.01s/it]

 73%|███████▎  | 2283/3107 [4:53:09<2:00:01,  8.74s/it]

 74%|███████▎  | 2284/3107 [4:53:18<2:00:58,  8.82s/it]


 74%|███████▎  | 2286/3107 [4:53:32<1:47:44,  7.87s/it]
{'loss': 0.8849, 'grad_norm': 0.16561047348028152, 'learning_rate': 3.445663602723199e-05, 'epoch': 0.74}

 74%|███████▎  | 2287/3107 [4:53:38<1:41:16,  7.41s/it]


 74%|███████▎  | 2289/3107 [4:53:51<1:35:22,  7.00s/it]
{'loss': 0.9377, 'grad_norm': 0.173671431906554, 'learning_rate': 3.422071114821304e-05, 'epoch': 0.74}

 74%|███████▎  | 2290/3107 [4:53:59<1:36:52,  7.11s/it]

 74%|███████▎  | 2291/3107 [4:54:05<1:32:44,  6.82s/it]


 74%|███████▍  | 2293/3107 [4:54:20<1:36:56,  7.15s/it]
{'loss': 1.0142, 'grad_norm': 0.16250558235823653, 'learning_rate': 3.390714623516522e-05, 'epoch': 0.74}

 74%|███████▍  | 2294/3107 [4:54:29<1:42:37,  7.57s/it]


 74%|███████▍  | 2296/3107 [4:54:46<1:49:08,  8.07s/it]

 74%|███████▍  | 2297/3107 [4:54:56<1:56:54,  8.66s/it]
[2024-05-29 00:06:19,800] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 2298/3107 [4:55:04<1:53:30,  8.42s/it]

 74%|███████▍  | 2299/3107 [4:55:12<1:53:01,  8.39s/it]

 74%|███████▍  | 2300/3107 [4:55:18<1:43:23,  7.69s/it]

 74%|███████▍  | 2301/3107 [4:55:24<1:36:54,  7.21s/it]

 74%|███████▍  | 2302/3107 [4:55:32<1:38:17,  7.33s/it]

 74%|███████▍  | 2303/3107 [4:55:42<1:48:37,  8.11s/it]
{'loss': 0.7535, 'grad_norm': 0.1521790709715107, 'learning_rate': 3.312827566588519e-05, 'epoch': 0.74}

 74%|███████▍  | 2304/3107 [4:55:51<1:53:12,  8.46s/it]


 74%|███████▍  | 2306/3107 [4:56:06<1:48:51,  8.15s/it]

 74%|███████▍  | 2307/3107 [4:56:14<1:48:09,  8.11s/it]
{'loss': 0.7998, 'grad_norm': 0.16625343339467255, 'learning_rate': 3.2818758349270475e-05, 'epoch': 0.74}


 74%|███████▍  | 2309/3107 [4:56:32<1:52:31,  8.46s/it]
{'loss': 1.1133, 'grad_norm': 0.18860769782841352, 'learning_rate': 3.26644375832619e-05, 'epoch': 0.74}

 74%|███████▍  | 2310/3107 [4:56:39<1:46:33,  8.02s/it]


 74%|███████▍  | 2312/3107 [4:56:52<1:34:51,  7.16s/it]
{'loss': 0.9498, 'grad_norm': 0.1748594351684399, 'learning_rate': 3.2433505687530505e-05, 'epoch': 0.74}

 74%|███████▍  | 2313/3107 [4:56:58<1:33:06,  7.04s/it]

 74%|███████▍  | 2314/3107 [4:57:05<1:30:53,  6.88s/it]

 75%|███████▍  | 2315/3107 [4:57:10<1:25:30,  6.48s/it]


 75%|███████▍  | 2317/3107 [4:57:26<1:33:59,  7.14s/it]

 75%|███████▍  | 2318/3107 [4:57:32<1:28:42,  6.75s/it]
{'loss': 0.9959, 'grad_norm': 0.17166334520425425, 'learning_rate': 3.197362748510181e-05, 'epoch': 0.75}

 75%|███████▍  | 2319/3107 [4:57:39<1:31:16,  6.95s/it]


 75%|███████▍  | 2321/3107 [4:57:52<1:26:16,  6.59s/it]

 75%|███████▍  | 2322/3107 [4:58:02<1:39:57,  7.64s/it]

 75%|███████▍  | 2323/3107 [4:58:08<1:34:55,  7.26s/it]

 75%|███████▍  | 2324/3107 [4:58:16<1:35:58,  7.35s/it]
{'loss': 0.8776, 'grad_norm': 0.1734334533034595, 'learning_rate': 3.151641172297891e-05, 'epoch': 0.75}

 75%|███████▍  | 2325/3107 [4:58:24<1:41:09,  7.76s/it]


 75%|███████▍  | 2327/3107 [4:58:40<1:40:16,  7.71s/it]

 75%|███████▍  | 2328/3107 [4:58:46<1:32:16,  7.11s/it]
{'loss': 1.0536, 'grad_norm': 0.18840345630437988, 'learning_rate': 3.121308918958503e-05, 'epoch': 0.75}


 75%|███████▍  | 2330/3107 [4:59:04<1:41:47,  7.86s/it]

 75%|███████▌  | 2331/3107 [4:59:10<1:32:45,  7.17s/it]

 75%|███████▌  | 2332/3107 [4:59:22<1:50:53,  8.58s/it]
{'loss': 0.9141, 'grad_norm': 0.16692598189992378, 'learning_rate': 3.0910963194591835e-05, 'epoch': 0.75}

 75%|███████▌  | 2333/3107 [4:59:29<1:47:09,  8.31s/it]

 75%|███████▌  | 2334/3107 [4:59:36<1:38:46,  7.67s/it]


 75%|███████▌  | 2336/3107 [4:59:50<1:39:24,  7.74s/it]
{'loss': 0.9837, 'grad_norm': 0.1775231014032268, 'learning_rate': 3.061003899343756e-05, 'epoch': 0.75}

 75%|███████▌  | 2337/3107 [5:00:01<1:48:37,  8.46s/it]

 75%|███████▌  | 2338/3107 [5:00:07<1:41:02,  7.88s/it]

 75%|███████▌  | 2339/3107 [5:00:13<1:32:27,  7.22s/it]


 75%|███████▌  | 2341/3107 [5:00:26<1:29:07,  6.98s/it]
{'loss': 0.9359, 'grad_norm': 0.17600150045814864, 'learning_rate': 3.023558173706683e-05, 'epoch': 0.75}


 75%|███████▌  | 2343/3107 [5:00:40<1:30:27,  7.10s/it]
{'loss': 0.91, 'grad_norm': 0.16971777026335755, 'learning_rate': 3.0086329190497997e-05, 'epoch': 0.75}

 75%|███████▌  | 2344/3107 [5:00:47<1:29:29,  7.04s/it]

 75%|███████▌  | 2345/3107 [5:00:55<1:31:19,  7.19s/it]

 76%|███████▌  | 2346/3107 [5:01:01<1:26:35,  6.83s/it]

 76%|███████▌  | 2347/3107 [5:01:08<1:25:52,  6.78s/it]


 76%|███████▌  | 2349/3107 [5:01:20<1:22:48,  6.55s/it]
{'loss': 1.0827, 'grad_norm': 0.1764362529404451, 'learning_rate': 2.9640398349733334e-05, 'epoch': 0.76}

 76%|███████▌  | 2350/3107 [5:01:29<1:31:02,  7.22s/it]


 76%|███████▌  | 2352/3107 [5:01:44<1:36:03,  7.63s/it]

 76%|███████▌  | 2353/3107 [5:01:50<1:30:11,  7.18s/it]
{'loss': 0.8579, 'grad_norm': 0.1741334869284027, 'learning_rate': 2.9344639915781093e-05, 'epoch': 0.76}

 76%|███████▌  | 2354/3107 [5:01:58<1:30:29,  7.21s/it]

 76%|███████▌  | 2355/3107 [5:02:05<1:31:57,  7.34s/it]

 76%|███████▌  | 2356/3107 [5:02:11<1:26:13,  6.89s/it]

 76%|███████▌  | 2357/3107 [5:02:21<1:35:50,  7.67s/it]

 76%|███████▌  | 2358/3107 [5:02:33<1:52:11,  8.99s/it]

 76%|███████▌  | 2359/3107 [5:02:40<1:44:26,  8.38s/it]

 76%|███████▌  | 2360/3107 [5:02:45<1:33:54,  7.54s/it]

 76%|███████▌  | 2361/3107 [5:02:55<1:42:43,  8.26s/it]


 76%|███████▌  | 2363/3107 [5:03:15<1:51:19,  8.98s/it]

 76%|███████▌  | 2364/3107 [5:03:21<1:39:57,  8.07s/it]

 76%|███████▌  | 2365/3107 [5:03:26<1:31:02,  7.36s/it]
{'loss': 0.9656, 'grad_norm': 0.1718729364310474, 'learning_rate': 2.8464759324481327e-05, 'epoch': 0.76}


 76%|███████▌  | 2367/3107 [5:03:41<1:29:33,  7.26s/it]
{'loss': 0.7745, 'grad_norm': 0.1909630517393332, 'learning_rate': 2.831919765420411e-05, 'epoch': 0.76}

 76%|███████▌  | 2368/3107 [5:03:47<1:26:18,  7.01s/it]

 76%|███████▌  | 2369/3107 [5:03:54<1:24:44,  6.89s/it]


 76%|███████▋  | 2371/3107 [5:04:09<1:27:38,  7.14s/it]
{'loss': 1.0033, 'grad_norm': 0.17550877659526906, 'learning_rate': 2.8029010104237785e-05, 'epoch': 0.76}


 76%|███████▋  | 2373/3107 [5:04:22<1:26:22,  7.06s/it]
{'loss': 0.9981, 'grad_norm': 0.17367328568839166, 'learning_rate': 2.7884385486492715e-05, 'epoch': 0.76}

 76%|███████▋  | 2374/3107 [5:04:31<1:32:30,  7.57s/it]

 76%|███████▋  | 2375/3107 [5:04:39<1:34:00,  7.71s/it]

 76%|███████▋  | 2376/3107 [5:04:45<1:26:19,  7.09s/it]

 77%|███████▋  | 2377/3107 [5:04:52<1:24:37,  6.96s/it]

 77%|███████▋  | 2378/3107 [5:05:01<1:35:17,  7.84s/it]

 77%|███████▋  | 2379/3107 [5:05:07<1:27:36,  7.22s/it]


 77%|███████▋  | 2381/3107 [5:05:20<1:22:46,  6.84s/it]
{'loss': 0.9156, 'grad_norm': 0.1895685662658659, 'learning_rate': 2.7309029389571162e-05, 'epoch': 0.77}

 77%|███████▋  | 2382/3107 [5:05:30<1:32:56,  7.69s/it]

 77%|███████▋  | 2383/3107 [5:05:36<1:27:10,  7.22s/it]


 77%|███████▋  | 2385/3107 [5:05:48<1:20:41,  6.71s/it]
{'loss': 0.8071, 'grad_norm': 0.16979579336800452, 'learning_rate': 2.70232455179658e-05, 'epoch': 0.77}

 77%|███████▋  | 2386/3107 [5:05:57<1:27:24,  7.27s/it]


 77%|███████▋  | 2388/3107 [5:06:17<1:44:17,  8.70s/it]
{'loss': 0.9621, 'grad_norm': 0.1786708380422724, 'learning_rate': 2.6809740400189133e-05, 'epoch': 0.77}

 77%|███████▋  | 2389/3107 [5:06:23<1:36:55,  8.10s/it]

 77%|███████▋  | 2390/3107 [5:06:30<1:31:46,  7.68s/it]


 77%|███████▋  | 2392/3107 [5:06:44<1:27:57,  7.38s/it]
{'loss': 0.9467, 'grad_norm': 0.162813289308332, 'learning_rate': 2.6526181262856863e-05, 'epoch': 0.77}

 77%|███████▋  | 2393/3107 [5:06:54<1:34:24,  7.93s/it]


 77%|███████▋  | 2395/3107 [5:07:08<1:29:47,  7.57s/it]
{'loss': 1.0169, 'grad_norm': 0.17205056660009757, 'learning_rate': 2.6314350372081475e-05, 'epoch': 0.77}


 77%|███████▋  | 2397/3107 [5:07:22<1:28:35,  7.49s/it]

 77%|███████▋  | 2398/3107 [5:07:29<1:24:47,  7.18s/it]

 77%|███████▋  | 2399/3107 [5:07:35<1:19:22,  6.73s/it]

 77%|███████▋  | 2400/3107 [5:07:43<1:24:26,  7.17s/it]
 77%|███████▋  | 2400/3107 [5:07:43<1:24:26,  7.17s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.8897, 'grad_norm': 0.18089816392403021, 'learning_rate': 2.589285361277637e-05, 'epoch': 0.77}

 77%|███████▋  | 2402/3107 [5:08:11<1:57:45, 10.02s/it]
{'loss': 0.9614, 'grad_norm': 0.17854356357352624, 'learning_rate': 2.5822885695461052e-05, 'epoch': 0.77}

 77%|███████▋  | 2403/3107 [5:08:18<1:47:42,  9.18s/it]


 77%|███████▋  | 2405/3107 [5:08:36<1:50:52,  9.48s/it]

 77%|███████▋  | 2406/3107 [5:08:43<1:40:23,  8.59s/it]
{'loss': 0.8821, 'grad_norm': 0.1777458959066738, 'learning_rate': 2.5543821224290765e-05, 'epoch': 0.77}


 78%|███████▊  | 2408/3107 [5:08:59<1:37:17,  8.35s/it]
{'loss': 0.8219, 'grad_norm': 0.16672578867876972, 'learning_rate': 2.540477436912927e-05, 'epoch': 0.78}

 78%|███████▊  | 2409/3107 [5:09:07<1:38:06,  8.43s/it]


 78%|███████▊  | 2411/3107 [5:09:20<1:26:44,  7.48s/it]

 78%|███████▊  | 2412/3107 [5:09:27<1:22:52,  7.16s/it]
{'loss': 0.917, 'grad_norm': 0.1808935098140443, 'learning_rate': 2.51276544430293e-05, 'epoch': 0.78}

 78%|███████▊  | 2413/3107 [5:09:36<1:30:03,  7.79s/it]

 78%|███████▊  | 2414/3107 [5:09:44<1:30:30,  7.84s/it]


 78%|███████▊  | 2416/3107 [5:10:03<1:39:26,  8.64s/it]
{'loss': 0.9244, 'grad_norm': 0.16334394541601385, 'learning_rate': 2.4851836910590508e-05, 'epoch': 0.78}


 78%|███████▊  | 2418/3107 [5:10:18<1:32:52,  8.09s/it]

 78%|███████▊  | 2419/3107 [5:10:31<1:47:50,  9.40s/it]

 78%|███████▊  | 2420/3107 [5:10:36<1:34:47,  8.28s/it]

 78%|███████▊  | 2421/3107 [5:10:45<1:35:44,  8.37s/it]
{'loss': 0.9841, 'grad_norm': 0.19111492495687898, 'learning_rate': 2.4508903793122173e-05, 'epoch': 0.78}

 78%|███████▊  | 2422/3107 [5:10:54<1:37:42,  8.56s/it]


 78%|███████▊  | 2424/3107 [5:11:11<1:35:47,  8.41s/it]
{'loss': 0.9685, 'grad_norm': 0.17446743845537308, 'learning_rate': 2.4304128195183086e-05, 'epoch': 0.78}


 78%|███████▊  | 2426/3107 [5:11:29<1:37:58,  8.63s/it]
{'loss': 0.9735, 'grad_norm': 0.17735648277734997, 'learning_rate': 2.4168022481386442e-05, 'epoch': 0.78}


 78%|███████▊  | 2428/3107 [5:11:43<1:28:28,  7.82s/it]
{'loss': 0.7853, 'grad_norm': 0.1693467107153911, 'learning_rate': 2.4032246539528548e-05, 'epoch': 0.78}

 78%|███████▊  | 2429/3107 [5:11:50<1:25:53,  7.60s/it]

 78%|███████▊  | 2430/3107 [5:11:58<1:27:09,  7.72s/it]

 78%|███████▊  | 2431/3107 [5:12:04<1:22:10,  7.29s/it]

 78%|███████▊  | 2432/3107 [5:12:14<1:30:54,  8.08s/it]

 78%|███████▊  | 2433/3107 [5:12:20<1:24:02,  7.48s/it]


 78%|███████▊  | 2435/3107 [5:12:35<1:25:15,  7.61s/it]
{'loss': 0.9263, 'grad_norm': 0.18513803392346814, 'learning_rate': 2.3559636208714785e-05, 'epoch': 0.78}


 78%|███████▊  | 2437/3107 [5:12:53<1:30:02,  8.06s/it]

 78%|███████▊  | 2438/3107 [5:13:01<1:29:49,  8.06s/it]
{'loss': 0.9136, 'grad_norm': 0.17371766602777772, 'learning_rate': 2.3358334024781815e-05, 'epoch': 0.78}

 79%|███████▊  | 2439/3107 [5:13:10<1:33:21,  8.39s/it]

 79%|███████▊  | 2440/3107 [5:13:18<1:30:04,  8.10s/it]


 79%|███████▊  | 2442/3107 [5:13:35<1:32:56,  8.39s/it]

 79%|███████▊  | 2443/3107 [5:13:43<1:29:26,  8.08s/it]

 79%|███████▊  | 2444/3107 [5:13:51<1:29:16,  8.08s/it]
{'loss': 0.8797, 'grad_norm': 0.18201504848554437, 'learning_rate': 2.295798134628404e-05, 'epoch': 0.79}


 79%|███████▊  | 2446/3107 [5:14:07<1:26:13,  7.83s/it]

 79%|███████▉  | 2447/3107 [5:14:15<1:29:15,  8.11s/it]
{'loss': 0.9935, 'grad_norm': 0.16337745030907033, 'learning_rate': 2.275893476901516e-05, 'epoch': 0.79}

 79%|███████▉  | 2448/3107 [5:14:24<1:32:18,  8.40s/it]


 79%|███████▉  | 2450/3107 [5:14:39<1:24:30,  7.72s/it]
{'loss': 1.0132, 'grad_norm': 0.17601593629663176, 'learning_rate': 2.256064396564974e-05, 'epoch': 0.79}

 79%|███████▉  | 2451/3107 [5:14:46<1:22:43,  7.57s/it]

 79%|███████▉  | 2452/3107 [5:14:52<1:16:18,  6.99s/it]

 79%|███████▉  | 2453/3107 [5:15:02<1:27:22,  8.02s/it]

 79%|███████▉  | 2454/3107 [5:15:09<1:22:11,  7.55s/it]

 79%|███████▉  | 2455/3107 [5:15:14<1:15:35,  6.96s/it]

 79%|███████▉  | 2456/3107 [5:15:22<1:17:31,  7.15s/it]

 79%|███████▉  | 2457/3107 [5:15:28<1:14:02,  6.84s/it]


 79%|███████▉  | 2459/3107 [5:15:41<1:12:16,  6.69s/it]
{'loss': 1.0161, 'grad_norm': 0.18310556100482805, 'learning_rate': 2.197032556387295e-05, 'epoch': 0.79}

 79%|███████▉  | 2460/3107 [5:15:50<1:19:53,  7.41s/it]

 79%|███████▉  | 2461/3107 [5:15:56<1:14:22,  6.91s/it]

 79%|███████▉  | 2462/3107 [5:16:07<1:25:34,  7.96s/it]

 79%|███████▉  | 2463/3107 [5:16:15<1:25:48,  8.00s/it]

 79%|███████▉  | 2464/3107 [5:16:24<1:28:52,  8.29s/it]


 79%|███████▉  | 2466/3107 [5:16:41<1:33:24,  8.74s/it]

 79%|███████▉  | 2467/3107 [5:16:47<1:24:40,  7.94s/it]
{'loss': 0.9881, 'grad_norm': 0.2018208929232415, 'learning_rate': 2.1451365047396454e-05, 'epoch': 0.79}

 79%|███████▉  | 2468/3107 [5:16:57<1:28:35,  8.32s/it]

 79%|███████▉  | 2469/3107 [5:17:07<1:33:40,  8.81s/it]


 80%|███████▉  | 2471/3107 [5:17:19<1:21:04,  7.65s/it]
{'loss': 0.9235, 'grad_norm': 0.18824822276401065, 'learning_rate': 2.1193932051547814e-05, 'epoch': 0.8}

 80%|███████▉  | 2472/3107 [5:17:30<1:29:48,  8.49s/it]

 80%|███████▉  | 2473/3107 [5:17:37<1:25:12,  8.06s/it]

 80%|███████▉  | 2474/3107 [5:17:45<1:24:12,  7.98s/it]


 80%|███████▉  | 2476/3107 [5:18:00<1:22:54,  7.88s/it]
{'loss': 0.9762, 'grad_norm': 0.18331282982444547, 'learning_rate': 2.0874069045134092e-05, 'epoch': 0.8}


 80%|███████▉  | 2478/3107 [5:18:17<1:28:11,  8.41s/it]
{'loss': 0.8995, 'grad_norm': 0.16741187035382177, 'learning_rate': 2.074672552526472e-05, 'epoch': 0.8}


 80%|███████▉  | 2480/3107 [5:18:32<1:20:41,  7.72s/it]
{'loss': 0.975, 'grad_norm': 0.16284872795279365, 'learning_rate': 2.061972665559213e-05, 'epoch': 0.8}


 80%|███████▉  | 2482/3107 [5:18:51<1:30:30,  8.69s/it]
{'loss': 0.7944, 'grad_norm': 0.1747576070748263, 'learning_rate': 2.04930729883987e-05, 'epoch': 0.8}

 80%|███████▉  | 2483/3107 [5:18:58<1:25:27,  8.22s/it]

 80%|███████▉  | 2484/3107 [5:19:07<1:26:38,  8.34s/it]


 80%|████████  | 2486/3107 [5:19:21<1:18:50,  7.62s/it]

 80%|████████  | 2487/3107 [5:19:28<1:14:44,  7.23s/it]

 80%|████████  | 2488/3107 [5:19:33<1:10:27,  6.83s/it]

 80%|████████  | 2489/3107 [5:19:42<1:14:24,  7.22s/it]
{'loss': 0.8406, 'grad_norm': 0.16003530397956853, 'learning_rate': 2.005251156107426e-05, 'epoch': 0.8}

 80%|████████  | 2490/3107 [5:19:48<1:12:24,  7.04s/it]


 80%|████████  | 2492/3107 [5:20:04<1:17:22,  7.55s/it]
{'loss': 0.934, 'grad_norm': 0.18030059476739205, 'learning_rate': 1.9865001914284354e-05, 'epoch': 0.8}

 80%|████████  | 2493/3107 [5:20:13<1:21:58,  8.01s/it]


 80%|████████  | 2495/3107 [5:20:28<1:19:00,  7.75s/it]
{'loss': 0.8944, 'grad_norm': 0.18650715239725957, 'learning_rate': 1.9678276357409908e-05, 'epoch': 0.8}


 80%|████████  | 2497/3107 [5:20:45<1:23:49,  8.24s/it]
{'loss': 0.9038, 'grad_norm': 0.18488171407126688, 'learning_rate': 1.9554229160892434e-05, 'epoch': 0.8}

 80%|████████  | 2498/3107 [5:20:51<1:15:28,  7.44s/it]

 80%|████████  | 2499/3107 [5:20:58<1:15:31,  7.45s/it]

 80%|████████  | 2500/3107 [5:21:04<1:10:44,  6.99s/it]


 81%|████████  | 2502/3107 [5:21:19<1:14:00,  7.34s/it]

 81%|████████  | 2503/3107 [5:21:26<1:11:46,  7.13s/it]
{'loss': 0.9109, 'grad_norm': 0.1856266924275311, 'learning_rate': 1.9184188737684682e-05, 'epoch': 0.81}

 81%|████████  | 2504/3107 [5:21:32<1:09:52,  6.95s/it]

 81%|████████  | 2505/3107 [5:21:39<1:08:24,  6.82s/it]

 81%|████████  | 2506/3107 [5:21:45<1:05:49,  6.57s/it]


 81%|████████  | 2508/3107 [5:21:58<1:05:52,  6.60s/it]
{'loss': 1.0247, 'grad_norm': 0.19732268153993943, 'learning_rate': 1.8878237286387968e-05, 'epoch': 0.81}

 81%|████████  | 2509/3107 [5:22:04<1:05:57,  6.62s/it]

 81%|████████  | 2510/3107 [5:22:12<1:09:28,  6.98s/it]


 81%|████████  | 2512/3107 [5:22:25<1:06:51,  6.74s/it]

 81%|████████  | 2513/3107 [5:22:33<1:10:26,  7.12s/it]

 81%|████████  | 2514/3107 [5:22:39<1:07:22,  6.82s/it]

 81%|████████  | 2515/3107 [5:22:48<1:11:58,  7.29s/it]
{'loss': 0.9676, 'grad_norm': 0.17236309026034408, 'learning_rate': 1.845361124394097e-05, 'epoch': 0.81}


 81%|████████  | 2517/3107 [5:23:06<1:18:25,  7.98s/it]
{'loss': 1.0706, 'grad_norm': 0.18422838102720235, 'learning_rate': 1.8333086431000424e-05, 'epoch': 0.81}

 81%|████████  | 2518/3107 [5:23:13<1:15:11,  7.66s/it]


 81%|████████  | 2520/3107 [5:23:32<1:26:06,  8.80s/it]

 81%|████████  | 2521/3107 [5:23:40<1:23:18,  8.53s/it]
{'loss': 0.8504, 'grad_norm': 0.1788038449545648, 'learning_rate': 1.8093102767006243e-05, 'epoch': 0.81}


 81%|████████  | 2523/3107 [5:23:54<1:16:10,  7.83s/it]
{'loss': 0.8837, 'grad_norm': 0.18931943279631774, 'learning_rate': 1.7973644959573997e-05, 'epoch': 0.81}


 81%|████████▏ | 2525/3107 [5:24:08<1:10:51,  7.31s/it]
{'loss': 0.9349, 'grad_norm': 0.17025737803266325, 'learning_rate': 1.785454386168587e-05, 'epoch': 0.81}


 81%|████████▏ | 2527/3107 [5:24:23<1:12:12,  7.47s/it]

 81%|████████▏ | 2528/3107 [5:24:34<1:20:07,  8.30s/it]
{'loss': 1.0617, 'grad_norm': 0.16980649070475998, 'learning_rate': 1.767656217781174e-05, 'epoch': 0.81}

 81%|████████▏ | 2529/3107 [5:24:41<1:16:34,  7.95s/it]


 81%|████████▏ | 2531/3107 [5:24:56<1:15:52,  7.90s/it]
{'loss': 0.8322, 'grad_norm': 0.16474936842244767, 'learning_rate': 1.7499385996888207e-05, 'epoch': 0.81}

 81%|████████▏ | 2532/3107 [5:25:04<1:17:12,  8.06s/it]


 82%|████████▏ | 2534/3107 [5:25:18<1:10:47,  7.41s/it]
{'loss': 0.9068, 'grad_norm': 0.17013724018198015, 'learning_rate': 1.73230170525156e-05, 'epoch': 0.82}

 82%|████████▏ | 2535/3107 [5:25:24<1:07:49,  7.11s/it]

 82%|████████▏ | 2536/3107 [5:25:32<1:10:28,  7.41s/it]

 82%|████████▏ | 2537/3107 [5:25:39<1:07:53,  7.15s/it]

 82%|████████▏ | 2538/3107 [5:25:47<1:11:36,  7.55s/it]


 82%|████████▏ | 2540/3107 [5:26:01<1:08:22,  7.24s/it]

 82%|████████▏ | 2541/3107 [5:26:08<1:05:50,  6.98s/it]

 82%|████████▏ | 2542/3107 [5:26:13<1:02:04,  6.59s/it]

 82%|████████▏ | 2543/3107 [5:26:22<1:07:29,  7.18s/it]
{'loss': 0.926, 'grad_norm': 0.17976578431443815, 'learning_rate': 1.6798770856127588e-05, 'epoch': 0.82}


 82%|████████▏ | 2545/3107 [5:26:38<1:09:33,  7.43s/it]

 82%|████████▏ | 2546/3107 [5:26:44<1:05:52,  7.05s/it]

 82%|████████▏ | 2547/3107 [5:26:53<1:13:27,  7.87s/it]
{'loss': 0.8212, 'grad_norm': 0.16011361243861347, 'learning_rate': 1.656812163124072e-05, 'epoch': 0.82}

 82%|████████▏ | 2548/3107 [5:27:05<1:22:49,  8.89s/it]

 82%|████████▏ | 2549/3107 [5:27:12<1:19:10,  8.51s/it]


 82%|████████▏ | 2551/3107 [5:27:32<1:25:55,  9.27s/it]
{'loss': 0.8582, 'grad_norm': 0.16902761931315244, 'learning_rate': 1.6338923691854824e-05, 'epoch': 0.82}


 82%|████████▏ | 2553/3107 [5:27:52<1:29:29,  9.69s/it]
{'loss': 0.9486, 'grad_norm': 0.16870917311433542, 'learning_rate': 1.622487020110812e-05, 'epoch': 0.82}

 82%|████████▏ | 2554/3107 [5:27:59<1:20:43,  8.76s/it]


 82%|████████▏ | 2556/3107 [5:28:14<1:15:37,  8.23s/it]

 82%|████████▏ | 2557/3107 [5:28:24<1:19:24,  8.66s/it]

 82%|████████▏ | 2558/3107 [5:28:32<1:18:27,  8.57s/it]

 82%|████████▏ | 2559/3107 [5:28:38<1:10:29,  7.72s/it]

 82%|████████▏ | 2560/3107 [5:28:44<1:06:49,  7.33s/it]

 82%|████████▏ | 2561/3107 [5:28:50<1:03:28,  6.98s/it]
{'loss': 0.8825, 'grad_norm': 0.1730450980504925, 'learning_rate': 1.5772304318945343e-05, 'epoch': 0.82}


 82%|████████▏ | 2563/3107 [5:29:10<1:15:44,  8.35s/it]
{'loss': 0.7841, 'grad_norm': 0.17422204719322637, 'learning_rate': 1.5660077328713886e-05, 'epoch': 0.82}

 83%|████████▎ | 2564/3107 [5:29:17<1:11:32,  7.90s/it]

 83%|████████▎ | 2565/3107 [5:29:23<1:07:02,  7.42s/it]


 83%|████████▎ | 2567/3107 [5:29:40<1:08:58,  7.66s/it]
{'loss': 0.8629, 'grad_norm': 0.1719517870626435, 'learning_rate': 1.5436724146496408e-05, 'epoch': 0.83}


 83%|████████▎ | 2569/3107 [5:29:56<1:11:58,  8.03s/it]
{'loss': 0.8291, 'grad_norm': 0.17623479991413607, 'learning_rate': 1.5325598925810548e-05, 'epoch': 0.83}

 83%|████████▎ | 2570/3107 [5:30:05<1:14:37,  8.34s/it]

 83%|████████▎ | 2571/3107 [5:30:11<1:07:43,  7.58s/it]


 83%|████████▎ | 2573/3107 [5:30:28<1:10:56,  7.97s/it]

 83%|████████▎ | 2574/3107 [5:30:36<1:09:50,  7.86s/it]

 83%|████████▎ | 2575/3107 [5:30:44<1:10:15,  7.92s/it]
{'loss': 0.8651, 'grad_norm': 0.16954280470001845, 'learning_rate': 1.4994434539662071e-05, 'epoch': 0.83}


 83%|████████▎ | 2577/3107 [5:30:56<1:01:40,  6.98s/it]
{'loss': 0.9791, 'grad_norm': 0.1731088893985968, 'learning_rate': 1.4884785103072252e-05, 'epoch': 0.83}


 83%|████████▎ | 2579/3107 [5:31:12<1:06:41,  7.58s/it]

 83%|████████▎ | 2580/3107 [5:31:18<1:00:49,  6.93s/it]
{'loss': 0.943, 'grad_norm': 0.1871531531405396, 'learning_rate': 1.472100511324589e-05, 'epoch': 0.83}

 83%|████████▎ | 2581/3107 [5:31:25<1:00:33,  6.91s/it]

 83%|████████▎ | 2582/3107 [5:31:33<1:04:45,  7.40s/it]

 83%|████████▎ | 2583/3107 [5:31:43<1:09:47,  7.99s/it]


 83%|████████▎ | 2585/3107 [5:31:58<1:07:46,  7.79s/it]
{'loss': 0.9684, 'grad_norm': 0.16351812559761325, 'learning_rate': 1.4449893522168833e-05, 'epoch': 0.83}

 83%|████████▎ | 2586/3107 [5:32:12<1:22:45,  9.53s/it]

 83%|████████▎ | 2587/3107 [5:32:19<1:16:13,  8.80s/it]


 83%|████████▎ | 2589/3107 [5:32:33<1:07:52,  7.86s/it]

 83%|████████▎ | 2590/3107 [5:32:39<1:02:45,  7.28s/it]
{'loss': 0.9806, 'grad_norm': 0.18505384031926914, 'learning_rate': 1.4181107135092165e-05, 'epoch': 0.83}

 83%|████████▎ | 2591/3107 [5:32:47<1:05:22,  7.60s/it]

 83%|████████▎ | 2592/3107 [5:32:56<1:08:14,  7.95s/it]


 83%|████████▎ | 2594/3107 [5:33:14<1:12:57,  8.53s/it]
{'loss': 0.936, 'grad_norm': 0.1641116856063986, 'learning_rate': 1.396775708371113e-05, 'epoch': 0.83}


 84%|████████▎ | 2596/3107 [5:33:30<1:10:05,  8.23s/it]

 84%|████████▎ | 2597/3107 [5:33:40<1:13:51,  8.69s/it]

 84%|████████▎ | 2598/3107 [5:33:47<1:07:32,  7.96s/it]

 84%|████████▎ | 2599/3107 [5:33:52<1:01:51,  7.31s/it]
{'loss': 1.0084, 'grad_norm': 0.1862961865498478, 'learning_rate': 1.3703174431007559e-05, 'epoch': 0.84}


 84%|████████▎ | 2601/3107 [5:34:06<59:34,  7.06s/it]

 84%|████████▎ | 2602/3107 [5:34:12<57:16,  6.80s/it]

 84%|████████▍ | 2603/3107 [5:34:20<1:00:14,  7.17s/it]

 84%|████████▍ | 2604/3107 [5:34:28<1:02:26,  7.45s/it]
{'loss': 0.8447, 'grad_norm': 0.17488528227871242, 'learning_rate': 1.344093727771124e-05, 'epoch': 0.84}


 84%|████████▍ | 2606/3107 [5:34:45<1:07:39,  8.10s/it]

 84%|████████▍ | 2607/3107 [5:34:52<1:06:00,  7.92s/it]

 84%|████████▍ | 2608/3107 [5:34:59<1:01:47,  7.43s/it]
{'loss': 0.9418, 'grad_norm': 0.16902600141194674, 'learning_rate': 1.3232841106446792e-05, 'epoch': 0.84}

 84%|████████▍ | 2609/3107 [5:35:07<1:04:51,  7.81s/it]

 84%|████████▍ | 2610/3107 [5:35:18<1:10:44,  8.54s/it]

 84%|████████▍ | 2611/3107 [5:35:28<1:14:19,  8.99s/it]


 84%|████████▍ | 2613/3107 [5:35:44<1:09:59,  8.50s/it]

 84%|████████▍ | 2614/3107 [5:35:53<1:09:48,  8.50s/it]
{'loss': 0.9064, 'grad_norm': 0.16750735566599034, 'learning_rate': 1.2923527915233336e-05, 'epoch': 0.84}

 84%|████████▍ | 2615/3107 [5:36:01<1:09:23,  8.46s/it]


 84%|████████▍ | 2617/3107 [5:36:15<1:03:24,  7.76s/it]
{'loss': 0.8131, 'grad_norm': 0.1747157689722004, 'learning_rate': 1.2770148579404295e-05, 'epoch': 0.84}


 84%|████████▍ | 2619/3107 [5:36:30<1:02:20,  7.67s/it]

 84%|████████▍ | 2620/3107 [5:36:36<57:55,  7.14s/it]

 84%|████████▍ | 2621/3107 [5:36:44<1:00:08,  7.43s/it]

 84%|████████▍ | 2622/3107 [5:36:54<1:06:03,  8.17s/it]

 84%|████████▍ | 2623/3107 [5:37:05<1:11:02,  8.81s/it]
{'loss': 0.9643, 'grad_norm': 0.1656196056579914, 'learning_rate': 1.2465951931073694e-05, 'epoch': 0.84}

 84%|████████▍ | 2624/3107 [5:37:16<1:17:21,  9.61s/it]


 85%|████████▍ | 2626/3107 [5:37:33<1:10:23,  8.78s/it]
{'loss': 0.9648, 'grad_norm': 0.17225430941350287, 'learning_rate': 1.2315137595018556e-05, 'epoch': 0.85}

 85%|████████▍ | 2627/3107 [5:37:40<1:05:54,  8.24s/it]


 85%|████████▍ | 2629/3107 [5:37:59<1:12:51,  9.15s/it]
{'loss': 0.9585, 'grad_norm': 0.1824525963824735, 'learning_rate': 1.2165181221376787e-05, 'epoch': 0.85}


 85%|████████▍ | 2631/3107 [5:38:11<59:47,  7.54s/it]

 85%|████████▍ | 2632/3107 [5:38:19<1:00:52,  7.69s/it]
{'loss': 0.8433, 'grad_norm': 0.16848869613125744, 'learning_rate': 1.2016084277413442e-05, 'epoch': 0.85}

 85%|████████▍ | 2633/3107 [5:38:25<57:00,  7.22s/it]

 85%|████████▍ | 2634/3107 [5:38:36<1:05:47,  8.35s/it]


 85%|████████▍ | 2636/3107 [5:38:51<1:01:00,  7.77s/it]
{'loss': 1.0865, 'grad_norm': 0.1952751655537362, 'learning_rate': 1.1818627763006084e-05, 'epoch': 0.85}


 85%|████████▍ | 2638/3107 [5:39:06<1:02:03,  7.94s/it]

 85%|████████▍ | 2639/3107 [5:39:13<58:06,  7.45s/it]
{'loss': 0.9337, 'grad_norm': 0.16772112869097586, 'learning_rate': 1.1671541813726194e-05, 'epoch': 0.85}


 85%|████████▌ | 2641/3107 [5:39:29<1:01:10,  7.88s/it]
{'loss': 1.0035, 'grad_norm': 0.17162285070752592, 'learning_rate': 1.1573964570020767e-05, 'epoch': 0.85}

 85%|████████▌ | 2642/3107 [5:39:38<1:03:21,  8.18s/it]

 85%|████████▌ | 2643/3107 [5:39:44<59:02,  7.63s/it]

 85%|████████▌ | 2644/3107 [5:39:50<54:01,  7.00s/it]


 85%|████████▌ | 2646/3107 [5:40:03<52:31,  6.84s/it]
{'loss': 0.995, 'grad_norm': 0.18599990037587788, 'learning_rate': 1.1331704747174732e-05, 'epoch': 0.85}

 85%|████████▌ | 2647/3107 [5:40:12<57:00,  7.44s/it]

 85%|████████▌ | 2648/3107 [5:40:21<1:01:35,  8.05s/it]

 85%|████████▌ | 2649/3107 [5:40:29<1:01:06,  8.01s/it]

 85%|████████▌ | 2650/3107 [5:40:35<56:40,  7.44s/it]


 85%|████████▌ | 2652/3107 [5:40:48<53:09,  7.01s/it]

 85%|████████▌ | 2653/3107 [5:40:54<50:35,  6.69s/it]
{'loss': 0.8949, 'grad_norm': 0.16362693259632866, 'learning_rate': 1.0996591181418926e-05, 'epoch': 0.85}


 85%|████████▌ | 2655/3107 [5:41:11<55:24,  7.36s/it]
{'loss': 0.99, 'grad_norm': 0.1712193769283624, 'learning_rate': 1.0901714534671848e-05, 'epoch': 0.85}


 86%|████████▌ | 2657/3107 [5:41:31<1:07:31,  9.00s/it]
{'loss': 1.0062, 'grad_norm': 0.17363109493906873, 'learning_rate': 1.0807225351304972e-05, 'epoch': 0.86}

 86%|████████▌ | 2658/3107 [5:41:38<1:02:59,  8.42s/it]

 86%|████████▌ | 2659/3107 [5:41:44<57:17,  7.67s/it]


 86%|████████▌ | 2661/3107 [5:41:57<52:37,  7.08s/it]

 86%|████████▌ | 2662/3107 [5:42:05<53:57,  7.27s/it]
{'loss': 0.8686, 'grad_norm': 0.18616064039372807, 'learning_rate': 1.0572700237601108e-05, 'epoch': 0.86}

 86%|████████▌ | 2663/3107 [5:42:16<1:01:13,  8.27s/it]

 86%|████████▌ | 2664/3107 [5:42:24<1:02:04,  8.41s/it]

 86%|████████▌ | 2665/3107 [5:42:30<55:39,  7.56s/it]


 86%|████████▌ | 2667/3107 [5:42:44<54:13,  7.39s/it]

 86%|████████▌ | 2668/3107 [5:42:51<52:28,  7.17s/it]

 86%|████████▌ | 2669/3107 [5:43:01<57:27,  7.87s/it]

 86%|████████▌ | 2670/3107 [5:43:13<1:07:52,  9.32s/it]
{'loss': 0.8651, 'grad_norm': 0.17865813239156775, 'learning_rate': 1.0202518297241237e-05, 'epoch': 0.86}

 86%|████████▌ | 2671/3107 [5:43:19<1:00:37,  8.34s/it]


 86%|████████▌ | 2673/3107 [5:43:33<54:49,  7.58s/it]

 86%|████████▌ | 2674/3107 [5:43:39<51:36,  7.15s/it]
{'loss': 0.8925, 'grad_norm': 0.17411735575018836, 'learning_rate': 1.0019768745405122e-05, 'epoch': 0.86}

 86%|████████▌ | 2675/3107 [5:43:46<51:25,  7.14s/it]

 86%|████████▌ | 2676/3107 [5:43:53<51:36,  7.18s/it]

 86%|████████▌ | 2677/3107 [5:44:00<50:28,  7.04s/it]

 86%|████████▌ | 2678/3107 [5:44:06<48:24,  6.77s/it]

 86%|████████▌ | 2679/3107 [5:44:12<45:44,  6.41s/it]


 86%|████████▋ | 2681/3107 [5:44:25<46:34,  6.56s/it]
{'loss': 0.9441, 'grad_norm': 0.17664431709536546, 'learning_rate': 9.703725172678092e-06, 'epoch': 0.86}

 86%|████████▋ | 2682/3107 [5:44:32<46:49,  6.61s/it]

 86%|████████▋ | 2683/3107 [5:44:38<44:51,  6.35s/it]

 86%|████████▋ | 2684/3107 [5:44:44<44:54,  6.37s/it]


 86%|████████▋ | 2686/3107 [5:44:59<48:11,  6.87s/it]

 86%|████████▋ | 2687/3107 [5:45:07<50:40,  7.24s/it]
{'loss': 0.9014, 'grad_norm': 0.18073839922010093, 'learning_rate': 9.436658597702242e-06, 'epoch': 0.86}

 87%|████████▋ | 2688/3107 [5:45:13<46:44,  6.69s/it]


 87%|████████▋ | 2690/3107 [5:45:27<49:25,  7.11s/it]
{'loss': 0.8835, 'grad_norm': 0.1725365249675617, 'learning_rate': 9.304453849916051e-06, 'epoch': 0.87}


 87%|████████▋ | 2692/3107 [5:45:41<47:59,  6.94s/it]

 87%|████████▋ | 2693/3107 [5:45:49<51:23,  7.45s/it]

 87%|████████▋ | 2694/3107 [5:45:57<51:07,  7.43s/it]
{'loss': 0.9654, 'grad_norm': 0.1814892912806886, 'learning_rate': 9.129561507534046e-06, 'epoch': 0.87}

 87%|████████▋ | 2695/3107 [5:46:06<54:26,  7.93s/it]


 87%|████████▋ | 2697/3107 [5:46:27<1:04:31,  9.44s/it]
{'loss': 0.8725, 'grad_norm': 0.16214884158405088, 'learning_rate': 8.999429405466208e-06, 'epoch': 0.87}

 87%|████████▋ | 2698/3107 [5:46:33<57:19,  8.41s/it]

 87%|████████▋ | 2699/3107 [5:46:38<51:12,  7.53s/it]

 87%|████████▋ | 2700/3107 [5:46:46<52:26,  7.73s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.9192, 'grad_norm': 0.17224576758900711, 'learning_rate': 8.827305229826456e-06, 'epoch': 0.87}
 87%|████████▋ | 2701/3107 [5:47:10<1:24:38, 12.51s/it]

 87%|████████▋ | 2702/3107 [5:47:16<1:11:02, 10.52s/it]

 87%|████████▋ | 2703/3107 [5:47:24<1:06:29,  9.87s/it]


 87%|████████▋ | 2705/3107 [5:47:45<1:06:40,  9.95s/it]
{'loss': 0.915, 'grad_norm': 0.17733421242375416, 'learning_rate': 8.65676699007344e-06, 'epoch': 0.87}

 87%|████████▋ | 2706/3107 [5:47:54<1:05:16,  9.77s/it]

 87%|████████▋ | 2707/3107 [5:48:01<58:05,  8.71s/it]


 87%|████████▋ | 2709/3107 [5:48:15<54:26,  8.21s/it]
{'loss': 0.7884, 'grad_norm': 0.195407821946248, 'learning_rate': 8.487817652695229e-06, 'epoch': 0.87}

 87%|████████▋ | 2710/3107 [5:48:22<51:38,  7.80s/it]


 87%|████████▋ | 2712/3107 [5:48:36<47:11,  7.17s/it]
{'loss': 0.8784, 'grad_norm': 0.16686680522965514, 'learning_rate': 8.362150135787261e-06, 'epoch': 0.87}

 87%|████████▋ | 2713/3107 [5:48:42<45:00,  6.86s/it]

 87%|████████▋ | 2714/3107 [5:48:50<48:10,  7.35s/it]

 87%|████████▋ | 2715/3107 [5:48:57<45:58,  7.04s/it]

 87%|████████▋ | 2716/3107 [5:49:04<46:21,  7.11s/it]

 87%|████████▋ | 2717/3107 [5:49:12<48:27,  7.46s/it]


 88%|████████▊ | 2719/3107 [5:49:26<46:20,  7.17s/it]

 88%|████████▊ | 2720/3107 [5:49:37<54:34,  8.46s/it]
{'loss': 1.0734, 'grad_norm': 0.1666826410078577, 'learning_rate': 8.031423646796343e-06, 'epoch': 0.88}


 88%|████████▊ | 2722/3107 [5:49:51<48:59,  7.64s/it]
{'loss': 0.9188, 'grad_norm': 0.17402619774135245, 'learning_rate': 7.949740993661192e-06, 'epoch': 0.88}

 88%|████████▊ | 2723/3107 [5:50:00<51:00,  7.97s/it]

 88%|████████▊ | 2724/3107 [5:50:07<48:18,  7.57s/it]


 88%|████████▊ | 2726/3107 [5:50:25<53:03,  8.36s/it]
{'loss': 0.8068, 'grad_norm': 0.17283788013964374, 'learning_rate': 7.787576942913166e-06, 'epoch': 0.88}


 88%|████████▊ | 2728/3107 [5:50:41<52:05,  8.25s/it]
{'loss': 0.889, 'grad_norm': 0.17443076424129042, 'learning_rate': 7.707096250506162e-06, 'epoch': 0.88}

 88%|████████▊ | 2729/3107 [5:50:50<53:44,  8.53s/it]

 88%|████████▊ | 2730/3107 [5:50:59<54:10,  8.62s/it]


 88%|████████▊ | 2732/3107 [5:51:12<46:18,  7.41s/it]
{'loss': 0.8767, 'grad_norm': 0.1760500697263794, 'learning_rate': 7.547339281564059e-06, 'epoch': 0.88}


 88%|████████▊ | 2734/3107 [5:51:32<55:05,  8.86s/it]

 88%|████████▊ | 2735/3107 [5:51:39<52:47,  8.51s/it]
{'loss': 1.0314, 'grad_norm': 0.17835851066996933, 'learning_rate': 7.4285767856509095e-06, 'epoch': 0.88}


 88%|████████▊ | 2737/3107 [5:51:54<48:21,  7.84s/it]
{'loss': 0.9513, 'grad_norm': 0.17721203661099028, 'learning_rate': 7.3499049256300355e-06, 'epoch': 0.88}

 88%|████████▊ | 2738/3107 [5:52:02<49:03,  7.98s/it]


 88%|████████▊ | 2740/3107 [5:52:20<53:00,  8.67s/it]
{'loss': 0.9343, 'grad_norm': 0.17896713056121263, 'learning_rate': 7.232652696784703e-06, 'epoch': 0.88}

 88%|████████▊ | 2741/3107 [5:52:26<49:15,  8.08s/it]

 88%|████████▊ | 2742/3107 [5:52:35<49:37,  8.16s/it]

 88%|████████▊ | 2743/3107 [5:52:43<49:40,  8.19s/it]

 88%|████████▊ | 2744/3107 [5:52:54<54:45,  9.05s/it]

 88%|████████▊ | 2745/3107 [5:53:01<50:21,  8.35s/it]


 88%|████████▊ | 2747/3107 [5:53:16<47:51,  7.98s/it]

 88%|████████▊ | 2748/3107 [5:53:24<48:03,  8.03s/it]
{'loss': 1.0679, 'grad_norm': 0.19104091555630212, 'learning_rate': 6.924420782486684e-06, 'epoch': 0.88}


 89%|████████▊ | 2750/3107 [5:53:42<49:07,  8.26s/it]

 89%|████████▊ | 2751/3107 [5:53:48<45:02,  7.59s/it]
{'loss': 0.873, 'grad_norm': 0.18531241316659589, 'learning_rate': 6.810502300978539e-06, 'epoch': 0.89}

 89%|████████▊ | 2752/3107 [5:53:55<44:09,  7.46s/it]

 89%|████████▊ | 2753/3107 [5:54:04<47:01,  7.97s/it]

 89%|████████▊ | 2754/3107 [5:54:11<44:28,  7.56s/it]

 89%|████████▊ | 2755/3107 [5:54:17<41:13,  7.03s/it]


 89%|████████▊ | 2757/3107 [5:54:31<42:40,  7.32s/it]

 89%|████████▉ | 2758/3107 [5:54:38<40:58,  7.04s/it]
{'loss': 1.0348, 'grad_norm': 0.18613365152341327, 'learning_rate': 6.548240399506422e-06, 'epoch': 0.89}

 89%|████████▉ | 2759/3107 [5:54:44<39:55,  6.88s/it]

 89%|████████▉ | 2760/3107 [5:54:51<39:35,  6.85s/it]

 89%|████████▉ | 2761/3107 [5:55:01<45:27,  7.88s/it]


 89%|████████▉ | 2763/3107 [5:55:13<39:58,  6.97s/it]
{'loss': 0.981, 'grad_norm': 0.18405973826575245, 'learning_rate': 6.363957618910454e-06, 'epoch': 0.89}

 89%|████████▉ | 2764/3107 [5:55:23<43:55,  7.68s/it]


 89%|████████▉ | 2766/3107 [5:55:38<44:17,  7.79s/it]
{'loss': 0.9725, 'grad_norm': 0.18132545277299061, 'learning_rate': 6.254609220973351e-06, 'epoch': 0.89}


 89%|████████▉ | 2768/3107 [5:55:56<47:48,  8.46s/it]

 89%|████████▉ | 2769/3107 [5:56:07<52:21,  9.29s/it]
{'loss': 0.9396, 'grad_norm': 0.17880306669434148, 'learning_rate': 6.146178085372156e-06, 'epoch': 0.89}


 89%|████████▉ | 2771/3107 [5:56:20<43:18,  7.73s/it]
{'loss': 0.9982, 'grad_norm': 0.1857900336954387, 'learning_rate': 6.074400776123557e-06, 'epoch': 0.89}


 89%|████████▉ | 2773/3107 [5:56:42<52:10,  9.37s/it]
{'loss': 1.0064, 'grad_norm': 0.16424471403198868, 'learning_rate': 6.00303192288475e-06, 'epoch': 0.89}

 89%|████████▉ | 2774/3107 [5:56:51<51:01,  9.19s/it]


 89%|████████▉ | 2776/3107 [5:57:08<49:17,  8.94s/it]

 89%|████████▉ | 2777/3107 [5:57:16<48:07,  8.75s/it]
{'loss': 1.0173, 'grad_norm': 0.17600589952381876, 'learning_rate': 5.861520824111244e-06, 'epoch': 0.89}

 89%|████████▉ | 2778/3107 [5:57:23<45:05,  8.22s/it]

 89%|████████▉ | 2779/3107 [5:57:29<40:15,  7.37s/it]


 90%|████████▉ | 2781/3107 [5:57:48<46:19,  8.53s/it]
{'loss': 0.8262, 'grad_norm': 0.16438034987032896, 'learning_rate': 5.721647250616801e-06, 'epoch': 0.9}


 90%|████████▉ | 2783/3107 [5:58:08<50:14,  9.30s/it]
{'loss': 0.8424, 'grad_norm': 0.16849067084304417, 'learning_rate': 5.652325297300775e-06, 'epoch': 0.9}

 90%|████████▉ | 2784/3107 [5:58:15<47:35,  8.84s/it]

 90%|████████▉ | 2785/3107 [5:58:23<46:03,  8.58s/it]


 90%|████████▉ | 2787/3107 [5:58:38<42:37,  7.99s/it]

 90%|████████▉ | 2788/3107 [5:58:46<41:47,  7.86s/it]
{'loss': 0.9934, 'grad_norm': 0.18310690040575736, 'learning_rate': 5.480816094367746e-06, 'epoch': 0.9}

 90%|████████▉ | 2789/3107 [5:58:53<40:59,  7.73s/it]

 90%|████████▉ | 2790/3107 [5:59:01<41:38,  7.88s/it]


 90%|████████▉ | 2792/3107 [5:59:14<37:39,  7.17s/it]

 90%|████████▉ | 2793/3107 [5:59:22<38:05,  7.28s/it]
{'loss': 0.9213, 'grad_norm': 0.17529697678799266, 'learning_rate': 5.3118758699384316e-06, 'epoch': 0.9}

 90%|████████▉ | 2794/3107 [5:59:27<35:30,  6.81s/it]


 90%|████████▉ | 2796/3107 [5:59:44<39:59,  7.72s/it]
{'loss': 0.9018, 'grad_norm': 0.15803074499484723, 'learning_rate': 5.211746758018521e-06, 'epoch': 0.9}

 90%|█████████ | 2797/3107 [5:59:55<43:52,  8.49s/it]


 90%|█████████ | 2799/3107 [6:00:10<41:23,  8.06s/it]
{'loss': 0.853, 'grad_norm': 0.17874193954626916, 'learning_rate': 5.112545112439782e-06, 'epoch': 0.9}

 90%|█████████ | 2800/3107 [6:00:17<39:02,  7.63s/it]

 90%|█████████ | 2801/3107 [6:00:26<40:25,  7.93s/it]


 90%|█████████ | 2803/3107 [6:00:44<42:29,  8.39s/it]

 90%|█████████ | 2804/3107 [6:00:56<47:55,  9.49s/it]
{'loss': 0.9232, 'grad_norm': 0.16678074656547706, 'learning_rate': 4.949272705009411e-06, 'epoch': 0.9}

 90%|█████████ | 2805/3107 [6:01:02<42:00,  8.35s/it]


 90%|█████████ | 2807/3107 [6:01:18<40:58,  8.20s/it]

 90%|█████████ | 2808/3107 [6:01:24<37:34,  7.54s/it]

 90%|█████████ | 2809/3107 [6:01:33<38:52,  7.83s/it]

 90%|█████████ | 2810/3107 [6:01:40<37:48,  7.64s/it]

 90%|█████████ | 2811/3107 [6:01:48<38:41,  7.84s/it]

 91%|█████████ | 2812/3107 [6:01:56<38:19,  7.80s/it]

 91%|█████████ | 2813/3107 [6:02:02<36:22,  7.42s/it]

 91%|█████████ | 2814/3107 [6:02:10<36:39,  7.51s/it]

 91%|█████████ | 2815/3107 [6:02:18<37:56,  7.80s/it]

 91%|█████████ | 2816/3107 [6:02:24<34:49,  7.18s/it]

 91%|█████████ | 2817/3107 [6:02:30<32:43,  6.77s/it]

 91%|█████████ | 2818/3107 [6:02:36<31:25,  6.52s/it]

 91%|█████████ | 2819/3107 [6:02:42<30:54,  6.44s/it]
{'loss': 0.9931, 'grad_norm': 0.18724354461506917, 'learning_rate': 4.474973435462526e-06, 'epoch': 0.91}


 91%|█████████ | 2821/3107 [6:02:58<34:29,  7.24s/it]
{'loss': 0.8703, 'grad_norm': 0.17699963940674518, 'learning_rate': 4.413496530312633e-06, 'epoch': 0.91}


 91%|█████████ | 2823/3107 [6:03:16<37:56,  8.01s/it]
{'loss': 1.1178, 'grad_norm': 0.1844425916069188, 'learning_rate': 4.352435303977909e-06, 'epoch': 0.91}


 91%|█████████ | 2825/3107 [6:03:28<33:13,  7.07s/it]

 91%|█████████ | 2826/3107 [6:03:34<31:12,  6.66s/it]
{'loss': 0.8755, 'grad_norm': 0.18768158579716496, 'learning_rate': 4.261623442624274e-06, 'epoch': 0.91}

 91%|█████████ | 2827/3107 [6:03:45<37:18,  7.99s/it]


 91%|█████████ | 2829/3107 [6:03:58<34:17,  7.40s/it]
{'loss': 1.0388, 'grad_norm': 0.18329238479601445, 'learning_rate': 4.171748344200399e-06, 'epoch': 0.91}


 91%|█████████ | 2831/3107 [6:04:12<32:51,  7.14s/it]

 91%|█████████ | 2832/3107 [6:04:18<31:46,  6.93s/it]
{'loss': 0.9641, 'grad_norm': 0.17496262344355565, 'learning_rate': 4.082810888099353e-06, 'epoch': 0.91}

 91%|█████████ | 2833/3107 [6:04:25<30:53,  6.76s/it]

 91%|█████████ | 2834/3107 [6:04:31<29:59,  6.59s/it]

 91%|█████████ | 2835/3107 [6:04:37<28:58,  6.39s/it]

 91%|█████████▏| 2836/3107 [6:04:46<31:58,  7.08s/it]

 91%|█████████▏| 2837/3107 [6:04:53<32:01,  7.11s/it]

 91%|█████████▏| 2838/3107 [6:04:59<31:08,  6.95s/it]

 91%|█████████▏| 2839/3107 [6:05:05<29:02,  6.50s/it]

 91%|█████████▏| 2840/3107 [6:05:13<31:36,  7.10s/it]

 91%|█████████▏| 2841/3107 [6:05:20<30:20,  6.84s/it]


 92%|█████████▏| 2843/3107 [6:05:41<38:06,  8.66s/it]

 92%|█████████▏| 2844/3107 [6:05:51<39:33,  9.02s/it]
{'loss': 0.9254, 'grad_norm': 0.17077536600706125, 'learning_rate': 3.7364547534991745e-06, 'epoch': 0.92}

 92%|█████████▏| 2845/3107 [6:05:59<38:38,  8.85s/it]

 92%|█████████▏| 2846/3107 [6:06:06<35:19,  8.12s/it]


 92%|█████████▏| 2848/3107 [6:06:27<40:28,  9.38s/it]
{'loss': 0.9093, 'grad_norm': 0.18109660835797534, 'learning_rate': 3.6243490410087012e-06, 'epoch': 0.92}

 92%|█████████▏| 2849/3107 [6:06:35<39:18,  9.14s/it]

 92%|█████████▏| 2850/3107 [6:06:44<38:30,  8.99s/it]

 92%|█████████▏| 2851/3107 [6:06:51<35:59,  8.44s/it]

 92%|█████████▏| 2852/3107 [6:06:58<33:50,  7.96s/it]

 92%|█████████▏| 2853/3107 [6:07:09<38:08,  9.01s/it]

 92%|█████████▏| 2854/3107 [6:07:15<34:15,  8.12s/it]

 92%|█████████▏| 2855/3107 [6:07:21<31:00,  7.38s/it]


 92%|█████████▏| 2857/3107 [6:07:41<36:14,  8.70s/it]

 92%|█████████▏| 2858/3107 [6:07:49<35:01,  8.44s/it]

 92%|█████████▏| 2859/3107 [6:07:56<34:05,  8.25s/it]
{'loss': 0.9834, 'grad_norm': 0.17378716162048527, 'learning_rate': 3.3247082395434835e-06, 'epoch': 0.92}

 92%|█████████▏| 2860/3107 [6:08:05<34:49,  8.46s/it]

 92%|█████████▏| 2861/3107 [6:08:16<37:00,  9.03s/it]

 92%|█████████▏| 2862/3107 [6:08:21<32:47,  8.03s/it]

 92%|█████████▏| 2863/3107 [6:08:28<30:30,  7.50s/it]


 92%|█████████▏| 2865/3107 [6:08:40<27:51,  6.91s/it]

 92%|█████████▏| 2866/3107 [6:08:50<31:12,  7.77s/it]

 92%|█████████▏| 2867/3107 [6:08:57<29:32,  7.38s/it]

 92%|█████████▏| 2868/3107 [6:09:02<27:32,  6.91s/it]
{'loss': 0.843, 'grad_norm': 0.181483413150064, 'learning_rate': 3.089005066859629e-06, 'epoch': 0.92}

 92%|█████████▏| 2869/3107 [6:09:11<29:50,  7.52s/it]


 92%|█████████▏| 2871/3107 [6:09:32<35:45,  9.09s/it]
{'loss': 0.936, 'grad_norm': 0.1690484581768291, 'learning_rate': 3.012332799985429e-06, 'epoch': 0.92}

 92%|█████████▏| 2872/3107 [6:09:39<33:09,  8.47s/it]


 93%|█████████▎| 2874/3107 [6:09:54<30:37,  7.89s/it]
{'loss': 0.9852, 'grad_norm': 0.18820921312797773, 'learning_rate': 2.9366095198660292e-06, 'epoch': 0.93}


 93%|█████████▎| 2876/3107 [6:10:12<31:51,  8.28s/it]
{'loss': 0.8358, 'grad_norm': 0.18161316597263016, 'learning_rate': 2.8866549142537723e-06, 'epoch': 0.93}

 93%|█████████▎| 2877/3107 [6:10:18<28:54,  7.54s/it]

 93%|█████████▎| 2878/3107 [6:10:25<28:21,  7.43s/it]

 93%|█████████▎| 2879/3107 [6:10:32<27:24,  7.21s/it]


 93%|█████████▎| 2881/3107 [6:10:47<27:54,  7.41s/it]

 93%|█████████▎| 2882/3107 [6:10:53<26:27,  7.06s/it]
{'loss': 1.1293, 'grad_norm': 0.17640916480391278, 'learning_rate': 2.7393258689065506e-06, 'epoch': 0.93}


 93%|█████████▎| 2884/3107 [6:11:07<25:47,  6.94s/it]

 93%|█████████▎| 2885/3107 [6:11:15<26:51,  7.26s/it]

 93%|█████████▎| 2886/3107 [6:11:24<29:36,  8.04s/it]

 93%|█████████▎| 2887/3107 [6:11:33<30:13,  8.24s/it]

 93%|█████████▎| 2888/3107 [6:11:39<27:40,  7.58s/it]

 93%|█████████▎| 2889/3107 [6:11:49<29:55,  8.23s/it]
{'loss': 0.7743, 'grad_norm': 0.17392239900001544, 'learning_rate': 2.5722535275000014e-06, 'epoch': 0.93}


 93%|█████████▎| 2891/3107 [6:12:07<31:18,  8.70s/it]
{'loss': 0.9475, 'grad_norm': 0.17202448114529803, 'learning_rate': 2.5254714798011113e-06, 'epoch': 0.93}


 93%|█████████▎| 2893/3107 [6:12:21<28:26,  7.97s/it]

 93%|█████████▎| 2894/3107 [6:12:29<27:53,  7.86s/it]
{'loss': 0.9148, 'grad_norm': 0.16000722024027692, 'learning_rate': 2.456093263771242e-06, 'epoch': 0.93}

 93%|█████████▎| 2895/3107 [6:12:36<27:38,  7.82s/it]

 93%|█████████▎| 2896/3107 [6:12:44<26:59,  7.68s/it]


 93%|█████████▎| 2898/3107 [6:12:59<26:53,  7.72s/it]

 93%|█████████▎| 2899/3107 [6:13:05<25:05,  7.24s/it]
{'loss': 0.939, 'grad_norm': 0.1900220325421423, 'learning_rate': 2.3425841888193744e-06, 'epoch': 0.93}

 93%|█████████▎| 2900/3107 [6:13:14<26:33,  7.70s/it]

 93%|█████████▎| 2901/3107 [6:13:21<26:14,  7.64s/it]

 93%|█████████▎| 2902/3107 [6:13:28<24:48,  7.26s/it]


 93%|█████████▎| 2904/3107 [6:13:41<23:56,  7.08s/it]

 93%|█████████▎| 2905/3107 [6:13:47<22:16,  6.62s/it]
{'loss': 0.8756, 'grad_norm': 0.19059128157350158, 'learning_rate': 2.2098772068240516e-06, 'epoch': 0.93}


 94%|█████████▎| 2907/3107 [6:14:01<22:30,  6.75s/it]

 94%|█████████▎| 2908/3107 [6:14:07<21:48,  6.57s/it]
{'loss': 0.9703, 'grad_norm': 0.180772521512279, 'learning_rate': 2.144958651251716e-06, 'epoch': 0.94}

 94%|█████████▎| 2909/3107 [6:14:14<22:05,  6.69s/it]


 94%|█████████▎| 2911/3107 [6:14:29<23:55,  7.33s/it]

 94%|█████████▎| 2912/3107 [6:14:37<24:20,  7.49s/it]

 94%|█████████▍| 2913/3107 [6:14:45<24:45,  7.66s/it]
{'loss': 0.9341, 'grad_norm': 0.16734398362990108, 'learning_rate': 2.0388890873483613e-06, 'epoch': 0.94}

 94%|█████████▍| 2914/3107 [6:14:52<24:15,  7.54s/it]

 94%|█████████▍| 2915/3107 [6:14:58<22:19,  6.98s/it]


 94%|█████████▍| 2917/3107 [6:15:11<21:28,  6.78s/it]
{'loss': 0.9544, 'grad_norm': 0.18206310587450206, 'learning_rate': 1.955950320546851e-06, 'epoch': 0.94}

 94%|█████████▍| 2918/3107 [6:15:17<20:10,  6.40s/it]


 94%|█████████▍| 2920/3107 [6:15:35<24:36,  7.90s/it]

 94%|█████████▍| 2921/3107 [6:15:43<24:12,  7.81s/it]

 94%|█████████▍| 2922/3107 [6:15:49<22:29,  7.30s/it]
{'loss': 0.9021, 'grad_norm': 0.1700616737337424, 'learning_rate': 1.8546753343874347e-06, 'epoch': 0.94}

 94%|█████████▍| 2923/3107 [6:15:56<21:52,  7.13s/it]

 94%|█████████▍| 2924/3107 [6:16:02<21:11,  6.95s/it]

 94%|█████████▍| 2925/3107 [6:16:10<21:24,  7.06s/it]

 94%|█████████▍| 2926/3107 [6:16:18<22:46,  7.55s/it]

 94%|█████████▍| 2927/3107 [6:16:24<21:23,  7.13s/it]

 94%|█████████▍| 2928/3107 [6:16:32<21:17,  7.14s/it]


 94%|█████████▍| 2930/3107 [6:16:47<22:01,  7.47s/it]
{'loss': 0.9382, 'grad_norm': 0.17269231738373747, 'learning_rate': 1.6981849472277212e-06, 'epoch': 0.94}

 94%|█████████▍| 2931/3107 [6:16:56<23:31,  8.02s/it]

 94%|█████████▍| 2932/3107 [6:17:03<22:05,  7.57s/it]


 94%|█████████▍| 2934/3107 [6:17:15<19:47,  6.87s/it]
{'loss': 0.9365, 'grad_norm': 0.18211056196266467, 'learning_rate': 1.6225039994061552e-06, 'epoch': 0.94}

 94%|█████████▍| 2935/3107 [6:17:22<20:02,  6.99s/it]

 94%|█████████▍| 2936/3107 [6:17:28<19:09,  6.73s/it]

 95%|█████████▍| 2937/3107 [6:17:34<18:19,  6.47s/it]

 95%|█████████▍| 2938/3107 [6:17:45<21:35,  7.66s/it]

 95%|█████████▍| 2939/3107 [6:17:55<23:28,  8.39s/it]


 95%|█████████▍| 2941/3107 [6:18:11<22:43,  8.22s/it]
{'loss': 0.9917, 'grad_norm': 0.17253432448420708, 'learning_rate': 1.494180840931747e-06, 'epoch': 0.95}

 95%|█████████▍| 2942/3107 [6:18:18<21:30,  7.82s/it]


 95%|█████████▍| 2944/3107 [6:18:31<19:25,  7.15s/it]

 95%|█████████▍| 2945/3107 [6:18:38<18:37,  6.90s/it]

 95%|█████████▍| 2946/3107 [6:18:45<19:21,  7.22s/it]
{'loss': 0.9119, 'grad_norm': 0.17644097282737756, 'learning_rate': 1.4057338471151427e-06, 'epoch': 0.95}

 95%|█████████▍| 2947/3107 [6:18:54<20:14,  7.59s/it]


 95%|█████████▍| 2949/3107 [6:19:09<19:44,  7.50s/it]

 95%|█████████▍| 2950/3107 [6:19:15<18:25,  7.04s/it]
{'loss': 1.0642, 'grad_norm': 0.17664899750359658, 'learning_rate': 1.336905549729095e-06, 'epoch': 0.95}

 95%|█████████▍| 2951/3107 [6:19:23<18:42,  7.20s/it]

 95%|█████████▌| 2952/3107 [6:19:28<17:26,  6.75s/it]


 95%|█████████▌| 2954/3107 [6:19:41<16:51,  6.61s/it]

 95%|█████████▌| 2955/3107 [6:19:47<16:20,  6.45s/it]
{'loss': 0.8742, 'grad_norm': 0.17273079620325593, 'learning_rate': 1.2532837643310814e-06, 'epoch': 0.95}


 95%|█████████▌| 2957/3107 [6:20:02<17:09,  6.86s/it]
{'loss': 0.9019, 'grad_norm': 0.16852440188391815, 'learning_rate': 1.2205864118762011e-06, 'epoch': 0.95}

 95%|█████████▌| 2958/3107 [6:20:08<16:33,  6.67s/it]


 95%|█████████▌| 2960/3107 [6:20:26<19:43,  8.05s/it]

 95%|█████████▌| 2961/3107 [6:20:31<17:56,  7.37s/it]
{'loss': 0.8466, 'grad_norm': 0.1678479463713732, 'learning_rate': 1.1564805389392863e-06, 'epoch': 0.95}


 95%|█████████▌| 2963/3107 [6:20:51<20:54,  8.71s/it]
{'loss': 0.8257, 'grad_norm': 0.17395941887411, 'learning_rate': 1.1250722972356742e-06, 'epoch': 0.95}

 95%|█████████▌| 2964/3107 [6:20:58<19:22,  8.13s/it]


 95%|█████████▌| 2966/3107 [6:21:13<18:23,  7.82s/it]
{'loss': 1.0042, 'grad_norm': 0.1656194617622369, 'learning_rate': 1.0787661879403277e-06, 'epoch': 0.95}

 95%|█████████▌| 2967/3107 [6:21:25<20:40,  8.86s/it]

 96%|█████████▌| 2968/3107 [6:21:33<20:13,  8.73s/it]

 96%|█████████▌| 2969/3107 [6:21:40<18:53,  8.21s/it]

 96%|█████████▌| 2970/3107 [6:21:52<21:30,  9.42s/it]

 96%|█████████▌| 2971/3107 [6:21:59<19:34,  8.64s/it]

 96%|█████████▌| 2972/3107 [6:22:09<20:07,  8.94s/it]

 96%|█████████▌| 2973/3107 [6:22:15<17:55,  8.03s/it]

 96%|█████████▌| 2974/3107 [6:22:24<19:02,  8.59s/it]


 96%|█████████▌| 2976/3107 [6:22:41<18:28,  8.46s/it]
{'loss': 0.7773, 'grad_norm': 0.17532596278061865, 'learning_rate': 9.314053963669245e-07, 'epoch': 0.96}

 96%|█████████▌| 2977/3107 [6:22:49<17:52,  8.25s/it]


 96%|█████████▌| 2979/3107 [6:23:02<15:27,  7.25s/it]
{'loss': 0.8922, 'grad_norm': 0.1893269871236835, 'learning_rate': 8.892966968779615e-07, 'epoch': 0.96}

 96%|█████████▌| 2980/3107 [6:23:08<14:53,  7.04s/it]

 96%|█████████▌| 2981/3107 [6:23:15<14:24,  6.86s/it]

 96%|█████████▌| 2982/3107 [6:23:22<14:41,  7.06s/it]


 96%|█████████▌| 2984/3107 [6:23:34<12:56,  6.32s/it]

 96%|█████████▌| 2985/3107 [6:23:39<12:35,  6.19s/it]
{'loss': 0.9789, 'grad_norm': 0.17402643395611717, 'learning_rate': 8.079889799507023e-07, 'epoch': 0.96}


 96%|█████████▌| 2987/3107 [6:23:55<14:27,  7.23s/it]

 96%|█████████▌| 2988/3107 [6:24:04<15:06,  7.62s/it]
{'loss': 0.9648, 'grad_norm': 0.17860798975617936, 'learning_rate': 7.687907580769427e-07, 'epoch': 0.96}


 96%|█████████▌| 2990/3107 [6:24:19<14:44,  7.56s/it]

 96%|█████████▋| 2991/3107 [6:24:26<13:59,  7.24s/it]
{'loss': 0.8111, 'grad_norm': 0.1802277673448718, 'learning_rate': 7.305634751477941e-07, 'epoch': 0.96}


 96%|█████████▋| 2993/3107 [6:24:38<12:24,  6.54s/it]

 96%|█████████▋| 2994/3107 [6:24:44<11:58,  6.36s/it]
{'loss': 0.9506, 'grad_norm': 0.17567183253877144, 'learning_rate': 6.933075052024562e-07, 'epoch': 0.96}


 96%|█████████▋| 2996/3107 [6:25:00<13:31,  7.31s/it]
{'loss': 0.743, 'grad_norm': 0.1669542241076161, 'learning_rate': 6.690099906384916e-07, 'epoch': 0.96}

 96%|█████████▋| 2997/3107 [6:25:09<14:11,  7.74s/it]


 97%|█████████▋| 2999/3107 [6:25:22<12:48,  7.12s/it]

 97%|█████████▋| 3000/3107 [6:25:30<13:25,  7.53s/it]
 97%|█████████▋| 3000/3107 [6:25:30<13:25,  7.53s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 97%|█████████▋| 3001/3107 [6:25:52<20:55, 11.84s/it]
{'loss': 1.0135, 'grad_norm': 0.1762026070463695, 'learning_rate': 6.101562668289873e-07, 'epoch': 0.97}

 97%|█████████▋| 3002/3107 [6:26:01<19:08, 10.94s/it]


 97%|█████████▋| 3004/3107 [6:26:16<16:13,  9.45s/it]
{'loss': 0.9066, 'grad_norm': 0.17157963604987894, 'learning_rate': 5.761405861713142e-07, 'epoch': 0.97}


 97%|█████████▋| 3006/3107 [6:26:30<13:48,  8.20s/it]
{'loss': 0.8465, 'grad_norm': 0.16908730216240328, 'learning_rate': 5.540039033305466e-07, 'epoch': 0.97}

 97%|█████████▋| 3007/3107 [6:26:37<13:08,  7.88s/it]

 97%|█████████▋| 3008/3107 [6:26:45<13:02,  7.90s/it]

 97%|█████████▋| 3009/3107 [6:26:51<11:49,  7.24s/it]

 97%|█████████▋| 3010/3107 [6:26:57<11:02,  6.83s/it]


 97%|█████████▋| 3012/3107 [6:27:10<10:42,  6.76s/it]

 97%|█████████▋| 3013/3107 [6:27:16<10:03,  6.42s/it]
{'loss': 0.9062, 'grad_norm': 0.1727556056138344, 'learning_rate': 4.799317716342677e-07, 'epoch': 0.97}


 97%|█████████▋| 3015/3107 [6:27:30<10:15,  6.69s/it]
{'loss': 0.9795, 'grad_norm': 0.1843704032407439, 'learning_rate': 4.5974190171735874e-07, 'epoch': 0.97}

 97%|█████████▋| 3016/3107 [6:27:40<11:33,  7.62s/it]

 97%|█████████▋| 3017/3107 [6:27:49<12:01,  8.02s/it]

 97%|█████████▋| 3018/3107 [6:27:55<11:06,  7.49s/it]

 97%|█████████▋| 3019/3107 [6:28:03<11:02,  7.53s/it]

 97%|█████████▋| 3020/3107 [6:28:14<12:24,  8.56s/it]

 97%|█████████▋| 3021/3107 [6:28:25<13:19,  9.29s/it]


 97%|█████████▋| 3023/3107 [6:28:38<11:13,  8.02s/it]
{'loss': 0.7943, 'grad_norm': 0.17589772505226633, 'learning_rate': 3.8331199763008695e-07, 'epoch': 0.97}

 97%|█████████▋| 3024/3107 [6:28:45<10:40,  7.72s/it]

 97%|█████████▋| 3025/3107 [6:28:55<11:23,  8.34s/it]

 97%|█████████▋| 3026/3107 [6:29:01<10:31,  7.80s/it]

 97%|█████████▋| 3027/3107 [6:29:07<09:27,  7.10s/it]

 97%|█████████▋| 3028/3107 [6:29:17<10:23,  7.90s/it]


 98%|█████████▊| 3030/3107 [6:29:34<10:44,  8.37s/it]
{'loss': 0.9818, 'grad_norm': 0.1743560330591629, 'learning_rate': 3.221214457484245e-07, 'epoch': 0.98}

 98%|█████████▊| 3031/3107 [6:29:40<09:30,  7.50s/it]

 98%|█████████▊| 3032/3107 [6:29:46<08:53,  7.11s/it]

 98%|█████████▊| 3033/3107 [6:29:52<08:22,  6.79s/it]

 98%|█████████▊| 3034/3107 [6:29:57<07:44,  6.36s/it]

 98%|█████████▊| 3035/3107 [6:30:07<08:57,  7.47s/it]

 98%|█████████▊| 3036/3107 [6:30:13<08:15,  6.98s/it]


 98%|█████████▊| 3038/3107 [6:30:28<08:14,  7.17s/it]
{'loss': 0.9816, 'grad_norm': 0.1740974828354181, 'learning_rate': 2.586916028770259e-07, 'epoch': 0.98}


 98%|█████████▊| 3040/3107 [6:30:46<09:08,  8.18s/it]
{'loss': 0.8015, 'grad_norm': 0.1623879849497158, 'learning_rate': 2.43918341706928e-07, 'epoch': 0.98}

 98%|█████████▊| 3041/3107 [6:30:53<08:39,  7.87s/it]

 98%|█████████▊| 3042/3107 [6:31:03<09:17,  8.58s/it]

 98%|█████████▊| 3043/3107 [6:31:09<08:09,  7.66s/it]


 98%|█████████▊| 3045/3107 [6:31:22<07:24,  7.17s/it]
{'loss': 0.8949, 'grad_norm': 0.1822653433888192, 'learning_rate': 2.08883248402314e-07, 'epoch': 0.98}

 98%|█████████▊| 3046/3107 [6:31:31<07:36,  7.48s/it]

 98%|█████████▊| 3047/3107 [6:31:40<07:58,  7.98s/it]

 98%|█████████▊| 3048/3107 [6:31:45<07:08,  7.26s/it]

 98%|█████████▊| 3049/3107 [6:31:52<06:51,  7.10s/it]


 98%|█████████▊| 3051/3107 [6:32:06<06:38,  7.11s/it]
{'loss': 0.9128, 'grad_norm': 0.17258548085617617, 'learning_rate': 1.704214070708532e-07, 'epoch': 0.98}


 98%|█████████▊| 3053/3107 [6:32:22<06:42,  7.45s/it]
{'loss': 0.8936, 'grad_norm': 0.17078013048593188, 'learning_rate': 1.5846898235422068e-07, 'epoch': 0.98}

 98%|█████████▊| 3054/3107 [6:32:31<06:48,  7.72s/it]


 98%|█████████▊| 3056/3107 [6:32:50<07:17,  8.57s/it]
{'loss': 0.9529, 'grad_norm': 0.17663329897254718, 'learning_rate': 1.4135445363046008e-07, 'epoch': 0.98}

 98%|█████████▊| 3057/3107 [6:33:01<07:44,  9.30s/it]

 98%|█████████▊| 3058/3107 [6:33:07<06:45,  8.27s/it]

 98%|█████████▊| 3059/3107 [6:33:14<06:12,  7.76s/it]


 99%|█████████▊| 3061/3107 [6:33:30<06:19,  8.25s/it]
{'loss': 1.008, 'grad_norm': 0.166469130867636, 'learning_rate': 1.150016022719691e-07, 'epoch': 0.99}

 99%|█████████▊| 3062/3107 [6:33:38<05:56,  7.93s/it]

 99%|█████████▊| 3063/3107 [6:33:44<05:24,  7.38s/it]

 99%|█████████▊| 3064/3107 [6:33:49<04:53,  6.83s/it]

 99%|█████████▊| 3065/3107 [6:33:56<04:47,  6.84s/it]


 99%|█████████▊| 3067/3107 [6:34:09<04:19,  6.48s/it]
{'loss': 0.9838, 'grad_norm': 0.17824315570569654, 'learning_rate': 8.696179868354915e-08, 'epoch': 0.99}

 99%|█████████▊| 3068/3107 [6:34:16<04:20,  6.68s/it]

 99%|█████████▉| 3069/3107 [6:34:23<04:18,  6.81s/it]

 99%|█████████▉| 3070/3107 [6:34:29<04:03,  6.57s/it]
[2024-05-29 01:46:04,160] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 3071/3107 [6:34:40<04:46,  7.96s/it]

 99%|█████████▉| 3072/3107 [6:34:47<04:26,  7.62s/it]

 99%|█████████▉| 3073/3107 [6:35:01<05:28,  9.68s/it]

 99%|█████████▉| 3074/3107 [6:35:07<04:41,  8.52s/it]

 99%|█████████▉| 3075/3107 [6:35:13<04:05,  7.66s/it]
[2024-05-29 01:46:48,439] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 3076/3107 [6:35:24<04:33,  8.81s/it]

 99%|█████████▉| 3077/3107 [6:35:31<04:09,  8.32s/it]

 99%|█████████▉| 3078/3107 [6:35:37<03:39,  7.56s/it]

 99%|█████████▉| 3079/3107 [6:35:46<03:37,  7.77s/it]

 99%|█████████▉| 3080/3107 [6:35:53<03:30,  7.78s/it]

 99%|█████████▉| 3081/3107 [6:36:01<03:20,  7.73s/it]

 99%|█████████▉| 3082/3107 [6:36:09<03:17,  7.88s/it]

 99%|█████████▉| 3083/3107 [6:36:17<03:11,  7.98s/it]

 99%|█████████▉| 3084/3107 [6:36:24<02:51,  7.47s/it]

 99%|█████████▉| 3085/3107 [6:36:31<02:43,  7.45s/it]

 99%|█████████▉| 3086/3107 [6:36:37<02:28,  7.09s/it]

 99%|█████████▉| 3087/3107 [6:36:45<02:27,  7.38s/it]

 99%|█████████▉| 3088/3107 [6:36:58<02:49,  8.90s/it]

 99%|█████████▉| 3089/3107 [6:37:06<02:36,  8.72s/it]

 99%|█████████▉| 3090/3107 [6:37:13<02:20,  8.26s/it]


100%|█████████▉| 3092/3107 [6:37:27<01:51,  7.41s/it]
{'loss': 0.9858, 'grad_norm': 0.19287754333498022, 'learning_rate': 1.2230526460066394e-08, 'epoch': 1.0}

100%|█████████▉| 3093/3107 [6:37:34<01:43,  7.41s/it]

100%|█████████▉| 3094/3107 [6:37:42<01:38,  7.57s/it]

100%|█████████▉| 3095/3107 [6:37:49<01:27,  7.27s/it]

100%|█████████▉| 3096/3107 [6:37:57<01:25,  7.78s/it]

100%|█████████▉| 3097/3107 [6:38:04<01:13,  7.39s/it]

100%|█████████▉| 3098/3107 [6:38:13<01:11,  7.98s/it]

100%|█████████▉| 3099/3107 [6:38:21<01:03,  7.91s/it]

100%|█████████▉| 3100/3107 [6:38:28<00:54,  7.75s/it]

100%|█████████▉| 3101/3107 [6:38:36<00:45,  7.57s/it]

100%|█████████▉| 3102/3107 [6:38:41<00:34,  6.93s/it]

100%|█████████▉| 3103/3107 [6:38:49<00:29,  7.38s/it]

100%|█████████▉| 3104/3107 [6:38:55<00:20,  6.85s/it]

100%|█████████▉| 3105/3107 [6:39:02<00:14,  7.01s/it]

100%|█████████▉| 3106/3107 [6:39:09<00:06,  6.93s/it]
{'loss': 0.8902, 'grad_norm': 0.18322677658747075, 'learning_rate': 0.0, 'epoch': 1.0}

100%|██████████| 3107/3107 [6:39:16<00:00,  7.71s/it]
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(