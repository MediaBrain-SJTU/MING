/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3106 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3106 [00:58<50:20:23, 58.37s/it]
{'loss': 1.5024, 'grad_norm': 3.565552084805972, 'learning_rate': 2.1276595744680853e-06, 'epoch': 0.0}

  0%|          | 2/3106 [01:10<26:49:20, 31.11s/it]

  0%|          | 3/3106 [01:21<19:05:31, 22.15s/it]

  0%|          | 4/3106 [01:35<16:18:10, 18.92s/it]


  0%|          | 6/3106 [02:04<13:52:03, 16.10s/it]
{'loss': 1.5725, 'grad_norm': 3.4742405093244404, 'learning_rate': 1.2765957446808511e-05, 'epoch': 0.0}

  0%|          | 7/3106 [02:17<12:59:20, 15.09s/it]

  0%|          | 8/3106 [02:32<12:54:03, 14.99s/it]

  0%|          | 9/3106 [02:47<13:00:24, 15.12s/it]

  0%|          | 10/3106 [03:05<13:45:43, 16.00s/it]

  0%|          | 11/3106 [03:19<13:06:49, 15.25s/it]


  0%|          | 13/3106 [03:47<12:39:16, 14.73s/it]

  0%|          | 14/3106 [04:00<12:20:09, 14.36s/it]
{'loss': 1.4802, 'grad_norm': 1.6559425904429155, 'learning_rate': 2.9787234042553192e-05, 'epoch': 0.0}


  1%|          | 16/3106 [04:26<11:57:27, 13.93s/it]
{'loss': 1.4449, 'grad_norm': 3.1328581193716025, 'learning_rate': 3.4042553191489365e-05, 'epoch': 0.01}

  1%|          | 17/3106 [04:39<11:35:09, 13.50s/it]

  1%|          | 18/3106 [04:52<11:28:37, 13.38s/it]

  1%|          | 19/3106 [05:03<10:56:21, 12.76s/it]


  1%|          | 21/3106 [05:28<10:49:16, 12.63s/it]
{'loss': 1.3315, 'grad_norm': 3.5828109468279057, 'learning_rate': 4.468085106382979e-05, 'epoch': 0.01}


  1%|          | 23/3106 [05:56<11:29:59, 13.43s/it]

  1%|          | 24/3106 [06:13<12:15:30, 14.32s/it]
{'loss': 1.3059, 'grad_norm': 1.6614323909832698, 'learning_rate': 5.1063829787234044e-05, 'epoch': 0.01}

  1%|          | 25/3106 [06:24<11:29:08, 13.42s/it]


  1%|          | 27/3106 [06:49<11:02:48, 12.92s/it]
{'loss': 1.3319, 'grad_norm': 1.1426634121187411, 'learning_rate': 5.744680851063831e-05, 'epoch': 0.01}


  1%|          | 29/3106 [07:21<12:31:17, 14.65s/it]
{'loss': 1.3431, 'grad_norm': 1.1022526527600123, 'learning_rate': 6.170212765957447e-05, 'epoch': 0.01}


  1%|          | 31/3106 [07:49<12:16:25, 14.37s/it]
{'loss': 1.2164, 'grad_norm': 0.8164470543281767, 'learning_rate': 6.595744680851063e-05, 'epoch': 0.01}

  1%|          | 32/3106 [08:03<12:21:44, 14.48s/it]

  1%|          | 33/3106 [08:17<12:12:10, 14.30s/it]


  1%|          | 35/3106 [08:47<12:19:38, 14.45s/it]

  1%|          | 36/3106 [09:03<12:48:21, 15.02s/it]

  1%|          | 37/3106 [09:17<12:32:56, 14.72s/it]
{'loss': 1.3005, 'grad_norm': 0.9992820048057838, 'learning_rate': 7.872340425531916e-05, 'epoch': 0.01}

  1%|          | 38/3106 [09:32<12:32:14, 14.71s/it]

  1%|▏         | 39/3106 [09:43<11:46:34, 13.82s/it]

  1%|▏         | 40/3106 [10:00<12:23:48, 14.56s/it]

  1%|▏         | 41/3106 [10:14<12:18:36, 14.46s/it]

  1%|▏         | 42/3106 [10:26<11:46:28, 13.83s/it]

  1%|▏         | 43/3106 [10:38<11:15:02, 13.22s/it]

  1%|▏         | 44/3106 [10:49<10:44:53, 12.64s/it]

  1%|▏         | 45/3106 [11:04<11:21:50, 13.37s/it]

  1%|▏         | 46/3106 [11:18<11:32:06, 13.57s/it]

  2%|▏         | 47/3106 [11:34<12:02:48, 14.18s/it]

  2%|▏         | 48/3106 [11:48<11:59:44, 14.12s/it]


  2%|▏         | 50/3106 [12:17<12:16:31, 14.46s/it]
{'loss': 1.1948, 'grad_norm': 0.37690337965663984, 'learning_rate': 0.00010638297872340425, 'epoch': 0.02}

  2%|▏         | 51/3106 [12:28<11:30:04, 13.55s/it]

  2%|▏         | 52/3106 [12:43<11:41:13, 13.78s/it]


  2%|▏         | 54/3106 [13:09<11:34:54, 13.66s/it]
{'loss': 1.1563, 'grad_norm': 0.33909773673676896, 'learning_rate': 0.00011489361702127661, 'epoch': 0.02}

  2%|▏         | 55/3106 [13:20<10:58:24, 12.95s/it]

  2%|▏         | 56/3106 [13:33<10:45:24, 12.70s/it]


  2%|▏         | 58/3106 [14:01<11:18:34, 13.36s/it]
{'loss': 1.0773, 'grad_norm': 0.350852595178957, 'learning_rate': 0.00012340425531914893, 'epoch': 0.02}

  2%|▏         | 59/3106 [14:14<11:09:40, 13.19s/it]

  2%|▏         | 60/3106 [14:28<11:30:43, 13.61s/it]


  2%|▏         | 62/3106 [14:53<11:01:16, 13.03s/it]
{'loss': 1.0638, 'grad_norm': 0.29243602757910325, 'learning_rate': 0.00013191489361702127, 'epoch': 0.02}

  2%|▏         | 63/3106 [15:06<11:03:48, 13.09s/it]

  2%|▏         | 64/3106 [15:26<12:36:08, 14.91s/it]

  2%|▏         | 65/3106 [15:37<11:35:43, 13.73s/it]

  2%|▏         | 66/3106 [15:48<10:55:04, 12.93s/it]

  2%|▏         | 67/3106 [16:02<11:10:24, 13.24s/it]

  2%|▏         | 68/3106 [16:16<11:33:11, 13.69s/it]

  2%|▏         | 69/3106 [16:28<11:04:40, 13.13s/it]

  2%|▏         | 70/3106 [16:40<10:46:57, 12.79s/it]

  2%|▏         | 71/3106 [16:52<10:35:19, 12.56s/it]

  2%|▏         | 72/3106 [17:09<11:36:12, 13.77s/it]

  2%|▏         | 73/3106 [17:26<12:31:05, 14.86s/it]

  2%|▏         | 74/3106 [17:46<13:41:38, 16.26s/it]

  2%|▏         | 75/3106 [18:00<13:06:12, 15.56s/it]

  2%|▏         | 76/3106 [18:12<12:17:00, 14.59s/it]


  3%|▎         | 78/3106 [18:36<11:01:56, 13.12s/it]
{'loss': 1.1383, 'grad_norm': 0.3725604271327518, 'learning_rate': 0.00016595744680851065, 'epoch': 0.03}

  3%|▎         | 79/3106 [18:48<10:47:53, 12.84s/it]


  3%|▎         | 81/3106 [19:16<11:17:36, 13.44s/it]

  3%|▎         | 82/3106 [19:27<10:54:03, 12.98s/it]
{'loss': 1.1313, 'grad_norm': 0.34059676252520715, 'learning_rate': 0.00017446808510638298, 'epoch': 0.03}


  3%|▎         | 84/3106 [19:54<10:56:29, 13.03s/it]
{'loss': 1.0646, 'grad_norm': 0.32736483062171134, 'learning_rate': 0.00017872340425531915, 'epoch': 0.03}


  3%|▎         | 86/3106 [20:22<11:26:01, 13.63s/it]
{'loss': 1.1716, 'grad_norm': 0.29933632065944343, 'learning_rate': 0.00018297872340425532, 'epoch': 0.03}

  3%|▎         | 87/3106 [20:34<11:10:28, 13.33s/it]

  3%|▎         | 88/3106 [20:48<11:17:50, 13.48s/it]

