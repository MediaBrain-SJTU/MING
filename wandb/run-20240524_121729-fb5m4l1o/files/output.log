/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/7689 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/7689 [00:08<17:58:32,  8.42s/it]
{'loss': 1.9573, 'grad_norm': 0.9483641528645097, 'learning_rate': 8.658008658008658e-07, 'epoch': 0.0}

  0%|          | 2/7689 [00:49<58:48:20, 27.54s/it]

  0%|          | 3/7689 [00:53<36:08:29, 16.93s/it]


  0%|          | 5/7689 [01:00<18:34:04,  8.70s/it]
{'loss': 1.9358, 'grad_norm': 0.9966629929565373, 'learning_rate': 4.329004329004329e-06, 'epoch': 0.0}

  0%|          | 6/7689 [01:06<16:05:26,  7.54s/it]


  0%|          | 8/7689 [01:15<12:41:20,  5.95s/it]
{'loss': 1.6943, 'grad_norm': 0.8861239429234375, 'learning_rate': 6.926406926406927e-06, 'epoch': 0.0}


  0%|          | 10/7689 [01:20<9:04:54,  4.26s/it]
{'loss': 2.0311, 'grad_norm': 1.0504249127882488, 'learning_rate': 8.658008658008657e-06, 'epoch': 0.0}


  0%|          | 12/7689 [01:29<8:54:38,  4.18s/it]
{'loss': 1.8681, 'grad_norm': 0.8819615314688959, 'learning_rate': 1.038961038961039e-05, 'epoch': 0.0}


  0%|          | 14/7689 [01:38<9:13:12,  4.32s/it]

  0%|          | 15/7689 [01:42<9:03:23,  4.25s/it]
{'loss': 1.6584, 'grad_norm': 0.8569559574071988, 'learning_rate': 1.2987012987012986e-05, 'epoch': 0.0}


  0%|          | 17/7689 [01:50<8:58:09,  4.21s/it]

  0%|          | 18/7689 [01:54<8:47:12,  4.12s/it]

  0%|          | 19/7689 [01:59<8:57:53,  4.21s/it]

  0%|          | 20/7689 [02:02<8:31:22,  4.00s/it]
{'loss': 1.7821, 'grad_norm': 0.8449217782292612, 'learning_rate': 1.7316017316017315e-05, 'epoch': 0.0}

  0%|          | 21/7689 [02:05<7:54:35,  3.71s/it]


  0%|          | 23/7689 [02:12<7:34:37,  3.56s/it]
{'loss': 1.6531, 'grad_norm': 0.7479883048197422, 'learning_rate': 1.9913419913419914e-05, 'epoch': 0.0}


  0%|          | 25/7689 [02:18<7:03:15,  3.31s/it]
{'loss': 1.78, 'grad_norm': 0.7817401223372394, 'learning_rate': 2.1645021645021645e-05, 'epoch': 0.0}

  0%|          | 26/7689 [02:21<6:52:15,  3.23s/it]


  0%|          | 28/7689 [02:30<7:54:42,  3.72s/it]
{'loss': 1.5561, 'grad_norm': 0.6019003328282225, 'learning_rate': 2.4242424242424244e-05, 'epoch': 0.0}


  0%|          | 30/7689 [02:37<7:23:32,  3.47s/it]
{'loss': 1.7448, 'grad_norm': 0.5809090733740394, 'learning_rate': 2.5974025974025972e-05, 'epoch': 0.0}


  0%|          | 32/7689 [02:43<6:53:36,  3.24s/it]
{'loss': 1.4886, 'grad_norm': 0.3473092704537025, 'learning_rate': 2.7705627705627707e-05, 'epoch': 0.0}

  0%|          | 33/7689 [02:45<6:34:56,  3.10s/it]

  0%|          | 34/7689 [02:51<8:03:01,  3.79s/it]

  0%|          | 35/7689 [02:54<7:28:05,  3.51s/it]


  0%|          | 37/7689 [03:01<7:16:25,  3.42s/it]
{'loss': 1.7094, 'grad_norm': 0.375568463675453, 'learning_rate': 3.2034632034632034e-05, 'epoch': 0.0}

  0%|          | 38/7689 [03:06<8:21:47,  3.94s/it]

  1%|          | 39/7689 [03:10<8:31:17,  4.01s/it]


  1%|          | 41/7689 [03:17<7:37:32,  3.59s/it]
{'loss': 1.5878, 'grad_norm': 0.4004964846075548, 'learning_rate': 3.5497835497835503e-05, 'epoch': 0.01}

  1%|          | 42/7689 [03:20<7:27:05,  3.51s/it]

  1%|          | 43/7689 [03:24<7:33:13,  3.56s/it]


  1%|          | 45/7689 [03:34<9:41:33,  4.56s/it]
{'loss': 1.6116, 'grad_norm': 0.36250053138397526, 'learning_rate': 3.8961038961038966e-05, 'epoch': 0.01}

  1%|          | 46/7689 [03:38<8:58:27,  4.23s/it]

  1%|          | 47/7689 [03:41<8:28:21,  3.99s/it]

  1%|          | 48/7689 [03:44<7:39:34,  3.61s/it]

  1%|          | 49/7689 [03:50<9:02:39,  4.26s/it]


  1%|          | 51/7689 [04:01<10:38:17,  5.01s/it]

  1%|          | 52/7689 [04:05<10:02:02,  4.73s/it]
{'loss': 1.4383, 'grad_norm': 0.20584243181094786, 'learning_rate': 4.5021645021645025e-05, 'epoch': 0.01}


  1%|          | 54/7689 [04:10<8:01:25,  3.78s/it]
{'loss': 1.4509, 'grad_norm': 0.22913971791102658, 'learning_rate': 4.675324675324675e-05, 'epoch': 0.01}

  1%|          | 55/7689 [04:14<8:03:09,  3.80s/it]

  1%|          | 56/7689 [04:17<7:26:03,  3.51s/it]


  1%|          | 58/7689 [04:23<6:44:45,  3.18s/it]
{'loss': 1.5017, 'grad_norm': 0.22632913629301757, 'learning_rate': 5.0216450216450216e-05, 'epoch': 0.01}

  1%|          | 59/7689 [04:27<7:30:37,  3.54s/it]


  1%|          | 61/7689 [04:33<6:39:18,  3.14s/it]
{'loss': 1.5275, 'grad_norm': 0.23317200626200177, 'learning_rate': 5.281385281385282e-05, 'epoch': 0.01}

  1%|          | 62/7689 [04:37<7:45:49,  3.66s/it]

  1%|          | 63/7689 [04:41<7:44:24,  3.65s/it]

  1%|          | 64/7689 [04:45<7:52:23,  3.72s/it]

  1%|          | 65/7689 [04:48<7:18:52,  3.45s/it]

  1%|          | 66/7689 [04:54<9:01:30,  4.26s/it]

  1%|          | 67/7689 [04:59<9:34:55,  4.53s/it]

  1%|          | 68/7689 [05:02<8:38:48,  4.08s/it]

  1%|          | 69/7689 [05:07<9:22:24,  4.43s/it]

  1%|          | 70/7689 [05:10<8:20:19,  3.94s/it]


  1%|          | 72/7689 [05:20<9:26:44,  4.46s/it]
{'loss': 1.4967, 'grad_norm': 0.230719380385916, 'learning_rate': 6.233766233766233e-05, 'epoch': 0.01}


  1%|          | 74/7689 [05:26<7:57:21,  3.76s/it]
{'loss': 1.2821, 'grad_norm': 0.21810989580021586, 'learning_rate': 6.406926406926407e-05, 'epoch': 0.01}

  1%|          | 75/7689 [05:31<8:25:26,  3.98s/it]


  1%|          | 77/7689 [05:37<7:16:41,  3.44s/it]
{'loss': 1.3964, 'grad_norm': 0.23324316551392543, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}

  1%|          | 78/7689 [05:40<6:58:43,  3.30s/it]

  1%|          | 79/7689 [05:43<6:52:18,  3.25s/it]

  1%|          | 80/7689 [05:46<6:45:49,  3.20s/it]


  1%|          | 82/7689 [05:53<7:01:58,  3.33s/it]

  1%|          | 83/7689 [05:56<7:06:33,  3.36s/it]
{'loss': 1.3381, 'grad_norm': 0.23712636663723924, 'learning_rate': 7.186147186147186e-05, 'epoch': 0.01}

  1%|          | 84/7689 [06:00<7:08:05,  3.38s/it]

  1%|          | 85/7689 [06:03<7:19:48,  3.47s/it]

  1%|          | 86/7689 [06:08<8:08:11,  3.85s/it]

  1%|          | 87/7689 [06:11<7:47:37,  3.69s/it]

  1%|          | 88/7689 [06:17<9:07:25,  4.32s/it]

  1%|          | 89/7689 [06:20<8:18:03,  3.93s/it]

  1%|          | 90/7689 [06:24<7:57:09,  3.77s/it]


  1%|          | 92/7689 [06:31<7:44:58,  3.67s/it]
{'loss': 1.373, 'grad_norm': 0.2426617255271591, 'learning_rate': 7.965367965367965e-05, 'epoch': 0.01}


  1%|          | 94/7689 [06:37<7:01:55,  3.33s/it]
{'loss': 1.2167, 'grad_norm': 0.2312727527805423, 'learning_rate': 8.138528138528139e-05, 'epoch': 0.01}


  1%|          | 96/7689 [06:43<6:27:44,  3.06s/it]

  1%|▏         | 97/7689 [06:46<6:54:11,  3.27s/it]
{'loss': 1.2387, 'grad_norm': 0.22278927294704406, 'learning_rate': 8.398268398268399e-05, 'epoch': 0.01}

  1%|▏         | 98/7689 [06:49<6:39:54,  3.16s/it]


  1%|▏         | 100/7689 [06:57<7:15:30,  3.44s/it]

  1%|▏         | 101/7689 [07:00<7:19:52,  3.48s/it]
{'loss': 1.4489, 'grad_norm': 0.22042384406163987, 'learning_rate': 8.744588744588745e-05, 'epoch': 0.01}


  1%|▏         | 103/7689 [07:09<7:50:21,  3.72s/it]
{'loss': 1.3283, 'grad_norm': 0.22694858103666296, 'learning_rate': 8.917748917748918e-05, 'epoch': 0.01}

  1%|▏         | 104/7689 [07:16<9:56:35,  4.72s/it]

  1%|▏         | 105/7689 [07:19<9:20:18,  4.43s/it]


  1%|▏         | 107/7689 [07:25<7:38:07,  3.63s/it]
{'loss': 1.3897, 'grad_norm': 0.22356347111972458, 'learning_rate': 9.264069264069265e-05, 'epoch': 0.01}


  1%|▏         | 109/7689 [07:37<9:55:11,  4.71s/it]
{'loss': 1.6185, 'grad_norm': 0.2256973234654395, 'learning_rate': 9.437229437229437e-05, 'epoch': 0.01}


  1%|▏         | 111/7689 [07:47<10:14:13,  4.86s/it]
{'loss': 1.2512, 'grad_norm': 0.2415827535436577, 'learning_rate': 9.610389610389611e-05, 'epoch': 0.01}


  1%|▏         | 113/7689 [07:57<10:50:48,  5.15s/it]

  1%|▏         | 114/7689 [08:01<10:18:56,  4.90s/it]
{'loss': 1.1606, 'grad_norm': 0.23555467467187793, 'learning_rate': 9.870129870129871e-05, 'epoch': 0.01}


  2%|▏         | 116/7689 [08:07<8:10:13,  3.88s/it]
{'loss': 1.0824, 'grad_norm': 0.2446515998996809, 'learning_rate': 0.00010043290043290043, 'epoch': 0.02}

  2%|▏         | 117/7689 [08:10<7:33:01,  3.59s/it]

  2%|▏         | 118/7689 [08:12<6:58:23,  3.32s/it]


  2%|▏         | 120/7689 [08:19<6:52:17,  3.27s/it]
{'loss': 1.2416, 'grad_norm': 0.24944402063329205, 'learning_rate': 0.00010389610389610389, 'epoch': 0.02}


  2%|▏         | 122/7689 [08:25<6:31:30,  3.10s/it]
{'loss': 1.2566, 'grad_norm': 0.21705781926951073, 'learning_rate': 0.00010562770562770564, 'epoch': 0.02}


  2%|▏         | 124/7689 [08:31<6:32:39,  3.11s/it]
{'loss': 1.3586, 'grad_norm': 0.22647136438479334, 'learning_rate': 0.00010735930735930737, 'epoch': 0.02}

  2%|▏         | 125/7689 [08:36<7:41:53,  3.66s/it]

  2%|▏         | 126/7689 [08:40<8:12:19,  3.91s/it]

  2%|▏         | 127/7689 [08:43<7:37:27,  3.63s/it]


  2%|▏         | 129/7689 [08:49<6:50:27,  3.26s/it]

  2%|▏         | 130/7689 [08:53<7:08:10,  3.40s/it]
{'loss': 1.1878, 'grad_norm': 0.2770029729772734, 'learning_rate': 0.00011255411255411256, 'epoch': 0.02}


  2%|▏         | 132/7689 [09:01<8:01:27,  3.82s/it]

  2%|▏         | 133/7689 [09:05<8:02:50,  3.83s/it]
{'loss': 1.3005, 'grad_norm': 0.24440714514256065, 'learning_rate': 0.00011515151515151516, 'epoch': 0.02}

  2%|▏         | 134/7689 [09:10<8:55:22,  4.25s/it]


  2%|▏         | 136/7689 [09:17<7:58:31,  3.80s/it]
{'loss': 1.3099, 'grad_norm': 0.24095181995083292, 'learning_rate': 0.00011774891774891777, 'epoch': 0.02}


  2%|▏         | 138/7689 [09:23<7:25:49,  3.54s/it]

  2%|▏         | 139/7689 [09:27<7:20:44,  3.50s/it]
{'loss': 1.324, 'grad_norm': 0.23349408727415383, 'learning_rate': 0.00012034632034632037, 'epoch': 0.02}

  2%|▏         | 140/7689 [09:31<8:05:19,  3.86s/it]

  2%|▏         | 141/7689 [09:34<7:42:00,  3.67s/it]


  2%|▏         | 143/7689 [09:41<6:58:40,  3.33s/it]
{'loss': 1.1504, 'grad_norm': 0.25400998395741314, 'learning_rate': 0.0001238095238095238, 'epoch': 0.02}

  2%|▏         | 144/7689 [09:44<6:53:15,  3.29s/it]

  2%|▏         | 145/7689 [09:48<7:16:18,  3.47s/it]


  2%|▏         | 147/7689 [09:53<6:32:05,  3.12s/it]
{'loss': 1.4194, 'grad_norm': 0.2641907440041888, 'learning_rate': 0.00012727272727272728, 'epoch': 0.02}

  2%|▏         | 148/7689 [09:56<6:30:53,  3.11s/it]


  2%|▏         | 150/7689 [10:03<6:31:27,  3.12s/it]
{'loss': 1.3675, 'grad_norm': 0.2390850712637811, 'learning_rate': 0.00012987012987012987, 'epoch': 0.02}

  2%|▏         | 151/7689 [10:06<6:31:10,  3.11s/it]

  2%|▏         | 152/7689 [10:10<6:59:54,  3.34s/it]

  2%|▏         | 153/7689 [10:12<6:37:07,  3.16s/it]


  2%|▏         | 155/7689 [10:21<8:13:57,  3.93s/it]

  2%|▏         | 156/7689 [10:25<8:00:46,  3.83s/it]

  2%|▏         | 157/7689 [10:29<8:17:24,  3.96s/it]
{'loss': 1.2942, 'grad_norm': 0.2324876476558205, 'learning_rate': 0.00013593073593073593, 'epoch': 0.02}


  2%|▏         | 159/7689 [10:35<7:08:46,  3.42s/it]
{'loss': 1.2861, 'grad_norm': 0.2594873243552533, 'learning_rate': 0.00013766233766233766, 'epoch': 0.02}

  2%|▏         | 160/7689 [10:38<7:13:49,  3.46s/it]

  2%|▏         | 161/7689 [10:42<7:06:00,  3.40s/it]

  2%|▏         | 162/7689 [10:46<7:51:05,  3.76s/it]


  2%|▏         | 164/7689 [10:55<8:36:31,  4.12s/it]

  2%|▏         | 165/7689 [11:01<9:32:57,  4.57s/it]
{'loss': 1.0071, 'grad_norm': 0.2224286635590244, 'learning_rate': 0.00014285714285714287, 'epoch': 0.02}


  2%|▏         | 167/7689 [11:13<11:08:46,  5.33s/it]
{'loss': 1.2353, 'grad_norm': 0.22868385581856537, 'learning_rate': 0.00014458874458874458, 'epoch': 0.02}

  2%|▏         | 168/7689 [11:18<10:28:56,  5.02s/it]


  2%|▏         | 170/7689 [11:25<8:52:22,  4.25s/it]
{'loss': 1.3322, 'grad_norm': 0.2535040042505454, 'learning_rate': 0.0001471861471861472, 'epoch': 0.02}


  2%|▏         | 172/7689 [11:31<7:29:52,  3.59s/it]
{'loss': 1.3065, 'grad_norm': 0.25710772494564166, 'learning_rate': 0.00014891774891774893, 'epoch': 0.02}

  2%|▏         | 173/7689 [11:36<8:48:09,  4.22s/it]

  2%|▏         | 174/7689 [11:40<8:24:57,  4.03s/it]

  2%|▏         | 175/7689 [11:44<8:10:22,  3.92s/it]


  2%|▏         | 177/7689 [11:49<6:57:13,  3.33s/it]
{'loss': 1.2575, 'grad_norm': 0.2418605194992333, 'learning_rate': 0.00015324675324675325, 'epoch': 0.02}

  2%|▏         | 178/7689 [11:52<6:48:09,  3.26s/it]

  2%|▏         | 179/7689 [11:56<6:47:57,  3.26s/it]

  2%|▏         | 180/7689 [12:01<7:50:01,  3.76s/it]

  2%|▏         | 181/7689 [12:03<7:18:03,  3.50s/it]


  2%|▏         | 183/7689 [12:09<6:33:55,  3.15s/it]

  2%|▏         | 184/7689 [12:15<8:11:15,  3.93s/it]

  2%|▏         | 185/7689 [12:19<8:26:20,  4.05s/it]
{'loss': 1.2893, 'grad_norm': 0.2559210912488746, 'learning_rate': 0.00016017316017316016, 'epoch': 0.02}

  2%|▏         | 186/7689 [12:22<7:50:45,  3.76s/it]

  2%|▏         | 187/7689 [12:26<7:35:16,  3.64s/it]


  2%|▏         | 189/7689 [12:31<6:50:21,  3.28s/it]
{'loss': 1.4517, 'grad_norm': 0.2561014178542454, 'learning_rate': 0.00016363636363636366, 'epoch': 0.02}


  2%|▏         | 191/7689 [12:41<8:35:34,  4.13s/it]
{'loss': 1.0691, 'grad_norm': 0.22109019776429384, 'learning_rate': 0.00016536796536796537, 'epoch': 0.02}


  3%|▎         | 193/7689 [12:47<7:34:57,  3.64s/it]
{'loss': 1.3723, 'grad_norm': 0.22673247137535363, 'learning_rate': 0.0001670995670995671, 'epoch': 0.03}

  3%|▎         | 194/7689 [12:50<6:56:44,  3.34s/it]


  3%|▎         | 196/7689 [12:57<6:59:12,  3.36s/it]

  3%|▎         | 197/7689 [13:01<7:22:03,  3.54s/it]
{'loss': 1.0372, 'grad_norm': 0.2228610660692561, 'learning_rate': 0.00017056277056277057, 'epoch': 0.03}

  3%|▎         | 198/7689 [13:04<7:15:39,  3.49s/it]


  3%|▎         | 200/7689 [13:11<7:01:30,  3.38s/it]
{'loss': 1.2872, 'grad_norm': 0.29832994401901675, 'learning_rate': 0.00017316017316017316, 'epoch': 0.03}


  3%|▎         | 202/7689 [13:17<6:25:16,  3.09s/it]
{'loss': 1.3427, 'grad_norm': 0.2654214468864349, 'learning_rate': 0.0001748917748917749, 'epoch': 0.03}

  3%|▎         | 203/7689 [13:22<7:23:27,  3.55s/it]

  3%|▎         | 204/7689 [13:24<6:54:10,  3.32s/it]

  3%|▎         | 205/7689 [13:28<6:50:47,  3.29s/it]

  3%|▎         | 206/7689 [13:30<6:29:56,  3.13s/it]


  3%|▎         | 208/7689 [13:37<6:33:56,  3.16s/it]
{'loss': 1.2485, 'grad_norm': 0.23610674521174918, 'learning_rate': 0.0001800865800865801, 'epoch': 0.03}


  3%|▎         | 210/7689 [13:43<6:33:35,  3.16s/it]
{'loss': 1.3752, 'grad_norm': 0.22847132342337007, 'learning_rate': 0.00018181818181818183, 'epoch': 0.03}

  3%|▎         | 211/7689 [13:46<6:43:44,  3.24s/it]


  3%|▎         | 213/7689 [13:53<6:42:13,  3.23s/it]
{'loss': 1.2367, 'grad_norm': 0.22537193513147494, 'learning_rate': 0.00018441558441558442, 'epoch': 0.03}


  3%|▎         | 215/7689 [13:59<6:22:08,  3.07s/it]
{'loss': 1.5215, 'grad_norm': 0.23364815232073388, 'learning_rate': 0.00018614718614718616, 'epoch': 0.03}

  3%|▎         | 216/7689 [14:02<6:28:36,  3.12s/it]

  3%|▎         | 217/7689 [14:07<7:20:02,  3.53s/it]

  3%|▎         | 218/7689 [14:10<7:21:55,  3.55s/it]

  3%|▎         | 219/7689 [14:14<7:26:05,  3.58s/it]

  3%|▎         | 220/7689 [14:18<7:59:42,  3.85s/it]

  3%|▎         | 221/7689 [14:22<7:39:46,  3.69s/it]


  3%|▎         | 223/7689 [14:31<8:42:10,  4.20s/it]
{'loss': 1.0593, 'grad_norm': 0.2028019243515975, 'learning_rate': 0.00019307359307359307, 'epoch': 0.03}


  3%|▎         | 225/7689 [14:37<7:24:46,  3.58s/it]

  3%|▎         | 226/7689 [14:45<10:08:06,  4.89s/it]

  3%|▎         | 227/7689 [14:49<9:37:06,  4.64s/it]
{'loss': 1.3559, 'grad_norm': 0.22764120154994966, 'learning_rate': 0.00019653679653679654, 'epoch': 0.03}


  3%|▎         | 229/7689 [14:55<8:07:37,  3.92s/it]
{'loss': 1.1027, 'grad_norm': 0.19935850898592247, 'learning_rate': 0.00019826839826839827, 'epoch': 0.03}


  3%|▎         | 231/7689 [15:01<7:05:23,  3.42s/it]

  3%|▎         | 232/7689 [15:05<7:18:04,  3.52s/it]
{'loss': 1.2371, 'grad_norm': 0.2373154612896198, 'learning_rate': 0.00019999999112792948, 'epoch': 0.03}


  3%|▎         | 234/7689 [15:11<6:52:15,  3.32s/it]
{'loss': 1.344, 'grad_norm': 0.2373096611121237, 'learning_rate': 0.00019999992015137474, 'epoch': 0.03}


  3%|▎         | 236/7689 [15:17<6:23:37,  3.09s/it]

  3%|▎         | 237/7689 [15:21<6:56:02,  3.35s/it]
{'loss': 1.213, 'grad_norm': 0.23534654793932444, 'learning_rate': 0.00019999968060562654, 'epoch': 0.03}

  3%|▎         | 238/7689 [15:24<6:36:15,  3.19s/it]


  3%|▎         | 240/7689 [15:33<8:21:26,  4.04s/it]
{'loss': 1.365, 'grad_norm': 0.274935804377069, 'learning_rate': 0.00019999928136313786, 'epoch': 0.03}


  3%|▎         | 242/7689 [15:39<7:06:01,  3.43s/it]
{'loss': 1.3189, 'grad_norm': 0.221621571402656, 'learning_rate': 0.00019999892648137174, 'epoch': 0.03}


  3%|▎         | 244/7689 [15:45<6:48:07,  3.29s/it]
{'loss': 1.123, 'grad_norm': 0.23059096589701422, 'learning_rate': 0.00019999850062380655, 'epoch': 0.03}

  3%|▎         | 245/7689 [15:48<6:36:18,  3.19s/it]

  3%|▎         | 246/7689 [15:52<6:54:01,  3.34s/it]


  3%|▎         | 248/7689 [16:00<7:38:40,  3.70s/it]
{'loss': 1.1571, 'grad_norm': 0.2262589619842707, 'learning_rate': 0.00019999743598253834, 'epoch': 0.03}


  3%|▎         | 250/7689 [16:06<6:57:11,  3.36s/it]

  3%|▎         | 251/7689 [16:09<7:03:16,  3.41s/it]
{'loss': 1.3619, 'grad_norm': 0.2142581834543258, 'learning_rate': 0.00019999645119272904, 'epoch': 0.03}

  3%|▎         | 252/7689 [16:14<7:54:43,  3.83s/it]


  3%|▎         | 254/7689 [16:21<7:30:28,  3.64s/it]
{'loss': 1.3658, 'grad_norm': 0.24633228047757727, 'learning_rate': 0.00019999530671133662, 'epoch': 0.03}

  3%|▎         | 255/7689 [16:24<6:58:08,  3.37s/it]

  3%|▎         | 256/7689 [16:31<9:12:07,  4.46s/it]

  3%|▎         | 257/7689 [16:34<8:22:18,  4.06s/it]


  3%|▎         | 259/7689 [16:41<7:56:24,  3.85s/it]
{'loss': 1.3288, 'grad_norm': 0.2211117317782374, 'learning_rate': 0.00019999304437724402, 'epoch': 0.03}

  3%|▎         | 260/7689 [16:45<7:43:30,  3.74s/it]

  3%|▎         | 261/7689 [16:48<7:35:10,  3.68s/it]


  3%|▎         | 263/7689 [16:58<8:16:33,  4.01s/it]
{'loss': 1.1354, 'grad_norm': 0.1981257557260797, 'learning_rate': 0.00019999091513721252, 'epoch': 0.03}

  3%|▎         | 264/7689 [17:00<7:29:05,  3.63s/it]

  3%|▎         | 265/7689 [17:08<9:59:55,  4.85s/it]

  3%|▎         | 266/7689 [17:13<9:56:14,  4.82s/it]

  3%|▎         | 267/7689 [17:16<9:09:20,  4.44s/it]


  3%|▎         | 269/7689 [17:23<8:00:08,  3.88s/it]
{'loss': 1.3231, 'grad_norm': 0.20023261190564864, 'learning_rate': 0.00019998718900352244, 'epoch': 0.03}


  4%|▎         | 271/7689 [17:33<9:28:05,  4.59s/it]
{'loss': 1.2133, 'grad_norm': 0.20797345288165694, 'learning_rate': 0.0001999858050227968, 'epoch': 0.04}


  4%|▎         | 273/7689 [17:40<7:59:26,  3.88s/it]

  4%|▎         | 274/7689 [17:45<9:05:41,  4.42s/it]
{'loss': 1.2991, 'grad_norm': 0.21195407616180495, 'learning_rate': 0.00019998359598986807, 'epoch': 0.04}

  4%|▎         | 275/7689 [17:50<9:23:07,  4.56s/it]

  4%|▎         | 276/7689 [17:54<9:03:41,  4.40s/it]


  4%|▎         | 278/7689 [18:00<7:25:13,  3.60s/it]
{'loss': 1.1511, 'grad_norm': 0.20959895716429833, 'learning_rate': 0.0001999804022360815, 'epoch': 0.04}


  4%|▎         | 280/7689 [18:05<6:33:27,  3.19s/it]
{'loss': 1.2167, 'grad_norm': 0.20612819763260373, 'learning_rate': 0.00019997869891463208, 'epoch': 0.04}

  4%|▎         | 281/7689 [18:09<6:49:41,  3.32s/it]

  4%|▎         | 282/7689 [18:12<6:48:23,  3.31s/it]

  4%|▎         | 283/7689 [18:15<6:24:47,  3.12s/it]


  4%|▎         | 285/7689 [18:25<8:31:28,  4.14s/it]
{'loss': 1.1765, 'grad_norm': 0.19994247394338072, 'learning_rate': 0.0001999741301574673, 'epoch': 0.04}

  4%|▎         | 286/7689 [18:29<8:08:20,  3.96s/it]


  4%|▎         | 288/7689 [18:38<8:42:12,  4.23s/it]
{'loss': 1.1786, 'grad_norm': 0.21596458793705728, 'learning_rate': 0.00019997117602725707, 'epoch': 0.04}

  4%|▍         | 289/7689 [18:40<7:47:15,  3.79s/it]


  4%|▍         | 291/7689 [18:48<7:33:50,  3.68s/it]
{'loss': 1.439, 'grad_norm': 0.22754384763801733, 'learning_rate': 0.00019996806224582744, 'epoch': 0.04}


  4%|▍         | 293/7689 [18:56<8:07:54,  3.96s/it]
{'loss': 1.2852, 'grad_norm': 0.2029272953172058, 'learning_rate': 0.00019996589769886427, 'epoch': 0.04}


  4%|▍         | 295/7689 [19:02<7:01:30,  3.42s/it]
{'loss': 1.2546, 'grad_norm': 0.21322704911199752, 'learning_rate': 0.0001999636621995447, 'epoch': 0.04}

  4%|▍         | 296/7689 [19:05<6:38:06,  3.23s/it]

  4%|▍         | 297/7689 [19:08<6:49:14,  3.32s/it]


  4%|▍         | 299/7689 [19:14<6:19:12,  3.08s/it]
{'loss': 1.1376, 'grad_norm': 0.2286001652883449, 'learning_rate': 0.00019995897835023345, 'epoch': 0.04}
  4%|▍         | 300/7689 [19:18<7:09:43,  3.49s/it]Traceback (most recent call last):
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/train/train_mem.py", line 13, in <module>
    train()
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/train/train.py", line 585, in train
    trainer.train()
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 2291, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 2721, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 3572, in evaluate
    output = eval_loop(
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 3757, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 3971, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/trainer.py", line 3264, in compute_loss
    outputs = model(**inputs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/peft_model.py", line 1091, in forward
    return self.base_model(
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 160, in forward
    return self.model.forward(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/models/qwen2/modeling_qwen2.py", line 1168, in forward
    outputs = self.model(
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/model/modeling_molora_qwen.py", line 278, in forward
    layer_outputs = decoder_layer(
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/model/modeling_molora_qwen.py", line 141, in forward
    hidden_states = self.mlp(hidden_states)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/model/modeling_molora_qwen.py", line 81, in forward
    return super().forward(x)
  File "/mnt/petrelfs/liaoyusheng/projects/MING-MOE-DATA/transformers/src/transformers/models/qwen2/modeling_qwen2.py", line 180, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/model/utils.py", line 224, in forward
    moe_result = self.molora_helper2(x) if self.training else self.molora_helper(x)
  File "/mnt/petrelfs/liaoyusheng/projects/MING/ming/model/utils.py", line 300, in molora_helper
    assert selected_experts.shape[0] == 1
AssertionError