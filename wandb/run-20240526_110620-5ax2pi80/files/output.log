/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3844 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3844 [00:48<51:49:30, 48.55s/it]
{'loss': 1.6778, 'grad_norm': 0.7143369235752248, 'learning_rate': 1.7241379310344828e-07, 'epoch': 0.0}

  0%|          | 2/3844 [00:57<26:56:55, 25.25s/it]


  0%|          | 4/3844 [01:14<15:05:49, 14.15s/it]
{'loss': 1.5565, 'grad_norm': 0.7004841606406373, 'learning_rate': 6.896551724137931e-07, 'epoch': 0.0}


  0%|          | 6/3844 [01:28<10:46:46, 10.11s/it]

  0%|          | 7/3844 [01:38<10:41:55, 10.04s/it]
{'loss': 1.7005, 'grad_norm': 0.7736107505943814, 'learning_rate': 1.2068965517241381e-06, 'epoch': 0.0}

  0%|          | 8/3844 [01:46<9:49:16,  9.22s/it]

  0%|          | 9/3844 [01:53<9:14:39,  8.68s/it]

  0%|          | 10/3844 [01:59<8:29:36,  7.98s/it]


  0%|          | 12/3844 [02:12<7:33:32,  7.10s/it]
{'loss': 1.7022, 'grad_norm': 0.7978750633655292, 'learning_rate': 2.0689655172413796e-06, 'epoch': 0.0}

  0%|          | 13/3844 [02:19<7:17:44,  6.86s/it]


  0%|          | 15/3844 [02:34<7:40:02,  7.21s/it]

  0%|          | 16/3844 [02:40<7:21:36,  6.92s/it]
{'loss': 1.6401, 'grad_norm': 0.6974742099465747, 'learning_rate': 2.7586206896551725e-06, 'epoch': 0.0}

  0%|          | 17/3844 [02:49<7:46:59,  7.32s/it]

  0%|          | 18/3844 [02:56<7:43:10,  7.26s/it]

  0%|          | 19/3844 [03:04<7:58:46,  7.51s/it]


  1%|          | 21/3844 [03:19<7:47:43,  7.34s/it]

  1%|          | 22/3844 [03:27<8:00:34,  7.54s/it]

  1%|          | 23/3844 [03:36<8:42:00,  8.20s/it]

  1%|          | 24/3844 [03:43<8:02:27,  7.58s/it]

  1%|          | 25/3844 [03:52<8:48:18,  8.30s/it]
{'loss': 1.6241, 'grad_norm': 0.7581385726378801, 'learning_rate': 4.310344827586207e-06, 'epoch': 0.01}

  1%|          | 26/3844 [04:03<9:31:18,  8.98s/it]

  1%|          | 27/3844 [04:09<8:29:28,  8.01s/it]


  1%|          | 29/3844 [04:21<7:19:37,  6.91s/it]
{'loss': 1.6868, 'grad_norm': 0.7597504764576475, 'learning_rate': 5e-06, 'epoch': 0.01}

  1%|          | 30/3844 [04:29<7:41:37,  7.26s/it]


  1%|          | 32/3844 [04:44<8:03:45,  7.61s/it]

  1%|          | 33/3844 [04:54<8:41:45,  8.21s/it]
{'loss': 1.5957, 'grad_norm': 0.6651702746809995, 'learning_rate': 5.689655172413794e-06, 'epoch': 0.01}

  1%|          | 34/3844 [05:02<8:30:23,  8.04s/it]

  1%|          | 35/3844 [05:09<8:27:34,  8.00s/it]

  1%|          | 36/3844 [05:19<9:01:32,  8.53s/it]

  1%|          | 37/3844 [05:25<8:13:02,  7.77s/it]


  1%|          | 39/3844 [05:39<7:32:03,  7.13s/it]

  1%|          | 40/3844 [05:45<7:11:37,  6.81s/it]
{'loss': 1.657, 'grad_norm': 0.6274698802848517, 'learning_rate': 6.896551724137932e-06, 'epoch': 0.01}


  1%|          | 42/3844 [05:58<7:10:22,  6.79s/it]

  1%|          | 43/3844 [06:07<7:45:18,  7.35s/it]
{'loss': 1.4629, 'grad_norm': 0.4524921217638823, 'learning_rate': 7.413793103448277e-06, 'epoch': 0.01}


  1%|          | 45/3844 [06:22<7:50:55,  7.44s/it]
{'loss': 1.5894, 'grad_norm': 0.584140112446114, 'learning_rate': 7.758620689655173e-06, 'epoch': 0.01}

  1%|          | 46/3844 [06:29<7:46:39,  7.37s/it]


  1%|          | 48/3844 [06:41<6:52:49,  6.53s/it]
{'loss': 1.6445, 'grad_norm': 0.4383930378677893, 'learning_rate': 8.275862068965518e-06, 'epoch': 0.01}

  1%|▏         | 49/3844 [06:47<6:48:17,  6.46s/it]

  1%|▏         | 50/3844 [06:53<6:43:37,  6.38s/it]

  1%|▏         | 51/3844 [07:01<7:10:06,  6.80s/it]


  1%|▏         | 53/3844 [07:17<7:29:01,  7.11s/it]

  1%|▏         | 54/3844 [07:27<8:23:44,  7.97s/it]

  1%|▏         | 55/3844 [07:34<8:08:13,  7.73s/it]
{'loss': 1.6479, 'grad_norm': 0.2556547180174985, 'learning_rate': 9.482758620689655e-06, 'epoch': 0.01}


  1%|▏         | 57/3844 [07:55<9:41:45,  9.22s/it]

  2%|▏         | 58/3844 [08:01<8:36:13,  8.18s/it]

  2%|▏         | 59/3844 [08:06<7:46:49,  7.40s/it]

  2%|▏         | 60/3844 [08:13<7:28:36,  7.11s/it]

  2%|▏         | 61/3844 [08:18<7:02:58,  6.71s/it]

  2%|▏         | 62/3844 [08:25<6:53:33,  6.56s/it]

  2%|▏         | 63/3844 [08:34<7:52:26,  7.50s/it]

  2%|▏         | 64/3844 [08:41<7:27:32,  7.10s/it]

  2%|▏         | 65/3844 [08:47<7:13:10,  6.88s/it]

  2%|▏         | 66/3844 [08:55<7:34:36,  7.22s/it]
{'loss': 1.4844, 'grad_norm': 0.221460919848267, 'learning_rate': 1.1379310344827587e-05, 'epoch': 0.02}


  2%|▏         | 68/3844 [09:10<7:46:05,  7.41s/it]

  2%|▏         | 69/3844 [09:17<7:24:16,  7.06s/it]
{'loss': 1.4116, 'grad_norm': 0.22200859876545045, 'learning_rate': 1.1896551724137933e-05, 'epoch': 0.02}


  2%|▏         | 71/3844 [09:31<7:16:13,  6.94s/it]

  2%|▏         | 72/3844 [09:36<6:55:37,  6.61s/it]
{'loss': 1.4342, 'grad_norm': 0.1985406087467983, 'learning_rate': 1.2413793103448277e-05, 'epoch': 0.02}


  2%|▏         | 74/3844 [09:49<6:43:08,  6.42s/it]
{'loss': 1.5482, 'grad_norm': 0.18376519272963693, 'learning_rate': 1.2758620689655174e-05, 'epoch': 0.02}

  2%|▏         | 75/3844 [09:57<7:15:28,  6.93s/it]


  2%|▏         | 77/3844 [10:11<7:02:34,  6.73s/it]
{'loss': 1.5186, 'grad_norm': 0.1823210827787182, 'learning_rate': 1.327586206896552e-05, 'epoch': 0.02}

  2%|▏         | 78/3844 [10:19<7:37:32,  7.29s/it]

  2%|▏         | 79/3844 [10:26<7:29:06,  7.16s/it]

  2%|▏         | 80/3844 [10:33<7:16:05,  6.95s/it]

  2%|▏         | 81/3844 [10:40<7:23:31,  7.07s/it]


  2%|▏         | 83/3844 [11:01<9:31:09,  9.11s/it]
{'loss': 1.2991, 'grad_norm': 0.1668070273720188, 'learning_rate': 1.4310344827586209e-05, 'epoch': 0.02}

  2%|▏         | 84/3844 [11:11<9:55:07,  9.50s/it]

  2%|▏         | 85/3844 [11:18<9:06:41,  8.73s/it]


  2%|▏         | 87/3844 [11:33<8:24:32,  8.06s/it]

  2%|▏         | 88/3844 [11:39<7:53:58,  7.57s/it]

  2%|▏         | 89/3844 [11:45<7:16:57,  6.98s/it]
{'loss': 1.4964, 'grad_norm': 0.1675072197164456, 'learning_rate': 1.5344827586206898e-05, 'epoch': 0.02}

  2%|▏         | 90/3844 [11:53<7:33:23,  7.25s/it]


  2%|▏         | 92/3844 [12:07<7:35:57,  7.29s/it]
{'loss': 1.4711, 'grad_norm': 0.16071703585895097, 'learning_rate': 1.586206896551724e-05, 'epoch': 0.02}

  2%|▏         | 93/3844 [12:14<7:30:48,  7.21s/it]

  2%|▏         | 94/3844 [12:20<7:10:12,  6.88s/it]

  2%|▏         | 95/3844 [12:28<7:27:08,  7.16s/it]

  2%|▏         | 96/3844 [12:36<7:55:01,  7.60s/it]


  3%|▎         | 98/3844 [12:49<7:13:04,  6.94s/it]
{'loss': 1.5148, 'grad_norm': 0.16206855313810412, 'learning_rate': 1.6896551724137932e-05, 'epoch': 0.03}


  3%|▎         | 100/3844 [13:03<7:14:36,  6.96s/it]
{'loss': 1.3512, 'grad_norm': 0.1949751690198925, 'learning_rate': 1.7241379310344828e-05, 'epoch': 0.03}

  3%|▎         | 101/3844 [13:09<6:51:11,  6.59s/it]

  3%|▎         | 102/3844 [13:16<7:14:26,  6.97s/it]


  3%|▎         | 104/3844 [13:29<6:54:52,  6.66s/it]

  3%|▎         | 105/3844 [13:35<6:42:54,  6.47s/it]
{'loss': 1.4785, 'grad_norm': 0.17114291059952014, 'learning_rate': 1.810344827586207e-05, 'epoch': 0.03}

  3%|▎         | 106/3844 [13:42<6:48:31,  6.56s/it]

  3%|▎         | 107/3844 [13:48<6:41:18,  6.44s/it]

  3%|▎         | 108/3844 [13:54<6:32:38,  6.31s/it]

  3%|▎         | 109/3844 [14:02<7:05:32,  6.84s/it]

  3%|▎         | 110/3844 [14:10<7:24:08,  7.14s/it]

  3%|▎         | 111/3844 [14:18<7:49:47,  7.55s/it]


  3%|▎         | 113/3844 [14:37<8:50:49,  8.54s/it]

  3%|▎         | 114/3844 [14:43<8:05:49,  7.81s/it]
{'loss': 1.427, 'grad_norm': 0.16639705210270855, 'learning_rate': 1.9655172413793106e-05, 'epoch': 0.03}


  3%|▎         | 116/3844 [14:57<7:43:53,  7.47s/it]

  3%|▎         | 117/3844 [15:03<7:16:42,  7.03s/it]
{'loss': 1.4489, 'grad_norm': 0.1717768937305447, 'learning_rate': 1.9999996449267818e-05, 'epoch': 0.03}

  3%|▎         | 118/3844 [15:09<6:51:17,  6.62s/it]

  3%|▎         | 119/3844 [15:16<7:06:49,  6.88s/it]

  3%|▎         | 120/3844 [15:26<8:02:04,  7.77s/it]

  3%|▎         | 121/3844 [15:32<7:24:38,  7.17s/it]

  3%|▎         | 122/3844 [15:38<7:03:21,  6.82s/it]

  3%|▎         | 123/3844 [15:44<6:53:37,  6.67s/it]

  3%|▎         | 124/3844 [15:52<7:10:50,  6.95s/it]

  3%|▎         | 125/3844 [15:58<7:01:58,  6.81s/it]

  3%|▎         | 126/3844 [16:07<7:32:05,  7.30s/it]

  3%|▎         | 127/3844 [16:14<7:29:20,  7.25s/it]

  3%|▎         | 128/3844 [16:24<8:14:02,  7.98s/it]


  3%|▎         | 130/3844 [16:38<7:41:30,  7.46s/it]
{'loss': 1.4354, 'grad_norm': 0.16526281440601534, 'learning_rate': 1.9999304064522956e-05, 'epoch': 0.03}

  3%|▎         | 131/3844 [16:48<8:31:10,  8.26s/it]

  3%|▎         | 132/3844 [16:54<7:58:40,  7.74s/it]

  3%|▎         | 133/3844 [17:06<9:13:10,  8.94s/it]

  3%|▎         | 134/3844 [17:12<8:22:26,  8.13s/it]

  4%|▎         | 135/3844 [17:20<8:22:45,  8.13s/it]

  4%|▎         | 136/3844 [17:31<9:02:17,  8.77s/it]

  4%|▎         | 137/3844 [17:41<9:34:47,  9.30s/it]

  4%|▎         | 138/3844 [17:50<9:21:06,  9.08s/it]

  4%|▎         | 139/3844 [17:55<8:13:02,  7.98s/it]

  4%|▎         | 140/3844 [18:01<7:27:41,  7.25s/it]


  4%|▎         | 142/3844 [18:16<7:35:40,  7.39s/it]

  4%|▎         | 143/3844 [18:25<8:18:07,  8.08s/it]

  4%|▎         | 144/3844 [18:33<8:16:46,  8.06s/it]
{'loss': 1.3277, 'grad_norm': 0.1692838769463066, 'learning_rate': 1.9997216354957054e-05, 'epoch': 0.04}

  4%|▍         | 145/3844 [18:41<8:02:28,  7.83s/it]

  4%|▍         | 146/3844 [18:47<7:31:20,  7.32s/it]

  4%|▍         | 147/3844 [18:54<7:36:09,  7.40s/it]

  4%|▍         | 148/3844 [19:00<7:00:15,  6.82s/it]

  4%|▍         | 149/3844 [19:06<6:48:43,  6.64s/it]

  4%|▍         | 150/3844 [19:13<6:59:55,  6.82s/it]

  4%|▍         | 151/3844 [19:19<6:42:02,  6.53s/it]

  4%|▍         | 152/3844 [19:26<6:54:56,  6.74s/it]

  4%|▍         | 153/3844 [19:33<6:47:48,  6.63s/it]

  4%|▍         | 154/3844 [19:38<6:32:04,  6.38s/it]

  4%|▍         | 155/3844 [19:45<6:28:23,  6.32s/it]

  4%|▍         | 156/3844 [19:51<6:23:10,  6.23s/it]

  4%|▍         | 157/3844 [19:58<6:47:20,  6.63s/it]

  4%|▍         | 158/3844 [20:07<7:22:16,  7.20s/it]

  4%|▍         | 159/3844 [20:13<7:00:14,  6.84s/it]

  4%|▍         | 160/3844 [20:19<6:49:26,  6.67s/it]

  4%|▍         | 161/3844 [20:25<6:32:27,  6.39s/it]

  4%|▍         | 162/3844 [20:32<6:55:18,  6.77s/it]

  4%|▍         | 163/3844 [20:39<6:45:19,  6.61s/it]

  4%|▍         | 164/3844 [20:45<6:36:06,  6.46s/it]


  4%|▍         | 166/3844 [21:02<7:48:29,  7.64s/it]
{'loss': 1.2354, 'grad_norm': 0.16686314291552617, 'learning_rate': 1.999112448223846e-05, 'epoch': 0.04}

  4%|▍         | 167/3844 [21:08<7:25:12,  7.26s/it]

  4%|▍         | 168/3844 [21:14<6:58:31,  6.83s/it]


  4%|▍         | 170/3844 [21:26<6:26:39,  6.31s/it]
{'loss': 1.2416, 'grad_norm': 0.1762285215397176, 'learning_rate': 1.9989647850947708e-05, 'epoch': 0.04}

  4%|▍         | 171/3844 [21:33<6:35:54,  6.47s/it]


  5%|▍         | 173/3844 [21:44<6:03:48,  5.95s/it]

  5%|▍         | 174/3844 [21:54<7:23:03,  7.24s/it]

  5%|▍         | 175/3844 [22:04<8:15:15,  8.10s/it]
{'loss': 1.3475, 'grad_norm': 0.15988764189913787, 'learning_rate': 1.9987642446526327e-05, 'epoch': 0.05}

  5%|▍         | 176/3844 [22:13<8:33:45,  8.40s/it]

  5%|▍         | 177/3844 [22:21<8:20:15,  8.19s/it]

  5%|▍         | 178/3844 [22:26<7:34:53,  7.44s/it]

  5%|▍         | 179/3844 [22:33<7:15:30,  7.13s/it]


  5%|▍         | 181/3844 [22:48<7:31:26,  7.39s/it]
{'loss': 1.2367, 'grad_norm': 0.17182898703508256, 'learning_rate': 1.9985001906180847e-05, 'epoch': 0.05}

  5%|▍         | 182/3844 [22:53<6:54:26,  6.79s/it]

  5%|▍         | 183/3844 [22:58<6:27:41,  6.35s/it]

  5%|▍         | 184/3844 [23:09<7:50:34,  7.71s/it]


  5%|▍         | 186/3844 [23:28<8:30:07,  8.37s/it]
{'loss': 1.3237, 'grad_norm': 0.17083505974419216, 'learning_rate': 1.9982606455863856e-05, 'epoch': 0.05}

  5%|▍         | 187/3844 [23:35<8:10:59,  8.06s/it]

  5%|▍         | 188/3844 [23:44<8:28:11,  8.34s/it]

  5%|▍         | 189/3844 [23:56<9:21:30,  9.22s/it]

  5%|▍         | 190/3844 [24:03<8:54:24,  8.78s/it]

  5%|▍         | 191/3844 [24:09<8:07:43,  8.01s/it]

  5%|▍         | 192/3844 [24:15<7:23:05,  7.28s/it]

  5%|▌         | 193/3844 [24:21<6:49:39,  6.73s/it]

  5%|▌         | 194/3844 [24:29<7:30:43,  7.41s/it]

  5%|▌         | 195/3844 [24:38<7:41:37,  7.59s/it]

  5%|▌         | 196/3844 [24:48<8:38:44,  8.53s/it]


  5%|▌         | 198/3844 [25:04<8:10:53,  8.08s/it]

  5%|▌         | 199/3844 [25:10<7:32:22,  7.45s/it]
{'loss': 1.1943, 'grad_norm': 0.18836974387507197, 'learning_rate': 1.997554897524732e-05, 'epoch': 0.05}

  5%|▌         | 200/3844 [25:16<6:59:09,  6.90s/it]

  5%|▌         | 201/3844 [25:21<6:36:12,  6.53s/it]

  5%|▌         | 202/3844 [25:27<6:20:20,  6.27s/it]

  5%|▌         | 203/3844 [25:32<6:04:56,  6.01s/it]

  5%|▌         | 204/3844 [25:41<6:46:17,  6.70s/it]

  5%|▌         | 205/3844 [25:46<6:26:01,  6.36s/it]


  5%|▌         | 207/3844 [26:02<7:06:22,  7.03s/it]
{'loss': 1.1827, 'grad_norm': 0.192336336024911, 'learning_rate': 1.9970610791760655e-05, 'epoch': 0.05}

  5%|▌         | 208/3844 [26:11<7:36:31,  7.53s/it]


  5%|▌         | 210/3844 [26:22<6:33:59,  6.50s/it]

  5%|▌         | 211/3844 [26:28<6:34:36,  6.52s/it]

  6%|▌         | 212/3844 [26:37<7:06:49,  7.05s/it]
{'loss': 1.1644, 'grad_norm': 0.16809937947408715, 'learning_rate': 1.996729429353878e-05, 'epoch': 0.06}

  6%|▌         | 213/3844 [26:46<7:38:42,  7.58s/it]


  6%|▌         | 215/3844 [27:01<7:44:38,  7.68s/it]
{'loss': 1.3772, 'grad_norm': 0.1929529123953434, 'learning_rate': 1.996521945196495e-05, 'epoch': 0.06}


  6%|▌         | 217/3844 [27:14<7:13:41,  7.17s/it]
{'loss': 1.3018, 'grad_norm': 0.20247666268643705, 'learning_rate': 1.996380083960505e-05, 'epoch': 0.06}


  6%|▌         | 219/3844 [27:32<7:59:06,  7.93s/it]
{'loss': 1.3331, 'grad_norm': 0.19592789149932846, 'learning_rate': 1.9962353924219524e-05, 'epoch': 0.06}

  6%|▌         | 220/3844 [27:38<7:15:18,  7.21s/it]

  6%|▌         | 221/3844 [27:43<6:42:30,  6.67s/it]


  6%|▌         | 223/3844 [27:57<6:42:03,  6.66s/it]
{'loss': 1.2213, 'grad_norm': 0.19631522682871194, 'learning_rate': 1.9959375200892307e-05, 'epoch': 0.06}

  6%|▌         | 224/3844 [28:03<6:35:25,  6.55s/it]


  6%|▌         | 226/3844 [28:17<6:46:42,  6.74s/it]

  6%|▌         | 227/3844 [28:22<6:22:37,  6.35s/it]

  6%|▌         | 228/3844 [28:29<6:27:50,  6.44s/it]
{'loss': 1.2588, 'grad_norm': 0.19330342510099185, 'learning_rate': 1.995549266712994e-05, 'epoch': 0.06}


  6%|▌         | 230/3844 [28:41<6:07:54,  6.11s/it]
{'loss': 1.2856, 'grad_norm': 0.19833291690174265, 'learning_rate': 1.9953890160726296e-05, 'epoch': 0.06}

  6%|▌         | 231/3844 [28:47<6:15:02,  6.23s/it]

  6%|▌         | 232/3844 [28:55<6:51:57,  6.84s/it]

  6%|▌         | 233/3844 [29:03<7:06:32,  7.09s/it]


  6%|▌         | 235/3844 [29:23<8:15:48,  8.24s/it]
{'loss': 1.2413, 'grad_norm': 0.2048564941647575, 'learning_rate': 1.9949760202300888e-05, 'epoch': 0.06}


  6%|▌         | 237/3844 [29:40<8:35:12,  8.57s/it]
{'loss': 1.2536, 'grad_norm': 0.19434489200337599, 'learning_rate': 1.9948058754282282e-05, 'epoch': 0.06}


  6%|▌         | 239/3844 [29:55<7:55:39,  7.92s/it]
{'loss': 1.2609, 'grad_norm': 0.20121480297729658, 'learning_rate': 1.994632904795478e-05, 'epoch': 0.06}


  6%|▋         | 241/3844 [30:13<8:17:27,  8.28s/it]
{'loss': 1.2326, 'grad_norm': 0.20461669315746456, 'learning_rate': 1.9944571088231762e-05, 'epoch': 0.06}

  6%|▋         | 242/3844 [30:22<8:32:56,  8.54s/it]


  6%|▋         | 244/3844 [30:38<8:23:36,  8.39s/it]
{'loss': 1.2908, 'grad_norm': 0.19619286972164598, 'learning_rate': 1.9941881184476154e-05, 'epoch': 0.06}

  6%|▋         | 245/3844 [30:46<8:10:48,  8.18s/it]


  6%|▋         | 247/3844 [31:00<7:37:25,  7.63s/it]
{'loss': 1.3285, 'grad_norm': 0.1977152981299321, 'learning_rate': 1.9939127739027143e-05, 'epoch': 0.06}


  6%|▋         | 249/3844 [31:15<7:20:52,  7.36s/it]

  7%|▋         | 250/3844 [31:21<6:56:25,  6.95s/it]

  7%|▋         | 251/3844 [31:27<6:39:48,  6.68s/it]

  7%|▋         | 252/3844 [31:32<6:25:07,  6.43s/it]
{'loss': 1.3504, 'grad_norm': 0.21035598333646321, 'learning_rate': 1.9934397507496865e-05, 'epoch': 0.07}

  7%|▋         | 253/3844 [31:40<6:40:30,  6.69s/it]


  7%|▋         | 255/3844 [31:53<6:34:11,  6.59s/it]

  7%|▋         | 256/3844 [32:04<8:04:55,  8.11s/it]
{'loss': 1.1771, 'grad_norm': 0.2110399380980287, 'learning_rate': 1.993048633053094e-05, 'epoch': 0.07}

  7%|▋         | 257/3844 [32:10<7:21:46,  7.39s/it]

  7%|▋         | 258/3844 [32:16<6:49:49,  6.86s/it]


  7%|▋         | 260/3844 [32:31<7:12:48,  7.25s/it]
{'loss': 1.2364, 'grad_norm': 0.20191949674060347, 'learning_rate': 1.992646232007343e-05, 'epoch': 0.07}


  7%|▋         | 262/3844 [32:46<7:28:18,  7.51s/it]
{'loss': 1.2614, 'grad_norm': 0.2086037893577066, 'learning_rate': 1.992440801652344e-05, 'epoch': 0.07}

  7%|▋         | 263/3844 [32:52<6:51:11,  6.89s/it]


  7%|▋         | 265/3844 [33:07<7:23:00,  7.43s/it]

  7%|▋         | 266/3844 [33:12<6:46:56,  6.82s/it]
{'loss': 1.3517, 'grad_norm': 0.21640560595615063, 'learning_rate': 1.9920214841958083e-05, 'epoch': 0.07}


  7%|▋         | 268/3844 [33:25<6:29:06,  6.53s/it]
{'loss': 1.1611, 'grad_norm': 0.21149468590754722, 'learning_rate': 1.9918075982853793e-05, 'epoch': 0.07}

  7%|▋         | 269/3844 [33:32<6:34:38,  6.62s/it]

  7%|▋         | 270/3844 [33:38<6:16:16,  6.32s/it]


  7%|▋         | 272/3844 [33:52<6:57:26,  7.01s/it]
{'loss': 1.2999, 'grad_norm': 0.19358866592773152, 'learning_rate': 1.991371375138001e-05, 'epoch': 0.07}


  7%|▋         | 274/3844 [34:04<6:28:10,  6.52s/it]

  7%|▋         | 275/3844 [34:11<6:22:14,  6.43s/it]

  7%|▋         | 276/3844 [34:19<6:59:11,  7.05s/it]
{'loss': 1.2153, 'grad_norm': 0.2108391756429763, 'learning_rate': 1.9909238876990283e-05, 'epoch': 0.07}


  7%|▋         | 278/3844 [34:31<6:27:08,  6.51s/it]
{'loss': 1.2839, 'grad_norm': 0.22202391098040838, 'learning_rate': 1.9906959214541023e-05, 'epoch': 0.07}

  7%|▋         | 279/3844 [34:38<6:24:57,  6.48s/it]

  7%|▋         | 280/3844 [34:46<6:52:22,  6.94s/it]

  7%|▋         | 281/3844 [34:54<7:13:59,  7.31s/it]

  7%|▋         | 282/3844 [35:02<7:24:21,  7.49s/it]

  7%|▋         | 283/3844 [35:12<8:12:23,  8.30s/it]


  7%|▋         | 285/3844 [35:29<8:03:36,  8.15s/it]
{'loss': 1.1957, 'grad_norm': 0.220805562170875, 'learning_rate': 1.9898758824383925e-05, 'epoch': 0.07}


  7%|▋         | 287/3844 [35:45<8:02:55,  8.15s/it]
{'loss': 1.2364, 'grad_norm': 0.21196424479210124, 'learning_rate': 1.9896352577054094e-05, 'epoch': 0.07}


  8%|▊         | 289/3844 [35:57<7:03:54,  7.15s/it]

  8%|▊         | 290/3844 [36:05<7:16:11,  7.36s/it]

  8%|▊         | 291/3844 [36:17<8:42:01,  8.82s/it]

  8%|▊         | 292/3844 [36:27<8:49:16,  8.94s/it]

  8%|▊         | 293/3844 [36:33<8:12:00,  8.31s/it]
{'loss': 1.3787, 'grad_norm': 0.21725711885487656, 'learning_rate': 1.9888965194205908e-05, 'epoch': 0.08}

  8%|▊         | 294/3844 [36:40<7:35:27,  7.70s/it]


  8%|▊         | 296/3844 [36:53<7:00:08,  7.11s/it]
{'loss': 1.2464, 'grad_norm': 0.20782051428089007, 'learning_rate': 1.9885176685641043e-05, 'epoch': 0.08}

  8%|▊         | 297/3844 [37:00<7:00:35,  7.11s/it]


  8%|▊         | 299/3844 [37:13<6:45:06,  6.86s/it]

  8%|▊         | 300/3844 [37:23<7:30:37,  7.63s/it]
  8%|▊         | 300/3844 [37:23<7:30:37,  7.63s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1532, 'grad_norm': 0.207805767732269, 'learning_rate': 1.9878722118446153e-05, 'epoch': 0.08}
  8%|▊         | 301/3844 [37:56<15:07:17, 15.36s/it]

  8%|▊         | 302/3844 [38:02<12:15:12, 12.45s/it]

  8%|▊         | 303/3844 [38:10<11:03:12, 11.24s/it]

  8%|▊         | 304/3844 [38:18<10:04:09, 10.24s/it]

  8%|▊         | 305/3844 [38:26<9:20:15,  9.50s/it]

  8%|▊         | 306/3844 [38:32<8:21:53,  8.51s/it]


  8%|▊         | 308/3844 [38:45<7:26:56,  7.58s/it]
{'loss': 1.3244, 'grad_norm': 0.22591284330672626, 'learning_rate': 1.9869391106802152e-05, 'epoch': 0.08}

  8%|▊         | 309/3844 [38:52<7:08:06,  7.27s/it]


  8%|▊         | 311/3844 [39:05<6:58:07,  7.10s/it]

  8%|▊         | 312/3844 [39:14<7:15:13,  7.39s/it]

  8%|▊         | 313/3844 [39:19<6:44:27,  6.87s/it]
{'loss': 1.3474, 'grad_norm': 0.22658307697358757, 'learning_rate': 1.9862515818288995e-05, 'epoch': 0.08}
[2024-05-26 11:46:04,747] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  8%|▊         | 315/3844 [39:39<7:56:15,  8.10s/it]

  8%|▊         | 316/3844 [39:45<7:18:19,  7.45s/it]

  8%|▊         | 317/3844 [39:55<8:12:38,  8.38s/it]
{'loss': 1.2407, 'grad_norm': 0.21606769378980034, 'learning_rate': 1.9856889512745716e-05, 'epoch': 0.08}


  8%|▊         | 319/3844 [40:12<8:01:17,  8.19s/it]

  8%|▊         | 320/3844 [40:19<7:46:54,  7.95s/it]
{'loss': 1.3561, 'grad_norm': 0.23451406537494277, 'learning_rate': 1.9852596281842444e-05, 'epoch': 0.08}

  8%|▊         | 321/3844 [40:31<8:54:05,  9.10s/it]

  8%|▊         | 322/3844 [40:38<8:17:51,  8.48s/it]


  8%|▊         | 324/3844 [40:50<7:00:07,  7.16s/it]
{'loss': 1.1822, 'grad_norm': 0.2188273005184588, 'learning_rate': 1.9846774023812366e-05, 'epoch': 0.08}

  8%|▊         | 325/3844 [40:57<6:56:57,  7.11s/it]


  9%|▊         | 327/3844 [41:09<6:27:36,  6.61s/it]
{'loss': 1.3828, 'grad_norm': 0.26752801272792204, 'learning_rate': 1.984233390384874e-05, 'epoch': 0.09}

  9%|▊         | 328/3844 [41:15<6:12:41,  6.36s/it]

  9%|▊         | 329/3844 [41:22<6:30:11,  6.66s/it]


  9%|▊         | 331/3844 [41:37<6:53:47,  7.07s/it]

  9%|▊         | 332/3844 [41:44<6:46:32,  6.95s/it]

  9%|▊         | 333/3844 [41:53<7:31:41,  7.72s/it]
{'loss': 1.1874, 'grad_norm': 0.21370380868635405, 'learning_rate': 1.983326497633558e-05, 'epoch': 0.09}


  9%|▊         | 335/3844 [42:07<7:11:06,  7.37s/it]

  9%|▊         | 336/3844 [42:16<7:27:25,  7.65s/it]
{'loss': 1.2953, 'grad_norm': 0.22613193169410084, 'learning_rate': 1.9828636226748416e-05, 'epoch': 0.09}

  9%|▉         | 337/3844 [42:24<7:39:01,  7.85s/it]


  9%|▉         | 339/3844 [42:38<7:08:30,  7.34s/it]

  9%|▉         | 340/3844 [42:43<6:39:00,  6.83s/it]
{'loss': 1.3159, 'grad_norm': 0.2567937720433989, 'learning_rate': 1.9822366849055603e-05, 'epoch': 0.09}

  9%|▉         | 341/3844 [42:51<6:52:38,  7.07s/it]

  9%|▉         | 342/3844 [42:59<7:08:59,  7.35s/it]


  9%|▉         | 344/3844 [43:15<7:39:52,  7.88s/it]
{'loss': 1.1212, 'grad_norm': 0.23102462281991495, 'learning_rate': 1.9815985866360755e-05, 'epoch': 0.09}

  9%|▉         | 345/3844 [43:22<7:22:14,  7.58s/it]


  9%|▉         | 347/3844 [43:38<7:14:56,  7.46s/it]

  9%|▉         | 348/3844 [43:43<6:40:29,  6.87s/it]
{'loss': 1.2752, 'grad_norm': 0.23264856978661583, 'learning_rate': 1.9809493351166712e-05, 'epoch': 0.09}

  9%|▉         | 349/3844 [43:51<6:52:52,  7.09s/it]


  9%|▉         | 351/3844 [44:03<6:30:43,  6.71s/it]
{'loss': 1.1933, 'grad_norm': 0.22294631380837024, 'learning_rate': 1.9804550815898303e-05, 'epoch': 0.09}

  9%|▉         | 352/3844 [44:12<7:12:02,  7.42s/it]


  9%|▉         | 354/3844 [44:24<6:17:55,  6.50s/it]
{'loss': 1.3842, 'grad_norm': 0.23540241621574104, 'learning_rate': 1.979954561665812e-05, 'epoch': 0.09}

  9%|▉         | 355/3844 [44:32<6:56:38,  7.16s/it]


  9%|▉         | 357/3844 [44:46<6:41:18,  6.91s/it]

  9%|▉         | 358/3844 [44:54<6:59:10,  7.21s/it]
{'loss': 1.3035, 'grad_norm': 0.24477547044821643, 'learning_rate': 1.9792774595730462e-05, 'epoch': 0.09}

  9%|▉         | 359/3844 [45:00<6:48:14,  7.03s/it]


  9%|▉         | 361/3844 [45:15<7:07:37,  7.37s/it]

  9%|▉         | 362/3844 [45:21<6:49:27,  7.06s/it]
{'loss': 1.1343, 'grad_norm': 0.2397058539296334, 'learning_rate': 1.9785892306037796e-05, 'epoch': 0.09}


  9%|▉         | 364/3844 [45:38<7:37:38,  7.89s/it]
{'loss': 1.1701, 'grad_norm': 0.22359302276079004, 'learning_rate': 1.9782409459792374e-05, 'epoch': 0.09}


 10%|▉         | 366/3844 [45:51<7:05:39,  7.34s/it]
{'loss': 1.3241, 'grad_norm': 0.23570917994118967, 'learning_rate': 1.9778898825778996e-05, 'epoch': 0.1}

 10%|▉         | 367/3844 [45:59<7:01:31,  7.27s/it]

 10%|▉         | 368/3844 [46:06<7:09:56,  7.42s/it]


 10%|▉         | 370/3844 [46:20<6:52:02,  7.12s/it]

 10%|▉         | 371/3844 [46:29<7:30:56,  7.79s/it]

 10%|▉         | 372/3844 [46:35<6:57:44,  7.22s/it]
{'loss': 1.303, 'grad_norm': 0.2232088966578438, 'learning_rate': 1.9768200297248193e-05, 'epoch': 0.1}

 10%|▉         | 373/3844 [46:41<6:28:39,  6.72s/it]


 10%|▉         | 375/3844 [46:54<6:21:27,  6.60s/it]

 10%|▉         | 376/3844 [47:00<6:03:05,  6.28s/it]

 10%|▉         | 377/3844 [47:08<6:35:08,  6.84s/it]
{'loss': 1.2241, 'grad_norm': 0.2453120298681118, 'learning_rate': 1.9759094081954235e-05, 'epoch': 0.1}

 10%|▉         | 378/3844 [47:14<6:29:06,  6.74s/it]


 10%|▉         | 380/3844 [47:29<7:03:08,  7.33s/it]
{'loss': 1.143, 'grad_norm': 0.2574336215764051, 'learning_rate': 1.975354717779053e-05, 'epoch': 0.1}


 10%|▉         | 382/3844 [47:42<6:32:41,  6.81s/it]
{'loss': 1.3142, 'grad_norm': 0.2415762899670088, 'learning_rate': 1.9749814607259123e-05, 'epoch': 0.1}


 10%|▉         | 384/3844 [48:00<7:31:25,  7.83s/it]

 10%|█         | 385/3844 [48:08<7:35:08,  7.89s/it]
{'loss': 1.2148, 'grad_norm': 0.22678743692134568, 'learning_rate': 1.9744163826335306e-05, 'epoch': 0.1}

 10%|█         | 386/3844 [48:17<8:02:41,  8.38s/it]


 10%|█         | 388/3844 [48:35<8:21:21,  8.70s/it]
{'loss': 1.2987, 'grad_norm': 0.21931207618785714, 'learning_rate': 1.973845076739198e-05, 'epoch': 0.1}


 10%|█         | 390/3844 [48:50<7:32:27,  7.86s/it]

 10%|█         | 391/3844 [48:58<7:36:45,  7.94s/it]

 10%|█         | 392/3844 [49:06<7:42:57,  8.05s/it]

 10%|█         | 393/3844 [49:13<7:32:37,  7.87s/it]

 10%|█         | 394/3844 [49:22<7:41:38,  8.03s/it]

 10%|█         | 395/3844 [49:30<7:45:08,  8.09s/it]
{'loss': 1.2265, 'grad_norm': 0.24624592294136005, 'learning_rate': 1.9724878310080244e-05, 'epoch': 0.1}

 10%|█         | 396/3844 [49:37<7:26:18,  7.77s/it]


 10%|█         | 398/3844 [49:52<7:18:28,  7.63s/it]
{'loss': 1.2615, 'grad_norm': 0.24299881235201187, 'learning_rate': 1.971895792368463e-05, 'epoch': 0.1}

 10%|█         | 399/3844 [49:58<7:06:09,  7.42s/it]


 10%|█         | 401/3844 [50:14<7:20:52,  7.68s/it]

 10%|█         | 402/3844 [50:20<6:48:24,  7.12s/it]

 10%|█         | 403/3844 [50:32<8:11:30,  8.57s/it]
{'loss': 1.1104, 'grad_norm': 0.24531985303088977, 'learning_rate': 1.9708952594289506e-05, 'epoch': 0.1}

 11%|█         | 404/3844 [50:41<8:08:53,  8.53s/it]


 11%|█         | 406/3844 [50:56<7:43:46,  8.09s/it]

 11%|█         | 407/3844 [51:02<6:59:41,  7.33s/it]
{'loss': 1.3548, 'grad_norm': 0.23384646732092126, 'learning_rate': 1.970082421631491e-05, 'epoch': 0.11}


 11%|█         | 409/3844 [51:14<6:23:07,  6.69s/it]
{'loss': 1.289, 'grad_norm': 0.25778790199296936, 'learning_rate': 1.969671868750853e-05, 'epoch': 0.11}

 11%|█         | 410/3844 [51:21<6:22:07,  6.68s/it]

 11%|█         | 411/3844 [51:27<6:16:24,  6.58s/it]

 11%|█         | 412/3844 [51:37<7:15:18,  7.61s/it]

 11%|█         | 413/3844 [51:43<6:50:36,  7.18s/it]

 11%|█         | 414/3844 [51:49<6:19:01,  6.63s/it]

 11%|█         | 415/3844 [51:57<6:42:36,  7.04s/it]


 11%|█         | 417/3844 [52:12<7:06:24,  7.47s/it]
{'loss': 1.1055, 'grad_norm': 0.2391734179597336, 'learning_rate': 1.9680021246517368e-05, 'epoch': 0.11}

 11%|█         | 418/3844 [52:19<6:52:33,  7.23s/it]


 11%|█         | 420/3844 [52:34<7:00:16,  7.36s/it]

 11%|█         | 421/3844 [52:42<7:16:42,  7.65s/it]

 11%|█         | 422/3844 [52:50<7:20:44,  7.73s/it]

 11%|█         | 423/3844 [52:56<6:50:18,  7.20s/it]
{'loss': 1.1411, 'grad_norm': 0.2530829651468022, 'learning_rate': 1.9667209406827693e-05, 'epoch': 0.11}


 11%|█         | 425/3844 [53:10<6:51:03,  7.21s/it]
{'loss': 1.1694, 'grad_norm': 0.25317919310343023, 'learning_rate': 1.966288385628026e-05, 'epoch': 0.11}

 11%|█         | 426/3844 [53:19<7:12:15,  7.59s/it]


 11%|█         | 428/3844 [53:32<6:45:53,  7.13s/it]
{'loss': 1.2425, 'grad_norm': 0.24258160044144436, 'learning_rate': 1.9656344068860235e-05, 'epoch': 0.11}


 11%|█         | 430/3844 [53:46<6:58:55,  7.36s/it]
{'loss': 1.1699, 'grad_norm': 0.2330338440888197, 'learning_rate': 1.965194992090843e-05, 'epoch': 0.11}


 11%|█         | 432/3844 [54:02<7:02:17,  7.43s/it]
{'loss': 1.1069, 'grad_norm': 0.25584816619990774, 'learning_rate': 1.964752835577012e-05, 'epoch': 0.11}


 11%|█▏        | 434/3844 [54:18<7:26:39,  7.86s/it]
{'loss': 1.1643, 'grad_norm': 0.24838037341017616, 'learning_rate': 1.9643079386005126e-05, 'epoch': 0.11}


 11%|█▏        | 436/3844 [54:36<8:01:22,  8.47s/it]
{'loss': 1.0923, 'grad_norm': 0.2431327105138063, 'learning_rate': 1.963860302425113e-05, 'epoch': 0.11}

 11%|█▏        | 437/3844 [54:43<7:28:24,  7.90s/it]


 11%|█▏        | 439/3844 [54:56<6:49:42,  7.22s/it]

 11%|█▏        | 440/3844 [55:02<6:28:06,  6.84s/it]

 11%|█▏        | 441/3844 [55:10<6:55:53,  7.33s/it]
{'loss': 1.2683, 'grad_norm': 0.2566743609021765, 'learning_rate': 1.962729236355103e-05, 'epoch': 0.11}

 11%|█▏        | 442/3844 [55:17<6:46:37,  7.17s/it]

 12%|█▏        | 443/3844 [55:23<6:30:21,  6.89s/it]


 12%|█▏        | 445/3844 [55:40<7:11:59,  7.63s/it]

 12%|█▏        | 446/3844 [55:50<7:50:47,  8.31s/it]
{'loss': 1.2681, 'grad_norm': 0.24401800685879166, 'learning_rate': 1.961581078340947e-05, 'epoch': 0.12}

 12%|█▏        | 447/3844 [55:57<7:30:06,  7.95s/it]

 12%|█▏        | 448/3844 [56:03<6:58:08,  7.39s/it]

 12%|█▏        | 449/3844 [56:09<6:32:09,  6.93s/it]

 12%|█▏        | 450/3844 [56:18<7:01:34,  7.45s/it]

 12%|█▏        | 451/3844 [56:23<6:33:31,  6.96s/it]

 12%|█▏        | 452/3844 [56:31<6:49:33,  7.24s/it]

 12%|█▏        | 453/3844 [56:37<6:29:40,  6.89s/it]


 12%|█▏        | 455/3844 [56:53<6:48:17,  7.23s/it]

 12%|█▏        | 456/3844 [56:59<6:29:25,  6.90s/it]
{'loss': 1.2761, 'grad_norm': 0.25758814004666425, 'learning_rate': 1.9592335683191972e-05, 'epoch': 0.12}


 12%|█▏        | 458/3844 [57:15<6:56:09,  7.37s/it]

 12%|█▏        | 459/3844 [57:20<6:27:24,  6.87s/it]
{'loss': 1.2331, 'grad_norm': 0.2558079193873319, 'learning_rate': 1.9585160243508153e-05, 'epoch': 0.12}


 12%|█▏        | 461/3844 [57:34<6:24:16,  6.82s/it]
{'loss': 1.292, 'grad_norm': 0.2585392853296379, 'learning_rate': 1.9580342579884408e-05, 'epoch': 0.12}

 12%|█▏        | 462/3844 [57:42<6:38:53,  7.08s/it]

 12%|█▏        | 463/3844 [57:49<6:44:46,  7.18s/it]

 12%|█▏        | 464/3844 [57:56<6:30:11,  6.93s/it]

 12%|█▏        | 465/3844 [58:01<6:06:25,  6.51s/it]

 12%|█▏        | 466/3844 [58:07<5:56:23,  6.33s/it]


 12%|█▏        | 468/3844 [58:23<6:50:13,  7.29s/it]
{'loss': 1.0812, 'grad_norm': 0.24026543889323324, 'learning_rate': 1.9563266539190864e-05, 'epoch': 0.12}


 12%|█▏        | 470/3844 [58:39<7:07:00,  7.59s/it]

 12%|█▏        | 471/3844 [58:45<6:40:13,  7.12s/it]

 12%|█▏        | 472/3844 [58:52<6:45:02,  7.21s/it]
{'loss': 1.1626, 'grad_norm': 0.24232374230994558, 'learning_rate': 1.955335935445422e-05, 'epoch': 0.12}

 12%|█▏        | 473/3844 [58:58<6:15:51,  6.69s/it]


 12%|█▏        | 475/3844 [59:13<6:26:16,  6.88s/it]
{'loss': 1.2284, 'grad_norm': 0.26727127926007926, 'learning_rate': 1.9545857724779375e-05, 'epoch': 0.12}


 12%|█▏        | 477/3844 [59:25<6:02:57,  6.47s/it]

 12%|█▏        | 478/3844 [59:32<6:16:55,  6.72s/it]
{'loss': 1.322, 'grad_norm': 0.27410774810653116, 'learning_rate': 1.9538295084521767e-05, 'epoch': 0.12}

 12%|█▏        | 479/3844 [59:42<7:04:34,  7.57s/it]

 12%|█▏        | 480/3844 [59:48<6:33:44,  7.02s/it]


 13%|█▎        | 482/3844 [1:00:03<6:46:15,  7.25s/it]
{'loss': 1.1567, 'grad_norm': 0.25622165988941376, 'learning_rate': 1.9528116742425792e-05, 'epoch': 0.13}


 13%|█▎        | 484/3844 [1:00:19<7:03:20,  7.56s/it]

 13%|█▎        | 485/3844 [1:00:25<6:38:39,  7.12s/it]

 13%|█▎        | 486/3844 [1:00:34<7:21:12,  7.88s/it]
{'loss': 1.2304, 'grad_norm': 0.2585024236461594, 'learning_rate': 1.9517830138695452e-05, 'epoch': 0.13}


 13%|█▎        | 488/3844 [1:00:47<6:33:58,  7.04s/it]

 13%|█▎        | 489/3844 [1:00:53<6:17:48,  6.76s/it]
{'loss': 1.1577, 'grad_norm': 0.2859781966063214, 'learning_rate': 1.951004420946444e-05, 'epoch': 0.13}

 13%|█▎        | 490/3844 [1:00:59<6:10:15,  6.62s/it]

 13%|█▎        | 491/3844 [1:01:07<6:38:13,  7.13s/it]

 13%|█▎        | 492/3844 [1:01:17<7:24:24,  7.95s/it]


 13%|█▎        | 494/3844 [1:01:33<7:12:10,  7.74s/it]

 13%|█▎        | 495/3844 [1:01:38<6:37:45,  7.13s/it]
{'loss': 1.2536, 'grad_norm': 0.25324261483133853, 'learning_rate': 1.9494290056091287e-05, 'epoch': 0.13}


 13%|█▎        | 497/3844 [1:01:58<7:53:37,  8.49s/it]
{'loss': 1.1849, 'grad_norm': 0.27204512116123847, 'learning_rate': 1.9488984712987106e-05, 'epoch': 0.13}

 13%|█▎        | 498/3844 [1:02:06<7:32:34,  8.12s/it]


 13%|█▎        | 500/3844 [1:02:21<7:08:50,  7.69s/it]

 13%|█▎        | 501/3844 [1:02:29<7:21:16,  7.92s/it]

 13%|█▎        | 502/3844 [1:02:35<6:49:34,  7.35s/it]

 13%|█▎        | 503/3844 [1:02:41<6:27:11,  6.95s/it]
{'loss': 1.1754, 'grad_norm': 0.2576195771149429, 'learning_rate': 1.9472907018718778e-05, 'epoch': 0.13}


 13%|█▎        | 505/3844 [1:02:55<6:23:42,  6.90s/it]
{'loss': 1.1739, 'grad_norm': 0.2594928585163972, 'learning_rate': 1.9467493949721923e-05, 'epoch': 0.13}


 13%|█▎        | 507/3844 [1:03:07<6:06:23,  6.59s/it]
{'loss': 1.2146, 'grad_norm': 0.2574400407659652, 'learning_rate': 1.946205398750147e-05, 'epoch': 0.13}

 13%|█▎        | 508/3844 [1:03:13<6:04:16,  6.55s/it]


 13%|█▎        | 510/3844 [1:03:29<6:42:22,  7.24s/it]
{'loss': 1.225, 'grad_norm': 0.25242949046796287, 'learning_rate': 1.9453843653200134e-05, 'epoch': 0.13}


 13%|█▎        | 512/3844 [1:03:45<7:02:48,  7.61s/it]
{'loss': 1.1737, 'grad_norm': 0.2783085706927507, 'learning_rate': 1.9448336525693133e-05, 'epoch': 0.13}


 13%|█▎        | 514/3844 [1:04:04<7:51:36,  8.50s/it]

 13%|█▎        | 515/3844 [1:04:13<7:47:56,  8.43s/it]
{'loss': 1.1192, 'grad_norm': 0.2498249182289798, 'learning_rate': 1.944002551658211e-05, 'epoch': 0.13}


 13%|█▎        | 517/3844 [1:04:30<8:01:22,  8.68s/it]

 13%|█▎        | 518/3844 [1:04:37<7:32:54,  8.17s/it]
{'loss': 1.2803, 'grad_norm': 0.2642519517708745, 'learning_rate': 1.943165417329529e-05, 'epoch': 0.13}


 14%|█▎        | 520/3844 [1:04:53<7:19:31,  7.93s/it]

 14%|█▎        | 521/3844 [1:04:59<6:44:54,  7.31s/it]
{'loss': 1.3321, 'grad_norm': 0.28833818496687114, 'learning_rate': 1.9423222549336564e-05, 'epoch': 0.14}

 14%|█▎        | 522/3844 [1:05:04<6:17:05,  6.81s/it]

 14%|█▎        | 523/3844 [1:05:12<6:33:40,  7.11s/it]

 14%|█▎        | 524/3844 [1:05:18<6:17:03,  6.81s/it]


 14%|█▎        | 526/3844 [1:05:31<6:10:13,  6.69s/it]
{'loss': 1.2002, 'grad_norm': 0.268367756816642, 'learning_rate': 1.9409036032229904e-05, 'epoch': 0.14}

 14%|█▎        | 527/3844 [1:05:37<6:00:41,  6.52s/it]

 14%|█▎        | 528/3844 [1:05:44<6:06:41,  6.64s/it]


 14%|█▍        | 530/3844 [1:05:57<5:55:09,  6.43s/it]

 14%|█▍        | 531/3844 [1:06:04<6:17:59,  6.85s/it]

 14%|█▍        | 532/3844 [1:06:13<6:39:34,  7.24s/it]

 14%|█▍        | 533/3844 [1:06:23<7:35:33,  8.26s/it]

 14%|█▍        | 534/3844 [1:06:30<7:13:05,  7.85s/it]
{'loss': 1.2861, 'grad_norm': 0.2877818344916714, 'learning_rate': 1.9385990257980455e-05, 'epoch': 0.14}
[2024-05-26 12:13:13,003] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 535/3844 [1:06:40<7:48:30,  8.50s/it]

 14%|█▍        | 536/3844 [1:06:48<7:37:31,  8.30s/it]

 14%|█▍        | 537/3844 [1:06:54<6:59:05,  7.60s/it]


 14%|█▍        | 539/3844 [1:07:05<6:06:32,  6.65s/it]
{'loss': 1.153, 'grad_norm': 0.28102708246752844, 'learning_rate': 1.93713699561929e-05, 'epoch': 0.14}


 14%|█▍        | 541/3844 [1:07:23<7:17:56,  7.96s/it]

 14%|█▍        | 542/3844 [1:07:29<6:38:06,  7.23s/it]

 14%|█▍        | 543/3844 [1:07:35<6:19:59,  6.91s/it]
{'loss': 1.298, 'grad_norm': 0.2627881927918096, 'learning_rate': 1.935955391156497e-05, 'epoch': 0.14}


 14%|█▍        | 545/3844 [1:07:51<6:59:03,  7.62s/it]

 14%|█▍        | 546/3844 [1:08:00<7:09:19,  7.81s/it]

 14%|█▍        | 547/3844 [1:08:07<7:07:29,  7.78s/it]

 14%|█▍        | 548/3844 [1:08:15<7:11:51,  7.86s/it]
{'loss': 1.13, 'grad_norm': 0.3016040059356064, 'learning_rate': 1.9344634322049357e-05, 'epoch': 0.14}


 14%|█▍        | 550/3844 [1:08:29<6:51:23,  7.49s/it]
{'loss': 1.1409, 'grad_norm': 0.28418728146194744, 'learning_rate': 1.933862001896559e-05, 'epoch': 0.14}

 14%|█▍        | 551/3844 [1:08:36<6:36:45,  7.23s/it]

 14%|█▍        | 552/3844 [1:08:45<6:56:55,  7.60s/it]

 14%|█▍        | 553/3844 [1:08:54<7:22:02,  8.06s/it]


 14%|█▍        | 555/3844 [1:09:07<6:52:50,  7.53s/it]

 14%|█▍        | 556/3844 [1:09:13<6:25:46,  7.04s/it]

 14%|█▍        | 557/3844 [1:09:23<7:06:31,  7.79s/it]

 15%|█▍        | 558/3844 [1:09:30<6:48:16,  7.45s/it]
{'loss': 1.1091, 'grad_norm': 0.28481766539666403, 'learning_rate': 1.9314297707139758e-05, 'epoch': 0.15}

 15%|█▍        | 559/3844 [1:09:36<6:35:45,  7.23s/it]

 15%|█▍        | 560/3844 [1:09:42<6:15:14,  6.86s/it]


 15%|█▍        | 562/3844 [1:09:57<6:31:39,  7.16s/it]

 15%|█▍        | 563/3844 [1:10:03<6:13:43,  6.83s/it]

 15%|█▍        | 564/3844 [1:10:11<6:41:53,  7.35s/it]

 15%|█▍        | 565/3844 [1:10:17<6:20:45,  6.97s/it]
{'loss': 1.3291, 'grad_norm': 0.3049576385505797, 'learning_rate': 1.9292668385275916e-05, 'epoch': 0.15}


 15%|█▍        | 567/3844 [1:10:30<5:53:10,  6.47s/it]

 15%|█▍        | 568/3844 [1:10:35<5:44:20,  6.31s/it]
{'loss': 1.228, 'grad_norm': 0.2843876690284035, 'learning_rate': 1.9283299644572557e-05, 'epoch': 0.15}


 15%|█▍        | 570/3844 [1:10:55<7:11:20,  7.90s/it]
{'loss': 1.2621, 'grad_norm': 0.2827186263780593, 'learning_rate': 1.9277020851229945e-05, 'epoch': 0.15}


 15%|█▍        | 572/3844 [1:11:08<6:24:56,  7.06s/it]

 15%|█▍        | 573/3844 [1:11:13<6:04:01,  6.68s/it]
{'loss': 1.2332, 'grad_norm': 0.2758972754614408, 'learning_rate': 1.9267553256494085e-05, 'epoch': 0.15}

 15%|█▍        | 574/3844 [1:11:20<6:06:54,  6.73s/it]


 15%|█▍        | 576/3844 [1:11:37<6:50:57,  7.55s/it]
{'loss': 1.2498, 'grad_norm': 0.2549744023968618, 'learning_rate': 1.9258026429906956e-05, 'epoch': 0.15}

 15%|█▌        | 577/3844 [1:11:44<6:49:16,  7.52s/it]

 15%|█▌        | 578/3844 [1:11:50<6:23:27,  7.04s/it]

 15%|█▌        | 579/3844 [1:11:57<6:11:26,  6.83s/it]


 15%|█▌        | 581/3844 [1:12:09<5:55:16,  6.53s/it]

 15%|█▌        | 582/3844 [1:12:16<5:56:16,  6.55s/it]
{'loss': 1.2787, 'grad_norm': 0.2908597206846861, 'learning_rate': 1.9238795325112867e-05, 'epoch': 0.15}

 15%|█▌        | 583/3844 [1:12:23<6:06:53,  6.75s/it]

 15%|█▌        | 584/3844 [1:12:30<6:20:55,  7.01s/it]


 15%|█▌        | 586/3844 [1:12:45<6:35:59,  7.29s/it]

 15%|█▌        | 587/3844 [1:12:53<6:48:26,  7.52s/it]
{'loss': 1.1853, 'grad_norm': 0.27718929871051357, 'learning_rate': 1.9222588959095093e-05, 'epoch': 0.15}

 15%|█▌        | 588/3844 [1:12:59<6:18:13,  6.97s/it]

 15%|█▌        | 589/3844 [1:13:06<6:26:42,  7.13s/it]

 15%|█▌        | 590/3844 [1:13:15<6:52:38,  7.61s/it]


 15%|█▌        | 592/3844 [1:13:34<7:45:24,  8.59s/it]

 15%|█▌        | 593/3844 [1:13:39<6:55:24,  7.67s/it]
{'loss': 1.2929, 'grad_norm': 0.2918392601634759, 'learning_rate': 1.920292521587826e-05, 'epoch': 0.15}

 15%|█▌        | 594/3844 [1:13:46<6:37:16,  7.33s/it]
[2024-05-26 12:20:29,601] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 16%|█▌        | 596/3844 [1:14:04<7:09:38,  7.94s/it]

 16%|█▌        | 597/3844 [1:14:11<7:03:54,  7.83s/it]
{'loss': 1.303, 'grad_norm': 0.26176874986925897, 'learning_rate': 1.91896853142344e-05, 'epoch': 0.16}

 16%|█▌        | 598/3844 [1:14:19<6:56:11,  7.69s/it]

 16%|█▌        | 599/3844 [1:14:24<6:19:52,  7.02s/it]

 16%|█▌        | 600/3844 [1:14:34<7:06:46,  7.89s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 16%|█▌        | 601/3844 [1:15:07<13:55:27, 15.46s/it]
{'loss': 1.2621, 'grad_norm': 0.2562860736467353, 'learning_rate': 1.9176340996326727e-05, 'epoch': 0.16}

 16%|█▌        | 602/3844 [1:15:17<12:21:19, 13.72s/it]

 16%|█▌        | 603/3844 [1:15:22<10:07:58, 11.26s/it]

 16%|█▌        | 604/3844 [1:15:29<8:53:23,  9.88s/it]

 16%|█▌        | 605/3844 [1:15:35<7:43:21,  8.58s/it]


 16%|█▌        | 607/3844 [1:15:48<6:54:10,  7.68s/it]
{'loss': 1.123, 'grad_norm': 0.2742740490258326, 'learning_rate': 1.9156129070968937e-05, 'epoch': 0.16}

 16%|█▌        | 608/3844 [1:15:55<6:40:18,  7.42s/it]


 16%|█▌        | 610/3844 [1:16:09<6:41:39,  7.45s/it]
{'loss': 1.1573, 'grad_norm': 0.3098406241872606, 'learning_rate': 1.9145935296344924e-05, 'epoch': 0.16}


 16%|█▌        | 612/3844 [1:16:27<7:26:22,  8.29s/it]
{'loss': 1.1974, 'grad_norm': 0.2648149386755405, 'learning_rate': 1.9139106967807063e-05, 'epoch': 0.16}


 16%|█▌        | 614/3844 [1:16:40<6:29:14,  7.23s/it]
{'loss': 1.2194, 'grad_norm': 0.27211046333611133, 'learning_rate': 1.913225267885681e-05, 'epoch': 0.16}

 16%|█▌        | 615/3844 [1:16:49<7:07:04,  7.94s/it]


 16%|█▌        | 617/3844 [1:17:06<7:07:19,  7.95s/it]
{'loss': 1.2279, 'grad_norm': 0.3040816315895793, 'learning_rate': 1.9121922612269405e-05, 'epoch': 0.16}

 16%|█▌        | 618/3844 [1:17:12<6:48:30,  7.60s/it]

 16%|█▌        | 619/3844 [1:17:19<6:23:56,  7.14s/it]

 16%|█▌        | 620/3844 [1:17:27<6:46:48,  7.57s/it]


 16%|█▌        | 622/3844 [1:17:41<6:41:20,  7.47s/it]
{'loss': 1.1485, 'grad_norm': 0.2604916790289985, 'learning_rate': 1.9104576309440744e-05, 'epoch': 0.16}


 16%|█▌        | 624/3844 [1:17:58<6:49:50,  7.64s/it]
{'loss': 1.2561, 'grad_norm': 0.292903984762296, 'learning_rate': 1.909759251195435e-05, 'epoch': 0.16}

 16%|█▋        | 625/3844 [1:18:04<6:28:07,  7.23s/it]

 16%|█▋        | 626/3844 [1:18:13<6:49:07,  7.63s/it]


 16%|█▋        | 628/3844 [1:18:27<6:45:34,  7.57s/it]
{'loss': 1.2082, 'grad_norm': 0.27816019219438165, 'learning_rate': 1.908354740943193e-05, 'epoch': 0.16}


 16%|█▋        | 630/3844 [1:18:44<7:05:30,  7.94s/it]

 16%|█▋        | 631/3844 [1:18:52<7:05:12,  7.94s/it]
{'loss': 1.1823, 'grad_norm': 0.2648192389021806, 'learning_rate': 1.907294584201614e-05, 'epoch': 0.16}

 16%|█▋        | 632/3844 [1:18:57<6:22:10,  7.14s/it]

 16%|█▋        | 633/3844 [1:19:02<5:55:48,  6.65s/it]


 17%|█▋        | 635/3844 [1:19:16<5:52:58,  6.60s/it]

 17%|█▋        | 636/3844 [1:19:22<5:43:05,  6.42s/it]
{'loss': 1.2206, 'grad_norm': 0.30245419055030004, 'learning_rate': 1.90551477342761e-05, 'epoch': 0.17}

 17%|█▋        | 637/3844 [1:19:29<5:52:24,  6.59s/it]


 17%|█▋        | 639/3844 [1:19:48<6:57:51,  7.82s/it]

 17%|█▋        | 640/3844 [1:19:54<6:36:56,  7.43s/it]
{'loss': 1.0558, 'grad_norm': 0.28937245173406473, 'learning_rate': 1.9040793484329248e-05, 'epoch': 0.17}


 17%|█▋        | 642/3844 [1:20:08<6:18:07,  7.09s/it]

 17%|█▋        | 643/3844 [1:20:16<6:40:47,  7.51s/it]
{'loss': 1.1855, 'grad_norm': 0.30631683861071024, 'learning_rate': 1.9029960374949213e-05, 'epoch': 0.17}


 17%|█▋        | 645/3844 [1:20:30<6:20:46,  7.14s/it]

 17%|█▋        | 646/3844 [1:20:36<6:00:57,  6.77s/it]
{'loss': 1.3037, 'grad_norm': 0.28316982047011535, 'learning_rate': 1.9019069552248833e-05, 'epoch': 0.17}


 17%|█▋        | 648/3844 [1:20:50<6:07:07,  6.89s/it]
{'loss': 1.2325, 'grad_norm': 0.298054106463646, 'learning_rate': 1.901177697518467e-05, 'epoch': 0.17}

 17%|█▋        | 649/3844 [1:20:59<6:40:23,  7.52s/it]

 17%|█▋        | 650/3844 [1:21:05<6:25:04,  7.23s/it]

 17%|█▋        | 651/3844 [1:21:13<6:38:35,  7.49s/it]


 17%|█▋        | 653/3844 [1:21:32<7:12:06,  8.12s/it]
{'loss': 1.186, 'grad_norm': 0.3009200562021216, 'learning_rate': 1.8993433583614642e-05, 'epoch': 0.17}

 17%|█▋        | 654/3844 [1:21:41<7:23:40,  8.35s/it]

 17%|█▋        | 655/3844 [1:21:53<8:26:59,  9.54s/it]

 17%|█▋        | 656/3844 [1:21:59<7:29:05,  8.45s/it]


 17%|█▋        | 658/3844 [1:22:14<6:50:42,  7.73s/it]
{'loss': 1.1763, 'grad_norm': 0.27954339773386294, 'learning_rate': 1.897493052590103e-05, 'epoch': 0.17}

 17%|█▋        | 659/3844 [1:22:21<6:44:34,  7.62s/it]

 17%|█▋        | 660/3844 [1:22:27<6:22:45,  7.21s/it]


 17%|█▋        | 662/3844 [1:22:40<5:55:20,  6.70s/it]
{'loss': 1.3935, 'grad_norm': 0.3003021478576682, 'learning_rate': 1.8960013340761335e-05, 'epoch': 0.17}


 17%|█▋        | 664/3844 [1:22:58<7:11:22,  8.14s/it]

 17%|█▋        | 665/3844 [1:23:06<7:12:37,  8.17s/it]

 17%|█▋        | 666/3844 [1:23:12<6:41:43,  7.58s/it]
{'loss': 1.2242, 'grad_norm': 0.30359950188970036, 'learning_rate': 1.8944994348967247e-05, 'epoch': 0.17}

 17%|█▋        | 667/3844 [1:23:19<6:21:59,  7.21s/it]


 17%|█▋        | 669/3844 [1:23:38<7:18:35,  8.29s/it]

 17%|█▋        | 670/3844 [1:23:49<7:49:23,  8.87s/it]
{'loss': 1.1615, 'grad_norm': 0.289964427230589, 'learning_rate': 1.8929873721169546e-05, 'epoch': 0.17}


 17%|█▋        | 672/3844 [1:24:06<7:57:42,  9.04s/it]
{'loss': 1.1717, 'grad_norm': 0.29352018565426774, 'learning_rate': 1.8922275347413536e-05, 'epoch': 0.17}

 18%|█▊        | 673/3844 [1:24:18<8:34:43,  9.74s/it]


 18%|█▊        | 675/3844 [1:24:34<7:46:44,  8.84s/it]

 18%|█▊        | 676/3844 [1:24:40<7:04:08,  8.03s/it]

 18%|█▊        | 677/3844 [1:24:46<6:22:47,  7.25s/it]

 18%|█▊        | 678/3844 [1:24:52<6:12:07,  7.05s/it]

 18%|█▊        | 679/3844 [1:25:00<6:26:30,  7.33s/it]

 18%|█▊        | 680/3844 [1:25:09<6:43:34,  7.65s/it]

 18%|█▊        | 681/3844 [1:25:16<6:36:20,  7.52s/it]
{'loss': 1.206, 'grad_norm': 0.29978053546367345, 'learning_rate': 1.8887769340830694e-05, 'epoch': 0.18}


 18%|█▊        | 683/3844 [1:25:30<6:29:29,  7.39s/it]

 18%|█▊        | 684/3844 [1:25:38<6:48:11,  7.75s/it]

 18%|█▊        | 685/3844 [1:25:44<6:17:11,  7.16s/it]

 18%|█▊        | 686/3844 [1:25:53<6:37:48,  7.56s/it]

 18%|█▊        | 687/3844 [1:26:02<7:04:53,  8.08s/it]
{'loss': 1.1488, 'grad_norm': 0.3064796691280127, 'learning_rate': 1.8864481191256533e-05, 'epoch': 0.18}

 18%|█▊        | 688/3844 [1:26:10<7:01:24,  8.01s/it]

 18%|█▊        | 689/3844 [1:26:16<6:29:17,  7.40s/it]

 18%|█▊        | 690/3844 [1:26:23<6:22:52,  7.28s/it]

 18%|█▊        | 691/3844 [1:26:30<6:14:51,  7.13s/it]

 18%|█▊        | 692/3844 [1:26:36<5:56:21,  6.78s/it]


 18%|█▊        | 694/3844 [1:26:47<5:23:55,  6.17s/it]
{'loss': 1.1303, 'grad_norm': 0.30061320546822073, 'learning_rate': 1.8837025300011932e-05, 'epoch': 0.18}

 18%|█▊        | 695/3844 [1:26:55<5:55:20,  6.77s/it]


 18%|█▊        | 697/3844 [1:27:14<7:31:51,  8.61s/it]

 18%|█▊        | 698/3844 [1:27:20<6:42:43,  7.68s/it]

 18%|█▊        | 699/3844 [1:27:27<6:28:19,  7.41s/it]

 18%|█▊        | 700/3844 [1:27:33<6:05:20,  6.97s/it]
{'loss': 1.2733, 'grad_norm': 0.29476188709251905, 'learning_rate': 1.8813246895441423e-05, 'epoch': 0.18}


 18%|█▊        | 702/3844 [1:27:50<6:57:59,  7.98s/it]
{'loss': 1.0813, 'grad_norm': 0.2822226720389529, 'learning_rate': 1.8805270660935413e-05, 'epoch': 0.18}

 18%|█▊        | 703/3844 [1:28:00<7:16:00,  8.33s/it]

 18%|█▊        | 704/3844 [1:28:09<7:36:04,  8.71s/it]

 18%|█▊        | 705/3844 [1:28:20<8:04:16,  9.26s/it]


 18%|█▊        | 707/3844 [1:28:35<7:28:21,  8.58s/it]
{'loss': 1.155, 'grad_norm': 0.28523386895960495, 'learning_rate': 1.8785220696374476e-05, 'epoch': 0.18}

 18%|█▊        | 708/3844 [1:28:41<6:55:52,  7.96s/it]


 18%|█▊        | 710/3844 [1:28:59<7:19:46,  8.42s/it]

 18%|█▊        | 711/3844 [1:29:05<6:44:16,  7.74s/it]

 19%|█▊        | 712/3844 [1:29:10<6:10:36,  7.10s/it]

 19%|█▊        | 713/3844 [1:29:18<6:23:33,  7.35s/it]
{'loss': 1.3084, 'grad_norm': 0.2846165071307322, 'learning_rate': 1.8760954890538473e-05, 'epoch': 0.19}

 19%|█▊        | 714/3844 [1:29:25<6:14:35,  7.18s/it]

 19%|█▊        | 715/3844 [1:29:32<6:03:12,  6.96s/it]

 19%|█▊        | 716/3844 [1:29:37<5:42:02,  6.56s/it]

 19%|█▊        | 717/3844 [1:29:44<5:38:13,  6.49s/it]

 19%|█▊        | 718/3844 [1:29:49<5:24:41,  6.23s/it]

 19%|█▊        | 719/3844 [1:29:57<5:55:30,  6.83s/it]

 19%|█▊        | 720/3844 [1:30:07<6:41:36,  7.71s/it]

 19%|█▉        | 721/3844 [1:30:16<6:58:36,  8.04s/it]

 19%|█▉        | 722/3844 [1:30:23<6:46:17,  7.81s/it]

 19%|█▉        | 723/3844 [1:30:30<6:21:23,  7.33s/it]

 19%|█▉        | 724/3844 [1:30:36<6:07:01,  7.06s/it]

 19%|█▉        | 725/3844 [1:30:45<6:33:14,  7.56s/it]

 19%|█▉        | 726/3844 [1:30:53<6:44:54,  7.79s/it]

 19%|█▉        | 727/3844 [1:30:59<6:21:51,  7.35s/it]

 19%|█▉        | 728/3844 [1:31:09<6:57:18,  8.04s/it]

 19%|█▉        | 729/3844 [1:31:16<6:38:16,  7.67s/it]

 19%|█▉        | 730/3844 [1:31:21<6:06:59,  7.07s/it]


 19%|█▉        | 732/3844 [1:31:36<6:15:28,  7.24s/it]
{'loss': 1.2364, 'grad_norm': 0.29839564331218343, 'learning_rate': 1.8682638545429408e-05, 'epoch': 0.19}

 19%|█▉        | 733/3844 [1:31:43<6:07:50,  7.09s/it]


 19%|█▉        | 735/3844 [1:31:59<6:21:29,  7.36s/it]

 19%|█▉        | 736/3844 [1:32:04<5:58:37,  6.92s/it]

 19%|█▉        | 737/3844 [1:32:11<5:47:03,  6.70s/it]

 19%|█▉        | 738/3844 [1:32:16<5:32:17,  6.42s/it]
{'loss': 1.1753, 'grad_norm': 0.31827329924998, 'learning_rate': 1.8657443665007796e-05, 'epoch': 0.19}


 19%|█▉        | 740/3844 [1:32:28<5:24:18,  6.27s/it]
{'loss': 1.2736, 'grad_norm': 0.2967904119038087, 'learning_rate': 1.864899615524244e-05, 'epoch': 0.19}

 19%|█▉        | 741/3844 [1:32:34<5:17:08,  6.13s/it]

 19%|█▉        | 742/3844 [1:32:43<6:03:50,  7.04s/it]

 19%|█▉        | 743/3844 [1:32:49<5:39:01,  6.56s/it]

 19%|█▉        | 744/3844 [1:32:57<6:01:17,  6.99s/it]

 19%|█▉        | 745/3844 [1:33:06<6:32:11,  7.59s/it]

 19%|█▉        | 746/3844 [1:33:14<6:36:31,  7.68s/it]

 19%|█▉        | 747/3844 [1:33:20<6:14:47,  7.26s/it]


 19%|█▉        | 749/3844 [1:33:35<6:10:02,  7.17s/it]
{'loss': 1.1553, 'grad_norm': 0.2895989571166099, 'learning_rate': 1.861067867776602e-05, 'epoch': 0.19}

 20%|█▉        | 750/3844 [1:33:41<6:01:29,  7.01s/it]

 20%|█▉        | 751/3844 [1:33:47<5:41:40,  6.63s/it]


 20%|█▉        | 753/3844 [1:34:03<6:07:13,  7.13s/it]
{'loss': 1.2159, 'grad_norm': 0.29912755326236834, 'learning_rate': 1.8593489570565077e-05, 'epoch': 0.2}

 20%|█▉        | 754/3844 [1:34:13<7:06:28,  8.28s/it]

 20%|█▉        | 755/3844 [1:34:21<6:52:42,  8.02s/it]

 20%|█▉        | 756/3844 [1:34:28<6:32:11,  7.62s/it]

 20%|█▉        | 757/3844 [1:34:33<6:02:58,  7.05s/it]

 20%|█▉        | 758/3844 [1:34:41<6:11:27,  7.22s/it]

 20%|█▉        | 759/3844 [1:34:50<6:36:03,  7.70s/it]


 20%|█▉        | 761/3844 [1:35:09<7:33:54,  8.83s/it]
{'loss': 1.1252, 'grad_norm': 0.2805939620525781, 'learning_rate': 1.8558818626313077e-05, 'epoch': 0.2}

 20%|█▉        | 762/3844 [1:35:17<7:31:20,  8.79s/it]

 20%|█▉        | 763/3844 [1:35:27<7:42:14,  9.00s/it]

 20%|█▉        | 764/3844 [1:35:34<7:07:17,  8.32s/it]


 20%|█▉        | 766/3844 [1:35:51<7:04:03,  8.27s/it]
{'loss': 1.2556, 'grad_norm': 0.32295041635330884, 'learning_rate': 1.8536951650657586e-05, 'epoch': 0.2}


 20%|█▉        | 768/3844 [1:36:05<6:26:30,  7.54s/it]
{'loss': 1.1396, 'grad_norm': 0.3121794494816895, 'learning_rate': 1.852816240122438e-05, 'epoch': 0.2}

 20%|██        | 769/3844 [1:36:11<6:13:12,  7.28s/it]

 20%|██        | 770/3844 [1:36:20<6:27:12,  7.56s/it]

 20%|██        | 771/3844 [1:36:28<6:41:21,  7.84s/it]

 20%|██        | 772/3844 [1:36:35<6:29:26,  7.61s/it]

 20%|██        | 773/3844 [1:36:43<6:34:29,  7.71s/it]


 20%|██        | 775/3844 [1:36:57<6:11:16,  7.26s/it]
{'loss': 1.3359, 'grad_norm': 0.3316983713476545, 'learning_rate': 1.849720942101568e-05, 'epoch': 0.2}


 20%|██        | 777/3844 [1:37:13<6:33:10,  7.69s/it]
{'loss': 1.2716, 'grad_norm': 0.303081529719095, 'learning_rate': 1.8488311356812186e-05, 'epoch': 0.2}

 20%|██        | 778/3844 [1:37:18<5:59:31,  7.04s/it]

 20%|██        | 779/3844 [1:37:24<5:45:48,  6.77s/it]

 20%|██        | 780/3844 [1:37:30<5:32:28,  6.51s/it]

 20%|██        | 781/3844 [1:37:36<5:29:04,  6.45s/it]

 20%|██        | 782/3844 [1:37:43<5:35:10,  6.57s/it]


 20%|██        | 784/3844 [1:37:59<6:11:08,  7.28s/it]

 20%|██        | 785/3844 [1:38:05<5:54:26,  6.95s/it]
{'loss': 1.3044, 'grad_norm': 0.30892756325329146, 'learning_rate': 1.8452478236062465e-05, 'epoch': 0.2}

 20%|██        | 786/3844 [1:38:14<6:25:02,  7.55s/it]

 20%|██        | 787/3844 [1:38:20<6:02:55,  7.12s/it]

 20%|██        | 788/3844 [1:38:30<6:47:50,  8.01s/it]

 21%|██        | 789/3844 [1:38:38<6:40:24,  7.86s/it]

 21%|██        | 790/3844 [1:38:44<6:14:02,  7.35s/it]

 21%|██        | 791/3844 [1:38:51<6:02:43,  7.13s/it]

 21%|██        | 792/3844 [1:38:56<5:35:42,  6.60s/it]

 21%|██        | 793/3844 [1:39:02<5:26:48,  6.43s/it]


 21%|██        | 795/3844 [1:39:23<7:19:19,  8.65s/it]

 21%|██        | 796/3844 [1:39:33<7:41:22,  9.08s/it]
{'loss': 1.1943, 'grad_norm': 0.30606605100534373, 'learning_rate': 1.8402580771807593e-05, 'epoch': 0.21}


 21%|██        | 798/3844 [1:39:47<6:46:32,  8.01s/it]
{'loss': 1.2999, 'grad_norm': 0.2820386708103177, 'learning_rate': 1.839343080768971e-05, 'epoch': 0.21}

 21%|██        | 799/3844 [1:39:53<6:26:24,  7.61s/it]

 21%|██        | 800/3844 [1:40:02<6:42:35,  7.94s/it]

 21%|██        | 801/3844 [1:40:10<6:35:10,  7.79s/it]

 21%|██        | 802/3844 [1:40:18<6:38:57,  7.87s/it]

 21%|██        | 803/3844 [1:40:25<6:26:56,  7.63s/it]

 21%|██        | 804/3844 [1:40:39<8:11:26,  9.70s/it]

 21%|██        | 805/3844 [1:40:47<7:36:48,  9.02s/it]

 21%|██        | 806/3844 [1:40:54<7:02:48,  8.35s/it]

 21%|██        | 807/3844 [1:41:04<7:42:30,  9.14s/it]

 21%|██        | 808/3844 [1:41:13<7:25:50,  8.81s/it]

 21%|██        | 809/3844 [1:41:22<7:35:01,  9.00s/it]


 21%|██        | 811/3844 [1:41:35<6:34:04,  7.80s/it]
{'loss': 1.2579, 'grad_norm': 0.30647865509637345, 'learning_rate': 1.8333376053850103e-05, 'epoch': 0.21}


 21%|██        | 813/3844 [1:41:53<6:59:18,  8.30s/it]
{'loss': 1.1581, 'grad_norm': 0.3135288310994048, 'learning_rate': 1.832404791102161e-05, 'epoch': 0.21}

 21%|██        | 814/3844 [1:42:00<6:31:06,  7.74s/it]

 21%|██        | 815/3844 [1:42:12<7:37:46,  9.07s/it]

 21%|██        | 816/3844 [1:42:18<6:54:50,  8.22s/it]

 21%|██▏       | 817/3844 [1:42:24<6:23:55,  7.61s/it]

 21%|██▏       | 818/3844 [1:42:30<5:56:17,  7.06s/it]

 21%|██▏       | 819/3844 [1:42:36<5:38:31,  6.71s/it]

 21%|██▏       | 820/3844 [1:42:42<5:25:55,  6.47s/it]

 21%|██▏       | 821/3844 [1:42:48<5:24:33,  6.44s/it]

 21%|██▏       | 822/3844 [1:42:53<5:07:20,  6.10s/it]

 21%|██▏       | 823/3844 [1:43:05<6:23:24,  7.61s/it]

 21%|██▏       | 824/3844 [1:43:11<6:11:40,  7.38s/it]

 21%|██▏       | 825/3844 [1:43:18<5:56:22,  7.08s/it]

 21%|██▏       | 826/3844 [1:43:24<5:37:30,  6.71s/it]


 22%|██▏       | 828/3844 [1:43:39<6:07:54,  7.32s/it]
{'loss': 1.0555, 'grad_norm': 0.2788361040004876, 'learning_rate': 1.8253334991067583e-05, 'epoch': 0.22}

 22%|██▏       | 829/3844 [1:43:45<5:39:22,  6.75s/it]

 22%|██▏       | 830/3844 [1:43:50<5:23:26,  6.44s/it]

 22%|██▏       | 831/3844 [1:43:57<5:29:17,  6.56s/it]

 22%|██▏       | 832/3844 [1:44:04<5:37:37,  6.73s/it]

 22%|██▏       | 833/3844 [1:44:10<5:23:00,  6.44s/it]

 22%|██▏       | 834/3844 [1:44:17<5:26:22,  6.51s/it]

 22%|██▏       | 835/3844 [1:44:22<5:14:58,  6.28s/it]

 22%|██▏       | 836/3844 [1:44:28<5:03:05,  6.05s/it]

 22%|██▏       | 837/3844 [1:44:34<4:59:49,  5.98s/it]

 22%|██▏       | 838/3844 [1:44:44<5:58:46,  7.16s/it]

 22%|██▏       | 839/3844 [1:44:50<5:43:50,  6.87s/it]

 22%|██▏       | 840/3844 [1:45:01<6:50:47,  8.20s/it]

 22%|██▏       | 841/3844 [1:45:07<6:10:01,  7.39s/it]

 22%|██▏       | 842/3844 [1:45:12<5:45:49,  6.91s/it]

 22%|██▏       | 843/3844 [1:45:18<5:31:20,  6.62s/it]

 22%|██▏       | 844/3844 [1:45:26<5:45:33,  6.91s/it]

 22%|██▏       | 845/3844 [1:45:32<5:30:28,  6.61s/it]

 22%|██▏       | 846/3844 [1:45:39<5:35:39,  6.72s/it]

 22%|██▏       | 847/3844 [1:45:48<6:12:46,  7.46s/it]

 22%|██▏       | 848/3844 [1:45:55<6:02:55,  7.27s/it]


 22%|██▏       | 850/3844 [1:46:09<6:05:54,  7.33s/it]
{'loss': 1.0706, 'grad_norm': 0.2862086838904499, 'learning_rate': 1.8147240512634207e-05, 'epoch': 0.22}


 22%|██▏       | 852/3844 [1:46:28<7:09:06,  8.61s/it]

 22%|██▏       | 853/3844 [1:46:35<6:54:37,  8.32s/it]
{'loss': 1.3047, 'grad_norm': 0.3140992827059492, 'learning_rate': 1.8132555300084004e-05, 'epoch': 0.22}

 22%|██▏       | 854/3844 [1:46:44<6:57:10,  8.37s/it]

 22%|██▏       | 855/3844 [1:46:52<6:54:13,  8.31s/it]

 22%|██▏       | 856/3844 [1:47:02<7:20:40,  8.85s/it]

 22%|██▏       | 857/3844 [1:47:10<7:05:24,  8.55s/it]

 22%|██▏       | 858/3844 [1:47:21<7:46:52,  9.38s/it]

 22%|██▏       | 859/3844 [1:47:28<7:11:28,  8.67s/it]


 22%|██▏       | 861/3844 [1:47:43<6:40:07,  8.05s/it]

 22%|██▏       | 862/3844 [1:47:50<6:13:28,  7.51s/it]
{'loss': 1.1983, 'grad_norm': 0.3203754085368972, 'learning_rate': 1.8088188173193624e-05, 'epoch': 0.22}

 22%|██▏       | 863/3844 [1:47:56<5:53:19,  7.11s/it]

 22%|██▏       | 864/3844 [1:48:04<6:01:42,  7.28s/it]

 23%|██▎       | 865/3844 [1:48:10<5:47:57,  7.01s/it]

 23%|██▎       | 866/3844 [1:48:16<5:36:29,  6.78s/it]

 23%|██▎       | 867/3844 [1:48:22<5:27:43,  6.61s/it]

 23%|██▎       | 868/3844 [1:48:28<5:16:25,  6.38s/it]

 23%|██▎       | 869/3844 [1:48:34<5:03:53,  6.13s/it]

 23%|██▎       | 870/3844 [1:48:40<5:02:17,  6.10s/it]

 23%|██▎       | 871/3844 [1:48:46<5:00:44,  6.07s/it]

 23%|██▎       | 872/3844 [1:48:55<5:46:24,  6.99s/it]

 23%|██▎       | 873/3844 [1:49:01<5:33:34,  6.74s/it]

 23%|██▎       | 874/3844 [1:49:08<5:34:16,  6.75s/it]

 23%|██▎       | 875/3844 [1:49:14<5:26:16,  6.59s/it]

 23%|██▎       | 876/3844 [1:49:21<5:33:18,  6.74s/it]

 23%|██▎       | 877/3844 [1:49:27<5:20:43,  6.49s/it]

 23%|██▎       | 878/3844 [1:49:36<5:51:28,  7.11s/it]

 23%|██▎       | 879/3844 [1:49:41<5:30:18,  6.68s/it]

 23%|██▎       | 880/3844 [1:49:48<5:28:56,  6.66s/it]

 23%|██▎       | 881/3844 [1:49:58<6:13:58,  7.57s/it]

 23%|██▎       | 882/3844 [1:50:06<6:30:55,  7.92s/it]

 23%|██▎       | 883/3844 [1:50:16<6:51:25,  8.34s/it]

 23%|██▎       | 884/3844 [1:50:22<6:21:29,  7.73s/it]

 23%|██▎       | 885/3844 [1:50:32<6:56:35,  8.45s/it]

 23%|██▎       | 886/3844 [1:50:39<6:28:21,  7.88s/it]

 23%|██▎       | 887/3844 [1:50:47<6:30:37,  7.93s/it]

 23%|██▎       | 888/3844 [1:50:57<7:02:19,  8.57s/it]

 23%|██▎       | 889/3844 [1:51:05<6:51:23,  8.35s/it]

 23%|██▎       | 890/3844 [1:51:10<6:09:33,  7.51s/it]

 23%|██▎       | 891/3844 [1:51:16<5:39:59,  6.91s/it]

 23%|██▎       | 892/3844 [1:51:22<5:31:47,  6.74s/it]

 23%|██▎       | 893/3844 [1:51:29<5:38:21,  6.88s/it]

 23%|██▎       | 894/3844 [1:51:35<5:26:24,  6.64s/it]

 23%|██▎       | 895/3844 [1:51:42<5:23:43,  6.59s/it]

 23%|██▎       | 896/3844 [1:51:48<5:17:09,  6.46s/it]

 23%|██▎       | 897/3844 [1:51:58<6:14:23,  7.62s/it]

 23%|██▎       | 898/3844 [1:52:05<5:58:24,  7.30s/it]

 23%|██▎       | 899/3844 [1:52:15<6:37:21,  8.10s/it]

 23%|██▎       | 900/3844 [1:52:22<6:17:55,  7.70s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1744, 'grad_norm': 0.3351819815578476, 'learning_rate': 1.789058776377008e-05, 'epoch': 0.23}
 23%|██▎       | 901/3844 [1:52:49<11:12:55, 13.72s/it]

 23%|██▎       | 902/3844 [1:53:00<10:23:44, 12.72s/it]

 23%|██▎       | 903/3844 [1:53:07<9:10:24, 11.23s/it]

 24%|██▎       | 904/3844 [1:53:16<8:34:50, 10.51s/it]

 24%|██▎       | 905/3844 [1:53:23<7:43:43,  9.47s/it]

 24%|██▎       | 906/3844 [1:53:29<6:45:52,  8.29s/it]

 24%|██▎       | 907/3844 [1:53:34<6:06:34,  7.49s/it]

 24%|██▎       | 908/3844 [1:53:48<7:39:44,  9.40s/it]

 24%|██▎       | 909/3844 [1:53:55<7:02:12,  8.63s/it]

 24%|██▎       | 910/3844 [1:54:01<6:14:29,  7.66s/it]

 24%|██▎       | 911/3844 [1:54:07<6:01:16,  7.39s/it]

 24%|██▎       | 912/3844 [1:54:13<5:36:32,  6.89s/it]

 24%|██▍       | 913/3844 [1:54:18<5:13:58,  6.43s/it]

 24%|██▍       | 914/3844 [1:54:24<5:01:29,  6.17s/it]

 24%|██▍       | 915/3844 [1:54:30<4:59:16,  6.13s/it]

 24%|██▍       | 916/3844 [1:54:38<5:20:33,  6.57s/it]

 24%|██▍       | 917/3844 [1:54:46<5:44:50,  7.07s/it]

 24%|██▍       | 918/3844 [1:54:53<5:50:26,  7.19s/it]

 24%|██▍       | 919/3844 [1:55:02<6:15:10,  7.70s/it]

 24%|██▍       | 920/3844 [1:55:10<6:20:17,  7.80s/it]

 24%|██▍       | 921/3844 [1:55:16<5:56:26,  7.32s/it]

 24%|██▍       | 922/3844 [1:55:22<5:29:25,  6.76s/it]

 24%|██▍       | 923/3844 [1:55:32<6:23:48,  7.88s/it]

 24%|██▍       | 924/3844 [1:55:41<6:29:55,  8.01s/it]

 24%|██▍       | 925/3844 [1:55:47<5:59:37,  7.39s/it]

 24%|██▍       | 926/3844 [1:55:53<5:46:00,  7.11s/it]

 24%|██▍       | 927/3844 [1:56:02<6:04:42,  7.50s/it]

 24%|██▍       | 928/3844 [1:56:09<5:57:11,  7.35s/it]

 24%|██▍       | 929/3844 [1:56:14<5:29:09,  6.78s/it]

 24%|██▍       | 930/3844 [1:56:22<5:47:58,  7.17s/it]

 24%|██▍       | 931/3844 [1:56:29<5:44:04,  7.09s/it]

 24%|██▍       | 932/3844 [1:56:36<5:38:02,  6.97s/it]

 24%|██▍       | 933/3844 [1:56:44<5:51:53,  7.25s/it]

 24%|██▍       | 934/3844 [1:56:52<6:09:25,  7.62s/it]

 24%|██▍       | 935/3844 [1:56:58<5:38:57,  6.99s/it]

 24%|██▍       | 936/3844 [1:57:08<6:23:14,  7.91s/it]

 24%|██▍       | 937/3844 [1:57:14<6:04:21,  7.52s/it]

 24%|██▍       | 938/3844 [1:57:20<5:37:25,  6.97s/it]

 24%|██▍       | 939/3844 [1:57:26<5:28:29,  6.78s/it]

 24%|██▍       | 940/3844 [1:57:32<5:17:57,  6.57s/it]

 24%|██▍       | 941/3844 [1:57:38<5:08:21,  6.37s/it]

 25%|██▍       | 942/3844 [1:57:44<4:58:23,  6.17s/it]

 25%|██▍       | 943/3844 [1:57:52<5:28:59,  6.80s/it]

 25%|██▍       | 944/3844 [1:57:58<5:15:28,  6.53s/it]

 25%|██▍       | 945/3844 [1:58:04<5:00:42,  6.22s/it]

 25%|██▍       | 946/3844 [1:58:13<5:43:30,  7.11s/it]

 25%|██▍       | 947/3844 [1:58:21<5:56:38,  7.39s/it]

 25%|██▍       | 948/3844 [1:58:26<5:30:15,  6.84s/it]

 25%|██▍       | 949/3844 [1:58:36<6:10:35,  7.68s/it]

 25%|██▍       | 950/3844 [1:58:42<5:46:38,  7.19s/it]

 25%|██▍       | 951/3844 [1:58:50<5:59:06,  7.45s/it]

 25%|██▍       | 952/3844 [1:59:00<6:27:24,  8.04s/it]

 25%|██▍       | 953/3844 [1:59:06<6:09:23,  7.67s/it]

 25%|██▍       | 954/3844 [1:59:13<5:59:01,  7.45s/it]

 25%|██▍       | 955/3844 [1:59:22<6:19:17,  7.88s/it]

 25%|██▍       | 956/3844 [1:59:29<6:00:41,  7.49s/it]

 25%|██▍       | 957/3844 [1:59:34<5:33:34,  6.93s/it]

 25%|██▍       | 958/3844 [1:59:40<5:17:18,  6.60s/it]

 25%|██▍       | 959/3844 [1:59:47<5:24:18,  6.74s/it]

 25%|██▍       | 960/3844 [1:59:56<5:47:02,  7.22s/it]

 25%|██▌       | 961/3844 [2:00:03<5:52:27,  7.34s/it]

 25%|██▌       | 962/3844 [2:00:09<5:37:06,  7.02s/it]

 25%|██▌       | 963/3844 [2:00:22<6:57:58,  8.70s/it]

 25%|██▌       | 964/3844 [2:00:30<6:39:00,  8.31s/it]

 25%|██▌       | 965/3844 [2:00:37<6:23:53,  8.00s/it]

 25%|██▌       | 966/3844 [2:00:42<5:46:17,  7.22s/it]

 25%|██▌       | 967/3844 [2:00:48<5:21:25,  6.70s/it]

 25%|██▌       | 968/3844 [2:00:56<5:37:24,  7.04s/it]

 25%|██▌       | 969/3844 [2:01:01<5:22:04,  6.72s/it]

 25%|██▌       | 970/3844 [2:01:13<6:32:40,  8.20s/it]

 25%|██▌       | 971/3844 [2:01:19<5:58:24,  7.49s/it]

 25%|██▌       | 972/3844 [2:01:27<6:00:45,  7.54s/it]

 25%|██▌       | 973/3844 [2:01:32<5:35:31,  7.01s/it]

 25%|██▌       | 974/3844 [2:01:40<5:41:58,  7.15s/it]

 25%|██▌       | 975/3844 [2:01:47<5:36:44,  7.04s/it]

 25%|██▌       | 976/3844 [2:01:53<5:23:49,  6.77s/it]

 25%|██▌       | 977/3844 [2:02:01<5:42:49,  7.17s/it]

 25%|██▌       | 978/3844 [2:02:08<5:36:58,  7.05s/it]

 25%|██▌       | 979/3844 [2:02:17<6:07:45,  7.70s/it]

 25%|██▌       | 980/3844 [2:02:24<5:54:53,  7.43s/it]

 26%|██▌       | 981/3844 [2:02:29<5:28:24,  6.88s/it]

 26%|██▌       | 982/3844 [2:02:38<5:47:32,  7.29s/it]

 26%|██▌       | 983/3844 [2:02:43<5:26:48,  6.85s/it]

 26%|██▌       | 984/3844 [2:02:52<5:47:24,  7.29s/it]

 26%|██▌       | 985/3844 [2:02:58<5:27:46,  6.88s/it]

 26%|██▌       | 986/3844 [2:03:04<5:26:02,  6.84s/it]

 26%|██▌       | 987/3844 [2:03:13<5:49:22,  7.34s/it]

 26%|██▌       | 988/3844 [2:03:22<6:08:39,  7.75s/it]

 26%|██▌       | 989/3844 [2:03:29<6:04:32,  7.66s/it]

 26%|██▌       | 990/3844 [2:03:35<5:46:52,  7.29s/it]

 26%|██▌       | 991/3844 [2:03:42<5:42:22,  7.20s/it]

 26%|██▌       | 992/3844 [2:03:49<5:30:47,  6.96s/it]

 26%|██▌       | 993/3844 [2:03:57<5:52:19,  7.41s/it]

 26%|██▌       | 994/3844 [2:04:04<5:38:25,  7.12s/it]

 26%|██▌       | 995/3844 [2:04:09<5:14:56,  6.63s/it]

 26%|██▌       | 996/3844 [2:04:15<5:02:01,  6.36s/it]

 26%|██▌       | 997/3844 [2:04:22<5:08:40,  6.51s/it]

 26%|██▌       | 998/3844 [2:04:27<4:56:40,  6.25s/it]

 26%|██▌       | 999/3844 [2:04:37<5:41:04,  7.19s/it]

 26%|██▌       | 1000/3844 [2:04:43<5:21:00,  6.77s/it]

 26%|██▌       | 1001/3844 [2:04:49<5:10:18,  6.55s/it]

 26%|██▌       | 1002/3844 [2:04:57<5:28:40,  6.94s/it]

 26%|██▌       | 1003/3844 [2:05:06<5:59:08,  7.58s/it]

 26%|██▌       | 1004/3844 [2:05:12<5:37:52,  7.14s/it]

 26%|██▌       | 1005/3844 [2:05:18<5:31:19,  7.00s/it]

 26%|██▌       | 1006/3844 [2:05:27<5:46:48,  7.33s/it]

 26%|██▌       | 1007/3844 [2:05:34<5:42:53,  7.25s/it]

 26%|██▌       | 1008/3844 [2:05:39<5:21:09,  6.79s/it]

 26%|██▌       | 1009/3844 [2:05:46<5:15:49,  6.68s/it]

 26%|██▋       | 1010/3844 [2:05:54<5:43:58,  7.28s/it]

 26%|██▋       | 1011/3844 [2:06:03<6:01:15,  7.65s/it]

 26%|██▋       | 1012/3844 [2:06:11<6:03:50,  7.71s/it]

 26%|██▋       | 1013/3844 [2:06:19<6:12:08,  7.89s/it]

 26%|██▋       | 1014/3844 [2:06:25<5:47:21,  7.36s/it]

 26%|██▋       | 1015/3844 [2:06:31<5:26:43,  6.93s/it]

 26%|██▋       | 1016/3844 [2:06:37<5:15:09,  6.69s/it]

 26%|██▋       | 1017/3844 [2:06:44<5:18:20,  6.76s/it]

 26%|██▋       | 1018/3844 [2:06:52<5:30:58,  7.03s/it]

 27%|██▋       | 1019/3844 [2:06:59<5:39:14,  7.21s/it]

 27%|██▋       | 1020/3844 [2:07:06<5:25:57,  6.93s/it]

 27%|██▋       | 1021/3844 [2:07:14<5:45:12,  7.34s/it]

 27%|██▋       | 1022/3844 [2:07:24<6:18:23,  8.05s/it]
[2024-05-26 13:14:08,318] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 1023/3844 [2:07:36<7:11:03,  9.17s/it]

 27%|██▋       | 1024/3844 [2:07:42<6:31:49,  8.34s/it]

 27%|██▋       | 1025/3844 [2:07:48<5:56:24,  7.59s/it]

 27%|██▋       | 1026/3844 [2:07:54<5:44:42,  7.34s/it]

 27%|██▋       | 1027/3844 [2:08:00<5:19:17,  6.80s/it]

 27%|██▋       | 1028/3844 [2:08:08<5:38:34,  7.21s/it]

 27%|██▋       | 1029/3844 [2:08:15<5:38:58,  7.23s/it]

 27%|██▋       | 1030/3844 [2:08:24<6:02:12,  7.72s/it]

 27%|██▋       | 1031/3844 [2:08:30<5:31:28,  7.07s/it]

 27%|██▋       | 1032/3844 [2:08:36<5:11:27,  6.65s/it]

 27%|██▋       | 1033/3844 [2:08:41<4:55:57,  6.32s/it]

 27%|██▋       | 1034/3844 [2:08:48<5:02:29,  6.46s/it]

 27%|██▋       | 1035/3844 [2:08:53<4:48:18,  6.16s/it]

 27%|██▋       | 1036/3844 [2:08:59<4:43:14,  6.05s/it]

 27%|██▋       | 1037/3844 [2:09:05<4:45:09,  6.10s/it]

 27%|██▋       | 1038/3844 [2:09:12<4:57:56,  6.37s/it]

 27%|██▋       | 1039/3844 [2:09:20<5:14:10,  6.72s/it]

 27%|██▋       | 1040/3844 [2:09:27<5:12:28,  6.69s/it]

 27%|██▋       | 1041/3844 [2:09:35<5:33:03,  7.13s/it]

 27%|██▋       | 1042/3844 [2:09:41<5:17:01,  6.79s/it]

 27%|██▋       | 1043/3844 [2:09:50<5:58:26,  7.68s/it]

 27%|██▋       | 1044/3844 [2:09:58<5:58:17,  7.68s/it]

 27%|██▋       | 1045/3844 [2:10:08<6:28:39,  8.33s/it]

 27%|██▋       | 1046/3844 [2:10:14<5:50:28,  7.52s/it]

 27%|██▋       | 1047/3844 [2:10:19<5:24:28,  6.96s/it]

 27%|██▋       | 1048/3844 [2:10:27<5:34:05,  7.17s/it]

 27%|██▋       | 1049/3844 [2:10:34<5:26:47,  7.02s/it]

 27%|██▋       | 1050/3844 [2:10:42<5:51:37,  7.55s/it]

 27%|██▋       | 1051/3844 [2:10:48<5:28:27,  7.06s/it]

 27%|██▋       | 1052/3844 [2:10:54<5:05:27,  6.56s/it]

 27%|██▋       | 1053/3844 [2:11:03<5:41:29,  7.34s/it]

 27%|██▋       | 1054/3844 [2:11:11<5:49:21,  7.51s/it]

 27%|██▋       | 1055/3844 [2:11:17<5:26:26,  7.02s/it]

 27%|██▋       | 1056/3844 [2:11:23<5:16:12,  6.81s/it]

 27%|██▋       | 1057/3844 [2:11:32<5:42:36,  7.38s/it]

 28%|██▊       | 1058/3844 [2:11:38<5:26:50,  7.04s/it]

 28%|██▊       | 1059/3844 [2:11:47<5:54:52,  7.65s/it]

 28%|██▊       | 1060/3844 [2:11:56<6:10:36,  7.99s/it]

 28%|██▊       | 1061/3844 [2:12:03<5:55:31,  7.66s/it]

 28%|██▊       | 1062/3844 [2:12:09<5:42:14,  7.38s/it]

 28%|██▊       | 1063/3844 [2:12:18<5:58:51,  7.74s/it]

 28%|██▊       | 1064/3844 [2:12:27<6:12:58,  8.05s/it]

 28%|██▊       | 1065/3844 [2:12:35<6:12:10,  8.04s/it]

 28%|██▊       | 1066/3844 [2:12:44<6:30:19,  8.43s/it]

 28%|██▊       | 1067/3844 [2:12:50<5:53:54,  7.65s/it]

 28%|██▊       | 1068/3844 [2:13:01<6:43:18,  8.72s/it]

 28%|██▊       | 1069/3844 [2:13:07<6:09:25,  7.99s/it]

 28%|██▊       | 1070/3844 [2:13:13<5:40:44,  7.37s/it]

 28%|██▊       | 1071/3844 [2:13:20<5:31:06,  7.16s/it]

 28%|██▊       | 1072/3844 [2:13:27<5:28:17,  7.11s/it]

 28%|██▊       | 1073/3844 [2:13:33<5:17:33,  6.88s/it]

 28%|██▊       | 1074/3844 [2:13:39<5:04:08,  6.59s/it]

 28%|██▊       | 1075/3844 [2:13:51<6:22:50,  8.30s/it]

 28%|██▊       | 1076/3844 [2:14:01<6:44:08,  8.76s/it]

 28%|██▊       | 1077/3844 [2:14:07<6:03:16,  7.88s/it]

 28%|██▊       | 1078/3844 [2:14:14<5:47:51,  7.55s/it]

 28%|██▊       | 1079/3844 [2:14:22<5:53:39,  7.67s/it]

 28%|██▊       | 1080/3844 [2:14:31<6:08:35,  8.00s/it]

 28%|██▊       | 1081/3844 [2:14:40<6:26:57,  8.40s/it]

 28%|██▊       | 1082/3844 [2:14:49<6:35:48,  8.60s/it]

 28%|██▊       | 1083/3844 [2:14:56<6:06:38,  7.97s/it]

 28%|██▊       | 1084/3844 [2:15:03<6:02:50,  7.89s/it]

 28%|██▊       | 1085/3844 [2:15:10<5:50:39,  7.63s/it]

 28%|██▊       | 1086/3844 [2:15:18<5:52:42,  7.67s/it]

 28%|██▊       | 1087/3844 [2:15:24<5:24:22,  7.06s/it]

 28%|██▊       | 1088/3844 [2:15:31<5:26:32,  7.11s/it]

 28%|██▊       | 1089/3844 [2:15:37<5:09:34,  6.74s/it]

 28%|██▊       | 1090/3844 [2:15:43<5:01:34,  6.57s/it]

 28%|██▊       | 1091/3844 [2:15:51<5:24:03,  7.06s/it]

 28%|██▊       | 1092/3844 [2:15:59<5:31:21,  7.22s/it]

 28%|██▊       | 1093/3844 [2:16:08<6:03:54,  7.94s/it]

 28%|██▊       | 1094/3844 [2:16:17<6:13:06,  8.14s/it]

 28%|██▊       | 1095/3844 [2:16:23<5:39:45,  7.42s/it]

 29%|██▊       | 1096/3844 [2:16:29<5:20:28,  7.00s/it]

 29%|██▊       | 1097/3844 [2:16:35<5:11:15,  6.80s/it]

 29%|██▊       | 1098/3844 [2:16:41<4:54:56,  6.44s/it]

 29%|██▊       | 1099/3844 [2:16:47<4:46:23,  6.26s/it]

 29%|██▊       | 1100/3844 [2:16:53<4:48:25,  6.31s/it]

 29%|██▊       | 1101/3844 [2:17:00<4:58:55,  6.54s/it]

 29%|██▊       | 1102/3844 [2:17:06<4:53:31,  6.42s/it]

 29%|██▊       | 1103/3844 [2:17:14<5:10:15,  6.79s/it]

 29%|██▊       | 1104/3844 [2:17:21<5:09:59,  6.79s/it]

 29%|██▊       | 1105/3844 [2:17:31<6:02:25,  7.94s/it]

 29%|██▉       | 1106/3844 [2:17:39<6:06:22,  8.03s/it]

 29%|██▉       | 1107/3844 [2:17:47<5:55:28,  7.79s/it]

 29%|██▉       | 1108/3844 [2:17:54<5:50:33,  7.69s/it]

 29%|██▉       | 1109/3844 [2:18:01<5:44:29,  7.56s/it]

 29%|██▉       | 1110/3844 [2:18:08<5:30:39,  7.26s/it]

 29%|██▉       | 1111/3844 [2:18:15<5:31:24,  7.28s/it]

 29%|██▉       | 1112/3844 [2:18:22<5:21:23,  7.06s/it]

 29%|██▉       | 1113/3844 [2:18:28<5:15:44,  6.94s/it]

 29%|██▉       | 1114/3844 [2:18:38<5:45:23,  7.59s/it]

 29%|██▉       | 1115/3844 [2:18:44<5:22:41,  7.09s/it]

 29%|██▉       | 1116/3844 [2:18:57<6:49:59,  9.02s/it]

 29%|██▉       | 1117/3844 [2:19:04<6:22:15,  8.41s/it]

 29%|██▉       | 1118/3844 [2:19:14<6:49:54,  9.02s/it]

 29%|██▉       | 1119/3844 [2:19:21<6:11:39,  8.18s/it]

 29%|██▉       | 1120/3844 [2:19:27<5:50:09,  7.71s/it]

 29%|██▉       | 1121/3844 [2:19:34<5:41:30,  7.53s/it]

 29%|██▉       | 1122/3844 [2:19:43<5:53:53,  7.80s/it]

 29%|██▉       | 1123/3844 [2:19:49<5:37:17,  7.44s/it]

 29%|██▉       | 1124/3844 [2:19:56<5:31:29,  7.31s/it]

 29%|██▉       | 1125/3844 [2:20:05<5:45:15,  7.62s/it]

 29%|██▉       | 1126/3844 [2:20:11<5:23:01,  7.13s/it]

 29%|██▉       | 1127/3844 [2:20:19<5:32:48,  7.35s/it]

 29%|██▉       | 1128/3844 [2:20:24<5:05:59,  6.76s/it]

 29%|██▉       | 1129/3844 [2:20:31<5:14:11,  6.94s/it]

 29%|██▉       | 1130/3844 [2:20:38<5:03:44,  6.71s/it]

 29%|██▉       | 1131/3844 [2:20:43<4:51:05,  6.44s/it]

 29%|██▉       | 1132/3844 [2:20:49<4:42:10,  6.24s/it]

 29%|██▉       | 1133/3844 [2:20:55<4:35:03,  6.09s/it]

 30%|██▉       | 1134/3844 [2:21:04<5:15:19,  6.98s/it]

 30%|██▉       | 1135/3844 [2:21:15<6:11:54,  8.24s/it]

 30%|██▉       | 1136/3844 [2:21:26<6:49:53,  9.08s/it]

 30%|██▉       | 1137/3844 [2:21:34<6:27:04,  8.58s/it]

 30%|██▉       | 1138/3844 [2:21:41<6:17:18,  8.37s/it]

 30%|██▉       | 1139/3844 [2:21:54<7:09:15,  9.52s/it]

 30%|██▉       | 1140/3844 [2:22:01<6:34:31,  8.75s/it]

 30%|██▉       | 1141/3844 [2:22:06<5:52:21,  7.82s/it]

 30%|██▉       | 1142/3844 [2:22:17<6:36:02,  8.79s/it]

 30%|██▉       | 1143/3844 [2:22:25<6:17:58,  8.40s/it]

 30%|██▉       | 1144/3844 [2:22:33<6:08:34,  8.19s/it]

 30%|██▉       | 1145/3844 [2:22:39<5:42:31,  7.61s/it]

 30%|██▉       | 1146/3844 [2:22:45<5:29:49,  7.33s/it]

 30%|██▉       | 1147/3844 [2:22:54<5:39:51,  7.56s/it]

 30%|██▉       | 1148/3844 [2:23:01<5:40:02,  7.57s/it]

 30%|██▉       | 1149/3844 [2:23:07<5:17:04,  7.06s/it]

 30%|██▉       | 1150/3844 [2:23:13<5:05:53,  6.81s/it]

 30%|██▉       | 1151/3844 [2:23:19<4:47:47,  6.41s/it]

 30%|██▉       | 1152/3844 [2:23:25<4:39:33,  6.23s/it]

 30%|██▉       | 1153/3844 [2:23:35<5:34:03,  7.45s/it]

 30%|███       | 1154/3844 [2:23:44<5:54:26,  7.91s/it]

 30%|███       | 1155/3844 [2:23:49<5:19:31,  7.13s/it]

 30%|███       | 1156/3844 [2:23:55<4:57:37,  6.64s/it]

 30%|███       | 1157/3844 [2:24:04<5:34:05,  7.46s/it]

 30%|███       | 1158/3844 [2:24:10<5:16:15,  7.06s/it]

 30%|███       | 1159/3844 [2:24:20<6:00:24,  8.05s/it]

 30%|███       | 1160/3844 [2:24:29<6:10:08,  8.27s/it]

 30%|███       | 1161/3844 [2:24:35<5:37:08,  7.54s/it]

 30%|███       | 1162/3844 [2:24:42<5:22:09,  7.21s/it]

 30%|███       | 1163/3844 [2:24:50<5:36:40,  7.53s/it]

 30%|███       | 1164/3844 [2:24:58<5:46:29,  7.76s/it]

 30%|███       | 1165/3844 [2:25:05<5:28:47,  7.36s/it]

 30%|███       | 1166/3844 [2:25:12<5:35:43,  7.52s/it]

 30%|███       | 1167/3844 [2:25:19<5:26:46,  7.32s/it]

 30%|███       | 1168/3844 [2:25:27<5:28:18,  7.36s/it]

 30%|███       | 1169/3844 [2:25:34<5:29:38,  7.39s/it]

 30%|███       | 1170/3844 [2:25:40<5:05:35,  6.86s/it]

 30%|███       | 1171/3844 [2:25:46<4:57:19,  6.67s/it]

 30%|███       | 1172/3844 [2:25:52<4:46:34,  6.44s/it]

 31%|███       | 1173/3844 [2:26:00<5:02:15,  6.79s/it]

 31%|███       | 1174/3844 [2:26:07<5:14:27,  7.07s/it]

 31%|███       | 1175/3844 [2:26:14<5:08:59,  6.95s/it]

 31%|███       | 1176/3844 [2:26:22<5:23:32,  7.28s/it]

 31%|███       | 1177/3844 [2:26:29<5:16:32,  7.12s/it]

 31%|███       | 1178/3844 [2:26:36<5:20:01,  7.20s/it]

 31%|███       | 1179/3844 [2:26:42<5:03:32,  6.83s/it]

 31%|███       | 1180/3844 [2:26:48<4:48:52,  6.51s/it]

 31%|███       | 1181/3844 [2:26:55<4:58:49,  6.73s/it]

 31%|███       | 1182/3844 [2:27:02<4:58:19,  6.72s/it]

 31%|███       | 1183/3844 [2:27:08<4:48:51,  6.51s/it]

 31%|███       | 1184/3844 [2:27:14<4:43:44,  6.40s/it]

 31%|███       | 1185/3844 [2:27:21<4:49:05,  6.52s/it]

 31%|███       | 1186/3844 [2:27:28<5:03:05,  6.84s/it]

 31%|███       | 1187/3844 [2:27:34<4:49:09,  6.53s/it]

 31%|███       | 1188/3844 [2:27:46<6:05:06,  8.25s/it]

 31%|███       | 1189/3844 [2:27:54<6:00:05,  8.14s/it]

 31%|███       | 1190/3844 [2:28:00<5:32:14,  7.51s/it]

 31%|███       | 1191/3844 [2:28:09<5:47:09,  7.85s/it]

 31%|███       | 1192/3844 [2:28:20<6:34:30,  8.93s/it]

 31%|███       | 1193/3844 [2:28:26<5:53:02,  7.99s/it]

 31%|███       | 1194/3844 [2:28:35<6:02:00,  8.20s/it]

 31%|███       | 1195/3844 [2:28:41<5:34:54,  7.59s/it]

 31%|███       | 1196/3844 [2:28:49<5:39:54,  7.70s/it]

 31%|███       | 1197/3844 [2:28:55<5:18:39,  7.22s/it]

 31%|███       | 1198/3844 [2:29:02<5:10:04,  7.03s/it]

 31%|███       | 1199/3844 [2:29:08<4:53:34,  6.66s/it]

 31%|███       | 1200/3844 [2:29:14<4:45:17,  6.47s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0893, 'grad_norm': 0.32961304978875544, 'learning_rate': 1.6103202253360718e-05, 'epoch': 0.31}
 31%|███       | 1201/3844 [2:29:43<9:49:55, 13.39s/it]

 31%|███▏      | 1202/3844 [2:29:50<8:20:45, 11.37s/it]

 31%|███▏      | 1203/3844 [2:29:57<7:20:23, 10.00s/it]

 31%|███▏      | 1204/3844 [2:30:03<6:26:39,  8.79s/it]

 31%|███▏      | 1205/3844 [2:30:11<6:26:29,  8.79s/it]

 31%|███▏      | 1206/3844 [2:30:17<5:48:00,  7.92s/it]

 31%|███▏      | 1207/3844 [2:30:27<6:17:17,  8.58s/it]

 31%|███▏      | 1208/3844 [2:30:34<5:56:07,  8.11s/it]

 31%|███▏      | 1209/3844 [2:30:44<6:12:46,  8.49s/it]

 31%|███▏      | 1210/3844 [2:30:51<5:59:13,  8.18s/it]

 32%|███▏      | 1211/3844 [2:30:58<5:35:43,  7.65s/it]

 32%|███▏      | 1212/3844 [2:31:06<5:39:26,  7.74s/it]

 32%|███▏      | 1213/3844 [2:31:13<5:38:36,  7.72s/it]

 32%|███▏      | 1214/3844 [2:31:19<5:18:48,  7.27s/it]

 32%|███▏      | 1215/3844 [2:31:26<5:02:36,  6.91s/it]

 32%|███▏      | 1216/3844 [2:31:32<5:00:01,  6.85s/it]

 32%|███▏      | 1217/3844 [2:31:40<5:11:15,  7.11s/it]

 32%|███▏      | 1218/3844 [2:31:47<5:05:06,  6.97s/it]

 32%|███▏      | 1219/3844 [2:31:54<5:06:38,  7.01s/it]

 32%|███▏      | 1220/3844 [2:32:00<4:52:21,  6.69s/it]

 32%|███▏      | 1221/3844 [2:32:08<5:12:22,  7.15s/it]

 32%|███▏      | 1222/3844 [2:32:14<4:55:16,  6.76s/it]

 32%|███▏      | 1223/3844 [2:32:20<4:54:08,  6.73s/it]

 32%|███▏      | 1224/3844 [2:32:27<4:53:38,  6.72s/it]

 32%|███▏      | 1225/3844 [2:32:37<5:38:33,  7.76s/it]

 32%|███▏      | 1226/3844 [2:32:43<5:12:53,  7.17s/it]

 32%|███▏      | 1227/3844 [2:32:52<5:41:27,  7.83s/it]


 32%|███▏      | 1229/3844 [2:33:09<5:41:41,  7.84s/it]
{'loss': 1.3058, 'grad_norm': 0.3134118102442189, 'learning_rate': 1.59146065852636e-05, 'epoch': 0.32}

 32%|███▏      | 1230/3844 [2:33:15<5:15:03,  7.23s/it]

 32%|███▏      | 1231/3844 [2:33:21<5:03:50,  6.98s/it]

 32%|███▏      | 1232/3844 [2:33:27<4:47:21,  6.60s/it]

 32%|███▏      | 1233/3844 [2:33:35<5:14:51,  7.24s/it]

 32%|███▏      | 1234/3844 [2:33:42<5:09:44,  7.12s/it]

 32%|███▏      | 1235/3844 [2:33:48<4:56:00,  6.81s/it]

 32%|███▏      | 1236/3844 [2:33:56<5:10:29,  7.14s/it]

 32%|███▏      | 1237/3844 [2:34:04<5:13:36,  7.22s/it]

 32%|███▏      | 1238/3844 [2:34:09<4:52:44,  6.74s/it]

 32%|███▏      | 1239/3844 [2:34:16<4:50:37,  6.69s/it]

 32%|███▏      | 1240/3844 [2:34:25<5:25:26,  7.50s/it]

 32%|███▏      | 1241/3844 [2:34:31<5:07:59,  7.10s/it]

 32%|███▏      | 1242/3844 [2:34:42<5:55:44,  8.20s/it]

 32%|███▏      | 1243/3844 [2:34:49<5:31:43,  7.65s/it]

 32%|███▏      | 1244/3844 [2:34:58<5:50:18,  8.08s/it]

 32%|███▏      | 1245/3844 [2:35:04<5:30:11,  7.62s/it]

 32%|███▏      | 1246/3844 [2:35:12<5:33:14,  7.70s/it]

 32%|███▏      | 1247/3844 [2:35:19<5:16:13,  7.31s/it]

 32%|███▏      | 1248/3844 [2:35:28<5:37:58,  7.81s/it]

 32%|███▏      | 1249/3844 [2:35:33<5:07:40,  7.11s/it]

 33%|███▎      | 1250/3844 [2:35:39<4:56:36,  6.86s/it]

 33%|███▎      | 1251/3844 [2:35:45<4:36:47,  6.40s/it]

 33%|███▎      | 1252/3844 [2:35:54<5:09:00,  7.15s/it]

 33%|███▎      | 1253/3844 [2:36:00<5:01:56,  6.99s/it]

 33%|███▎      | 1254/3844 [2:36:06<4:46:08,  6.63s/it]

 33%|███▎      | 1255/3844 [2:36:13<4:53:30,  6.80s/it]

 33%|███▎      | 1256/3844 [2:36:20<4:48:03,  6.68s/it]

 33%|███▎      | 1257/3844 [2:36:27<4:59:06,  6.94s/it]

 33%|███▎      | 1258/3844 [2:36:33<4:45:15,  6.62s/it]

 33%|███▎      | 1259/3844 [2:36:39<4:36:45,  6.42s/it]

 33%|███▎      | 1260/3844 [2:36:47<4:52:51,  6.80s/it]

 33%|███▎      | 1261/3844 [2:36:54<5:03:27,  7.05s/it]

 33%|███▎      | 1262/3844 [2:37:02<5:18:18,  7.40s/it]

 33%|███▎      | 1263/3844 [2:37:09<5:12:10,  7.26s/it]

 33%|███▎      | 1264/3844 [2:37:17<5:15:09,  7.33s/it]

 33%|███▎      | 1265/3844 [2:37:26<5:32:17,  7.73s/it]

 33%|███▎      | 1266/3844 [2:37:31<5:04:23,  7.08s/it]

 33%|███▎      | 1267/3844 [2:37:37<4:44:13,  6.62s/it]

 33%|███▎      | 1268/3844 [2:37:47<5:29:48,  7.68s/it]

 33%|███▎      | 1269/3844 [2:37:54<5:20:20,  7.46s/it]

 33%|███▎      | 1270/3844 [2:38:01<5:21:20,  7.49s/it]

 33%|███▎      | 1271/3844 [2:38:07<4:59:34,  6.99s/it]

 33%|███▎      | 1272/3844 [2:38:15<5:14:10,  7.33s/it]

 33%|███▎      | 1273/3844 [2:38:21<4:50:38,  6.78s/it]

 33%|███▎      | 1274/3844 [2:38:27<4:43:34,  6.62s/it]

 33%|███▎      | 1275/3844 [2:38:34<4:52:44,  6.84s/it]

 33%|███▎      | 1276/3844 [2:38:40<4:38:18,  6.50s/it]

 33%|███▎      | 1277/3844 [2:38:49<5:11:50,  7.29s/it]

 33%|███▎      | 1278/3844 [2:38:56<5:11:30,  7.28s/it]

 33%|███▎      | 1279/3844 [2:39:04<5:10:11,  7.26s/it]

 33%|███▎      | 1280/3844 [2:39:10<5:02:55,  7.09s/it]

 33%|███▎      | 1281/3844 [2:39:16<4:49:52,  6.79s/it]

 33%|███▎      | 1282/3844 [2:39:22<4:36:49,  6.48s/it]

 33%|███▎      | 1283/3844 [2:39:28<4:33:05,  6.40s/it]

 33%|███▎      | 1284/3844 [2:39:36<4:50:38,  6.81s/it]

 33%|███▎      | 1285/3844 [2:39:43<4:47:42,  6.75s/it]

 33%|███▎      | 1286/3844 [2:39:50<5:00:18,  7.04s/it]

 33%|███▎      | 1287/3844 [2:39:58<5:03:51,  7.13s/it]

 34%|███▎      | 1288/3844 [2:40:04<4:56:13,  6.95s/it]

 34%|███▎      | 1289/3844 [2:40:11<4:53:58,  6.90s/it]

 34%|███▎      | 1290/3844 [2:40:17<4:44:02,  6.67s/it]

 34%|███▎      | 1291/3844 [2:40:26<5:06:10,  7.20s/it]

 34%|███▎      | 1292/3844 [2:40:31<4:46:52,  6.74s/it]

 34%|███▎      | 1293/3844 [2:40:38<4:49:39,  6.81s/it]

 34%|███▎      | 1294/3844 [2:40:45<4:51:34,  6.86s/it]

 34%|███▎      | 1295/3844 [2:40:51<4:33:01,  6.43s/it]

 34%|███▎      | 1296/3844 [2:40:56<4:20:49,  6.14s/it]

 34%|███▎      | 1297/3844 [2:41:05<4:52:45,  6.90s/it]

 34%|███▍      | 1298/3844 [2:41:10<4:33:15,  6.44s/it]

 34%|███▍      | 1299/3844 [2:41:16<4:28:55,  6.34s/it]

 34%|███▍      | 1300/3844 [2:41:23<4:29:14,  6.35s/it]

 34%|███▍      | 1301/3844 [2:41:30<4:46:15,  6.75s/it]

 34%|███▍      | 1302/3844 [2:41:38<5:01:04,  7.11s/it]

 34%|███▍      | 1303/3844 [2:41:44<4:45:01,  6.73s/it]

 34%|███▍      | 1304/3844 [2:41:51<4:50:59,  6.87s/it]

 34%|███▍      | 1305/3844 [2:41:57<4:37:50,  6.57s/it]

 34%|███▍      | 1306/3844 [2:42:03<4:29:10,  6.36s/it]

 34%|███▍      | 1307/3844 [2:42:12<5:06:35,  7.25s/it]

 34%|███▍      | 1308/3844 [2:42:20<5:06:19,  7.25s/it]

 34%|███▍      | 1309/3844 [2:42:26<4:50:24,  6.87s/it]

 34%|███▍      | 1310/3844 [2:42:34<5:05:52,  7.24s/it]

 34%|███▍      | 1311/3844 [2:42:41<4:59:28,  7.09s/it]

 34%|███▍      | 1312/3844 [2:42:50<5:30:56,  7.84s/it]

 34%|███▍      | 1313/3844 [2:42:56<5:03:37,  7.20s/it]


 34%|███▍      | 1315/3844 [2:43:10<5:01:46,  7.16s/it]
{'loss': 1.2722, 'grad_norm': 0.32704316196894984, 'learning_rate': 1.531522311531326e-05, 'epoch': 0.34}

 34%|███▍      | 1316/3844 [2:43:17<5:01:12,  7.15s/it]

 34%|███▍      | 1317/3844 [2:43:24<5:02:39,  7.19s/it]

 34%|███▍      | 1318/3844 [2:43:32<5:17:44,  7.55s/it]

 34%|███▍      | 1319/3844 [2:43:39<5:06:59,  7.29s/it]

 34%|███▍      | 1320/3844 [2:43:46<4:59:18,  7.11s/it]

 34%|███▍      | 1321/3844 [2:43:55<5:20:38,  7.63s/it]

 34%|███▍      | 1322/3844 [2:44:00<4:54:31,  7.01s/it]

 34%|███▍      | 1323/3844 [2:44:12<5:59:20,  8.55s/it]

 34%|███▍      | 1324/3844 [2:44:18<5:26:51,  7.78s/it]

 34%|███▍      | 1325/3844 [2:44:28<5:49:41,  8.33s/it]

 34%|███▍      | 1326/3844 [2:44:35<5:36:08,  8.01s/it]

 35%|███▍      | 1327/3844 [2:44:41<5:10:25,  7.40s/it]

 35%|███▍      | 1328/3844 [2:44:47<4:52:10,  6.97s/it]

 35%|███▍      | 1329/3844 [2:44:56<5:17:14,  7.57s/it]

 35%|███▍      | 1330/3844 [2:45:04<5:18:57,  7.61s/it]

 35%|███▍      | 1331/3844 [2:45:10<4:55:36,  7.06s/it]

 35%|███▍      | 1332/3844 [2:45:18<5:09:14,  7.39s/it]

 35%|███▍      | 1333/3844 [2:45:23<4:48:33,  6.90s/it]

 35%|███▍      | 1334/3844 [2:45:30<4:49:08,  6.91s/it]

 35%|███▍      | 1335/3844 [2:45:38<4:54:37,  7.05s/it]

 35%|███▍      | 1336/3844 [2:45:43<4:38:00,  6.65s/it]

 35%|███▍      | 1337/3844 [2:45:50<4:31:26,  6.50s/it]

 35%|███▍      | 1338/3844 [2:45:59<5:09:48,  7.42s/it]

 35%|███▍      | 1339/3844 [2:46:09<5:40:40,  8.16s/it]

 35%|███▍      | 1340/3844 [2:46:15<5:13:04,  7.50s/it]

 35%|███▍      | 1341/3844 [2:46:21<4:53:31,  7.04s/it]

 35%|███▍      | 1342/3844 [2:46:27<4:38:54,  6.69s/it]

 35%|███▍      | 1343/3844 [2:46:33<4:29:55,  6.48s/it]

 35%|███▍      | 1344/3844 [2:46:39<4:22:11,  6.29s/it]

 35%|███▍      | 1345/3844 [2:46:48<4:58:33,  7.17s/it]

 35%|███▌      | 1346/3844 [2:46:53<4:35:51,  6.63s/it]

 35%|███▌      | 1347/3844 [2:46:59<4:23:06,  6.32s/it]

 35%|███▌      | 1348/3844 [2:47:05<4:15:42,  6.15s/it]

 35%|███▌      | 1349/3844 [2:47:10<4:09:37,  6.00s/it]

 35%|███▌      | 1350/3844 [2:47:19<4:38:07,  6.69s/it]

 35%|███▌      | 1351/3844 [2:47:24<4:26:22,  6.41s/it]

 35%|███▌      | 1352/3844 [2:47:30<4:18:50,  6.23s/it]


 35%|███▌      | 1354/3844 [2:47:41<4:05:23,  5.91s/it]

 35%|███▌      | 1355/3844 [2:47:48<4:08:14,  5.98s/it]

 35%|███▌      | 1356/3844 [2:47:55<4:29:18,  6.49s/it]

 35%|███▌      | 1357/3844 [2:48:02<4:29:07,  6.49s/it]

 35%|███▌      | 1358/3844 [2:48:13<5:30:44,  7.98s/it]

 35%|███▌      | 1359/3844 [2:48:19<5:06:02,  7.39s/it]

 35%|███▌      | 1360/3844 [2:48:26<4:58:52,  7.22s/it]

 35%|███▌      | 1361/3844 [2:48:35<5:19:09,  7.71s/it]

 35%|███▌      | 1362/3844 [2:48:41<5:00:05,  7.25s/it]

 35%|███▌      | 1363/3844 [2:48:49<5:01:32,  7.29s/it]

 35%|███▌      | 1364/3844 [2:48:55<4:53:45,  7.11s/it]

 36%|███▌      | 1365/3844 [2:49:02<4:55:41,  7.16s/it]

 36%|███▌      | 1366/3844 [2:49:09<4:41:54,  6.83s/it]

 36%|███▌      | 1367/3844 [2:49:15<4:36:18,  6.69s/it]

 36%|███▌      | 1368/3844 [2:49:22<4:37:56,  6.74s/it]

 36%|███▌      | 1369/3844 [2:49:30<4:58:37,  7.24s/it]

 36%|███▌      | 1370/3844 [2:49:39<5:12:26,  7.58s/it]

 36%|███▌      | 1371/3844 [2:49:45<5:04:13,  7.38s/it]

 36%|███▌      | 1372/3844 [2:49:54<5:16:46,  7.69s/it]

 36%|███▌      | 1373/3844 [2:50:01<5:13:37,  7.62s/it]

 36%|███▌      | 1374/3844 [2:50:09<5:18:02,  7.73s/it]

 36%|███▌      | 1375/3844 [2:50:15<4:58:19,  7.25s/it]

 36%|███▌      | 1376/3844 [2:50:22<4:47:58,  7.00s/it]

 36%|███▌      | 1377/3844 [2:50:30<5:03:31,  7.38s/it]

 36%|███▌      | 1378/3844 [2:50:37<4:58:29,  7.26s/it]

 36%|███▌      | 1379/3844 [2:50:45<5:01:30,  7.34s/it]

 36%|███▌      | 1380/3844 [2:50:50<4:43:21,  6.90s/it]

 36%|███▌      | 1381/3844 [2:50:57<4:44:00,  6.92s/it]

 36%|███▌      | 1382/3844 [2:51:05<4:45:50,  6.97s/it]

 36%|███▌      | 1383/3844 [2:51:13<5:10:08,  7.56s/it]

 36%|███▌      | 1384/3844 [2:51:22<5:17:44,  7.75s/it]

 36%|███▌      | 1385/3844 [2:51:31<5:37:50,  8.24s/it]

 36%|███▌      | 1386/3844 [2:51:37<5:10:25,  7.58s/it]

 36%|███▌      | 1387/3844 [2:51:44<4:59:11,  7.31s/it]

 36%|███▌      | 1388/3844 [2:51:52<5:08:44,  7.54s/it]

 36%|███▌      | 1389/3844 [2:52:01<5:26:14,  7.97s/it]

 36%|███▌      | 1390/3844 [2:52:07<5:07:25,  7.52s/it]

 36%|███▌      | 1391/3844 [2:52:14<5:01:48,  7.38s/it]

 36%|███▌      | 1392/3844 [2:52:27<6:05:58,  8.96s/it]

 36%|███▌      | 1393/3844 [2:52:33<5:28:11,  8.03s/it]

 36%|███▋      | 1394/3844 [2:52:41<5:34:54,  8.20s/it]

 36%|███▋      | 1395/3844 [2:52:48<5:12:42,  7.66s/it]

 36%|███▋      | 1396/3844 [2:52:55<5:04:02,  7.45s/it]

 36%|███▋      | 1397/3844 [2:53:03<5:16:33,  7.76s/it]

 36%|███▋      | 1398/3844 [2:53:10<5:03:40,  7.45s/it]

 36%|███▋      | 1399/3844 [2:53:17<4:54:54,  7.24s/it]

 36%|███▋      | 1400/3844 [2:53:23<4:42:37,  6.94s/it]

 36%|███▋      | 1401/3844 [2:53:31<4:58:08,  7.32s/it]

 36%|███▋      | 1402/3844 [2:53:37<4:40:00,  6.88s/it]

 36%|███▋      | 1403/3844 [2:53:43<4:28:36,  6.60s/it]

 37%|███▋      | 1404/3844 [2:53:50<4:37:00,  6.81s/it]

 37%|███▋      | 1405/3844 [2:53:56<4:25:43,  6.54s/it]

 37%|███▋      | 1406/3844 [2:54:04<4:44:15,  7.00s/it]

 37%|███▋      | 1407/3844 [2:54:10<4:32:04,  6.70s/it]

 37%|███▋      | 1408/3844 [2:54:20<5:12:33,  7.70s/it]

 37%|███▋      | 1409/3844 [2:54:26<4:46:19,  7.06s/it]

 37%|███▋      | 1410/3844 [2:54:34<4:58:11,  7.35s/it]

 37%|███▋      | 1411/3844 [2:54:40<4:40:09,  6.91s/it]

 37%|███▋      | 1412/3844 [2:54:46<4:37:29,  6.85s/it]

 37%|███▋      | 1413/3844 [2:54:56<5:05:48,  7.55s/it]

 37%|███▋      | 1414/3844 [2:55:02<4:51:38,  7.20s/it]

 37%|███▋      | 1415/3844 [2:55:08<4:32:41,  6.74s/it]

 37%|███▋      | 1416/3844 [2:55:15<4:37:21,  6.85s/it]

 37%|███▋      | 1417/3844 [2:55:22<4:37:25,  6.86s/it]

 37%|███▋      | 1418/3844 [2:55:28<4:35:04,  6.80s/it]

 37%|███▋      | 1419/3844 [2:55:36<4:46:57,  7.10s/it]

 37%|███▋      | 1420/3844 [2:55:44<4:53:12,  7.26s/it]

 37%|███▋      | 1421/3844 [2:55:54<5:33:08,  8.25s/it]

 37%|███▋      | 1422/3844 [2:56:00<5:03:00,  7.51s/it]

 37%|███▋      | 1423/3844 [2:56:07<4:52:00,  7.24s/it]

 37%|███▋      | 1424/3844 [2:56:15<4:59:36,  7.43s/it]

 37%|███▋      | 1425/3844 [2:56:21<4:40:59,  6.97s/it]

 37%|███▋      | 1426/3844 [2:56:27<4:40:51,  6.97s/it]

 37%|███▋      | 1427/3844 [2:56:38<5:25:06,  8.07s/it]

 37%|███▋      | 1428/3844 [2:56:45<5:06:56,  7.62s/it]

 37%|███▋      | 1429/3844 [2:56:51<4:51:36,  7.24s/it]

 37%|███▋      | 1430/3844 [2:57:01<5:25:49,  8.10s/it]

 37%|███▋      | 1431/3844 [2:57:08<5:07:24,  7.64s/it]

 37%|███▋      | 1432/3844 [2:57:16<5:14:02,  7.81s/it]

 37%|███▋      | 1433/3844 [2:57:27<5:52:55,  8.78s/it]

 37%|███▋      | 1434/3844 [2:57:34<5:28:45,  8.18s/it]

 37%|███▋      | 1435/3844 [2:57:41<5:18:05,  7.92s/it]

 37%|███▋      | 1436/3844 [2:57:48<5:00:08,  7.48s/it]

 37%|███▋      | 1437/3844 [2:57:54<4:52:46,  7.30s/it]

 37%|███▋      | 1438/3844 [2:58:01<4:41:04,  7.01s/it]

 37%|███▋      | 1439/3844 [2:58:11<5:18:56,  7.96s/it]

 37%|███▋      | 1440/3844 [2:58:19<5:15:48,  7.88s/it]

 37%|███▋      | 1441/3844 [2:58:24<4:47:55,  7.19s/it]

 38%|███▊      | 1442/3844 [2:58:33<5:03:44,  7.59s/it]

 38%|███▊      | 1443/3844 [2:58:39<4:46:19,  7.16s/it]

 38%|███▊      | 1444/3844 [2:58:49<5:19:02,  7.98s/it]

 38%|███▊      | 1445/3844 [2:58:58<5:28:50,  8.22s/it]

 38%|███▊      | 1446/3844 [2:59:04<5:06:30,  7.67s/it]

 38%|███▊      | 1447/3844 [2:59:10<4:51:35,  7.30s/it]

 38%|███▊      | 1448/3844 [2:59:19<5:03:22,  7.60s/it]

 38%|███▊      | 1449/3844 [2:59:25<4:43:39,  7.11s/it]

 38%|███▊      | 1450/3844 [2:59:32<4:52:37,  7.33s/it]

 38%|███▊      | 1451/3844 [2:59:40<4:54:21,  7.38s/it]

 38%|███▊      | 1452/3844 [2:59:46<4:42:23,  7.08s/it]

 38%|███▊      | 1453/3844 [2:59:54<4:44:33,  7.14s/it]

 38%|███▊      | 1454/3844 [3:00:00<4:35:26,  6.91s/it]

 38%|███▊      | 1455/3844 [3:00:06<4:25:08,  6.66s/it]

 38%|███▊      | 1456/3844 [3:00:12<4:12:21,  6.34s/it]

 38%|███▊      | 1457/3844 [3:00:18<4:13:34,  6.37s/it]

 38%|███▊      | 1458/3844 [3:00:27<4:46:40,  7.21s/it]

 38%|███▊      | 1459/3844 [3:00:34<4:45:03,  7.17s/it]

 38%|███▊      | 1460/3844 [3:00:40<4:26:26,  6.71s/it]

 38%|███▊      | 1461/3844 [3:00:47<4:33:54,  6.90s/it]

 38%|███▊      | 1462/3844 [3:00:56<4:56:37,  7.47s/it]

 38%|███▊      | 1463/3844 [3:01:06<5:26:50,  8.24s/it]

 38%|███▊      | 1464/3844 [3:01:15<5:31:54,  8.37s/it]

 38%|███▊      | 1465/3844 [3:01:22<5:18:11,  8.02s/it]

 38%|███▊      | 1466/3844 [3:01:28<4:56:41,  7.49s/it]

 38%|███▊      | 1467/3844 [3:01:37<5:11:39,  7.87s/it]

 38%|███▊      | 1468/3844 [3:01:46<5:27:28,  8.27s/it]

 38%|███▊      | 1469/3844 [3:01:55<5:27:58,  8.29s/it]

 38%|███▊      | 1470/3844 [3:02:02<5:15:22,  7.97s/it]

 38%|███▊      | 1471/3844 [3:02:08<4:58:21,  7.54s/it]

 38%|███▊      | 1472/3844 [3:02:15<4:43:30,  7.17s/it]

 38%|███▊      | 1473/3844 [3:02:23<4:51:17,  7.37s/it]

 38%|███▊      | 1474/3844 [3:02:31<5:04:10,  7.70s/it]

 38%|███▊      | 1475/3844 [3:02:38<4:56:59,  7.52s/it]

 38%|███▊      | 1476/3844 [3:02:45<4:48:39,  7.31s/it]

 38%|███▊      | 1477/3844 [3:02:52<4:48:17,  7.31s/it]

 38%|███▊      | 1478/3844 [3:03:00<4:56:48,  7.53s/it]

 38%|███▊      | 1479/3844 [3:03:06<4:32:18,  6.91s/it]

 39%|███▊      | 1480/3844 [3:03:16<5:06:41,  7.78s/it]

 39%|███▊      | 1481/3844 [3:03:25<5:21:13,  8.16s/it]

 39%|███▊      | 1482/3844 [3:03:31<5:03:10,  7.70s/it]

 39%|███▊      | 1483/3844 [3:03:40<5:14:44,  8.00s/it]

 39%|███▊      | 1484/3844 [3:03:46<4:53:41,  7.47s/it]

 39%|███▊      | 1485/3844 [3:03:55<5:15:17,  8.02s/it]

 39%|███▊      | 1486/3844 [3:04:04<5:20:33,  8.16s/it]

 39%|███▊      | 1487/3844 [3:04:11<5:07:38,  7.83s/it]

 39%|███▊      | 1488/3844 [3:04:17<4:42:02,  7.18s/it]

 39%|███▊      | 1489/3844 [3:04:22<4:20:34,  6.64s/it]

 39%|███▉      | 1490/3844 [3:04:31<4:49:33,  7.38s/it]

 39%|███▉      | 1491/3844 [3:04:38<4:47:24,  7.33s/it]

 39%|███▉      | 1492/3844 [3:04:46<4:55:57,  7.55s/it]

 39%|███▉      | 1493/3844 [3:04:56<5:25:16,  8.30s/it]

 39%|███▉      | 1494/3844 [3:05:04<5:11:50,  7.96s/it]

 39%|███▉      | 1495/3844 [3:05:10<4:51:52,  7.46s/it]

 39%|███▉      | 1496/3844 [3:05:18<5:00:52,  7.69s/it]

 39%|███▉      | 1497/3844 [3:05:26<5:02:53,  7.74s/it]

 39%|███▉      | 1498/3844 [3:05:33<4:51:30,  7.46s/it]

 39%|███▉      | 1499/3844 [3:05:41<4:54:30,  7.54s/it]

 39%|███▉      | 1500/3844 [3:05:52<5:40:15,  8.71s/it]
 39%|███▉      | 1500/3844 [3:05:52<5:40:15,  8.71s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 39%|███▉      | 1501/3844 [3:06:19<9:18:39, 14.31s/it]

 39%|███▉      | 1502/3844 [3:06:26<7:49:20, 12.02s/it]

 39%|███▉      | 1503/3844 [3:06:32<6:33:53, 10.10s/it]

 39%|███▉      | 1504/3844 [3:06:37<5:42:10,  8.77s/it]

 39%|███▉      | 1505/3844 [3:06:43<5:07:48,  7.90s/it]

 39%|███▉      | 1506/3844 [3:06:50<4:49:53,  7.44s/it]

 39%|███▉      | 1507/3844 [3:06:59<5:11:49,  8.01s/it]

 39%|███▉      | 1508/3844 [3:07:05<4:49:43,  7.44s/it]

 39%|███▉      | 1509/3844 [3:07:12<4:41:03,  7.22s/it]

 39%|███▉      | 1510/3844 [3:07:19<4:43:51,  7.30s/it]

 39%|███▉      | 1511/3844 [3:07:30<5:21:27,  8.27s/it]

 39%|███▉      | 1512/3844 [3:07:36<4:59:44,  7.71s/it]

 39%|███▉      | 1513/3844 [3:07:43<4:50:44,  7.48s/it]

 39%|███▉      | 1514/3844 [3:07:51<4:53:19,  7.55s/it]

 39%|███▉      | 1515/3844 [3:07:57<4:31:48,  7.00s/it]

 39%|███▉      | 1516/3844 [3:08:03<4:28:52,  6.93s/it]

 39%|███▉      | 1517/3844 [3:08:11<4:40:21,  7.23s/it]

 39%|███▉      | 1518/3844 [3:08:18<4:35:21,  7.10s/it]

 40%|███▉      | 1519/3844 [3:08:25<4:33:10,  7.05s/it]

 40%|███▉      | 1520/3844 [3:08:37<5:26:33,  8.43s/it]

 40%|███▉      | 1521/3844 [3:08:44<5:15:58,  8.16s/it]

 40%|███▉      | 1522/3844 [3:08:50<4:53:24,  7.58s/it]

 40%|███▉      | 1523/3844 [3:08:59<5:08:45,  7.98s/it]

 40%|███▉      | 1524/3844 [3:09:06<4:52:30,  7.56s/it]

 40%|███▉      | 1525/3844 [3:09:14<4:57:01,  7.69s/it]

 40%|███▉      | 1526/3844 [3:09:20<4:37:28,  7.18s/it]

 40%|███▉      | 1527/3844 [3:09:29<4:58:51,  7.74s/it]

 40%|███▉      | 1528/3844 [3:09:35<4:41:24,  7.29s/it]

 40%|███▉      | 1529/3844 [3:09:43<4:44:36,  7.38s/it]

 40%|███▉      | 1530/3844 [3:09:49<4:35:21,  7.14s/it]

 40%|███▉      | 1531/3844 [3:10:00<5:13:32,  8.13s/it]

 40%|███▉      | 1532/3844 [3:10:06<4:49:30,  7.51s/it]

 40%|███▉      | 1533/3844 [3:10:13<4:43:41,  7.37s/it]

 40%|███▉      | 1534/3844 [3:10:20<4:37:37,  7.21s/it]

 40%|███▉      | 1535/3844 [3:10:26<4:22:38,  6.82s/it]

 40%|███▉      | 1536/3844 [3:10:33<4:26:49,  6.94s/it]

 40%|███▉      | 1537/3844 [3:10:40<4:24:55,  6.89s/it]

 40%|████      | 1538/3844 [3:10:47<4:26:51,  6.94s/it]

 40%|████      | 1539/3844 [3:10:56<4:48:57,  7.52s/it]

 40%|████      | 1540/3844 [3:11:05<5:16:29,  8.24s/it]

 40%|████      | 1541/3844 [3:11:13<5:09:32,  8.06s/it]

 40%|████      | 1542/3844 [3:11:20<4:50:47,  7.58s/it]

 40%|████      | 1543/3844 [3:11:26<4:36:38,  7.21s/it]

 40%|████      | 1544/3844 [3:11:31<4:17:50,  6.73s/it]

 40%|████      | 1545/3844 [3:11:38<4:09:58,  6.52s/it]

 40%|████      | 1546/3844 [3:11:46<4:26:47,  6.97s/it]

 40%|████      | 1547/3844 [3:11:52<4:15:51,  6.68s/it]


 40%|████      | 1548/3844 [3:12:04<5:16:22,  8.27s/it]

 40%|████      | 1549/3844 [3:12:10<4:57:27,  7.78s/it]

 40%|████      | 1550/3844 [3:12:18<4:59:14,  7.83s/it]

 40%|████      | 1551/3844 [3:12:25<4:53:47,  7.69s/it]

 40%|████      | 1552/3844 [3:12:36<5:22:44,  8.45s/it]

 40%|████      | 1553/3844 [3:12:46<5:49:25,  9.15s/it]

 40%|████      | 1554/3844 [3:12:53<5:14:34,  8.24s/it]

 40%|████      | 1555/3844 [3:13:00<5:07:12,  8.05s/it]

 40%|████      | 1556/3844 [3:13:06<4:43:38,  7.44s/it]

 41%|████      | 1557/3844 [3:13:16<5:11:25,  8.17s/it]

 41%|████      | 1558/3844 [3:13:22<4:42:03,  7.40s/it]

 41%|████      | 1559/3844 [3:13:29<4:44:00,  7.46s/it]

 41%|████      | 1560/3844 [3:13:36<4:35:15,  7.23s/it]

 41%|████      | 1561/3844 [3:13:42<4:20:22,  6.84s/it]

 41%|████      | 1562/3844 [3:13:48<4:08:57,  6.55s/it]

 41%|████      | 1563/3844 [3:13:55<4:16:24,  6.74s/it]

 41%|████      | 1564/3844 [3:14:02<4:18:41,  6.81s/it]

 41%|████      | 1565/3844 [3:14:11<4:40:49,  7.39s/it]

 41%|████      | 1566/3844 [3:14:18<4:38:42,  7.34s/it]

 41%|████      | 1567/3844 [3:14:24<4:21:29,  6.89s/it]

 41%|████      | 1568/3844 [3:14:30<4:13:38,  6.69s/it]

 41%|████      | 1569/3844 [3:14:37<4:18:28,  6.82s/it]

 41%|████      | 1570/3844 [3:14:43<4:05:42,  6.48s/it]

 41%|████      | 1571/3844 [3:14:50<4:09:21,  6.58s/it]

 41%|████      | 1572/3844 [3:14:58<4:27:26,  7.06s/it]

 41%|████      | 1573/3844 [3:15:06<4:45:29,  7.54s/it]

 41%|████      | 1574/3844 [3:15:13<4:28:42,  7.10s/it]

 41%|████      | 1575/3844 [3:15:20<4:30:49,  7.16s/it]

 41%|████      | 1576/3844 [3:15:32<5:27:08,  8.65s/it]

 41%|████      | 1577/3844 [3:15:40<5:17:40,  8.41s/it]

 41%|████      | 1578/3844 [3:15:46<4:47:17,  7.61s/it]

 41%|████      | 1579/3844 [3:15:56<5:23:11,  8.56s/it]

 41%|████      | 1580/3844 [3:16:02<4:56:04,  7.85s/it]

 41%|████      | 1581/3844 [3:16:10<4:47:36,  7.63s/it]
{'loss': 1.0998, 'grad_norm': 0.3777685016555069, 'learning_rate': 1.3299381494504499e-05, 'epoch': 0.41}


 41%|████      | 1583/3844 [3:16:21<4:10:23,  6.64s/it]

 41%|████      | 1584/3844 [3:16:27<4:03:53,  6.47s/it]
{'loss': 1.1541, 'grad_norm': 0.36222127250921693, 'learning_rate': 1.3275505594141622e-05, 'epoch': 0.41}

 41%|████      | 1585/3844 [3:16:34<4:05:29,  6.52s/it]


 41%|████▏     | 1587/3844 [3:16:49<4:25:20,  7.05s/it]

 41%|████▏     | 1588/3844 [3:16:56<4:27:17,  7.11s/it]

 41%|████▏     | 1589/3844 [3:17:03<4:24:42,  7.04s/it]

 41%|████▏     | 1590/3844 [3:17:11<4:26:13,  7.09s/it]

 41%|████▏     | 1591/3844 [3:17:20<4:48:18,  7.68s/it]

 41%|████▏     | 1592/3844 [3:17:30<5:17:41,  8.46s/it]

 41%|████▏     | 1593/3844 [3:17:37<4:59:57,  8.00s/it]

 41%|████▏     | 1594/3844 [3:17:44<4:49:25,  7.72s/it]

 41%|████▏     | 1595/3844 [3:17:52<4:48:40,  7.70s/it]
{'loss': 1.1392, 'grad_norm': 0.36359920746925045, 'learning_rate': 1.3187782679344376e-05, 'epoch': 0.41}

 42%|████▏     | 1596/3844 [3:17:58<4:36:02,  7.37s/it]

 42%|████▏     | 1597/3844 [3:18:04<4:20:53,  6.97s/it]


 42%|████▏     | 1599/3844 [3:18:16<3:56:29,  6.32s/it]

 42%|████▏     | 1600/3844 [3:18:25<4:28:10,  7.17s/it]

 42%|████▏     | 1601/3844 [3:18:33<4:34:16,  7.34s/it]

 42%|████▏     | 1602/3844 [3:18:38<4:12:37,  6.76s/it]

 42%|████▏     | 1603/3844 [3:18:44<4:02:42,  6.50s/it]
{'loss': 1.2366, 'grad_norm': 0.37123134962600124, 'learning_rate': 1.3123811717517181e-05, 'epoch': 0.42}


 42%|████▏     | 1605/3844 [3:19:02<4:46:13,  7.67s/it]
{'loss': 1.0913, 'grad_norm': 0.3816355900747795, 'learning_rate': 1.3107796679791231e-05, 'epoch': 0.42}


 42%|████▏     | 1607/3844 [3:19:18<4:57:10,  7.97s/it]

 42%|████▏     | 1608/3844 [3:19:24<4:35:17,  7.39s/it]

 42%|████▏     | 1609/3844 [3:19:29<4:12:56,  6.79s/it]

 42%|████▏     | 1610/3844 [3:19:39<4:47:17,  7.72s/it]
{'loss': 1.2045, 'grad_norm': 0.3464763248746239, 'learning_rate': 1.3067720562720889e-05, 'epoch': 0.42}

 42%|████▏     | 1611/3844 [3:19:44<4:19:57,  6.99s/it]

 42%|████▏     | 1612/3844 [3:19:52<4:27:56,  7.20s/it]


 42%|████▏     | 1614/3844 [3:20:05<4:19:00,  6.97s/it]
{'loss': 1.1199, 'grad_norm': 0.3389960538208865, 'learning_rate': 1.3035620421380992e-05, 'epoch': 0.42}


 42%|████▏     | 1616/3844 [3:20:23<4:58:10,  8.03s/it]

 42%|████▏     | 1617/3844 [3:20:32<5:08:29,  8.31s/it]
{'loss': 1.229, 'grad_norm': 0.34828704219078854, 'learning_rate': 1.3011522660217306e-05, 'epoch': 0.42}


 42%|████▏     | 1619/3844 [3:20:52<5:38:58,  9.14s/it]

 42%|████▏     | 1620/3844 [3:20:57<4:56:15,  7.99s/it]

 42%|████▏     | 1621/3844 [3:21:05<4:54:36,  7.95s/it]
{'loss': 1.2773, 'grad_norm': 0.35096037311563216, 'learning_rate': 1.2979362397941873e-05, 'epoch': 0.42}

 42%|████▏     | 1622/3844 [3:21:14<5:06:33,  8.28s/it]

 42%|████▏     | 1623/3844 [3:21:22<5:05:32,  8.25s/it]


 42%|████▏     | 1625/3844 [3:21:38<4:49:33,  7.83s/it]

 42%|████▏     | 1626/3844 [3:21:44<4:24:37,  7.16s/it]

 42%|████▏     | 1627/3844 [3:21:52<4:33:32,  7.40s/it]

 42%|████▏     | 1628/3844 [3:21:57<4:16:42,  6.95s/it]

 42%|████▏     | 1629/3844 [3:22:05<4:23:50,  7.15s/it]

 42%|████▏     | 1630/3844 [3:22:12<4:16:19,  6.95s/it]

 42%|████▏     | 1631/3844 [3:22:20<4:28:23,  7.28s/it]
{'loss': 1.216, 'grad_norm': 0.35842917618609177, 'learning_rate': 1.2898814437825597e-05, 'epoch': 0.42}


 42%|████▏     | 1633/3844 [3:22:36<4:43:57,  7.71s/it]

 43%|████▎     | 1634/3844 [3:22:45<4:56:44,  8.06s/it]

 43%|████▎     | 1635/3844 [3:22:55<5:19:28,  8.68s/it]

 43%|████▎     | 1636/3844 [3:23:06<5:39:20,  9.22s/it]

 43%|████▎     | 1637/3844 [3:23:14<5:31:45,  9.02s/it]

 43%|████▎     | 1638/3844 [3:23:21<5:13:30,  8.53s/it]

 43%|████▎     | 1639/3844 [3:23:27<4:38:22,  7.57s/it]
{'loss': 1.2735, 'grad_norm': 0.40441283508992676, 'learning_rate': 1.2834227577586433e-05, 'epoch': 0.43}


 43%|████▎     | 1641/3844 [3:23:38<4:03:16,  6.63s/it]

 43%|████▎     | 1642/3844 [3:23:46<4:11:57,  6.87s/it]

 43%|████▎     | 1643/3844 [3:23:53<4:18:47,  7.05s/it]

 43%|████▎     | 1644/3844 [3:24:04<5:03:21,  8.27s/it]

 43%|████▎     | 1645/3844 [3:24:13<5:07:46,  8.40s/it]

 43%|████▎     | 1646/3844 [3:24:20<4:51:58,  7.97s/it]

 43%|████▎     | 1647/3844 [3:24:28<4:55:00,  8.06s/it]

 43%|████▎     | 1648/3844 [3:24:34<4:33:59,  7.49s/it]

 43%|████▎     | 1649/3844 [3:24:40<4:11:14,  6.87s/it]

 43%|████▎     | 1650/3844 [3:24:50<4:42:49,  7.73s/it]

 43%|████▎     | 1651/3844 [3:24:56<4:30:34,  7.40s/it]
{'loss': 1.2755, 'grad_norm': 0.3834870426724577, 'learning_rate': 1.2737106681057944e-05, 'epoch': 0.43}

 43%|████▎     | 1652/3844 [3:25:06<5:02:26,  8.28s/it]


 43%|████▎     | 1654/3844 [3:25:18<4:12:28,  6.92s/it]

 43%|████▎     | 1655/3844 [3:25:25<4:14:59,  6.99s/it]

 43%|████▎     | 1656/3844 [3:25:32<4:12:53,  6.93s/it]

 43%|████▎     | 1657/3844 [3:25:38<4:09:41,  6.85s/it]
{'loss': 1.1426, 'grad_norm': 0.3620200127240949, 'learning_rate': 1.2688440649512623e-05, 'epoch': 0.43}


 43%|████▎     | 1659/3844 [3:25:53<4:17:44,  7.08s/it]

 43%|████▎     | 1660/3844 [3:26:00<4:08:51,  6.84s/it]

 43%|████▎     | 1661/3844 [3:26:06<4:00:25,  6.61s/it]
{'loss': 1.1248, 'grad_norm': 0.35709653681941916, 'learning_rate': 1.265595836795802e-05, 'epoch': 0.43}


 43%|████▎     | 1663/3844 [3:26:22<4:31:30,  7.47s/it]

 43%|████▎     | 1664/3844 [3:26:32<4:55:39,  8.14s/it]

 43%|████▎     | 1665/3844 [3:26:38<4:26:56,  7.35s/it]

 43%|████▎     | 1666/3844 [3:26:44<4:17:56,  7.11s/it]

 43%|████▎     | 1667/3844 [3:26:52<4:23:25,  7.26s/it]

 43%|████▎     | 1668/3844 [3:26:57<4:02:27,  6.69s/it]

 43%|████▎     | 1669/3844 [3:27:04<4:00:46,  6.64s/it]

 43%|████▎     | 1670/3844 [3:27:13<4:25:15,  7.32s/it]

 43%|████▎     | 1671/3844 [3:27:22<4:48:48,  7.97s/it]

 43%|████▎     | 1672/3844 [3:27:31<5:03:46,  8.39s/it]
{'loss': 1.2544, 'grad_norm': 0.3545614156024918, 'learning_rate': 1.2566477600302207e-05, 'epoch': 0.43}


 44%|████▎     | 1674/3844 [3:27:45<4:27:05,  7.38s/it]

 44%|████▎     | 1675/3844 [3:27:51<4:20:38,  7.21s/it]
{'loss': 1.1659, 'grad_norm': 0.3636286109257053, 'learning_rate': 1.254203515682114e-05, 'epoch': 0.44}

 44%|████▎     | 1676/3844 [3:27:57<4:02:43,  6.72s/it]


 44%|████▎     | 1678/3844 [3:28:13<4:23:49,  7.31s/it]

 44%|████▎     | 1679/3844 [3:28:20<4:15:47,  7.09s/it]

 44%|████▎     | 1680/3844 [3:28:30<4:47:16,  7.97s/it]
{'loss': 1.2507, 'grad_norm': 0.36845089352929866, 'learning_rate': 1.2501261723884665e-05, 'epoch': 0.44}


 44%|████▍     | 1682/3844 [3:28:43<4:20:33,  7.23s/it]

 44%|████▍     | 1683/3844 [3:28:53<4:45:09,  7.92s/it]

 44%|████▍     | 1684/3844 [3:28:59<4:24:11,  7.34s/it]

 44%|████▍     | 1685/3844 [3:29:05<4:09:43,  6.94s/it]

 44%|████▍     | 1686/3844 [3:29:10<3:57:21,  6.60s/it]

 44%|████▍     | 1687/3844 [3:29:22<4:45:46,  7.95s/it]
{'loss': 1.1484, 'grad_norm': 0.3792069768983302, 'learning_rate': 1.2444104477170105e-05, 'epoch': 0.44}


 44%|████▍     | 1689/3844 [3:29:38<4:58:56,  8.32s/it]

 44%|████▍     | 1690/3844 [3:29:44<4:32:54,  7.60s/it]

 44%|████▍     | 1691/3844 [3:29:51<4:32:49,  7.60s/it]

 44%|████▍     | 1692/3844 [3:29:57<4:14:02,  7.08s/it]

 44%|████▍     | 1693/3844 [3:30:04<4:13:29,  7.07s/it]

 44%|████▍     | 1694/3844 [3:30:11<4:04:58,  6.84s/it]
{'loss': 1.112, 'grad_norm': 0.3758027786273229, 'learning_rate': 1.2386862182764925e-05, 'epoch': 0.44}

 44%|████▍     | 1695/3844 [3:30:17<3:58:03,  6.65s/it]


 44%|████▍     | 1697/3844 [3:30:33<4:25:02,  7.41s/it]
{'loss': 1.1232, 'grad_norm': 0.3444281707643815, 'learning_rate': 1.2362304229377311e-05, 'epoch': 0.44}


 44%|████▍     | 1699/3844 [3:30:48<4:28:55,  7.52s/it]

 44%|████▍     | 1700/3844 [3:30:56<4:32:08,  7.62s/it]

 44%|████▍     | 1701/3844 [3:31:02<4:19:38,  7.27s/it]

 44%|████▍     | 1702/3844 [3:31:09<4:08:47,  6.97s/it]

 44%|████▍     | 1703/3844 [3:31:16<4:12:39,  7.08s/it]

 44%|████▍     | 1704/3844 [3:31:23<4:06:35,  6.91s/it]

 44%|████▍     | 1705/3844 [3:31:28<3:53:56,  6.56s/it]

 44%|████▍     | 1706/3844 [3:31:34<3:41:11,  6.21s/it]

 44%|████▍     | 1707/3844 [3:31:41<3:57:08,  6.66s/it]

 44%|████▍     | 1708/3844 [3:31:48<4:01:45,  6.79s/it]
{'loss': 1.2269, 'grad_norm': 0.3662152298914699, 'learning_rate': 1.2272130421230819e-05, 'epoch': 0.44}


 44%|████▍     | 1710/3844 [3:32:10<5:10:50,  8.74s/it]
[2024-05-26 14:38:42,547] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 1711/3844 [3:32:18<5:01:44,  8.49s/it]

 45%|████▍     | 1712/3844 [3:32:24<4:38:24,  7.84s/it]

 45%|████▍     | 1713/3844 [3:32:30<4:20:04,  7.32s/it]

 45%|████▍     | 1714/3844 [3:32:36<4:01:05,  6.79s/it]

 45%|████▍     | 1715/3844 [3:32:41<3:50:33,  6.50s/it]
{'loss': 1.3119, 'grad_norm': 0.3744298472611073, 'learning_rate': 1.2214644946431786e-05, 'epoch': 0.45}


 45%|████▍     | 1717/3844 [3:32:56<4:03:28,  6.87s/it]

 45%|████▍     | 1718/3844 [3:33:03<3:59:57,  6.77s/it]

 45%|████▍     | 1719/3844 [3:33:09<3:59:07,  6.75s/it]
{'loss': 0.9833, 'grad_norm': 0.35722256083100673, 'learning_rate': 1.2181761375486767e-05, 'epoch': 0.45}


 45%|████▍     | 1721/3844 [3:33:24<4:08:41,  7.03s/it]

 45%|████▍     | 1722/3844 [3:33:32<4:18:19,  7.30s/it]

 45%|████▍     | 1723/3844 [3:33:40<4:25:14,  7.50s/it]

 45%|████▍     | 1724/3844 [3:33:47<4:16:16,  7.25s/it]

 45%|████▍     | 1725/3844 [3:33:54<4:16:18,  7.26s/it]

 45%|████▍     | 1726/3844 [3:34:02<4:22:21,  7.43s/it]
{'loss': 1.3385, 'grad_norm': 0.33918876414950083, 'learning_rate': 1.212415570056158e-05, 'epoch': 0.45}

 45%|████▍     | 1727/3844 [3:34:09<4:21:02,  7.40s/it]


 45%|████▍     | 1729/3844 [3:34:24<4:22:41,  7.45s/it]

 45%|████▌     | 1730/3844 [3:34:30<4:12:42,  7.17s/it]

 45%|████▌     | 1731/3844 [3:34:36<3:58:12,  6.76s/it]

 45%|████▌     | 1732/3844 [3:34:44<4:04:35,  6.95s/it]

 45%|████▌     | 1733/3844 [3:34:51<4:07:39,  7.04s/it]

 45%|████▌     | 1734/3844 [3:34:58<4:07:13,  7.03s/it]

 45%|████▌     | 1735/3844 [3:35:04<3:59:23,  6.81s/it]

 45%|████▌     | 1736/3844 [3:35:10<3:50:50,  6.57s/it]

 45%|████▌     | 1737/3844 [3:35:18<4:01:23,  6.87s/it]

 45%|████▌     | 1738/3844 [3:35:29<4:44:49,  8.11s/it]
{'loss': 0.9711, 'grad_norm': 0.3492859989675251, 'learning_rate': 1.2025232262535864e-05, 'epoch': 0.45}


 45%|████▌     | 1740/3844 [3:35:46<4:48:19,  8.22s/it]

 45%|████▌     | 1741/3844 [3:35:52<4:24:24,  7.54s/it]
{'loss': 1.2318, 'grad_norm': 0.39451533895680974, 'learning_rate': 1.2000468647918167e-05, 'epoch': 0.45}

 45%|████▌     | 1742/3844 [3:35:58<4:03:51,  6.96s/it]


 45%|████▌     | 1744/3844 [3:36:10<3:53:09,  6.66s/it]

 45%|████▌     | 1745/3844 [3:36:16<3:42:29,  6.36s/it]

 45%|████▌     | 1746/3844 [3:36:24<4:01:52,  6.92s/it]

 45%|████▌     | 1747/3844 [3:36:34<4:34:52,  7.86s/it]
{'loss': 1.1498, 'grad_norm': 0.35900639375097393, 'learning_rate': 1.1950903220161286e-05, 'epoch': 0.45}


 45%|████▌     | 1749/3844 [3:36:46<4:05:48,  7.04s/it]

 46%|████▌     | 1750/3844 [3:36:52<3:49:39,  6.58s/it]

 46%|████▌     | 1751/3844 [3:36:58<3:47:28,  6.52s/it]

 46%|████▌     | 1752/3844 [3:37:06<4:05:32,  7.04s/it]

 46%|████▌     | 1753/3844 [3:37:14<4:09:32,  7.16s/it]

 46%|████▌     | 1754/3844 [3:37:20<3:59:46,  6.88s/it]

 46%|████▌     | 1755/3844 [3:37:26<3:49:14,  6.58s/it]

 46%|████▌     | 1756/3844 [3:37:32<3:42:05,  6.38s/it]
{'loss': 1.246, 'grad_norm': 0.3827096934386487, 'learning_rate': 1.1876461958733381e-05, 'epoch': 0.46}


 46%|████▌     | 1758/3844 [3:37:44<3:37:08,  6.25s/it]

 46%|████▌     | 1759/3844 [3:37:51<3:42:16,  6.40s/it]
{'loss': 1.251, 'grad_norm': 0.39058147610366056, 'learning_rate': 1.1851624007270703e-05, 'epoch': 0.46}


 46%|████▌     | 1761/3844 [3:38:06<4:09:45,  7.19s/it]

 46%|████▌     | 1762/3844 [3:38:13<4:03:10,  7.01s/it]

 46%|████▌     | 1763/3844 [3:38:19<3:54:08,  6.75s/it]

 46%|████▌     | 1764/3844 [3:38:26<3:59:18,  6.90s/it]

 46%|████▌     | 1765/3844 [3:38:33<3:58:59,  6.90s/it]

 46%|████▌     | 1766/3844 [3:38:40<4:02:30,  7.00s/it]

 46%|████▌     | 1767/3844 [3:38:51<4:40:15,  8.10s/it]

 46%|████▌     | 1768/3844 [3:38:56<4:11:19,  7.26s/it]
{'loss': 1.1821, 'grad_norm': 0.3932900327183101, 'learning_rate': 1.1777039782375811e-05, 'epoch': 0.46}

 46%|████▌     | 1769/3844 [3:39:04<4:10:04,  7.23s/it]

 46%|████▌     | 1770/3844 [3:39:09<3:55:55,  6.83s/it]


 46%|████▌     | 1772/3844 [3:39:22<3:47:13,  6.58s/it]

 46%|████▌     | 1773/3844 [3:39:29<3:53:39,  6.77s/it]

 46%|████▌     | 1774/3844 [3:39:41<4:44:32,  8.25s/it]

 46%|████▌     | 1775/3844 [3:39:47<4:19:57,  7.54s/it]

 46%|████▌     | 1776/3844 [3:39:55<4:22:48,  7.62s/it]
{'loss': 1.3249, 'grad_norm': 0.37009464958001287, 'learning_rate': 1.1710656747282897e-05, 'epoch': 0.46}


 46%|████▋     | 1778/3844 [3:40:09<4:14:41,  7.40s/it]

 46%|████▋     | 1779/3844 [3:40:16<4:11:57,  7.32s/it]

 46%|████▋     | 1780/3844 [3:40:26<4:43:21,  8.24s/it]

 46%|████▋     | 1781/3844 [3:40:37<5:06:00,  8.90s/it]

 46%|████▋     | 1782/3844 [3:40:43<4:40:30,  8.16s/it]
{'loss': 1.1723, 'grad_norm': 0.3942602991138781, 'learning_rate': 1.1660818283718706e-05, 'epoch': 0.46}


 46%|████▋     | 1784/3844 [3:40:57<4:18:02,  7.52s/it]

 46%|████▋     | 1785/3844 [3:41:03<3:58:04,  6.94s/it]

 46%|████▋     | 1786/3844 [3:41:11<4:10:48,  7.31s/it]

 46%|████▋     | 1787/3844 [3:41:19<4:18:44,  7.55s/it]
{'loss': 1.2343, 'grad_norm': 0.3534484641264576, 'learning_rate': 1.1619253742545813e-05, 'epoch': 0.46}

 47%|████▋     | 1788/3844 [3:41:26<4:12:33,  7.37s/it]


 47%|████▋     | 1790/3844 [3:41:40<4:13:11,  7.40s/it]

 47%|████▋     | 1791/3844 [3:41:47<4:03:56,  7.13s/it]

 47%|████▋     | 1792/3844 [3:41:54<4:01:27,  7.06s/it]
{'loss': 1.1669, 'grad_norm': 0.3602446607671279, 'learning_rate': 1.1577660453731858e-05, 'epoch': 0.47}


 47%|████▋     | 1794/3844 [3:42:09<4:17:36,  7.54s/it]
{'loss': 1.022, 'grad_norm': 0.37734308569872493, 'learning_rate': 1.156101525425872e-05, 'epoch': 0.47}

 47%|████▋     | 1795/3844 [3:42:16<4:11:04,  7.35s/it]

 47%|████▋     | 1796/3844 [3:42:22<3:55:44,  6.91s/it]


 47%|████▋     | 1798/3844 [3:42:37<4:07:53,  7.27s/it]

 47%|████▋     | 1799/3844 [3:42:43<3:51:19,  6.79s/it]

 47%|████▋     | 1800/3844 [3:42:49<3:42:04,  6.52s/it]
 47%|████▋     | 1800/3844 [3:42:49<3:42:04,  6.52s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 47%|████▋     | 1801/3844 [3:43:17<7:20:53, 12.95s/it]
{'loss': 1.1802, 'grad_norm': 0.3740853923875236, 'learning_rate': 1.1502722447204313e-05, 'epoch': 0.47}


 47%|████▋     | 1803/3844 [3:43:29<5:23:08,  9.50s/it]
{'loss': 1.0886, 'grad_norm': 0.40054926576299366, 'learning_rate': 1.1486057666372384e-05, 'epoch': 0.47}


 47%|████▋     | 1805/3844 [3:43:46<5:04:30,  8.96s/it]

 47%|████▋     | 1806/3844 [3:43:55<4:59:50,  8.83s/it]

 47%|████▋     | 1807/3844 [3:44:05<5:14:55,  9.28s/it]

 47%|████▋     | 1808/3844 [3:44:11<4:38:02,  8.19s/it]

 47%|████▋     | 1809/3844 [3:44:22<5:13:06,  9.23s/it]
{'loss': 1.0772, 'grad_norm': 0.3585613858629371, 'learning_rate': 1.1436038185646477e-05, 'epoch': 0.47}

 47%|████▋     | 1810/3844 [3:44:30<4:57:55,  8.79s/it]

 47%|████▋     | 1811/3844 [3:44:36<4:26:51,  7.88s/it]


 47%|████▋     | 1813/3844 [3:44:51<4:24:27,  7.81s/it]

 47%|████▋     | 1814/3844 [3:44:57<4:00:18,  7.10s/it]

 47%|████▋     | 1815/3844 [3:45:03<3:45:16,  6.66s/it]
{'loss': 1.1653, 'grad_norm': 0.37133738317742054, 'learning_rate': 1.138598199229018e-05, 'epoch': 0.47}


 47%|████▋     | 1817/3844 [3:45:15<3:42:00,  6.57s/it]

 47%|████▋     | 1818/3844 [3:45:21<3:30:33,  6.24s/it]

 47%|████▋     | 1819/3844 [3:45:33<4:32:05,  8.06s/it]
{'loss': 1.1807, 'grad_norm': 0.37402520364002534, 'learning_rate': 1.135259143272531e-05, 'epoch': 0.47}

 47%|████▋     | 1820/3844 [3:45:42<4:42:21,  8.37s/it]


 47%|████▋     | 1822/3844 [3:45:57<4:27:47,  7.95s/it]
{'loss': 1.1198, 'grad_norm': 0.3679181324008862, 'learning_rate': 1.1327538406657003e-05, 'epoch': 0.47}


 47%|████▋     | 1824/3844 [3:46:14<4:43:01,  8.41s/it]

 47%|████▋     | 1825/3844 [3:46:23<4:39:04,  8.29s/it]
{'loss': 1.1958, 'grad_norm': 0.39846050653384907, 'learning_rate': 1.1302476895872689e-05, 'epoch': 0.47}

 48%|████▊     | 1826/3844 [3:46:28<4:09:57,  7.43s/it]


 48%|████▊     | 1828/3844 [3:46:47<4:46:08,  8.52s/it]

 48%|████▊     | 1829/3844 [3:46:54<4:31:59,  8.10s/it]
{'loss': 1.1206, 'grad_norm': 0.4008434193984333, 'learning_rate': 1.1269048626568253e-05, 'epoch': 0.48}


 48%|████▊     | 1831/3844 [3:47:09<4:16:34,  7.65s/it]

 48%|████▊     | 1832/3844 [3:47:15<4:04:14,  7.28s/it]

 48%|████▊     | 1833/3844 [3:47:25<4:29:57,  8.05s/it]

 48%|████▊     | 1834/3844 [3:47:33<4:24:41,  7.90s/it]

 48%|████▊     | 1835/3844 [3:47:40<4:13:04,  7.56s/it]

 48%|████▊     | 1836/3844 [3:47:49<4:31:49,  8.12s/it]

 48%|████▊     | 1837/3844 [3:47:54<4:04:56,  7.32s/it]

 48%|████▊     | 1838/3844 [3:48:01<3:58:29,  7.13s/it]

 48%|████▊     | 1839/3844 [3:48:07<3:47:24,  6.81s/it]

 48%|████▊     | 1840/3844 [3:48:18<4:23:18,  7.88s/it]

 48%|████▊     | 1841/3844 [3:48:27<4:38:19,  8.34s/it]

 48%|████▊     | 1842/3844 [3:48:36<4:42:28,  8.47s/it]

 48%|████▊     | 1843/3844 [3:48:43<4:30:12,  8.10s/it]
{'loss': 1.2214, 'grad_norm': 0.36682174258879047, 'learning_rate': 1.1151938625654056e-05, 'epoch': 0.48}


 48%|████▊     | 1845/3844 [3:48:57<4:07:06,  7.42s/it]

 48%|████▊     | 1846/3844 [3:49:03<3:54:00,  7.03s/it]

 48%|████▊     | 1847/3844 [3:49:09<3:49:47,  6.90s/it]

 48%|████▊     | 1848/3844 [3:49:15<3:36:32,  6.51s/it]

 48%|████▊     | 1849/3844 [3:49:23<3:53:06,  7.01s/it]

 48%|████▊     | 1850/3844 [3:49:34<4:27:39,  8.05s/it]

 48%|████▊     | 1851/3844 [3:49:41<4:15:35,  7.69s/it]
{'loss': 1.1212, 'grad_norm': 0.3514046454507539, 'learning_rate': 1.1084945591551808e-05, 'epoch': 0.48}


 48%|████▊     | 1853/3844 [3:49:53<3:55:33,  7.10s/it]
{'loss': 1.223, 'grad_norm': 0.3660999893556777, 'learning_rate': 1.1068189509365484e-05, 'epoch': 0.48}


 48%|████▊     | 1855/3844 [3:50:09<4:03:24,  7.34s/it]

 48%|████▊     | 1856/3844 [3:50:15<3:46:00,  6.82s/it]

 48%|████▊     | 1857/3844 [3:50:22<3:48:59,  6.91s/it]

 48%|████▊     | 1858/3844 [3:50:27<3:34:14,  6.47s/it]
{'loss': 1.2522, 'grad_norm': 0.396434957336833, 'learning_rate': 1.1026286133049782e-05, 'epoch': 0.48}


 48%|████▊     | 1860/3844 [3:50:46<4:14:33,  7.70s/it]

 48%|████▊     | 1861/3844 [3:50:53<4:08:19,  7.51s/it]

 48%|████▊     | 1862/3844 [3:51:00<4:02:28,  7.34s/it]

 48%|████▊     | 1863/3844 [3:51:08<4:04:16,  7.40s/it]

 48%|████▊     | 1864/3844 [3:51:14<3:48:58,  6.94s/it]

 49%|████▊     | 1865/3844 [3:51:20<3:45:33,  6.84s/it]

 49%|████▊     | 1866/3844 [3:51:26<3:37:33,  6.60s/it]

 49%|████▊     | 1867/3844 [3:51:31<3:24:50,  6.22s/it]
{'loss': 1.1892, 'grad_norm': 0.4037630105546517, 'learning_rate': 1.0950814640632668e-05, 'epoch': 0.49}


 49%|████▊     | 1869/3844 [3:51:44<3:22:11,  6.14s/it]
{'loss': 1.2238, 'grad_norm': 0.3592704534991357, 'learning_rate': 1.0934035617604688e-05, 'epoch': 0.49}


 49%|████▊     | 1871/3844 [3:51:58<3:39:47,  6.68s/it]
{'loss': 0.9973, 'grad_norm': 0.37282012954909677, 'learning_rate': 1.0917253941368915e-05, 'epoch': 0.49}


 49%|████▊     | 1873/3844 [3:52:14<4:00:00,  7.31s/it]
{'loss': 1.0295, 'grad_norm': 0.37095367056355816, 'learning_rate': 1.0900469659595136e-05, 'epoch': 0.49}


 49%|████▉     | 1875/3844 [3:52:28<3:56:14,  7.20s/it]

 49%|████▉     | 1876/3844 [3:52:36<4:02:10,  7.38s/it]
{'loss': 1.1579, 'grad_norm': 0.34581698792702975, 'learning_rate': 1.0875288455846521e-05, 'epoch': 0.49}

 49%|████▉     | 1877/3844 [3:52:42<3:57:26,  7.24s/it]


 49%|████▉     | 1879/3844 [3:52:57<3:56:06,  7.21s/it]

 49%|████▉     | 1880/3844 [3:53:07<4:27:35,  8.18s/it]

 49%|████▉     | 1881/3844 [3:53:19<4:59:20,  9.15s/it]

 49%|████▉     | 1882/3844 [3:53:27<4:52:19,  8.94s/it]

 49%|████▉     | 1883/3844 [3:53:36<4:44:45,  8.71s/it]

 49%|████▉     | 1884/3844 [3:53:44<4:39:56,  8.57s/it]

 49%|████▉     | 1885/3844 [3:53:54<4:54:48,  9.03s/it]

 49%|████▉     | 1886/3844 [3:54:02<4:43:42,  8.69s/it]

 49%|████▉     | 1887/3844 [3:54:07<4:12:31,  7.74s/it]

 49%|████▉     | 1888/3844 [3:54:14<3:57:45,  7.29s/it]

 49%|████▉     | 1889/3844 [3:54:24<4:27:16,  8.20s/it]

 49%|████▉     | 1890/3844 [3:54:30<4:05:39,  7.54s/it]
{'loss': 1.1639, 'grad_norm': 0.36621505772322566, 'learning_rate': 1.0757704805181278e-05, 'epoch': 0.49}


 49%|████▉     | 1892/3844 [3:54:46<4:07:06,  7.60s/it]

 49%|████▉     | 1893/3844 [3:54:53<4:00:53,  7.41s/it]

 49%|████▉     | 1894/3844 [3:54:59<3:48:25,  7.03s/it]

 49%|████▉     | 1895/3844 [3:55:06<3:47:55,  7.02s/it]

 49%|████▉     | 1896/3844 [3:55:12<3:34:18,  6.60s/it]

 49%|████▉     | 1897/3844 [3:55:18<3:25:20,  6.33s/it]

 49%|████▉     | 1898/3844 [3:55:24<3:22:04,  6.23s/it]

 49%|████▉     | 1899/3844 [3:55:29<3:16:55,  6.07s/it]

 49%|████▉     | 1900/3844 [3:55:35<3:13:07,  5.96s/it]
{'loss': 1.0943, 'grad_norm': 0.39542167924423544, 'learning_rate': 1.067365095936144e-05, 'epoch': 0.49}


 49%|████▉     | 1902/3844 [3:55:46<3:10:15,  5.88s/it]

 50%|████▉     | 1903/3844 [3:55:52<3:09:12,  5.85s/it]

 50%|████▉     | 1904/3844 [3:56:00<3:24:45,  6.33s/it]

 50%|████▉     | 1905/3844 [3:56:07<3:38:32,  6.76s/it]

 50%|████▉     | 1906/3844 [3:56:13<3:28:18,  6.45s/it]

 50%|████▉     | 1907/3844 [3:56:20<3:28:10,  6.45s/it]
{'loss': 1.1978, 'grad_norm': 0.3581615927739576, 'learning_rate': 1.0614784447902527e-05, 'epoch': 0.5}


 50%|████▉     | 1909/3844 [3:56:32<3:25:11,  6.36s/it]

 50%|████▉     | 1910/3844 [3:56:38<3:17:44,  6.13s/it]

 50%|████▉     | 1911/3844 [3:56:44<3:13:45,  6.01s/it]
{'loss': 1.1587, 'grad_norm': 0.37738250110893373, 'learning_rate': 1.0581136705053518e-05, 'epoch': 0.5}


 50%|████▉     | 1913/3844 [3:56:59<3:36:48,  6.74s/it]
{'loss': 1.2343, 'grad_norm': 0.39004298090159684, 'learning_rate': 1.0564310333580466e-05, 'epoch': 0.5}


 50%|████▉     | 1915/3844 [3:57:16<4:03:36,  7.58s/it]
{'loss': 1.1544, 'grad_norm': 0.3882683857788018, 'learning_rate': 1.0547482359135806e-05, 'epoch': 0.5}

 50%|████▉     | 1916/3844 [3:57:27<4:32:13,  8.47s/it]

 50%|████▉     | 1917/3844 [3:57:35<4:24:41,  8.24s/it]


 50%|████▉     | 1919/3844 [3:57:50<4:10:27,  7.81s/it]

 50%|████▉     | 1920/3844 [3:57:55<3:48:55,  7.14s/it]

 50%|████▉     | 1921/3844 [3:58:01<3:36:40,  6.76s/it]

 50%|█████     | 1922/3844 [3:58:08<3:39:52,  6.86s/it]

 50%|█████     | 1923/3844 [3:58:18<4:11:52,  7.87s/it]

 50%|█████     | 1924/3844 [3:58:26<4:04:45,  7.65s/it]

 50%|█████     | 1925/3844 [3:58:36<4:35:44,  8.62s/it]

 50%|█████     | 1926/3844 [3:58:44<4:29:20,  8.43s/it]

 50%|█████     | 1927/3844 [3:58:53<4:32:34,  8.53s/it]

 50%|█████     | 1928/3844 [3:59:00<4:19:55,  8.14s/it]

 50%|█████     | 1929/3844 [3:59:07<4:07:25,  7.75s/it]

 50%|█████     | 1930/3844 [3:59:15<4:08:32,  7.79s/it]

 50%|█████     | 1931/3844 [3:59:23<4:10:50,  7.87s/it]

 50%|█████     | 1932/3844 [3:59:31<4:11:46,  7.90s/it]

 50%|█████     | 1933/3844 [3:59:40<4:18:46,  8.12s/it]

 50%|█████     | 1934/3844 [3:59:46<3:55:34,  7.40s/it]

 50%|█████     | 1935/3844 [3:59:54<4:01:52,  7.60s/it]

 50%|█████     | 1936/3844 [4:00:02<4:07:40,  7.79s/it]

 50%|█████     | 1937/3844 [4:00:14<4:49:41,  9.11s/it]

 50%|█████     | 1938/3844 [4:00:25<5:10:25,  9.77s/it]

 50%|█████     | 1939/3844 [4:00:34<4:56:10,  9.33s/it]

 50%|█████     | 1940/3844 [4:00:40<4:31:55,  8.57s/it]

 50%|█████     | 1941/3844 [4:00:51<4:55:11,  9.31s/it]

 51%|█████     | 1942/3844 [4:00:58<4:25:49,  8.39s/it]

 51%|█████     | 1943/3844 [4:01:05<4:12:02,  7.95s/it]
{'loss': 1.1797, 'grad_norm': 0.33979428254278643, 'learning_rate': 1.0311749181064401e-05, 'epoch': 0.51}


 51%|█████     | 1945/3844 [4:01:18<3:50:41,  7.29s/it]

 51%|█████     | 1946/3844 [4:01:26<3:59:09,  7.56s/it]

 51%|█████     | 1947/3844 [4:01:32<3:45:24,  7.13s/it]

 51%|█████     | 1948/3844 [4:01:38<3:37:44,  6.89s/it]

 51%|█████     | 1949/3844 [4:01:44<3:23:52,  6.46s/it]

 51%|█████     | 1950/3844 [4:01:53<3:47:26,  7.21s/it]

 51%|█████     | 1951/3844 [4:02:01<4:00:30,  7.62s/it]
{'loss': 1.1315, 'grad_norm': 0.3824672889999653, 'learning_rate': 1.0244359223762018e-05, 'epoch': 0.51}

 51%|█████     | 1952/3844 [4:02:07<3:42:25,  7.05s/it]

 51%|█████     | 1953/3844 [4:02:13<3:30:47,  6.69s/it]

 51%|█████     | 1954/3844 [4:02:21<3:43:13,  7.09s/it]


 51%|█████     | 1956/3844 [4:02:35<3:35:25,  6.85s/it]
{'loss': 1.2589, 'grad_norm': 0.3936650327843686, 'learning_rate': 1.0202234666277116e-05, 'epoch': 0.51}


 51%|█████     | 1958/3844 [4:02:48<3:27:57,  6.62s/it]

 51%|█████     | 1959/3844 [4:02:56<3:48:36,  7.28s/it]

 51%|█████     | 1960/3844 [4:03:05<3:59:04,  7.61s/it]

 51%|█████     | 1961/3844 [4:03:12<3:51:37,  7.38s/it]
{'loss': 1.2, 'grad_norm': 0.36488220260977894, 'learning_rate': 1.0160106518391622e-05, 'epoch': 0.51}


 51%|█████     | 1963/3844 [4:03:26<3:48:45,  7.30s/it]

 51%|█████     | 1964/3844 [4:03:33<3:40:56,  7.05s/it]

 51%|█████     | 1965/3844 [4:03:45<4:28:04,  8.56s/it]

 51%|█████     | 1966/3844 [4:03:51<4:05:08,  7.83s/it]
{'loss': 1.2293, 'grad_norm': 0.36475098935914374, 'learning_rate': 1.011797552803333e-05, 'epoch': 0.51}


 51%|█████     | 1968/3844 [4:04:06<3:55:23,  7.53s/it]
{'loss': 1.2802, 'grad_norm': 0.3684399809640404, 'learning_rate': 1.0101122503543366e-05, 'epoch': 0.51}


 51%|█████     | 1970/3844 [4:04:17<3:24:54,  6.56s/it]

 51%|█████▏    | 1971/3844 [4:04:23<3:17:31,  6.33s/it]

 51%|█████▏    | 1972/3844 [4:04:31<3:33:23,  6.84s/it]
{'loss': 1.119, 'grad_norm': 0.4317407048848658, 'learning_rate': 1.006741564069543e-05, 'epoch': 0.51}


 51%|█████▏    | 1974/3844 [4:04:42<3:15:19,  6.27s/it]

 51%|█████▏    | 1975/3844 [4:04:49<3:13:59,  6.23s/it]

 51%|█████▏    | 1976/3844 [4:04:58<3:47:22,  7.30s/it]
{'loss': 1.0722, 'grad_norm': 0.33983231841274125, 'learning_rate': 1.0033708011848541e-05, 'epoch': 0.51}


 51%|█████▏    | 1978/3844 [4:05:15<4:05:51,  7.91s/it]

 51%|█████▏    | 1979/3844 [4:05:22<3:55:03,  7.56s/it]

 52%|█████▏    | 1980/3844 [4:05:29<3:51:05,  7.44s/it]
{'loss': 1.1832, 'grad_norm': 0.3421557575194915, 'learning_rate': 1e-05, 'epoch': 0.52}


 52%|█████▏    | 1982/3844 [4:05:43<3:43:40,  7.21s/it]

 52%|█████▏    | 1983/3844 [4:05:50<3:46:27,  7.30s/it]

 52%|█████▏    | 1984/3844 [4:05:56<3:32:43,  6.86s/it]

 52%|█████▏    | 1985/3844 [4:06:02<3:28:36,  6.73s/it]

 52%|█████▏    | 1986/3844 [4:06:10<3:37:48,  7.03s/it]

 52%|█████▏    | 1987/3844 [4:06:17<3:36:50,  7.01s/it]

 52%|█████▏    | 1988/3844 [4:06:24<3:37:38,  7.04s/it]

 52%|█████▏    | 1989/3844 [4:06:31<3:32:28,  6.87s/it]

 52%|█████▏    | 1990/3844 [4:06:38<3:34:44,  6.95s/it]

 52%|█████▏    | 1991/3844 [4:06:44<3:29:10,  6.77s/it]

 52%|█████▏    | 1992/3844 [4:06:53<3:45:59,  7.32s/it]

 52%|█████▏    | 1993/3844 [4:06:58<3:27:48,  6.74s/it]
{'loss': 1.2065, 'grad_norm': 0.3746992132379914, 'learning_rate': 9.89045094531372e-06, 'epoch': 0.52}


 52%|█████▏    | 1995/3844 [4:07:12<3:38:26,  7.09s/it]

 52%|█████▏    | 1996/3844 [4:07:21<3:49:25,  7.45s/it]

 52%|█████▏    | 1997/3844 [4:07:27<3:35:20,  7.00s/it]

 52%|█████▏    | 1998/3844 [4:07:33<3:29:14,  6.80s/it]

 52%|█████▏    | 1999/3844 [4:07:38<3:15:39,  6.36s/it]

 52%|█████▏    | 2000/3844 [4:07:45<3:21:28,  6.56s/it]
{'loss': 1.1947, 'grad_norm': 0.38161219558224246, 'learning_rate': 9.831467600698955e-06, 'epoch': 0.52}


 52%|█████▏    | 2002/3844 [4:08:01<3:44:44,  7.32s/it]

 52%|█████▏    | 2003/3844 [4:08:11<4:11:43,  8.20s/it]
[2024-05-26 15:14:44,000] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 2004/3844 [4:08:18<3:59:08,  7.80s/it]

 52%|█████▏    | 2005/3844 [4:08:26<4:04:14,  7.97s/it]

 52%|█████▏    | 2006/3844 [4:08:32<3:43:13,  7.29s/it]
{'loss': 1.111, 'grad_norm': 0.40641915053431105, 'learning_rate': 9.7809150379989e-06, 'epoch': 0.52}


 52%|█████▏    | 2008/3844 [4:08:46<3:39:07,  7.16s/it]

 52%|█████▏    | 2009/3844 [4:08:53<3:33:52,  6.99s/it]

 52%|█████▏    | 2010/3844 [4:09:01<3:43:36,  7.32s/it]

 52%|█████▏    | 2011/3844 [4:09:12<4:19:53,  8.51s/it]
[2024-05-26 15:15:45,050] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 2012/3844 [4:09:19<4:02:01,  7.93s/it]

 52%|█████▏    | 2013/3844 [4:09:25<3:44:03,  7.34s/it]

 52%|█████▏    | 2014/3844 [4:09:31<3:35:34,  7.07s/it]

 52%|█████▏    | 2015/3844 [4:09:41<3:57:01,  7.78s/it]
{'loss': 1.031, 'grad_norm': 0.3399249196590708, 'learning_rate': 9.705097099572197e-06, 'epoch': 0.52}


 52%|█████▏    | 2017/3844 [4:09:54<3:39:47,  7.22s/it]

 52%|█████▏    | 2018/3844 [4:10:02<3:50:18,  7.57s/it]
{'loss': 1.1794, 'grad_norm': 0.39860926832748705, 'learning_rate': 9.679828007707871e-06, 'epoch': 0.52}


 53%|█████▎    | 2020/3844 [4:10:13<3:17:47,  6.51s/it]

 53%|█████▎    | 2021/3844 [4:10:19<3:10:18,  6.26s/it]

 53%|█████▎    | 2022/3844 [4:10:25<3:06:06,  6.13s/it]

 53%|█████▎    | 2023/3844 [4:10:31<3:03:14,  6.04s/it]

 53%|█████▎    | 2024/3844 [4:10:37<3:01:54,  6.00s/it]

 53%|█████▎    | 2025/3844 [4:10:42<2:55:56,  5.80s/it]
{'loss': 1.2185, 'grad_norm': 0.40206144412825245, 'learning_rate': 9.620875030384882e-06, 'epoch': 0.53}

 53%|█████▎    | 2026/3844 [4:10:50<3:14:40,  6.42s/it]

 53%|█████▎    | 2027/3844 [4:10:56<3:08:50,  6.24s/it]


 53%|█████▎    | 2029/3844 [4:11:11<3:26:53,  6.84s/it]

 53%|█████▎    | 2030/3844 [4:11:19<3:37:02,  7.18s/it]
{'loss': 1.1515, 'grad_norm': 0.3624883567456835, 'learning_rate': 9.578773718275696e-06, 'epoch': 0.53}

 53%|█████▎    | 2031/3844 [4:11:28<3:46:59,  7.51s/it]


 53%|█████▎    | 2033/3844 [4:11:43<3:54:01,  7.75s/it]

 53%|█████▎    | 2034/3844 [4:11:50<3:48:46,  7.58s/it]

 53%|█████▎    | 2035/3844 [4:12:00<4:05:19,  8.14s/it]

 53%|█████▎    | 2036/3844 [4:12:06<3:52:33,  7.72s/it]

 53%|█████▎    | 2037/3844 [4:12:13<3:45:33,  7.49s/it]
{'loss': 1.2293, 'grad_norm': 0.37977239963086823, 'learning_rate': 9.519844612267526e-06, 'epoch': 0.53}

 53%|█████▎    | 2038/3844 [4:12:20<3:35:44,  7.17s/it]


 53%|█████▎    | 2040/3844 [4:12:32<3:23:40,  6.77s/it]

 53%|█████▎    | 2041/3844 [4:12:39<3:25:58,  6.85s/it]
{'loss': 1.066, 'grad_norm': 0.39573044267161045, 'learning_rate': 9.486178207458732e-06, 'epoch': 0.53}

 53%|█████▎    | 2042/3844 [4:12:46<3:22:52,  6.76s/it]

 53%|█████▎    | 2043/3844 [4:12:54<3:34:30,  7.15s/it]

 53%|█████▎    | 2044/3844 [4:13:02<3:39:01,  7.30s/it]


 53%|█████▎    | 2046/3844 [4:13:18<3:48:14,  7.62s/it]

 53%|█████▎    | 2047/3844 [4:13:25<3:49:02,  7.65s/it]

 53%|█████▎    | 2048/3844 [4:13:31<3:31:48,  7.08s/it]

 53%|█████▎    | 2049/3844 [4:13:37<3:20:25,  6.70s/it]

 53%|█████▎    | 2050/3844 [4:13:46<3:46:17,  7.57s/it]

 53%|█████▎    | 2051/3844 [4:13:53<3:32:31,  7.11s/it]

 53%|█████▎    | 2052/3844 [4:13:59<3:29:58,  7.03s/it]

 53%|█████▎    | 2053/3844 [4:14:06<3:22:47,  6.79s/it]
{'loss': 1.2063, 'grad_norm': 0.3576810913266379, 'learning_rate': 9.385215552097475e-06, 'epoch': 0.53}

 53%|█████▎    | 2054/3844 [4:14:14<3:36:13,  7.25s/it]

 53%|█████▎    | 2055/3844 [4:14:20<3:24:14,  6.85s/it]


 54%|█████▎    | 2057/3844 [4:14:33<3:21:54,  6.78s/it]

 54%|█████▎    | 2058/3844 [4:14:39<3:16:17,  6.59s/it]
{'loss': 1.0716, 'grad_norm': 0.4112613620725158, 'learning_rate': 9.343165741529927e-06, 'epoch': 0.54}


 54%|█████▎    | 2060/3844 [4:14:55<3:33:06,  7.17s/it]

 54%|█████▎    | 2061/3844 [4:15:03<3:38:01,  7.34s/it]

 54%|█████▎    | 2062/3844 [4:15:09<3:31:48,  7.13s/it]

 54%|█████▎    | 2063/3844 [4:15:21<4:13:03,  8.53s/it]
{'loss': 1.0002, 'grad_norm': 0.34243078760583884, 'learning_rate': 9.301127592158516e-06, 'epoch': 0.54}


 54%|█████▎    | 2065/3844 [4:15:34<3:37:21,  7.33s/it]

 54%|█████▎    | 2066/3844 [4:15:47<4:34:38,  9.27s/it]

 54%|█████▍    | 2067/3844 [4:15:56<4:24:41,  8.94s/it]
{'loss': 1.0164, 'grad_norm': 0.3591273843013035, 'learning_rate': 9.267505970263662e-06, 'epoch': 0.54}


 54%|█████▍    | 2069/3844 [4:16:09<3:52:10,  7.85s/it]

 54%|█████▍    | 2070/3844 [4:16:15<3:33:59,  7.24s/it]

 54%|█████▍    | 2071/3844 [4:16:21<3:20:17,  6.78s/it]

 54%|█████▍    | 2072/3844 [4:16:29<3:34:52,  7.28s/it]

 54%|█████▍    | 2073/3844 [4:16:35<3:22:25,  6.86s/it]

 54%|█████▍    | 2074/3844 [4:16:42<3:22:49,  6.88s/it]

 54%|█████▍    | 2075/3844 [4:16:49<3:23:16,  6.89s/it]

 54%|█████▍    | 2076/3844 [4:16:56<3:22:48,  6.88s/it]

 54%|█████▍    | 2077/3844 [4:17:01<3:10:04,  6.45s/it]

 54%|█████▍    | 2078/3844 [4:17:08<3:09:57,  6.45s/it]

 54%|█████▍    | 2079/3844 [4:17:13<3:01:28,  6.17s/it]

 54%|█████▍    | 2080/3844 [4:17:22<3:23:34,  6.92s/it]

 54%|█████▍    | 2081/3844 [4:17:33<3:57:59,  8.10s/it]

 54%|█████▍    | 2082/3844 [4:17:42<4:13:50,  8.64s/it]
{'loss': 1.3425, 'grad_norm': 0.3963649766072036, 'learning_rate': 9.141502131168753e-06, 'epoch': 0.54}


 54%|█████▍    | 2084/3844 [4:18:00<4:11:16,  8.57s/it]

 54%|█████▍    | 2085/3844 [4:18:06<3:46:03,  7.71s/it]
{'loss': 1.15, 'grad_norm': 0.387489994186217, 'learning_rate': 9.116317180039478e-06, 'epoch': 0.54}


 54%|█████▍    | 2087/3844 [4:18:21<3:45:15,  7.69s/it]

 54%|█████▍    | 2088/3844 [4:18:27<3:32:06,  7.25s/it]

 54%|█████▍    | 2089/3844 [4:18:33<3:26:11,  7.05s/it]

 54%|█████▍    | 2090/3844 [4:18:44<3:56:54,  8.10s/it]

 54%|█████▍    | 2091/3844 [4:18:51<3:43:56,  7.66s/it]

 54%|█████▍    | 2092/3844 [4:18:57<3:31:47,  7.25s/it]
{'loss': 1.0783, 'grad_norm': 0.3838313312040674, 'learning_rate': 9.057574536251282e-06, 'epoch': 0.54}


 54%|█████▍    | 2094/3844 [4:19:11<3:28:19,  7.14s/it]
{'loss': 1.0849, 'grad_norm': 0.39991874937854865, 'learning_rate': 9.04079685770102e-06, 'epoch': 0.54}


 55%|█████▍    | 2096/3844 [4:19:25<3:21:11,  6.91s/it]

 55%|█████▍    | 2097/3844 [4:19:34<3:40:01,  7.56s/it]

 55%|█████▍    | 2098/3844 [4:19:44<4:03:02,  8.35s/it]

 55%|█████▍    | 2099/3844 [4:19:49<3:37:44,  7.49s/it]

 55%|█████▍    | 2100/3844 [4:19:57<3:37:20,  7.48s/it]
 55%|█████▍    | 2100/3844 [4:19:57<3:37:20,  7.48s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 55%|█████▍    | 2101/3844 [4:20:29<7:10:54, 14.83s/it]

 55%|█████▍    | 2102/3844 [4:20:37<6:13:15, 12.86s/it]

 55%|█████▍    | 2103/3844 [4:20:47<5:45:48, 11.92s/it]

 55%|█████▍    | 2104/3844 [4:20:57<5:27:05, 11.28s/it]

 55%|█████▍    | 2105/3844 [4:21:03<4:42:44,  9.76s/it]

 55%|█████▍    | 2106/3844 [4:21:09<4:09:48,  8.62s/it]

 55%|█████▍    | 2107/3844 [4:21:15<3:51:34,  8.00s/it]

 55%|█████▍    | 2108/3844 [4:21:23<3:49:59,  7.95s/it]

 55%|█████▍    | 2109/3844 [4:21:30<3:37:56,  7.54s/it]

 55%|█████▍    | 2110/3844 [4:21:35<3:20:50,  6.95s/it]

 55%|█████▍    | 2111/3844 [4:21:41<3:12:25,  6.66s/it]

 55%|█████▍    | 2112/3844 [4:21:47<3:06:09,  6.45s/it]

 55%|█████▍    | 2113/3844 [4:21:54<3:06:00,  6.45s/it]

 55%|█████▍    | 2114/3844 [4:22:03<3:30:34,  7.30s/it]
{'loss': 0.9435, 'grad_norm': 0.3964200428284695, 'learning_rate': 8.873177790306335e-06, 'epoch': 0.55}

 55%|█████▌    | 2115/3844 [4:22:13<3:50:27,  8.00s/it]

 55%|█████▌    | 2116/3844 [4:22:19<3:31:43,  7.35s/it]

 55%|█████▌    | 2117/3844 [4:22:25<3:20:00,  6.95s/it]


 55%|█████▌    | 2119/3844 [4:22:36<3:02:41,  6.35s/it]

 55%|█████▌    | 2120/3844 [4:22:42<2:55:08,  6.10s/it]

 55%|█████▌    | 2121/3844 [4:22:48<2:56:44,  6.15s/it]

 55%|█████▌    | 2122/3844 [4:22:56<3:09:46,  6.61s/it]
{'loss': 1.174, 'grad_norm': 0.38213054237573263, 'learning_rate': 8.806217121224381e-06, 'epoch': 0.55}


 55%|█████▌    | 2124/3844 [4:23:08<3:01:50,  6.34s/it]

 55%|█████▌    | 2125/3844 [4:23:16<3:14:40,  6.79s/it]

 55%|█████▌    | 2126/3844 [4:23:21<3:03:05,  6.39s/it]

 55%|█████▌    | 2127/3844 [4:23:29<3:13:54,  6.78s/it]

 55%|█████▌    | 2128/3844 [4:23:35<3:07:11,  6.55s/it]

 55%|█████▌    | 2129/3844 [4:23:43<3:20:01,  7.00s/it]

 55%|█████▌    | 2130/3844 [4:23:49<3:14:15,  6.80s/it]
{'loss': 1.0416, 'grad_norm': 0.4269904475556787, 'learning_rate': 8.739310708622234e-06, 'epoch': 0.55}

 55%|█████▌    | 2131/3844 [4:23:59<3:34:28,  7.51s/it]

 55%|█████▌    | 2132/3844 [4:24:06<3:37:11,  7.61s/it]

 55%|█████▌    | 2133/3844 [4:24:12<3:22:59,  7.12s/it]


 56%|█████▌    | 2135/3844 [4:24:31<3:49:15,  8.05s/it]
{'loss': 1.1627, 'grad_norm': 0.3687593472562744, 'learning_rate': 8.697523104127313e-06, 'epoch': 0.56}


 56%|█████▌    | 2137/3844 [4:24:45<3:32:01,  7.45s/it]

 56%|█████▌    | 2138/3844 [4:24:53<3:35:40,  7.59s/it]

 56%|█████▌    | 2139/3844 [4:25:00<3:33:11,  7.50s/it]
{'loss': 1.3288, 'grad_norm': 0.349472213898835, 'learning_rate': 8.664109633998966e-06, 'epoch': 0.56}


 56%|█████▌    | 2141/3844 [4:25:18<3:55:34,  8.30s/it]

 56%|█████▌    | 2142/3844 [4:25:25<3:47:56,  8.04s/it]

 56%|█████▌    | 2143/3844 [4:25:37<4:20:29,  9.19s/it]

 56%|█████▌    | 2144/3844 [4:25:43<3:55:35,  8.31s/it]

 56%|█████▌    | 2145/3844 [4:25:51<3:53:17,  8.24s/it]

 56%|█████▌    | 2146/3844 [4:25:58<3:43:14,  7.89s/it]
{'loss': 1.1698, 'grad_norm': 0.36392848385268906, 'learning_rate': 8.605672813625649e-06, 'epoch': 0.56}

 56%|█████▌    | 2147/3844 [4:26:05<3:28:35,  7.38s/it]


 56%|█████▌    | 2149/3844 [4:26:20<3:30:48,  7.46s/it]

 56%|█████▌    | 2150/3844 [4:26:26<3:18:42,  7.04s/it]

 56%|█████▌    | 2151/3844 [4:26:32<3:06:21,  6.60s/it]

 56%|█████▌    | 2152/3844 [4:26:39<3:16:06,  6.95s/it]

 56%|█████▌    | 2153/3844 [4:26:46<3:16:04,  6.96s/it]

 56%|█████▌    | 2154/3844 [4:26:54<3:18:37,  7.05s/it]

 56%|█████▌    | 2155/3844 [4:27:01<3:25:26,  7.30s/it]
{'loss': 1.0799, 'grad_norm': 0.3742319141979581, 'learning_rate': 8.530611335733024e-06, 'epoch': 0.56}


 56%|█████▌    | 2157/3844 [4:27:14<3:09:59,  6.76s/it]

 56%|█████▌    | 2158/3844 [4:27:20<2:59:16,  6.38s/it]

 56%|█████▌    | 2159/3844 [4:27:25<2:51:56,  6.12s/it]
{'loss': 1.2007, 'grad_norm': 0.40727213758665687, 'learning_rate': 8.497277552795692e-06, 'epoch': 0.56}


 56%|█████▌    | 2161/3844 [4:27:40<3:13:29,  6.90s/it]

 56%|█████▌    | 2162/3844 [4:27:46<3:04:32,  6.58s/it]
{'loss': 1.1556, 'grad_norm': 0.3938901936590613, 'learning_rate': 8.472288399983543e-06, 'epoch': 0.56}


 56%|█████▋    | 2164/3844 [4:28:04<3:39:54,  7.85s/it]
{'loss': 1.1206, 'grad_norm': 0.4044103816513046, 'learning_rate': 8.455634379411314e-06, 'epoch': 0.56}

 56%|█████▋    | 2165/3844 [4:28:11<3:27:25,  7.41s/it]


 56%|█████▋    | 2167/3844 [4:28:24<3:19:17,  7.13s/it]

 56%|█████▋    | 2168/3844 [4:28:31<3:12:14,  6.88s/it]

 56%|█████▋    | 2169/3844 [4:28:38<3:14:54,  6.98s/it]

 56%|█████▋    | 2170/3844 [4:28:45<3:12:56,  6.92s/it]

 56%|█████▋    | 2171/3844 [4:28:54<3:32:27,  7.62s/it]

 57%|█████▋    | 2172/3844 [4:29:04<3:53:47,  8.39s/it]

 57%|█████▋    | 2173/3844 [4:29:11<3:38:49,  7.86s/it]

 57%|█████▋    | 2174/3844 [4:29:16<3:20:29,  7.20s/it]

 57%|█████▋    | 2175/3844 [4:29:26<3:39:09,  7.88s/it]

 57%|█████▋    | 2176/3844 [4:29:31<3:20:28,  7.21s/it]

 57%|█████▋    | 2177/3844 [4:29:37<3:09:18,  6.81s/it]

 57%|█████▋    | 2178/3844 [4:29:48<3:42:20,  8.01s/it]
{'loss': 1.1172, 'grad_norm': 0.3663924438941696, 'learning_rate': 8.339181716281294e-06, 'epoch': 0.57}


 57%|█████▋    | 2180/3844 [4:30:04<3:43:09,  8.05s/it]
{'loss': 1.195, 'grad_norm': 0.38222138619488844, 'learning_rate': 8.322564114463552e-06, 'epoch': 0.57}


 57%|█████▋    | 2182/3844 [4:30:23<3:58:11,  8.60s/it]

 57%|█████▋    | 2183/3844 [4:30:29<3:42:14,  8.03s/it]

 57%|█████▋    | 2184/3844 [4:30:37<3:41:02,  7.99s/it]
{'loss': 1.181, 'grad_norm': 0.3872102881320761, 'learning_rate': 8.2893432527171e-06, 'epoch': 0.57}

 57%|█████▋    | 2185/3844 [4:30:43<3:19:52,  7.23s/it]


 57%|█████▋    | 2187/3844 [4:31:00<3:37:35,  7.88s/it]
{'loss': 1.0153, 'grad_norm': 0.3673308638330518, 'learning_rate': 8.264440341337843e-06, 'epoch': 0.57}


 57%|█████▋    | 2189/3844 [4:31:15<3:30:59,  7.65s/it]

 57%|█████▋    | 2190/3844 [4:31:26<4:05:23,  8.90s/it]
{'loss': 0.9704, 'grad_norm': 0.3904259848075117, 'learning_rate': 8.239548522466904e-06, 'epoch': 0.57}


 57%|█████▋    | 2192/3844 [4:31:41<3:41:09,  8.03s/it]

 57%|█████▋    | 2193/3844 [4:31:47<3:25:46,  7.48s/it]

 57%|█████▋    | 2194/3844 [4:31:54<3:25:00,  7.45s/it]

 57%|█████▋    | 2195/3844 [4:32:02<3:24:19,  7.43s/it]
{'loss': 0.986, 'grad_norm': 0.3647299065025622, 'learning_rate': 8.198087239768202e-06, 'epoch': 0.57}

 57%|█████▋    | 2196/3844 [4:32:09<3:25:56,  7.50s/it]


 57%|█████▋    | 2198/3844 [4:32:22<3:09:18,  6.90s/it]
{'loss': 1.2658, 'grad_norm': 0.37744823246939674, 'learning_rate': 8.173225778504094e-06, 'epoch': 0.57}


 57%|█████▋    | 2200/3844 [4:32:38<3:25:24,  7.50s/it]
{'loss': 1.1964, 'grad_norm': 0.36770748573819695, 'learning_rate': 8.156657947572216e-06, 'epoch': 0.57}


 57%|█████▋    | 2202/3844 [4:32:54<3:25:55,  7.52s/it]

 57%|█████▋    | 2203/3844 [4:33:00<3:16:21,  7.18s/it]

 57%|█████▋    | 2204/3844 [4:33:06<3:06:48,  6.83s/it]

 57%|█████▋    | 2205/3844 [4:33:13<3:03:26,  6.72s/it]

 57%|█████▋    | 2206/3844 [4:33:19<2:57:08,  6.49s/it]

 57%|█████▋    | 2207/3844 [4:33:27<3:09:49,  6.96s/it]

 57%|█████▋    | 2208/3844 [4:33:33<3:02:07,  6.68s/it]

 57%|█████▋    | 2209/3844 [4:33:40<3:10:05,  6.98s/it]

 57%|█████▋    | 2210/3844 [4:33:46<2:58:18,  6.55s/it]

 58%|█████▊    | 2211/3844 [4:33:54<3:07:28,  6.89s/it]
{'loss': 1.2032, 'grad_norm': 0.40220494290854913, 'learning_rate': 8.065629735096219e-06, 'epoch': 0.58}


 58%|█████▊    | 2213/3844 [4:34:10<3:34:26,  7.89s/it]

 58%|█████▊    | 2214/3844 [4:34:18<3:34:38,  7.90s/it]

 58%|█████▊    | 2215/3844 [4:34:27<3:41:47,  8.17s/it]
{'loss': 1.0384, 'grad_norm': 0.3754017124165729, 'learning_rate': 8.032569366288116e-06, 'epoch': 0.58}


 58%|█████▊    | 2217/3844 [4:34:40<3:16:54,  7.26s/it]

 58%|█████▊    | 2218/3844 [4:34:50<3:41:51,  8.19s/it]

 58%|█████▊    | 2219/3844 [4:34:59<3:46:10,  8.35s/it]

 58%|█████▊    | 2220/3844 [4:35:05<3:25:09,  7.58s/it]

 58%|█████▊    | 2221/3844 [4:35:10<3:07:13,  6.92s/it]

 58%|█████▊    | 2222/3844 [4:35:16<2:57:38,  6.57s/it]

 58%|█████▊    | 2223/3844 [4:35:24<3:11:47,  7.10s/it]
{'loss': 1.1861, 'grad_norm': 0.40908964953821286, 'learning_rate': 7.9665160678663e-06, 'epoch': 0.58}

 58%|█████▊    | 2224/3844 [4:35:32<3:14:49,  7.22s/it]

 58%|█████▊    | 2225/3844 [4:35:37<3:04:29,  6.84s/it]


 58%|█████▊    | 2227/3844 [4:35:49<2:47:25,  6.21s/it]

 58%|█████▊    | 2228/3844 [4:35:55<2:45:20,  6.14s/it]

 58%|█████▊    | 2229/3844 [4:36:02<2:57:27,  6.59s/it]

 58%|█████▊    | 2230/3844 [4:36:12<3:23:03,  7.55s/it]

 58%|█████▊    | 2231/3844 [4:36:24<3:57:33,  8.84s/it]

 58%|█████▊    | 2232/3844 [4:36:31<3:42:35,  8.28s/it]

 58%|█████▊    | 2233/3844 [4:36:37<3:23:49,  7.59s/it]
{'loss': 1.2626, 'grad_norm': 0.3667038648632948, 'learning_rate': 7.88407976218472e-06, 'epoch': 0.58}


 58%|█████▊    | 2235/3844 [4:36:51<3:12:19,  7.17s/it]

 58%|█████▊    | 2236/3844 [4:36:59<3:20:34,  7.48s/it]

 58%|█████▊    | 2237/3844 [4:37:05<3:07:33,  7.00s/it]

 58%|█████▊    | 2238/3844 [4:37:10<2:58:09,  6.66s/it]

 58%|█████▊    | 2239/3844 [4:37:18<3:05:28,  6.93s/it]
{'loss': 1.2173, 'grad_norm': 0.34058617537418434, 'learning_rate': 7.834689729561081e-06, 'epoch': 0.58}


 58%|█████▊    | 2241/3844 [4:37:31<2:57:56,  6.66s/it]
{'loss': 1.1981, 'grad_norm': 0.379758987966434, 'learning_rate': 7.818238624513236e-06, 'epoch': 0.58}


 58%|█████▊    | 2243/3844 [4:37:45<3:03:47,  6.89s/it]

 58%|█████▊    | 2244/3844 [4:37:53<3:13:47,  7.27s/it]
{'loss': 1.35, 'grad_norm': 0.41306016741156143, 'learning_rate': 7.793573601813467e-06, 'epoch': 0.58}


 58%|█████▊    | 2246/3844 [4:38:08<3:19:19,  7.48s/it]

 58%|█████▊    | 2247/3844 [4:38:17<3:30:56,  7.93s/it]

 58%|█████▊    | 2248/3844 [4:38:24<3:22:15,  7.60s/it]

 59%|█████▊    | 2249/3844 [4:38:32<3:26:41,  7.78s/it]

 59%|█████▊    | 2250/3844 [4:38:43<3:47:00,  8.55s/it]

 59%|█████▊    | 2251/3844 [4:38:50<3:38:57,  8.25s/it]

 59%|█████▊    | 2252/3844 [4:38:56<3:23:10,  7.66s/it]

 59%|█████▊    | 2253/3844 [4:39:04<3:25:34,  7.75s/it]

 59%|█████▊    | 2254/3844 [4:39:10<3:10:24,  7.19s/it]

 59%|█████▊    | 2255/3844 [4:39:17<3:03:45,  6.94s/it]

 59%|█████▊    | 2256/3844 [4:39:23<2:57:47,  6.72s/it]
{'loss': 1.1615, 'grad_norm': 0.3744273636243238, 'learning_rate': 7.695056105734936e-06, 'epoch': 0.59}


 59%|█████▊    | 2258/3844 [4:39:35<2:46:52,  6.31s/it]

 59%|█████▉    | 2259/3844 [4:39:41<2:43:30,  6.19s/it]

 59%|█████▉    | 2260/3844 [4:39:49<2:57:58,  6.74s/it]

 59%|█████▉    | 2261/3844 [4:39:57<3:08:02,  7.13s/it]
{'loss': 1.1798, 'grad_norm': 0.38492925778101467, 'learning_rate': 7.654076137146179e-06, 'epoch': 0.59}


 59%|█████▉    | 2263/3844 [4:40:11<3:07:38,  7.12s/it]

 59%|█████▉    | 2264/3844 [4:40:17<2:57:01,  6.72s/it]

 59%|█████▉    | 2265/3844 [4:40:22<2:48:17,  6.40s/it]

 59%|█████▉    | 2266/3844 [4:40:37<3:51:05,  8.79s/it]

 59%|█████▉    | 2267/3844 [4:40:46<3:57:49,  9.05s/it]

 59%|█████▉    | 2268/3844 [4:40:57<4:10:12,  9.53s/it]
{'loss': 1.019, 'grad_norm': 0.38445961103621, 'learning_rate': 7.596774313729619e-06, 'epoch': 0.59}


 59%|█████▉    | 2270/3844 [4:41:12<3:42:39,  8.49s/it]

 59%|█████▉    | 2271/3844 [4:41:23<3:58:16,  9.09s/it]

 59%|█████▉    | 2272/3844 [4:41:29<3:31:41,  8.08s/it]

 59%|█████▉    | 2273/3844 [4:41:35<3:18:46,  7.59s/it]
{'loss': 1.2062, 'grad_norm': 0.37728671404232206, 'learning_rate': 7.555895522829897e-06, 'epoch': 0.59}


 59%|█████▉    | 2275/3844 [4:41:51<3:20:27,  7.67s/it]

 59%|█████▉    | 2276/3844 [4:41:58<3:16:51,  7.53s/it]
{'loss': 1.185, 'grad_norm': 0.37049423623874495, 'learning_rate': 7.531389029909592e-06, 'epoch': 0.59}


 59%|█████▉    | 2278/3844 [4:42:15<3:28:35,  7.99s/it]

 59%|█████▉    | 2279/3844 [4:42:23<3:28:42,  8.00s/it]

 59%|█████▉    | 2280/3844 [4:42:29<3:10:50,  7.32s/it]
{'loss': 1.1125, 'grad_norm': 0.42332115978739987, 'learning_rate': 7.498738276115336e-06, 'epoch': 0.59}


 59%|█████▉    | 2282/3844 [4:42:47<3:38:44,  8.40s/it]
{'loss': 1.1233, 'grad_norm': 0.35947315497858034, 'learning_rate': 7.48242353360712e-06, 'epoch': 0.59}


 59%|█████▉    | 2284/3844 [4:43:10<4:22:27, 10.09s/it]
{'loss': 1.1943, 'grad_norm': 0.3875007975863903, 'learning_rate': 7.466115942489465e-06, 'epoch': 0.59}


 59%|█████▉    | 2286/3844 [4:43:23<3:34:27,  8.26s/it]

 59%|█████▉    | 2287/3844 [4:43:34<3:57:06,  9.14s/it]

 60%|█████▉    | 2288/3844 [4:43:40<3:31:07,  8.14s/it]

 60%|█████▉    | 2289/3844 [4:43:49<3:35:06,  8.30s/it]

 60%|█████▉    | 2290/3844 [4:43:54<3:14:12,  7.50s/it]

 60%|█████▉    | 2291/3844 [4:44:00<3:00:44,  6.98s/it]

 60%|█████▉    | 2292/3844 [4:44:09<3:16:31,  7.60s/it]

 60%|█████▉    | 2293/3844 [4:44:16<3:06:29,  7.21s/it]

 60%|█████▉    | 2294/3844 [4:44:23<3:06:32,  7.22s/it]

 60%|█████▉    | 2295/3844 [4:44:29<2:55:09,  6.78s/it]

 60%|█████▉    | 2296/3844 [4:44:35<2:51:40,  6.65s/it]

 60%|█████▉    | 2297/3844 [4:44:42<2:51:20,  6.65s/it]

 60%|█████▉    | 2298/3844 [4:44:49<3:01:00,  7.03s/it]

 60%|█████▉    | 2299/3844 [4:44:56<3:00:28,  7.01s/it]

 60%|█████▉    | 2300/3844 [4:45:06<3:23:05,  7.89s/it]
{'loss': 1.1161, 'grad_norm': 0.37914846080722175, 'learning_rate': 7.335918218053926e-06, 'epoch': 0.6}


 60%|█████▉    | 2302/3844 [4:45:20<3:08:57,  7.35s/it]

 60%|█████▉    | 2303/3844 [4:45:31<3:37:25,  8.47s/it]

 60%|█████▉    | 2304/3844 [4:45:38<3:23:17,  7.92s/it]

 60%|█████▉    | 2305/3844 [4:45:44<3:15:25,  7.62s/it]

 60%|█████▉    | 2306/3844 [4:45:51<3:03:52,  7.17s/it]

 60%|██████    | 2307/3844 [4:46:01<3:32:20,  8.29s/it]

 60%|██████    | 2308/3844 [4:46:08<3:15:55,  7.65s/it]

 60%|██████    | 2309/3844 [4:46:16<3:19:06,  7.78s/it]

 60%|██████    | 2310/3844 [4:46:23<3:14:24,  7.60s/it]

 60%|██████    | 2311/3844 [4:46:29<3:04:32,  7.22s/it]
{'loss': 1.1898, 'grad_norm': 0.38385127118041157, 'learning_rate': 7.246686796978537e-06, 'epoch': 0.6}


 60%|██████    | 2313/3844 [4:46:46<3:10:11,  7.45s/it]

 60%|██████    | 2314/3844 [4:46:51<2:58:28,  7.00s/it]

 60%|██████    | 2315/3844 [4:46:59<2:59:51,  7.06s/it]
{'loss': 1.1865, 'grad_norm': 0.4157866161633859, 'learning_rate': 7.214297262127847e-06, 'epoch': 0.6}


 60%|██████    | 2317/3844 [4:47:11<2:49:15,  6.65s/it]
{'loss': 1.0651, 'grad_norm': 0.368748516522425, 'learning_rate': 7.198114341245914e-06, 'epoch': 0.6}


 60%|██████    | 2319/3844 [4:47:27<3:04:09,  7.25s/it]
{'loss': 1.0711, 'grad_norm': 0.3639674194529299, 'learning_rate': 7.181939379359036e-06, 'epoch': 0.6}


 60%|██████    | 2321/3844 [4:47:40<2:51:44,  6.77s/it]
{'loss': 1.1235, 'grad_norm': 0.3990599112850985, 'learning_rate': 7.165772422413571e-06, 'epoch': 0.6}

 60%|██████    | 2322/3844 [4:47:48<3:04:21,  7.27s/it]


 60%|██████    | 2324/3844 [4:48:00<2:42:37,  6.42s/it]

 60%|██████    | 2325/3844 [4:48:05<2:36:45,  6.19s/it]

 61%|██████    | 2326/3844 [4:48:16<3:07:47,  7.42s/it]

 61%|██████    | 2327/3844 [4:48:23<3:05:52,  7.35s/it]
{'loss': 1.1293, 'grad_norm': 0.3793227181867054, 'learning_rate': 7.117320040347371e-06, 'epoch': 0.61}


 61%|██████    | 2329/3844 [4:48:40<3:24:10,  8.09s/it]
{'loss': 1.122, 'grad_norm': 0.3855837176025653, 'learning_rate': 7.101185562174406e-06, 'epoch': 0.61}


 61%|██████    | 2331/3844 [4:48:53<3:07:18,  7.43s/it]

 61%|██████    | 2332/3844 [4:49:01<3:05:25,  7.36s/it]

 61%|██████    | 2333/3844 [4:49:09<3:15:20,  7.76s/it]
{'loss': 1.0356, 'grad_norm': 0.37816720695319656, 'learning_rate': 7.068941354624994e-06, 'epoch': 0.61}

 61%|██████    | 2334/3844 [4:49:16<3:09:24,  7.53s/it]


 61%|██████    | 2336/3844 [4:49:35<3:23:44,  8.11s/it]

 61%|██████    | 2337/3844 [4:49:41<3:11:00,  7.60s/it]

 61%|██████    | 2338/3844 [4:49:49<3:11:22,  7.62s/it]
{'loss': 1.0005, 'grad_norm': 0.3736672927808449, 'learning_rate': 7.028682971363798e-06, 'epoch': 0.61}


 61%|██████    | 2340/3844 [4:50:06<3:17:43,  7.89s/it]

 61%|██████    | 2341/3844 [4:50:12<3:06:38,  7.45s/it]

 61%|██████    | 2342/3844 [4:50:18<2:49:52,  6.79s/it]

 61%|██████    | 2343/3844 [4:50:25<2:53:56,  6.95s/it]

 61%|██████    | 2344/3844 [4:50:34<3:08:49,  7.55s/it]
{'loss': 1.1378, 'grad_norm': 0.38230905588704345, 'learning_rate': 6.980442606498773e-06, 'epoch': 0.61}

 61%|██████    | 2345/3844 [4:50:40<2:59:10,  7.17s/it]


 61%|██████    | 2347/3844 [4:50:55<3:04:51,  7.41s/it]

 61%|██████    | 2348/3844 [4:51:01<2:52:26,  6.92s/it]

 61%|██████    | 2349/3844 [4:51:09<2:59:18,  7.20s/it]

 61%|██████    | 2350/3844 [4:51:15<2:51:33,  6.89s/it]

 61%|██████    | 2351/3844 [4:51:24<3:04:53,  7.43s/it]
{'loss': 1.1171, 'grad_norm': 0.3366301562977088, 'learning_rate': 6.92425983402768e-06, 'epoch': 0.61}


 61%|██████    | 2353/3844 [4:51:41<3:15:08,  7.85s/it]

 61%|██████    | 2354/3844 [4:51:49<3:16:25,  7.91s/it]

 61%|██████▏   | 2355/3844 [4:51:55<3:06:14,  7.50s/it]

 61%|██████▏   | 2356/3844 [4:52:05<3:20:06,  8.07s/it]

 61%|██████▏   | 2357/3844 [4:52:12<3:11:40,  7.73s/it]

 61%|██████▏   | 2358/3844 [4:52:18<2:59:34,  7.25s/it]

 61%|██████▏   | 2359/3844 [4:52:24<2:48:54,  6.82s/it]

 61%|██████▏   | 2360/3844 [4:52:30<2:46:30,  6.73s/it]

 61%|██████▏   | 2361/3844 [4:52:36<2:36:51,  6.35s/it]

 61%|██████▏   | 2362/3844 [4:52:42<2:37:46,  6.39s/it]
{'loss': 1.1106, 'grad_norm': 0.3651341871219418, 'learning_rate': 6.836189608978725e-06, 'epoch': 0.61}

 61%|██████▏   | 2363/3844 [4:52:49<2:38:09,  6.41s/it]


 62%|██████▏   | 2365/3844 [4:53:00<2:30:47,  6.12s/it]

 62%|██████▏   | 2366/3844 [4:53:08<2:40:15,  6.51s/it]
{'loss': 1.1076, 'grad_norm': 0.360054312143519, 'learning_rate': 6.804231077901733e-06, 'epoch': 0.62}

 62%|██████▏   | 2367/3844 [4:53:16<2:57:06,  7.19s/it]


 62%|██████▏   | 2369/3844 [4:53:30<2:54:17,  7.09s/it]

 62%|██████▏   | 2370/3844 [4:53:38<2:55:47,  7.16s/it]

 62%|██████▏   | 2371/3844 [4:53:43<2:42:00,  6.60s/it]

 62%|██████▏   | 2372/3844 [4:53:54<3:14:30,  7.93s/it]

 62%|██████▏   | 2373/3844 [4:54:02<3:13:06,  7.88s/it]

 62%|██████▏   | 2374/3844 [4:54:09<3:09:12,  7.72s/it]

 62%|██████▏   | 2375/3844 [4:54:15<2:58:54,  7.31s/it]
{'loss': 1.075, 'grad_norm': 0.3661942209826331, 'learning_rate': 6.7324576990273095e-06, 'epoch': 0.62}


 62%|██████▏   | 2377/3844 [4:54:27<2:43:59,  6.71s/it]

 62%|██████▏   | 2378/3844 [4:54:38<3:14:34,  7.96s/it]

 62%|██████▏   | 2379/3844 [4:54:47<3:21:22,  8.25s/it]
{'loss': 1.1266, 'grad_norm': 0.39206047614007683, 'learning_rate': 6.7006185054955045e-06, 'epoch': 0.62}


 62%|██████▏   | 2381/3844 [4:55:02<3:13:53,  7.95s/it]

 62%|██████▏   | 2382/3844 [4:55:08<2:55:10,  7.19s/it]

 62%|██████▏   | 2383/3844 [4:55:18<3:15:47,  8.04s/it]

 62%|██████▏   | 2384/3844 [4:55:25<3:11:07,  7.85s/it]

 62%|██████▏   | 2385/3844 [4:55:31<2:56:22,  7.25s/it]

 62%|██████▏   | 2386/3844 [4:55:37<2:48:44,  6.94s/it]

 62%|██████▏   | 2387/3844 [4:55:44<2:49:39,  6.99s/it]

 62%|██████▏   | 2388/3844 [4:55:52<2:54:37,  7.20s/it]

 62%|██████▏   | 2389/3844 [4:55:58<2:48:33,  6.95s/it]


 62%|██████▏   | 2390/3844 [4:56:04<2:38:53,  6.56s/it]
{'loss': 1.1434, 'grad_norm': 0.4026135428389194, 'learning_rate': 6.613255110847678e-06, 'epoch': 0.62}


 62%|██████▏   | 2392/3844 [4:56:18<2:42:27,  6.71s/it]

 62%|██████▏   | 2393/3844 [4:56:25<2:46:49,  6.90s/it]

 62%|██████▏   | 2394/3844 [4:56:33<2:54:12,  7.21s/it]

 62%|██████▏   | 2395/3844 [4:56:42<3:05:11,  7.67s/it]
{'loss': 1.2007, 'grad_norm': 0.37031856140299735, 'learning_rate': 6.573640229338598e-06, 'epoch': 0.62}


 62%|██████▏   | 2397/3844 [4:56:54<2:45:21,  6.86s/it]

 62%|██████▏   | 2398/3844 [4:57:00<2:37:00,  6.52s/it]
{'loss': 1.1915, 'grad_norm': 0.40908653571478837, 'learning_rate': 6.549900454030926e-06, 'epoch': 0.62}

 62%|██████▏   | 2399/3844 [4:57:07<2:40:47,  6.68s/it]

 62%|██████▏   | 2400/3844 [4:57:15<2:47:52,  6.98s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 62%|██████▏   | 2401/3844 [4:57:44<5:32:22, 13.82s/it]

 62%|██████▏   | 2402/3844 [4:57:50<4:33:59, 11.40s/it]

 63%|██████▎   | 2403/3844 [4:57:56<3:56:17,  9.84s/it]

 63%|██████▎   | 2404/3844 [4:58:02<3:25:04,  8.54s/it]

 63%|██████▎   | 2405/3844 [4:58:09<3:15:20,  8.14s/it]

 63%|██████▎   | 2406/3844 [4:58:18<3:22:16,  8.44s/it]

 63%|██████▎   | 2407/3844 [4:58:29<3:36:14,  9.03s/it]

 63%|██████▎   | 2408/3844 [4:58:36<3:25:41,  8.59s/it]
{'loss': 1.1603, 'grad_norm': 0.3542251026750949, 'learning_rate': 6.47092797577399e-06, 'epoch': 0.63}


 63%|██████▎   | 2410/3844 [4:58:51<3:10:34,  7.97s/it]

 63%|██████▎   | 2411/3844 [4:59:01<3:19:31,  8.35s/it]
{'loss': 1.0194, 'grad_norm': 0.3913569611051592, 'learning_rate': 6.4472848473804684e-06, 'epoch': 0.63}

 63%|██████▎   | 2412/3844 [4:59:07<3:05:37,  7.78s/it]


 63%|██████▎   | 2414/3844 [4:59:20<2:48:28,  7.07s/it]

 63%|██████▎   | 2415/3844 [4:59:26<2:39:27,  6.70s/it]
{'loss': 1.1194, 'grad_norm': 0.381702762934303, 'learning_rate': 6.415796023541492e-06, 'epoch': 0.63}


 63%|██████▎   | 2417/3844 [4:59:41<2:53:59,  7.32s/it]

 63%|██████▎   | 2418/3844 [4:59:47<2:40:02,  6.73s/it]

 63%|██████▎   | 2419/3844 [4:59:54<2:41:05,  6.78s/it]

 63%|██████▎   | 2420/3844 [5:00:02<2:51:39,  7.23s/it]

 63%|██████▎   | 2421/3844 [5:00:10<3:00:17,  7.60s/it]

 63%|██████▎   | 2422/3844 [5:00:18<2:57:56,  7.51s/it]

 63%|██████▎   | 2423/3844 [5:00:26<3:00:29,  7.62s/it]

 63%|██████▎   | 2424/3844 [5:00:37<3:24:36,  8.65s/it]

 63%|██████▎   | 2425/3844 [5:00:46<3:32:33,  8.99s/it]

 63%|██████▎   | 2426/3844 [5:00:54<3:20:31,  8.48s/it]

 63%|██████▎   | 2427/3844 [5:01:00<3:08:50,  8.00s/it]

 63%|██████▎   | 2428/3844 [5:01:10<3:21:53,  8.55s/it]

 63%|██████▎   | 2429/3844 [5:01:21<3:35:02,  9.12s/it]

 63%|██████▎   | 2430/3844 [5:01:28<3:22:21,  8.59s/it]

 63%|██████▎   | 2431/3844 [5:01:35<3:08:24,  8.00s/it]

 63%|██████▎   | 2432/3844 [5:01:43<3:06:37,  7.93s/it]

 63%|██████▎   | 2433/3844 [5:01:49<2:53:18,  7.37s/it]

 63%|██████▎   | 2434/3844 [5:01:57<2:58:47,  7.61s/it]

 63%|██████▎   | 2435/3844 [5:02:02<2:44:07,  6.99s/it]
{'loss': 1.2812, 'grad_norm': 0.39840777921919596, 'learning_rate': 6.258969917577431e-06, 'epoch': 0.63}


 63%|██████▎   | 2437/3844 [5:02:15<2:34:24,  6.58s/it]
{'loss': 1.2198, 'grad_norm': 0.4017152092551795, 'learning_rate': 6.243345020094888e-06, 'epoch': 0.63}


 63%|██████▎   | 2439/3844 [5:02:31<2:54:41,  7.46s/it]

 63%|██████▎   | 2440/3844 [5:02:38<2:48:37,  7.21s/it]

 64%|██████▎   | 2441/3844 [5:02:44<2:44:26,  7.03s/it]

 64%|██████▎   | 2442/3844 [5:02:50<2:33:26,  6.57s/it]
{'loss': 1.0559, 'grad_norm': 0.4098347458254244, 'learning_rate': 6.204329559460392e-06, 'epoch': 0.64}


 64%|██████▎   | 2444/3844 [5:03:04<2:38:19,  6.79s/it]

 64%|██████▎   | 2445/3844 [5:03:11<2:40:27,  6.88s/it]

 64%|██████▎   | 2446/3844 [5:03:18<2:39:37,  6.85s/it]

 64%|██████▎   | 2447/3844 [5:03:27<2:53:36,  7.46s/it]
{'loss': 1.1497, 'grad_norm': 0.36399974766644577, 'learning_rate': 6.165381485776162e-06, 'epoch': 0.64}


 64%|██████▎   | 2449/3844 [5:03:43<2:55:47,  7.56s/it]

 64%|██████▎   | 2450/3844 [5:03:49<2:46:36,  7.17s/it]
{'loss': 1.2112, 'grad_norm': 0.3948041339424216, 'learning_rate': 6.142045275003365e-06, 'epoch': 0.64}

 64%|██████▍   | 2451/3844 [5:03:55<2:41:09,  6.94s/it]

 64%|██████▍   | 2452/3844 [5:04:03<2:47:26,  7.22s/it]


 64%|██████▍   | 2454/3844 [5:04:17<2:44:26,  7.10s/it]

 64%|██████▍   | 2455/3844 [5:04:22<2:31:58,  6.56s/it]

 64%|██████▍   | 2456/3844 [5:04:28<2:28:07,  6.40s/it]

 64%|██████▍   | 2457/3844 [5:04:35<2:29:24,  6.46s/it]

 64%|██████▍   | 2458/3844 [5:04:42<2:35:22,  6.73s/it]
{'loss': 1.1729, 'grad_norm': 0.3754492484495424, 'learning_rate': 6.079936331511911e-06, 'epoch': 0.64}


 64%|██████▍   | 2460/3844 [5:04:57<2:44:25,  7.13s/it]

 64%|██████▍   | 2461/3844 [5:05:03<2:32:46,  6.63s/it]

 64%|██████▍   | 2462/3844 [5:05:10<2:38:50,  6.90s/it]
{'loss': 1.1635, 'grad_norm': 0.3747076227643369, 'learning_rate': 6.04894849506869e-06, 'epoch': 0.64}

 64%|██████▍   | 2463/3844 [5:05:17<2:40:53,  6.99s/it]


 64%|██████▍   | 2465/3844 [5:05:32<2:49:06,  7.36s/it]

 64%|██████▍   | 2466/3844 [5:05:41<2:57:32,  7.73s/it]

 64%|██████▍   | 2467/3844 [5:05:47<2:46:43,  7.26s/it]
{'loss': 1.0686, 'grad_norm': 0.3875882886376904, 'learning_rate': 6.01027687173697e-06, 'epoch': 0.64}


 64%|██████▍   | 2469/3844 [5:06:05<3:06:25,  8.13s/it]

 64%|██████▍   | 2470/3844 [5:06:11<2:51:39,  7.50s/it]

 64%|██████▍   | 2471/3844 [5:06:17<2:41:10,  7.04s/it]

 64%|██████▍   | 2472/3844 [5:06:23<2:36:20,  6.84s/it]

 64%|██████▍   | 2473/3844 [5:06:29<2:26:59,  6.43s/it]

 64%|██████▍   | 2474/3844 [5:06:35<2:26:02,  6.40s/it]

 64%|██████▍   | 2475/3844 [5:06:40<2:19:41,  6.12s/it]

 64%|██████▍   | 2476/3844 [5:06:47<2:22:01,  6.23s/it]

 64%|██████▍   | 2477/3844 [5:06:58<2:54:39,  7.67s/it]

 64%|██████▍   | 2478/3844 [5:07:05<2:52:58,  7.60s/it]
{'loss': 1.0804, 'grad_norm': 0.3911053442243745, 'learning_rate': 5.925449594186787e-06, 'epoch': 0.64}


 65%|██████▍   | 2480/3844 [5:07:21<2:56:07,  7.75s/it]

 65%|██████▍   | 2481/3844 [5:07:26<2:40:15,  7.05s/it]

 65%|██████▍   | 2482/3844 [5:07:32<2:35:00,  6.83s/it]

 65%|██████▍   | 2483/3844 [5:07:39<2:31:40,  6.69s/it]

 65%|██████▍   | 2484/3844 [5:07:46<2:34:56,  6.84s/it]
{'loss': 1.1545, 'grad_norm': 0.3774468103742356, 'learning_rate': 5.879327274649868e-06, 'epoch': 0.65}


 65%|██████▍   | 2486/3844 [5:07:57<2:19:28,  6.16s/it]

 65%|██████▍   | 2487/3844 [5:08:03<2:19:20,  6.16s/it]

 65%|██████▍   | 2488/3844 [5:08:10<2:24:39,  6.40s/it]

 65%|██████▍   | 2489/3844 [5:08:17<2:25:21,  6.44s/it]

 65%|██████▍   | 2490/3844 [5:08:23<2:26:18,  6.48s/it]

 65%|██████▍   | 2491/3844 [5:08:32<2:42:23,  7.20s/it]

 65%|██████▍   | 2492/3844 [5:08:40<2:50:51,  7.58s/it]

 65%|██████▍   | 2493/3844 [5:08:49<2:56:24,  7.83s/it]

 65%|██████▍   | 2494/3844 [5:08:59<3:13:35,  8.60s/it]

 65%|██████▍   | 2495/3844 [5:09:05<2:51:46,  7.64s/it]

 65%|██████▍   | 2496/3844 [5:09:12<2:51:25,  7.63s/it]
{'loss': 1.0856, 'grad_norm': 0.38208460759845553, 'learning_rate': 5.787399849709189e-06, 'epoch': 0.65}


 65%|██████▍   | 2498/3844 [5:09:28<2:52:47,  7.70s/it]

 65%|██████▌   | 2499/3844 [5:09:34<2:38:36,  7.08s/it]
{'loss': 1.1666, 'grad_norm': 0.38951304876389, 'learning_rate': 5.764484936824395e-06, 'epoch': 0.65}

 65%|██████▌   | 2500/3844 [5:09:42<2:43:43,  7.31s/it]


 65%|██████▌   | 2502/3844 [5:09:57<2:48:19,  7.53s/it]

 65%|██████▌   | 2503/3844 [5:10:05<2:49:56,  7.60s/it]

 65%|██████▌   | 2504/3844 [5:10:11<2:38:26,  7.09s/it]

 65%|██████▌   | 2505/3844 [5:10:17<2:27:55,  6.63s/it]

 65%|██████▌   | 2506/3844 [5:10:23<2:28:10,  6.64s/it]

 65%|██████▌   | 2507/3844 [5:10:29<2:23:41,  6.45s/it]

 65%|██████▌   | 2508/3844 [5:10:36<2:28:10,  6.65s/it]
{'loss': 1.2521, 'grad_norm': 0.36134303072371327, 'learning_rate': 5.695903206193043e-06, 'epoch': 0.65}


 65%|██████▌   | 2510/3844 [5:10:55<3:04:09,  8.28s/it]

 65%|██████▌   | 2511/3844 [5:11:01<2:52:03,  7.74s/it]

 65%|██████▌   | 2512/3844 [5:11:07<2:41:41,  7.28s/it]

 65%|██████▌   | 2513/3844 [5:11:15<2:40:50,  7.25s/it]
{'loss': 1.2685, 'grad_norm': 0.37840360946260715, 'learning_rate': 5.657908971136288e-06, 'epoch': 0.65}


 65%|██████▌   | 2515/3844 [5:11:35<3:15:54,  8.84s/it]

 65%|██████▌   | 2516/3844 [5:11:41<3:00:36,  8.16s/it]

 65%|██████▌   | 2517/3844 [5:11:47<2:43:31,  7.39s/it]
{'loss': 1.1672, 'grad_norm': 0.399566372660942, 'learning_rate': 5.627569054048731e-06, 'epoch': 0.65}

 66%|██████▌   | 2518/3844 [5:11:54<2:40:17,  7.25s/it]


 66%|██████▌   | 2520/3844 [5:12:09<2:41:57,  7.34s/it]

 66%|██████▌   | 2521/3844 [5:12:16<2:37:30,  7.14s/it]

 66%|██████▌   | 2522/3844 [5:12:21<2:28:28,  6.74s/it]

 66%|██████▌   | 2523/3844 [5:12:27<2:19:57,  6.36s/it]

 66%|██████▌   | 2524/3844 [5:12:32<2:13:25,  6.06s/it]
{'loss': 1.2055, 'grad_norm': 0.4341078503800187, 'learning_rate': 5.574593951150016e-06, 'epoch': 0.66}

 66%|██████▌   | 2525/3844 [5:12:38<2:11:40,  5.99s/it]


 66%|██████▌   | 2527/3844 [5:12:53<2:33:16,  6.98s/it]

 66%|██████▌   | 2528/3844 [5:12:59<2:22:40,  6.50s/it]

 66%|██████▌   | 2529/3844 [5:13:04<2:16:07,  6.21s/it]

 66%|██████▌   | 2530/3844 [5:13:10<2:16:40,  6.24s/it]

 66%|██████▌   | 2531/3844 [5:13:17<2:15:40,  6.20s/it]

 66%|██████▌   | 2532/3844 [5:13:22<2:12:27,  6.06s/it]

 66%|██████▌   | 2533/3844 [5:13:29<2:15:41,  6.21s/it]

 66%|██████▌   | 2534/3844 [5:13:37<2:28:45,  6.81s/it]

 66%|██████▌   | 2535/3844 [5:13:43<2:21:28,  6.48s/it]

 66%|██████▌   | 2536/3844 [5:13:48<2:15:13,  6.20s/it]

 66%|██████▌   | 2537/3844 [5:13:55<2:19:16,  6.39s/it]
{'loss': 1.1735, 'grad_norm': 0.395735982100677, 'learning_rate': 5.476621558656485e-06, 'epoch': 0.66}


 66%|██████▌   | 2539/3844 [5:14:09<2:26:27,  6.73s/it]

 66%|██████▌   | 2540/3844 [5:14:17<2:35:26,  7.15s/it]
{'loss': 1.2788, 'grad_norm': 0.37241144773855184, 'learning_rate': 5.454089211668567e-06, 'epoch': 0.66}

 66%|██████▌   | 2541/3844 [5:14:26<2:44:38,  7.58s/it]


 66%|██████▌   | 2543/3844 [5:14:42<2:50:13,  7.85s/it]

 66%|██████▌   | 2544/3844 [5:14:48<2:39:03,  7.34s/it]

 66%|██████▌   | 2545/3844 [5:14:55<2:40:00,  7.39s/it]

 66%|██████▌   | 2546/3844 [5:15:02<2:32:51,  7.07s/it]

 66%|██████▋   | 2547/3844 [5:15:13<2:59:36,  8.31s/it]
{'loss': 1.1345, 'grad_norm': 0.3546433984077137, 'learning_rate': 5.401626973052782e-06, 'epoch': 0.66}


 66%|██████▋   | 2549/3844 [5:15:29<2:56:52,  8.20s/it]

 66%|██████▋   | 2550/3844 [5:15:37<2:53:25,  8.04s/it]

 66%|██████▋   | 2551/3844 [5:15:45<2:55:22,  8.14s/it]
{'loss': 1.1776, 'grad_norm': 0.38318268244011777, 'learning_rate': 5.371720275322676e-06, 'epoch': 0.66}


 66%|██████▋   | 2553/3844 [5:15:59<2:41:04,  7.49s/it]

 66%|██████▋   | 2554/3844 [5:16:08<2:47:35,  7.79s/it]

 66%|██████▋   | 2555/3844 [5:16:15<2:47:38,  7.80s/it]

 66%|██████▋   | 2556/3844 [5:16:22<2:38:10,  7.37s/it]

 67%|██████▋   | 2557/3844 [5:16:31<2:52:11,  8.03s/it]

 67%|██████▋   | 2558/3844 [5:16:37<2:36:15,  7.29s/it]

 67%|██████▋   | 2559/3844 [5:16:44<2:34:25,  7.21s/it]

 67%|██████▋   | 2560/3844 [5:16:51<2:31:50,  7.10s/it]
{'loss': 1.1515, 'grad_norm': 0.4063618755204836, 'learning_rate': 5.304622997217627e-06, 'epoch': 0.67}

 67%|██████▋   | 2561/3844 [5:16:56<2:21:11,  6.60s/it]


 67%|██████▋   | 2563/3844 [5:17:09<2:22:35,  6.68s/it]
{'loss': 1.216, 'grad_norm': 0.368324979376425, 'learning_rate': 5.2823170667103295e-06, 'epoch': 0.67}

 67%|██████▋   | 2564/3844 [5:17:16<2:23:01,  6.70s/it]

 67%|██████▋   | 2565/3844 [5:17:22<2:17:12,  6.44s/it]


 67%|██████▋   | 2567/3844 [5:17:35<2:19:53,  6.57s/it]

 67%|██████▋   | 2568/3844 [5:17:44<2:30:45,  7.09s/it]
{'loss': 1.1778, 'grad_norm': 0.4139849160136915, 'learning_rate': 5.245207591062495e-06, 'epoch': 0.67}


 67%|██████▋   | 2570/3844 [5:17:57<2:27:54,  6.97s/it]

 67%|██████▋   | 2571/3844 [5:18:03<2:21:47,  6.68s/it]

 67%|██████▋   | 2572/3844 [5:18:09<2:16:24,  6.43s/it]
{'loss': 1.051, 'grad_norm': 0.40106862436626445, 'learning_rate': 5.215580757664385e-06, 'epoch': 0.67}


 67%|██████▋   | 2574/3844 [5:18:25<2:36:54,  7.41s/it]

 67%|██████▋   | 2575/3844 [5:18:34<2:43:33,  7.73s/it]

 67%|██████▋   | 2576/3844 [5:18:40<2:33:13,  7.25s/it]

 67%|██████▋   | 2577/3844 [5:18:47<2:32:11,  7.21s/it]

 67%|██████▋   | 2578/3844 [5:18:53<2:22:19,  6.75s/it]

 67%|██████▋   | 2579/3844 [5:18:58<2:13:24,  6.33s/it]

 67%|██████▋   | 2580/3844 [5:19:04<2:11:10,  6.23s/it]

 67%|██████▋   | 2581/3844 [5:19:10<2:07:27,  6.06s/it]

 67%|██████▋   | 2582/3844 [5:19:15<2:03:31,  5.87s/it]

 67%|██████▋   | 2583/3844 [5:19:24<2:21:00,  6.71s/it]

 67%|██████▋   | 2584/3844 [5:19:33<2:37:36,  7.50s/it]
{'loss': 1.1476, 'grad_norm': 0.39127429002723957, 'learning_rate': 5.127027773882699e-06, 'epoch': 0.67}


 67%|██████▋   | 2586/3844 [5:19:46<2:27:27,  7.03s/it]

 67%|██████▋   | 2587/3844 [5:19:53<2:25:02,  6.92s/it]

 67%|██████▋   | 2588/3844 [5:19:59<2:19:25,  6.66s/it]

 67%|██████▋   | 2589/3844 [5:20:08<2:32:51,  7.31s/it]

 67%|██████▋   | 2590/3844 [5:20:14<2:22:39,  6.83s/it]

 67%|██████▋   | 2591/3844 [5:20:20<2:16:30,  6.54s/it]

 67%|██████▋   | 2592/3844 [5:20:27<2:20:08,  6.72s/it]

 67%|██████▋   | 2593/3844 [5:20:35<2:29:04,  7.15s/it]

 67%|██████▋   | 2594/3844 [5:20:43<2:35:48,  7.48s/it]

 68%|██████▊   | 2595/3844 [5:20:53<2:49:26,  8.14s/it]
{'loss': 1.1237, 'grad_norm': 0.37459435479217257, 'learning_rate': 5.04629173791236e-06, 'epoch': 0.67}


 68%|██████▊   | 2597/3844 [5:21:10<2:51:06,  8.23s/it]

 68%|██████▊   | 2598/3844 [5:21:17<2:43:49,  7.89s/it]

 68%|██████▊   | 2599/3844 [5:21:23<2:32:27,  7.35s/it]

 68%|██████▊   | 2600/3844 [5:21:30<2:29:54,  7.23s/it]

 68%|██████▊   | 2601/3844 [5:21:44<3:12:25,  9.29s/it]
{'loss': 1.0559, 'grad_norm': 0.3848952002608259, 'learning_rate': 5.0024328680520484e-06, 'epoch': 0.68}


 68%|██████▊   | 2603/3844 [5:21:57<2:42:14,  7.84s/it]

 68%|██████▊   | 2604/3844 [5:22:05<2:42:26,  7.86s/it]
{'loss': 1.1348, 'grad_norm': 0.3758446499262207, 'learning_rate': 4.980551274631696e-06, 'epoch': 0.68}


 68%|██████▊   | 2606/3844 [5:22:18<2:27:03,  7.13s/it]

 68%|██████▊   | 2607/3844 [5:22:24<2:19:37,  6.77s/it]
{'loss': 1.0132, 'grad_norm': 0.4046839034452618, 'learning_rate': 4.958701762088801e-06, 'epoch': 0.68}


 68%|██████▊   | 2609/3844 [5:22:37<2:16:39,  6.64s/it]

 68%|██████▊   | 2610/3844 [5:22:43<2:13:53,  6.51s/it]

 68%|██████▊   | 2611/3844 [5:22:50<2:12:56,  6.47s/it]

 68%|██████▊   | 2612/3844 [5:22:57<2:19:05,  6.77s/it]
{'loss': 1.2664, 'grad_norm': 0.37137155226028257, 'learning_rate': 4.922357577884709e-06, 'epoch': 0.68}

 68%|██████▊   | 2613/3844 [5:23:02<2:10:28,  6.36s/it]


 68%|██████▊   | 2615/3844 [5:23:21<2:41:46,  7.90s/it]

 68%|██████▊   | 2616/3844 [5:23:27<2:32:25,  7.45s/it]

 68%|██████▊   | 2617/3844 [5:23:33<2:22:05,  6.95s/it]

 68%|██████▊   | 2618/3844 [5:23:42<2:36:02,  7.64s/it]

 68%|██████▊   | 2619/3844 [5:23:48<2:25:25,  7.12s/it]

 68%|██████▊   | 2620/3844 [5:23:54<2:15:24,  6.64s/it]

 68%|██████▊   | 2621/3844 [5:23:59<2:09:11,  6.34s/it]
{'loss': 1.229, 'grad_norm': 0.3881816741549482, 'learning_rate': 4.857165648364932e-06, 'epoch': 0.68}


 68%|██████▊   | 2623/3844 [5:24:18<2:39:54,  7.86s/it]

 68%|██████▊   | 2624/3844 [5:24:24<2:26:15,  7.19s/it]

 68%|██████▊   | 2625/3844 [5:24:30<2:20:41,  6.92s/it]

 68%|██████▊   | 2626/3844 [5:24:38<2:27:48,  7.28s/it]

 68%|██████▊   | 2627/3844 [5:24:45<2:28:18,  7.31s/it]

 68%|██████▊   | 2628/3844 [5:24:52<2:22:59,  7.06s/it]

 68%|██████▊   | 2629/3844 [5:24:59<2:25:33,  7.19s/it]

 68%|██████▊   | 2630/3844 [5:25:06<2:19:54,  6.91s/it]

 68%|██████▊   | 2631/3844 [5:25:12<2:18:59,  6.87s/it]

 68%|██████▊   | 2632/3844 [5:25:20<2:21:10,  6.99s/it]
{'loss': 1.055, 'grad_norm': 0.3788266923284681, 'learning_rate': 4.777888735545477e-06, 'epoch': 0.68}


 69%|██████▊   | 2634/3844 [5:25:35<2:30:38,  7.47s/it]
{'loss': 1.1879, 'grad_norm': 0.3722382591844577, 'learning_rate': 4.763522762093818e-06, 'epoch': 0.69}


 69%|██████▊   | 2636/3844 [5:25:47<2:15:36,  6.74s/it]
{'loss': 1.1379, 'grad_norm': 0.401571522478042, 'learning_rate': 4.749171663302124e-06, 'epoch': 0.69}

 69%|██████▊   | 2637/3844 [5:25:53<2:09:20,  6.43s/it]


 69%|██████▊   | 2639/3844 [5:26:08<2:17:32,  6.85s/it]

 69%|██████▊   | 2640/3844 [5:26:15<2:17:49,  6.87s/it]

 69%|██████▊   | 2641/3844 [5:26:24<2:30:58,  7.53s/it]

 69%|██████▊   | 2642/3844 [5:26:34<2:49:25,  8.46s/it]

 69%|██████▉   | 2643/3844 [5:26:41<2:41:39,  8.08s/it]

 69%|██████▉   | 2644/3844 [5:26:48<2:35:18,  7.77s/it]

 69%|██████▉   | 2645/3844 [5:26:55<2:28:57,  7.45s/it]

 69%|██████▉   | 2646/3844 [5:27:01<2:16:42,  6.85s/it]

 69%|██████▉   | 2647/3844 [5:27:06<2:08:22,  6.43s/it]

 69%|██████▉   | 2648/3844 [5:27:16<2:27:30,  7.40s/it]

 69%|██████▉   | 2649/3844 [5:27:24<2:34:46,  7.77s/it]

 69%|██████▉   | 2650/3844 [5:27:30<2:23:15,  7.20s/it]

 69%|██████▉   | 2651/3844 [5:27:36<2:16:58,  6.89s/it]

 69%|██████▉   | 2652/3844 [5:27:46<2:30:55,  7.60s/it]

 69%|██████▉   | 2653/3844 [5:27:54<2:36:00,  7.86s/it]

 69%|██████▉   | 2654/3844 [5:28:03<2:43:02,  8.22s/it]

 69%|██████▉   | 2655/3844 [5:28:10<2:36:46,  7.91s/it]

 69%|██████▉   | 2656/3844 [5:28:17<2:26:48,  7.41s/it]

 69%|██████▉   | 2657/3844 [5:28:24<2:23:49,  7.27s/it]

 69%|██████▉   | 2658/3844 [5:28:29<2:15:29,  6.85s/it]

 69%|██████▉   | 2659/3844 [5:28:36<2:14:41,  6.82s/it]

 69%|██████▉   | 2660/3844 [5:28:43<2:12:36,  6.72s/it]
{'loss': 1.125, 'grad_norm': 0.40720507516244353, 'learning_rate': 4.578133497286259e-06, 'epoch': 0.69}


 69%|██████▉   | 2662/3844 [5:28:57<2:17:06,  6.96s/it]

 69%|██████▉   | 2663/3844 [5:29:05<2:22:08,  7.22s/it]

 69%|██████▉   | 2664/3844 [5:29:12<2:17:01,  6.97s/it]

 69%|██████▉   | 2665/3844 [5:29:20<2:26:13,  7.44s/it]

 69%|██████▉   | 2666/3844 [5:29:28<2:31:20,  7.71s/it]

 69%|██████▉   | 2667/3844 [5:29:38<2:43:03,  8.31s/it]

 69%|██████▉   | 2668/3844 [5:29:48<2:50:41,  8.71s/it]

 69%|██████▉   | 2669/3844 [5:29:56<2:44:54,  8.42s/it]
{'loss': 1.1448, 'grad_norm': 0.4034629641112055, 'learning_rate': 4.514562208999185e-06, 'epoch': 0.69}


 69%|██████▉   | 2671/3844 [5:30:12<2:45:48,  8.48s/it]

 70%|██████▉   | 2672/3844 [5:30:19<2:33:21,  7.85s/it]

 70%|██████▉   | 2673/3844 [5:30:24<2:20:26,  7.20s/it]

 70%|██████▉   | 2674/3844 [5:30:30<2:12:57,  6.82s/it]

 70%|██████▉   | 2675/3844 [5:30:36<2:07:59,  6.57s/it]

 70%|██████▉   | 2676/3844 [5:30:42<2:00:47,  6.21s/it]

 70%|██████▉   | 2677/3844 [5:30:53<2:28:22,  7.63s/it]

 70%|██████▉   | 2678/3844 [5:31:00<2:24:04,  7.41s/it]

 70%|██████▉   | 2679/3844 [5:31:05<2:11:46,  6.79s/it]

 70%|██████▉   | 2680/3844 [5:31:14<2:22:50,  7.36s/it]
{'loss': 1.0784, 'grad_norm': 0.4142770606893847, 'learning_rate': 4.437292833159642e-06, 'epoch': 0.7}


 70%|██████▉   | 2682/3844 [5:31:25<2:06:13,  6.52s/it]

 70%|██████▉   | 2683/3844 [5:31:31<2:02:40,  6.34s/it]

 70%|██████▉   | 2684/3844 [5:31:42<2:28:42,  7.69s/it]
{'loss': 1.2468, 'grad_norm': 0.4313887378134891, 'learning_rate': 4.409313039651519e-06, 'epoch': 0.7}


 70%|██████▉   | 2686/3844 [5:31:54<2:13:05,  6.90s/it]

 70%|██████▉   | 2687/3844 [5:32:00<2:10:29,  6.77s/it]
{'loss': 1.2059, 'grad_norm': 0.4151514052598175, 'learning_rate': 4.388369864291972e-06, 'epoch': 0.7}


 70%|██████▉   | 2689/3844 [5:32:13<2:05:09,  6.50s/it]

 70%|██████▉   | 2690/3844 [5:32:20<2:08:47,  6.70s/it]

 70%|███████   | 2691/3844 [5:32:27<2:07:33,  6.64s/it]

 70%|███████   | 2692/3844 [5:32:33<2:04:09,  6.47s/it]

 70%|███████   | 2693/3844 [5:32:39<2:03:13,  6.42s/it]

 70%|███████   | 2694/3844 [5:32:45<2:03:23,  6.44s/it]

 70%|███████   | 2695/3844 [5:32:52<2:06:19,  6.60s/it]

 70%|███████   | 2696/3844 [5:33:01<2:16:30,  7.13s/it]
{'loss': 1.2573, 'grad_norm': 0.3599828265024795, 'learning_rate': 4.325756066656029e-06, 'epoch': 0.7}


 70%|███████   | 2698/3844 [5:33:18<2:28:23,  7.77s/it]
{'loss': 1.1771, 'grad_norm': 0.3966854501853034, 'learning_rate': 4.311886087737877e-06, 'epoch': 0.7}


 70%|███████   | 2700/3844 [5:33:33<2:27:17,  7.73s/it]
 70%|███████   | 2700/3844 [5:33:33<2:27:17,  7.73s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|███████   | 2701/3844 [5:34:03<4:32:11, 14.29s/it]

 70%|███████   | 2702/3844 [5:34:10<3:53:17, 12.26s/it]

 70%|███████   | 2703/3844 [5:34:17<3:22:59, 10.67s/it]

 70%|███████   | 2704/3844 [5:34:24<3:01:57,  9.58s/it]
{'loss': 1.1901, 'grad_norm': 0.39664431661516053, 'learning_rate': 4.270373253783881e-06, 'epoch': 0.7}

 70%|███████   | 2705/3844 [5:34:31<2:48:15,  8.86s/it]


 70%|███████   | 2707/3844 [5:34:46<2:34:26,  8.15s/it]

 70%|███████   | 2708/3844 [5:34:54<2:32:23,  8.05s/it]

 70%|███████   | 2709/3844 [5:35:00<2:24:28,  7.64s/it]

 70%|███████   | 2710/3844 [5:35:07<2:18:15,  7.31s/it]
{'loss': 1.2088, 'grad_norm': 0.44723317764752935, 'learning_rate': 4.229006898991148e-06, 'epoch': 0.7}


 71%|███████   | 2712/3844 [5:35:22<2:23:16,  7.59s/it]

 71%|███████   | 2713/3844 [5:35:28<2:12:13,  7.01s/it]

 71%|███████   | 2714/3844 [5:35:34<2:06:49,  6.73s/it]

 71%|███████   | 2715/3844 [5:35:40<2:02:26,  6.51s/it]
{'loss': 1.0866, 'grad_norm': 0.4181668708000966, 'learning_rate': 4.1946475935436255e-06, 'epoch': 0.71}


 71%|███████   | 2717/3844 [5:35:56<2:16:57,  7.29s/it]

 71%|███████   | 2718/3844 [5:36:02<2:08:36,  6.85s/it]

 71%|███████   | 2719/3844 [5:36:08<2:03:34,  6.59s/it]

 71%|███████   | 2720/3844 [5:36:14<2:02:05,  6.52s/it]

 71%|███████   | 2721/3844 [5:36:24<2:20:28,  7.51s/it]

 71%|███████   | 2722/3844 [5:36:32<2:21:47,  7.58s/it]

 71%|███████   | 2723/3844 [5:36:38<2:14:52,  7.22s/it]

 71%|███████   | 2724/3844 [5:36:46<2:21:49,  7.60s/it]

 71%|███████   | 2725/3844 [5:36:54<2:23:43,  7.71s/it]

 71%|███████   | 2726/3844 [5:37:00<2:13:29,  7.16s/it]

 71%|███████   | 2727/3844 [5:37:06<2:04:56,  6.71s/it]
{'loss': 1.238, 'grad_norm': 0.4033151807747778, 'learning_rate': 4.11260692781639e-06, 'epoch': 0.71}

 71%|███████   | 2728/3844 [5:37:11<1:57:22,  6.31s/it]


 71%|███████   | 2730/3844 [5:37:24<1:57:14,  6.31s/it]

 71%|███████   | 2731/3844 [5:37:30<1:55:03,  6.20s/it]

 71%|███████   | 2732/3844 [5:37:37<1:58:42,  6.40s/it]

 71%|███████   | 2733/3844 [5:37:47<2:17:13,  7.41s/it]

 71%|███████   | 2734/3844 [5:37:52<2:05:25,  6.78s/it]
{'loss': 1.2403, 'grad_norm': 0.4121460244454997, 'learning_rate': 4.065027365492386e-06, 'epoch': 0.71}

 71%|███████   | 2735/3844 [5:37:57<1:58:54,  6.43s/it]


 71%|███████   | 2737/3844 [5:38:10<1:56:05,  6.29s/it]

 71%|███████   | 2738/3844 [5:38:18<2:07:52,  6.94s/it]

 71%|███████▏  | 2739/3844 [5:38:26<2:13:41,  7.26s/it]

 71%|███████▏  | 2740/3844 [5:38:32<2:06:50,  6.89s/it]

 71%|███████▏  | 2741/3844 [5:38:39<2:07:10,  6.92s/it]
{'loss': 1.2759, 'grad_norm': 0.3400925184457568, 'learning_rate': 4.017654322865534e-06, 'epoch': 0.71}

 71%|███████▏  | 2742/3844 [5:38:46<2:04:57,  6.80s/it]


 71%|███████▏  | 2744/3844 [5:39:03<2:17:22,  7.49s/it]

 71%|███████▏  | 2745/3844 [5:39:15<2:42:25,  8.87s/it]

 71%|███████▏  | 2746/3844 [5:39:21<2:27:43,  8.07s/it]

 71%|███████▏  | 2747/3844 [5:39:33<2:48:55,  9.24s/it]

 71%|███████▏  | 2748/3844 [5:39:42<2:50:14,  9.32s/it]

 72%|███████▏  | 2749/3844 [5:39:48<2:30:17,  8.23s/it]
{'loss': 1.154, 'grad_norm': 0.41262158493731277, 'learning_rate': 3.963768698272226e-06, 'epoch': 0.72}


 72%|███████▏  | 2751/3844 [5:40:05<2:34:11,  8.46s/it]

 72%|███████▏  | 2752/3844 [5:40:14<2:38:13,  8.69s/it]

 72%|███████▏  | 2753/3844 [5:40:26<2:55:15,  9.64s/it]

 72%|███████▏  | 2754/3844 [5:40:33<2:41:51,  8.91s/it]

 72%|███████▏  | 2755/3844 [5:40:44<2:48:29,  9.28s/it]
{'loss': 1.1934, 'grad_norm': 0.41184403174582485, 'learning_rate': 3.9235343832332116e-06, 'epoch': 0.72}


 72%|███████▏  | 2757/3844 [5:40:56<2:21:43,  7.82s/it]

 72%|███████▏  | 2758/3844 [5:41:04<2:21:23,  7.81s/it]

 72%|███████▏  | 2759/3844 [5:41:13<2:24:26,  7.99s/it]

 72%|███████▏  | 2760/3844 [5:41:20<2:22:08,  7.87s/it]

 72%|███████▏  | 2761/3844 [5:41:29<2:26:24,  8.11s/it]

 72%|███████▏  | 2762/3844 [5:41:41<2:46:10,  9.22s/it]
[2024-05-26 16:48:13,393] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 2763/3844 [5:41:47<2:31:20,  8.40s/it]

 72%|███████▏  | 2764/3844 [5:41:53<2:18:57,  7.72s/it]

 72%|███████▏  | 2765/3844 [5:42:01<2:18:43,  7.71s/it]

 72%|███████▏  | 2766/3844 [5:42:09<2:20:42,  7.83s/it]

 72%|███████▏  | 2767/3844 [5:42:14<2:07:00,  7.08s/it]

 72%|███████▏  | 2768/3844 [5:42:23<2:17:14,  7.65s/it]
{'loss': 1.2151, 'grad_norm': 0.3788207048379545, 'learning_rate': 3.83689432439715e-06, 'epoch': 0.72}


 72%|███████▏  | 2770/3844 [5:42:37<2:11:03,  7.32s/it]

 72%|███████▏  | 2771/3844 [5:42:43<2:01:55,  6.82s/it]

 72%|███████▏  | 2772/3844 [5:42:49<1:56:22,  6.51s/it]
{'loss': 1.2778, 'grad_norm': 0.42684438212101716, 'learning_rate': 3.8103841709519087e-06, 'epoch': 0.72}


 72%|███████▏  | 2774/3844 [5:43:06<2:12:21,  7.42s/it]

 72%|███████▏  | 2775/3844 [5:43:11<2:02:33,  6.88s/it]

 72%|███████▏  | 2776/3844 [5:43:20<2:13:37,  7.51s/it]

 72%|███████▏  | 2777/3844 [5:43:27<2:08:28,  7.22s/it]

 72%|███████▏  | 2778/3844 [5:43:35<2:13:11,  7.50s/it]

 72%|███████▏  | 2779/3844 [5:43:43<2:15:59,  7.66s/it]

 72%|███████▏  | 2780/3844 [5:43:48<2:04:15,  7.01s/it]

 72%|███████▏  | 2781/3844 [5:43:55<2:00:23,  6.80s/it]

 72%|███████▏  | 2782/3844 [5:44:02<2:01:59,  6.89s/it]

 72%|███████▏  | 2783/3844 [5:44:08<1:59:38,  6.77s/it]

 72%|███████▏  | 2784/3844 [5:44:17<2:12:14,  7.49s/it]

 72%|███████▏  | 2785/3844 [5:44:28<2:26:27,  8.30s/it]

 72%|███████▏  | 2786/3844 [5:44:36<2:24:33,  8.20s/it]

 73%|███████▎  | 2787/3844 [5:44:44<2:23:09,  8.13s/it]

 73%|███████▎  | 2788/3844 [5:44:49<2:10:50,  7.43s/it]

 73%|███████▎  | 2789/3844 [5:44:56<2:05:08,  7.12s/it]

 73%|███████▎  | 2790/3844 [5:45:02<1:58:37,  6.75s/it]

 73%|███████▎  | 2791/3844 [5:45:08<1:54:53,  6.55s/it]

 73%|███████▎  | 2792/3844 [5:45:14<1:52:17,  6.40s/it]

 73%|███████▎  | 2793/3844 [5:45:20<1:50:15,  6.29s/it]

 73%|███████▎  | 2794/3844 [5:45:31<2:15:14,  7.73s/it]
[2024-05-26 16:52:03,677] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 2795/3844 [5:45:41<2:26:24,  8.37s/it]

 73%|███████▎  | 2796/3844 [5:45:51<2:34:38,  8.85s/it]

 73%|███████▎  | 2797/3844 [5:45:57<2:22:49,  8.19s/it]

 73%|███████▎  | 2798/3844 [5:46:04<2:13:10,  7.64s/it]

 73%|███████▎  | 2799/3844 [5:46:10<2:05:38,  7.21s/it]

 73%|███████▎  | 2800/3844 [5:46:16<2:01:14,  6.97s/it]

 73%|███████▎  | 2801/3844 [5:46:23<2:00:32,  6.93s/it]

 73%|███████▎  | 2802/3844 [5:46:32<2:09:27,  7.45s/it]

 73%|███████▎  | 2803/3844 [5:46:39<2:06:30,  7.29s/it]

 73%|███████▎  | 2804/3844 [5:46:45<2:02:34,  7.07s/it]

 73%|███████▎  | 2805/3844 [5:46:51<1:55:08,  6.65s/it]

 73%|███████▎  | 2806/3844 [5:46:58<1:54:36,  6.63s/it]

 73%|███████▎  | 2807/3844 [5:47:03<1:49:21,  6.33s/it]

 73%|███████▎  | 2808/3844 [5:47:13<2:07:44,  7.40s/it]

 73%|███████▎  | 2809/3844 [5:47:21<2:11:24,  7.62s/it]

 73%|███████▎  | 2810/3844 [5:47:28<2:05:53,  7.31s/it]

 73%|███████▎  | 2811/3844 [5:47:34<1:58:01,  6.86s/it]

 73%|███████▎  | 2812/3844 [5:47:40<1:53:06,  6.58s/it]

 73%|███████▎  | 2813/3844 [5:47:47<1:56:07,  6.76s/it]

 73%|███████▎  | 2814/3844 [5:47:56<2:08:06,  7.46s/it]

 73%|███████▎  | 2815/3844 [5:48:03<2:06:52,  7.40s/it]

 73%|███████▎  | 2816/3844 [5:48:10<2:03:36,  7.21s/it]
{'loss': 1.074, 'grad_norm': 0.39923295049071816, 'learning_rate': 3.523479854494292e-06, 'epoch': 0.73}


 73%|███████▎  | 2818/3844 [5:48:24<1:58:09,  6.91s/it]
{'loss': 1.2715, 'grad_norm': 0.38538675535384154, 'learning_rate': 3.5106473565277186e-06, 'epoch': 0.73}


 73%|███████▎  | 2820/3844 [5:48:37<1:54:19,  6.70s/it]

 73%|███████▎  | 2821/3844 [5:48:45<2:03:23,  7.24s/it]

 73%|███████▎  | 2822/3844 [5:48:56<2:21:41,  8.32s/it]

 73%|███████▎  | 2823/3844 [5:49:03<2:16:57,  8.05s/it]

 73%|███████▎  | 2824/3844 [5:49:11<2:13:23,  7.85s/it]

 73%|███████▎  | 2825/3844 [5:49:19<2:13:13,  7.84s/it]

 74%|███████▎  | 2826/3844 [5:49:25<2:05:03,  7.37s/it]

 74%|███████▎  | 2827/3844 [5:49:33<2:08:27,  7.58s/it]

 74%|███████▎  | 2828/3844 [5:49:40<2:06:02,  7.44s/it]

 74%|███████▎  | 2829/3844 [5:49:47<2:01:00,  7.15s/it]
{'loss': 1.1406, 'grad_norm': 0.3753748051512073, 'learning_rate': 3.4403990923129903e-06, 'epoch': 0.74}


 74%|███████▎  | 2831/3844 [5:50:02<2:05:35,  7.44s/it]

 74%|███████▎  | 2832/3844 [5:50:13<2:23:47,  8.53s/it]

 74%|███████▎  | 2833/3844 [5:50:21<2:19:58,  8.31s/it]
{'loss': 1.2251, 'grad_norm': 0.4120130070023657, 'learning_rate': 3.4149937003283772e-06, 'epoch': 0.74}


 74%|███████▍  | 2835/3844 [5:50:32<1:54:44,  6.82s/it]

 74%|███████▍  | 2836/3844 [5:50:40<2:02:38,  7.30s/it]

 74%|███████▍  | 2837/3844 [5:50:48<2:04:02,  7.39s/it]
{'loss': 1.1629, 'grad_norm': 0.35822557856885734, 'learning_rate': 3.3896631293775064e-06, 'epoch': 0.74}


 74%|███████▍  | 2839/3844 [5:51:02<2:00:41,  7.21s/it]

 74%|███████▍  | 2840/3844 [5:51:12<2:13:56,  8.00s/it]

 74%|███████▍  | 2841/3844 [5:51:18<2:04:07,  7.43s/it]

 74%|███████▍  | 2842/3844 [5:51:24<1:56:42,  6.99s/it]

 74%|███████▍  | 2843/3844 [5:51:30<1:49:34,  6.57s/it]

 74%|███████▍  | 2844/3844 [5:51:35<1:43:42,  6.22s/it]

 74%|███████▍  | 2845/3844 [5:51:41<1:40:11,  6.02s/it]

 74%|███████▍  | 2846/3844 [5:51:48<1:46:51,  6.42s/it]

 74%|███████▍  | 2847/3844 [5:51:54<1:45:41,  6.36s/it]

 74%|███████▍  | 2848/3844 [5:52:00<1:41:49,  6.13s/it]
{'loss': 1.1926, 'grad_norm': 0.4166943432566709, 'learning_rate': 3.320392201902527e-06, 'epoch': 0.74}


 74%|███████▍  | 2850/3844 [5:52:15<1:55:53,  7.00s/it]
{'loss': 1.0471, 'grad_norm': 0.3653446250920604, 'learning_rate': 3.307858979250793e-06, 'epoch': 0.74}


 74%|███████▍  | 2852/3844 [5:52:33<2:10:32,  7.90s/it]

 74%|███████▍  | 2853/3844 [5:52:39<2:03:33,  7.48s/it]

 74%|███████▍  | 2854/3844 [5:52:50<2:18:14,  8.38s/it]
{'loss': 1.0733, 'grad_norm': 0.3971219153765516, 'learning_rate': 3.2828495982861018e-06, 'epoch': 0.74}


 74%|███████▍  | 2856/3844 [5:53:05<2:10:04,  7.90s/it]

 74%|███████▍  | 2857/3844 [5:53:12<2:02:44,  7.46s/it]

 74%|███████▍  | 2858/3844 [5:53:18<1:56:24,  7.08s/it]

 74%|███████▍  | 2859/3844 [5:53:28<2:10:26,  7.95s/it]

 74%|███████▍  | 2860/3844 [5:53:33<1:58:15,  7.21s/it]

 74%|███████▍  | 2861/3844 [5:53:39<1:50:33,  6.75s/it]

 74%|███████▍  | 2862/3844 [5:53:46<1:50:12,  6.73s/it]

 74%|███████▍  | 2863/3844 [5:53:51<1:43:32,  6.33s/it]

 75%|███████▍  | 2864/3844 [5:53:57<1:41:34,  6.22s/it]

 75%|███████▍  | 2865/3844 [5:54:03<1:42:52,  6.30s/it]

 75%|███████▍  | 2866/3844 [5:54:09<1:39:22,  6.10s/it]

 75%|███████▍  | 2867/3844 [5:54:16<1:42:22,  6.29s/it]

 75%|███████▍  | 2868/3844 [5:54:22<1:42:54,  6.33s/it]

 75%|███████▍  | 2869/3844 [5:54:29<1:46:18,  6.54s/it]
{'loss': 1.1003, 'grad_norm': 0.37066291366030624, 'learning_rate': 3.189746476675004e-06, 'epoch': 0.75}


 75%|███████▍  | 2871/3844 [5:54:46<1:59:39,  7.38s/it]

 75%|███████▍  | 2872/3844 [5:54:54<2:00:45,  7.45s/it]
{'loss': 0.9059, 'grad_norm': 0.4059982854106067, 'learning_rate': 3.1712559571182643e-06, 'epoch': 0.75}


 75%|███████▍  | 2874/3844 [5:55:11<2:08:10,  7.93s/it]
{'loss': 1.0592, 'grad_norm': 0.38844622913900356, 'learning_rate': 3.158953183826693e-06, 'epoch': 0.75}


 75%|███████▍  | 2876/3844 [5:55:24<1:57:12,  7.27s/it]

 75%|███████▍  | 2877/3844 [5:55:30<1:52:30,  6.98s/it]

 75%|███████▍  | 2878/3844 [5:55:40<2:08:46,  8.00s/it]

 75%|███████▍  | 2879/3844 [5:55:46<1:58:30,  7.37s/it]

 75%|███████▍  | 2880/3844 [5:55:52<1:51:48,  6.96s/it]

 75%|███████▍  | 2881/3844 [5:56:00<1:53:25,  7.07s/it]

 75%|███████▍  | 2882/3844 [5:56:06<1:48:07,  6.74s/it]

 75%|███████▌  | 2883/3844 [5:56:11<1:41:56,  6.36s/it]

 75%|███████▌  | 2884/3844 [5:56:17<1:40:40,  6.29s/it]
{'loss': 1.1406, 'grad_norm': 0.41070731468106914, 'learning_rate': 3.0977315030258002e-06, 'epoch': 0.75}


 75%|███████▌  | 2886/3844 [5:56:30<1:43:38,  6.49s/it]

 75%|███████▌  | 2887/3844 [5:56:36<1:38:30,  6.18s/it]

 75%|███████▌  | 2888/3844 [5:56:42<1:39:49,  6.27s/it]

 75%|███████▌  | 2889/3844 [5:56:48<1:39:02,  6.22s/it]

 75%|███████▌  | 2890/3844 [5:56:54<1:37:34,  6.14s/it]

 75%|███████▌  | 2891/3844 [5:57:01<1:42:16,  6.44s/it]
{'loss': 1.1103, 'grad_norm': 0.3977098706725562, 'learning_rate': 3.0551677139405412e-06, 'epoch': 0.75}

 75%|███████▌  | 2892/3844 [5:57:11<1:56:35,  7.35s/it]


 75%|███████▌  | 2894/3844 [5:57:23<1:47:46,  6.81s/it]

 75%|███████▌  | 2895/3844 [5:57:29<1:41:09,  6.40s/it]
{'loss': 1.2809, 'grad_norm': 0.40522306380413864, 'learning_rate': 3.030953954443362e-06, 'epoch': 0.75}


 75%|███████▌  | 2897/3844 [5:57:50<2:18:33,  8.78s/it]

 75%|███████▌  | 2898/3844 [5:57:57<2:08:00,  8.12s/it]

 75%|███████▌  | 2899/3844 [5:58:02<1:55:00,  7.30s/it]

 75%|███████▌  | 2900/3844 [5:58:11<2:01:45,  7.74s/it]
{'loss': 1.0837, 'grad_norm': 0.40058173144546566, 'learning_rate': 3.000798140600999e-06, 'epoch': 0.75}


 75%|███████▌  | 2902/3844 [5:58:23<1:47:06,  6.82s/it]

 76%|███████▌  | 2903/3844 [5:58:30<1:47:54,  6.88s/it]
{'loss': 1.144, 'grad_norm': 0.40588656888907804, 'learning_rate': 2.9827642635376475e-06, 'epoch': 0.76}


 76%|███████▌  | 2905/3844 [5:58:43<1:42:42,  6.56s/it]

 76%|███████▌  | 2906/3844 [5:58:51<1:52:55,  7.22s/it]
{'loss': 1.2355, 'grad_norm': 0.38109246405380137, 'learning_rate': 2.9647752358376525e-06, 'epoch': 0.76}


 76%|███████▌  | 2908/3844 [5:59:05<1:49:53,  7.04s/it]
{'loss': 1.1447, 'grad_norm': 0.38737433298456125, 'learning_rate': 2.952807523806176e-06, 'epoch': 0.76}

 76%|███████▌  | 2909/3844 [5:59:13<1:55:05,  7.39s/it]


 76%|███████▌  | 2911/3844 [5:59:25<1:42:00,  6.56s/it]

 76%|███████▌  | 2912/3844 [5:59:32<1:45:21,  6.78s/it]

 76%|███████▌  | 2913/3844 [5:59:38<1:41:56,  6.57s/it]
{'loss': 1.2698, 'grad_norm': 0.39739820980296686, 'learning_rate': 2.9229758973626766e-06, 'epoch': 0.76}


 76%|███████▌  | 2915/3844 [5:59:55<1:57:34,  7.59s/it]

 76%|███████▌  | 2916/3844 [6:00:01<1:51:05,  7.18s/it]

 76%|███████▌  | 2917/3844 [6:00:12<2:05:01,  8.09s/it]

 76%|███████▌  | 2918/3844 [6:00:23<2:19:06,  9.01s/it]

 76%|███████▌  | 2919/3844 [6:00:30<2:10:10,  8.44s/it]
{'loss': 1.261, 'grad_norm': 0.38271408357037734, 'learning_rate': 2.8873438407183916e-06, 'epoch': 0.76}


 76%|███████▌  | 2921/3844 [6:00:44<2:01:30,  7.90s/it]

 76%|███████▌  | 2922/3844 [6:00:49<1:49:54,  7.15s/it]

 76%|███████▌  | 2923/3844 [6:00:58<1:57:30,  7.66s/it]

 76%|███████▌  | 2924/3844 [6:01:04<1:48:41,  7.09s/it]

 76%|███████▌  | 2925/3844 [6:01:10<1:41:22,  6.62s/it]

 76%|███████▌  | 2926/3844 [6:01:16<1:38:13,  6.42s/it]

 76%|███████▌  | 2927/3844 [6:01:21<1:33:30,  6.12s/it]

 76%|███████▌  | 2928/3844 [6:01:28<1:37:24,  6.38s/it]

 76%|███████▌  | 2929/3844 [6:01:35<1:41:45,  6.67s/it]

 76%|███████▌  | 2930/3844 [6:01:41<1:39:18,  6.52s/it]

 76%|███████▌  | 2931/3844 [6:01:51<1:55:05,  7.56s/it]

 76%|███████▋  | 2932/3844 [6:01:57<1:47:30,  7.07s/it]

 76%|███████▋  | 2933/3844 [6:02:04<1:46:21,  7.00s/it]

 76%|███████▋  | 2934/3844 [6:02:11<1:44:51,  6.91s/it]

 76%|███████▋  | 2935/3844 [6:02:17<1:40:57,  6.66s/it]

 76%|███████▋  | 2936/3844 [6:02:24<1:44:05,  6.88s/it]

 76%|███████▋  | 2937/3844 [6:02:33<1:53:42,  7.52s/it]
{'loss': 1.1351, 'grad_norm': 0.39577305869612417, 'learning_rate': 2.7815423109488627e-06, 'epoch': 0.76}

 76%|███████▋  | 2938/3844 [6:02:39<1:46:01,  7.02s/it]


 76%|███████▋  | 2940/3844 [6:02:55<1:53:39,  7.54s/it]

 77%|███████▋  | 2941/3844 [6:03:01<1:46:18,  7.06s/it]

 77%|███████▋  | 2942/3844 [6:03:07<1:41:37,  6.76s/it]

 77%|███████▋  | 2943/3844 [6:03:16<1:50:53,  7.38s/it]

 77%|███████▋  | 2944/3844 [6:03:22<1:48:39,  7.24s/it]

 77%|███████▋  | 2945/3844 [6:03:28<1:41:42,  6.79s/it]

 77%|███████▋  | 2946/3844 [6:03:34<1:37:09,  6.49s/it]

 77%|███████▋  | 2947/3844 [6:03:40<1:35:07,  6.36s/it]

 77%|███████▋  | 2948/3844 [6:03:46<1:35:13,  6.38s/it]

 77%|███████▋  | 2949/3844 [6:03:57<1:53:13,  7.59s/it]

 77%|███████▋  | 2950/3844 [6:04:05<1:57:39,  7.90s/it]

 77%|███████▋  | 2951/3844 [6:04:15<2:02:31,  8.23s/it]

 77%|███████▋  | 2952/3844 [6:04:22<1:59:22,  8.03s/it]
{'loss': 1.2615, 'grad_norm': 0.40572796291248225, 'learning_rate': 2.6946420486873347e-06, 'epoch': 0.77}

 77%|███████▋  | 2953/3844 [6:04:29<1:56:34,  7.85s/it]


 77%|███████▋  | 2955/3844 [6:04:47<2:01:37,  8.21s/it]

 77%|███████▋  | 2956/3844 [6:04:57<2:11:06,  8.86s/it]

 77%|███████▋  | 2957/3844 [6:05:02<1:55:33,  7.82s/it]
{'loss': 1.1231, 'grad_norm': 0.3971147742301982, 'learning_rate': 2.665934004530203e-06, 'epoch': 0.77}

 77%|███████▋  | 2958/3844 [6:05:12<2:03:23,  8.36s/it]

 77%|███████▋  | 2959/3844 [6:05:19<1:59:38,  8.11s/it]


 77%|███████▋  | 2961/3844 [6:05:36<2:01:04,  8.23s/it]
{'loss': 1.1416, 'grad_norm': 0.37836686466296204, 'learning_rate': 2.6430612933797027e-06, 'epoch': 0.77}


 77%|███████▋  | 2963/3844 [6:05:47<1:40:35,  6.85s/it]

 77%|███████▋  | 2964/3844 [6:05:54<1:40:35,  6.86s/it]

 77%|███████▋  | 2965/3844 [6:06:02<1:46:52,  7.30s/it]

 77%|███████▋  | 2966/3844 [6:06:09<1:43:52,  7.10s/it]

 77%|███████▋  | 2967/3844 [6:06:21<2:03:16,  8.43s/it]

 77%|███████▋  | 2968/3844 [6:06:26<1:51:13,  7.62s/it]
{'loss': 1.2518, 'grad_norm': 0.4265111858133035, 'learning_rate': 2.603235347887322e-06, 'epoch': 0.77}


 77%|███████▋  | 2970/3844 [6:06:43<1:55:33,  7.93s/it]
{'loss': 1.1785, 'grad_norm': 0.39698019308681404, 'learning_rate': 2.591903720877085e-06, 'epoch': 0.77}


 77%|███████▋  | 2972/3844 [6:06:55<1:41:07,  6.96s/it]
{'loss': 1.2738, 'grad_norm': 0.37950536706488847, 'learning_rate': 2.580593137195816e-06, 'epoch': 0.77}

 77%|███████▋  | 2973/3844 [6:07:01<1:38:55,  6.82s/it]


 77%|███████▋  | 2975/3844 [6:07:18<1:53:03,  7.81s/it]

 77%|███████▋  | 2976/3844 [6:07:24<1:46:28,  7.36s/it]
{'loss': 1.3612, 'grad_norm': 0.38645935095789363, 'learning_rate': 2.558035228275035e-06, 'epoch': 0.77}


 77%|███████▋  | 2978/3844 [6:07:43<1:58:42,  8.22s/it]

 77%|███████▋  | 2979/3844 [6:07:51<1:58:08,  8.19s/it]

 78%|███████▊  | 2980/3844 [6:07:57<1:50:02,  7.64s/it]

 78%|███████▊  | 2981/3844 [6:08:05<1:51:40,  7.76s/it]

 78%|███████▊  | 2982/3844 [6:08:14<1:56:52,  8.14s/it]

 78%|███████▊  | 2983/3844 [6:08:21<1:50:28,  7.70s/it]

 78%|███████▊  | 2984/3844 [6:08:27<1:42:14,  7.13s/it]
{'loss': 1.1851, 'grad_norm': 0.40583932906255477, 'learning_rate': 2.513173340026741e-06, 'epoch': 0.78}

 78%|███████▊  | 2985/3844 [6:08:34<1:40:48,  7.04s/it]


 78%|███████▊  | 2987/3844 [6:08:47<1:38:10,  6.87s/it]
{'loss': 1.1301, 'grad_norm': 0.41369051088633974, 'learning_rate': 2.4964377488496317e-06, 'epoch': 0.78}

 78%|███████▊  | 2988/3844 [6:08:53<1:34:38,  6.63s/it]

 78%|███████▊  | 2989/3844 [6:09:04<1:49:10,  7.66s/it]


 78%|███████▊  | 2991/3844 [6:09:17<1:43:04,  7.25s/it]

 78%|███████▊  | 2992/3844 [6:09:23<1:37:04,  6.84s/it]

 78%|███████▊  | 2993/3844 [6:09:29<1:34:07,  6.64s/it]
{'loss': 1.2719, 'grad_norm': 0.36534916938721335, 'learning_rate': 2.463110546039121e-06, 'epoch': 0.78}

 78%|███████▊  | 2994/3844 [6:09:38<1:42:33,  7.24s/it]

 78%|███████▊  | 2995/3844 [6:09:44<1:39:10,  7.01s/it]


 78%|███████▊  | 2997/3844 [6:09:59<1:45:25,  7.47s/it]

 78%|███████▊  | 2998/3844 [6:10:05<1:37:39,  6.93s/it]

 78%|███████▊  | 2999/3844 [6:10:13<1:41:06,  7.18s/it]

 78%|███████▊  | 3000/3844 [6:10:19<1:35:29,  6.79s/it]
 78%|███████▊  | 3000/3844 [6:10:19<1:35:29,  6.79s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.1084, 'grad_norm': 0.4215332676991155, 'learning_rate': 2.4189741500881224e-06, 'epoch': 0.78}
 78%|███████▊  | 3001/3844 [6:10:50<3:18:27, 14.12s/it]

 78%|███████▊  | 3002/3844 [6:10:56<2:44:43, 11.74s/it]

 78%|███████▊  | 3003/3844 [6:11:04<2:28:02, 10.56s/it]

 78%|███████▊  | 3004/3844 [6:11:16<2:33:13, 10.95s/it]


 78%|███████▊  | 3006/3844 [6:11:29<2:04:46,  8.93s/it]

 78%|███████▊  | 3007/3844 [6:11:39<2:06:25,  9.06s/it]

 78%|███████▊  | 3008/3844 [6:11:49<2:10:13,  9.35s/it]

 78%|███████▊  | 3009/3844 [6:11:59<2:13:22,  9.58s/it]
{'loss': 1.1223, 'grad_norm': 0.3723063586346111, 'learning_rate': 2.3751823057162195e-06, 'epoch': 0.78}


 78%|███████▊  | 3011/3844 [6:12:14<1:57:37,  8.47s/it]
{'loss': 1.1012, 'grad_norm': 0.41900845108567214, 'learning_rate': 2.3642884143916554e-06, 'epoch': 0.78}

 78%|███████▊  | 3012/3844 [6:12:24<2:05:05,  9.02s/it]

 78%|███████▊  | 3013/3844 [6:12:32<2:02:03,  8.81s/it]


 78%|███████▊  | 3015/3844 [6:12:49<1:54:53,  8.32s/it]

 78%|███████▊  | 3016/3844 [6:12:57<1:53:39,  8.24s/it]

 78%|███████▊  | 3017/3844 [6:13:03<1:46:51,  7.75s/it]

 79%|███████▊  | 3018/3844 [6:13:11<1:45:13,  7.64s/it]

 79%|███████▊  | 3019/3844 [6:13:21<1:55:44,  8.42s/it]
[2024-05-26 17:19:53,684] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▊  | 3020/3844 [6:13:27<1:46:53,  7.78s/it]

 79%|███████▊  | 3021/3844 [6:13:37<1:54:43,  8.36s/it]
{'loss': 1.1731, 'grad_norm': 0.411350987268198, 'learning_rate': 2.310144922855858e-06, 'epoch': 0.79}


 79%|███████▊  | 3023/3844 [6:13:51<1:45:00,  7.67s/it]

 79%|███████▊  | 3024/3844 [6:14:01<1:54:25,  8.37s/it]

 79%|███████▊  | 3025/3844 [6:14:12<2:03:23,  9.04s/it]
{'loss': 1.0507, 'grad_norm': 0.38843629977344274, 'learning_rate': 2.2886402171772638e-06, 'epoch': 0.79}

 79%|███████▊  | 3026/3844 [6:14:24<2:16:51, 10.04s/it]

 79%|███████▊  | 3027/3844 [6:14:34<2:16:22, 10.02s/it]

 79%|███████▉  | 3028/3844 [6:14:41<2:02:40,  9.02s/it]


 79%|███████▉  | 3030/3844 [6:14:53<1:45:09,  7.75s/it]

 79%|███████▉  | 3031/3844 [6:15:00<1:39:00,  7.31s/it]

 79%|███████▉  | 3032/3844 [6:15:07<1:38:02,  7.24s/it]
{'loss': 1.1697, 'grad_norm': 0.38936362093699123, 'learning_rate': 2.251217961974067e-06, 'epoch': 0.79}


 79%|███████▉  | 3034/3844 [6:15:18<1:25:48,  6.36s/it]
{'loss': 1.1028, 'grad_norm': 0.41912789348494345, 'learning_rate': 2.240575357134984e-06, 'epoch': 0.79}

 79%|███████▉  | 3035/3844 [6:15:24<1:26:48,  6.44s/it]


 79%|███████▉  | 3037/3844 [6:15:38<1:27:02,  6.47s/it]
{'loss': 1.1723, 'grad_norm': 0.40412214994023476, 'learning_rate': 2.224652786757381e-06, 'epoch': 0.79}

 79%|███████▉  | 3038/3844 [6:15:46<1:34:35,  7.04s/it]


 79%|███████▉  | 3040/3844 [6:16:00<1:31:56,  6.86s/it]

 79%|███████▉  | 3041/3844 [6:16:07<1:32:11,  6.89s/it]

 79%|███████▉  | 3042/3844 [6:16:13<1:31:05,  6.82s/it]
{'loss': 1.1373, 'grad_norm': 0.35777101370021586, 'learning_rate': 2.1982256522226077e-06, 'epoch': 0.79}

 79%|███████▉  | 3043/3844 [6:16:20<1:30:01,  6.74s/it]

 79%|███████▉  | 3044/3844 [6:16:27<1:29:44,  6.73s/it]


 79%|███████▉  | 3046/3844 [6:16:44<1:41:09,  7.61s/it]

 79%|███████▉  | 3047/3844 [6:16:49<1:32:05,  6.93s/it]

 79%|███████▉  | 3048/3844 [6:16:59<1:44:10,  7.85s/it]
{'loss': 1.0413, 'grad_norm': 0.38667121904961, 'learning_rate': 2.166695964910204e-06, 'epoch': 0.79}

 79%|███████▉  | 3049/3844 [6:17:09<1:50:44,  8.36s/it]

 79%|███████▉  | 3050/3844 [6:17:15<1:41:00,  7.63s/it]


 79%|███████▉  | 3052/3844 [6:17:29<1:39:04,  7.51s/it]
{'loss': 1.2847, 'grad_norm': 0.4072487591755036, 'learning_rate': 2.145787379450612e-06, 'epoch': 0.79}

 79%|███████▉  | 3053/3844 [6:17:36<1:38:04,  7.44s/it]


 79%|███████▉  | 3055/3844 [6:17:51<1:39:08,  7.54s/it]

 80%|███████▉  | 3056/3844 [6:18:03<1:56:33,  8.88s/it]
{'loss': 0.9278, 'grad_norm': 0.36906836745692606, 'learning_rate': 2.12496803616951e-06, 'epoch': 0.79}

 80%|███████▉  | 3057/3844 [6:18:10<1:48:59,  8.31s/it]

 80%|███████▉  | 3058/3844 [6:18:18<1:48:14,  8.26s/it]


 80%|███████▉  | 3060/3844 [6:18:37<1:55:46,  8.86s/it]

 80%|███████▉  | 3061/3844 [6:18:43<1:42:50,  7.88s/it]

 80%|███████▉  | 3062/3844 [6:18:49<1:36:53,  7.43s/it]

 80%|███████▉  | 3063/3844 [6:18:56<1:32:12,  7.08s/it]
{'loss': 1.1934, 'grad_norm': 0.42728410746266066, 'learning_rate': 2.0887496353632964e-06, 'epoch': 0.8}

 80%|███████▉  | 3064/3844 [6:19:02<1:29:45,  6.90s/it]


 80%|███████▉  | 3066/3844 [6:19:24<1:55:41,  8.92s/it]

 80%|███████▉  | 3067/3844 [6:19:30<1:43:44,  8.01s/it]

 80%|███████▉  | 3068/3844 [6:19:43<2:05:42,  9.72s/it]

 80%|███████▉  | 3069/3844 [6:19:53<2:06:40,  9.81s/it]
{'loss': 1.151, 'grad_norm': 0.37967093667940593, 'learning_rate': 2.0579243513895565e-06, 'epoch': 0.8}


 80%|███████▉  | 3071/3844 [6:20:07<1:47:59,  8.38s/it]
{'loss': 1.239, 'grad_norm': 0.4419230725293989, 'learning_rate': 2.047694338165005e-06, 'epoch': 0.8}


 80%|███████▉  | 3073/3844 [6:20:24<1:45:14,  8.19s/it]

 80%|███████▉  | 3074/3844 [6:20:30<1:36:38,  7.53s/it]

 80%|███████▉  | 3075/3844 [6:20:36<1:31:50,  7.17s/it]

 80%|████████  | 3076/3844 [6:20:42<1:26:47,  6.78s/it]

 80%|████████  | 3077/3844 [6:20:49<1:27:41,  6.86s/it]
{'loss': 1.2169, 'grad_norm': 0.4038176832018227, 'learning_rate': 2.017139949620005e-06, 'epoch': 0.8}


 80%|████████  | 3079/3844 [6:21:04<1:31:26,  7.17s/it]

 80%|████████  | 3080/3844 [6:21:09<1:24:52,  6.66s/it]

 80%|████████  | 3081/3844 [6:21:18<1:31:45,  7.21s/it]

 80%|████████  | 3082/3844 [6:21:24<1:26:55,  6.84s/it]

 80%|████████  | 3083/3844 [6:21:30<1:22:45,  6.52s/it]

 80%|████████  | 3084/3844 [6:21:40<1:38:01,  7.74s/it]

 80%|████████  | 3085/3844 [6:21:46<1:29:15,  7.06s/it]

 80%|████████  | 3086/3844 [6:21:52<1:26:17,  6.83s/it]
{'loss': 1.1905, 'grad_norm': 0.36690480327230246, 'learning_rate': 1.9716912661612264e-06, 'epoch': 0.8}

 80%|████████  | 3087/3844 [6:22:02<1:39:53,  7.92s/it]


 80%|████████  | 3089/3844 [6:22:15<1:30:45,  7.21s/it]
{'loss': 1.1938, 'grad_norm': 0.37880501042254294, 'learning_rate': 1.956644199133092e-06, 'epoch': 0.8}


 80%|████████  | 3091/3844 [6:22:30<1:33:28,  7.45s/it]

 80%|████████  | 3092/3844 [6:22:38<1:34:05,  7.51s/it]

 80%|████████  | 3093/3844 [6:22:44<1:28:00,  7.03s/it]

 80%|████████  | 3094/3844 [6:22:52<1:32:00,  7.36s/it]
{'loss': 1.1402, 'grad_norm': 0.40166127672082175, 'learning_rate': 1.9316800405842783e-06, 'epoch': 0.8}

 81%|████████  | 3095/3844 [6:23:01<1:37:16,  7.79s/it]


 81%|████████  | 3097/3844 [6:23:12<1:23:55,  6.74s/it]

 81%|████████  | 3098/3844 [6:23:19<1:25:15,  6.86s/it]
{'loss': 1.094, 'grad_norm': 0.3839426370240406, 'learning_rate': 1.911811826806379e-06, 'epoch': 0.81}


 81%|████████  | 3100/3844 [6:23:36<1:33:21,  7.53s/it]

 81%|████████  | 3101/3844 [6:23:42<1:27:53,  7.10s/it]
{'loss': 1.1144, 'grad_norm': 0.4231753501910811, 'learning_rate': 1.8969709639921153e-06, 'epoch': 0.81}


 81%|████████  | 3103/3844 [6:23:56<1:24:59,  6.88s/it]
{'loss': 1.1996, 'grad_norm': 0.39294762159474805, 'learning_rate': 1.887105821280154e-06, 'epoch': 0.81}

 81%|████████  | 3104/3844 [6:24:03<1:25:13,  6.91s/it]

 81%|████████  | 3105/3844 [6:24:11<1:31:03,  7.39s/it]

 81%|████████  | 3106/3844 [6:24:19<1:33:02,  7.56s/it]

 81%|████████  | 3107/3844 [6:24:27<1:34:50,  7.72s/it]


 81%|████████  | 3109/3844 [6:24:42<1:33:20,  7.62s/it]

 81%|████████  | 3110/3844 [6:24:48<1:28:01,  7.20s/it]
{'loss': 1.102, 'grad_norm': 0.43627274244183234, 'learning_rate': 1.8527594873657918e-06, 'epoch': 0.81}

 81%|████████  | 3111/3844 [6:24:55<1:26:02,  7.04s/it]

 81%|████████  | 3112/3844 [6:25:01<1:23:12,  6.82s/it]

 81%|████████  | 3113/3844 [6:25:13<1:43:12,  8.47s/it]

 81%|████████  | 3114/3844 [6:25:21<1:39:40,  8.19s/it]


 81%|████████  | 3116/3844 [6:25:34<1:29:30,  7.38s/it]

 81%|████████  | 3117/3844 [6:25:41<1:24:33,  6.98s/it]
{'loss': 1.2053, 'grad_norm': 0.4209878798548166, 'learning_rate': 1.8186966536033146e-06, 'epoch': 0.81}

 81%|████████  | 3118/3844 [6:25:47<1:22:02,  6.78s/it]


 81%|████████  | 3120/3844 [6:26:00<1:21:50,  6.78s/it]

 81%|████████  | 3121/3844 [6:26:10<1:32:04,  7.64s/it]
{'loss': 1.158, 'grad_norm': 0.3885113334608179, 'learning_rate': 1.799359919503516e-06, 'epoch': 0.81}


 81%|████████  | 3123/3844 [6:26:24<1:28:18,  7.35s/it]

 81%|████████▏ | 3124/3844 [6:26:32<1:29:47,  7.48s/it]
{'loss': 1.1895, 'grad_norm': 0.4274426678442665, 'learning_rate': 1.7849185052797525e-06, 'epoch': 0.81}


 81%|████████▏ | 3126/3844 [6:26:51<1:37:11,  8.12s/it]

 81%|████████▏ | 3127/3844 [6:27:00<1:43:02,  8.62s/it]

 81%|████████▏ | 3128/3844 [6:27:07<1:33:52,  7.87s/it]

 81%|████████▏ | 3129/3844 [6:27:12<1:24:59,  7.13s/it]

 81%|████████▏ | 3130/3844 [6:27:22<1:34:56,  7.98s/it]
{'loss': 1.192, 'grad_norm': 0.3983062176511138, 'learning_rate': 1.7561932843147066e-06, 'epoch': 0.81}


 81%|████████▏ | 3132/3844 [6:27:38<1:34:39,  7.98s/it]
{'loss': 1.0764, 'grad_norm': 0.4002233903221632, 'learning_rate': 1.7466650089324211e-06, 'epoch': 0.81}


 82%|████████▏ | 3134/3844 [6:27:49<1:19:45,  6.74s/it]
{'loss': 1.1286, 'grad_norm': 0.4002239979082328, 'learning_rate': 1.7371601778517178e-06, 'epoch': 0.82}


 82%|████████▏ | 3136/3844 [6:28:01<1:14:38,  6.33s/it]
{'loss': 1.2169, 'grad_norm': 0.41413920507448876, 'learning_rate': 1.7276788180718784e-06, 'epoch': 0.82}

 82%|████████▏ | 3137/3844 [6:28:07<1:14:55,  6.36s/it]


 82%|████████▏ | 3139/3844 [6:28:20<1:15:40,  6.44s/it]

 82%|████████▏ | 3140/3844 [6:28:26<1:15:29,  6.43s/it]

 82%|████████▏ | 3141/3844 [6:28:36<1:26:46,  7.41s/it]
{'loss': 1.1205, 'grad_norm': 0.3791503951582666, 'learning_rate': 1.7040782821445389e-06, 'epoch': 0.82}
[2024-05-26 17:35:17,914] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 3142/3844 [6:28:45<1:32:34,  7.91s/it]


 82%|████████▏ | 3144/3844 [6:28:58<1:23:17,  7.14s/it]

 82%|████████▏ | 3145/3844 [6:29:04<1:20:59,  6.95s/it]

 82%|████████▏ | 3146/3844 [6:29:10<1:17:05,  6.63s/it]

 82%|████████▏ | 3147/3844 [6:29:16<1:13:30,  6.33s/it]
{'loss': 1.092, 'grad_norm': 0.39611700261331934, 'learning_rate': 1.675952088978392e-06, 'epoch': 0.82}

 82%|████████▏ | 3148/3844 [6:29:21<1:10:06,  6.04s/it]

 82%|████████▏ | 3149/3844 [6:29:29<1:16:41,  6.62s/it]


 82%|████████▏ | 3151/3844 [6:29:43<1:17:14,  6.69s/it]

 82%|████████▏ | 3152/3844 [6:29:50<1:20:05,  6.94s/it]

 82%|████████▏ | 3153/3844 [6:29:58<1:23:23,  7.24s/it]

 82%|████████▏ | 3154/3844 [6:30:10<1:39:23,  8.64s/it]

 82%|████████▏ | 3155/3844 [6:30:17<1:33:21,  8.13s/it]

 82%|████████▏ | 3156/3844 [6:30:23<1:25:34,  7.46s/it]

 82%|████████▏ | 3157/3844 [6:30:30<1:25:49,  7.50s/it]

 82%|████████▏ | 3158/3844 [6:30:41<1:35:26,  8.35s/it]
{'loss': 1.1093, 'grad_norm': 0.3884158163310495, 'learning_rate': 1.6249406212541507e-06, 'epoch': 0.82}


 82%|████████▏ | 3160/3844 [6:30:58<1:36:52,  8.50s/it]
{'loss': 1.0933, 'grad_norm': 0.3991568606700561, 'learning_rate': 1.6157429986838634e-06, 'epoch': 0.82}

 82%|████████▏ | 3161/3844 [6:31:03<1:26:50,  7.63s/it]

 82%|████████▏ | 3162/3844 [6:31:09<1:20:01,  7.04s/it]


 82%|████████▏ | 3164/3844 [6:31:26<1:28:56,  7.85s/it]

 82%|████████▏ | 3165/3844 [6:31:33<1:24:03,  7.43s/it]

 82%|████████▏ | 3166/3844 [6:31:41<1:25:09,  7.54s/it]

 82%|████████▏ | 3167/3844 [6:31:52<1:37:59,  8.68s/it]
{'loss': 1.0565, 'grad_norm': 0.3881525684649866, 'learning_rate': 1.5837390430823352e-06, 'epoch': 0.82}

 82%|████████▏ | 3168/3844 [6:31:58<1:28:10,  7.83s/it]


 82%|████████▏ | 3170/3844 [6:32:12<1:26:23,  7.69s/it]
{'loss': 1.0055, 'grad_norm': 0.38592409446661646, 'learning_rate': 1.570112648946366e-06, 'epoch': 0.82}


 83%|████████▎ | 3172/3844 [6:32:27<1:23:12,  7.43s/it]
{'loss': 1.233, 'grad_norm': 0.38201591104481813, 'learning_rate': 1.561058313085413e-06, 'epoch': 0.83}


 83%|████████▎ | 3174/3844 [6:32:41<1:21:48,  7.33s/it]

 83%|████████▎ | 3175/3844 [6:32:48<1:20:17,  7.20s/it]

 83%|████████▎ | 3176/3844 [6:32:54<1:16:17,  6.85s/it]

 83%|████████▎ | 3177/3844 [6:33:02<1:19:55,  7.19s/it]

 83%|████████▎ | 3178/3844 [6:33:13<1:31:22,  8.23s/it]

 83%|████████▎ | 3179/3844 [6:33:20<1:29:28,  8.07s/it]

 83%|████████▎ | 3180/3844 [6:33:27<1:23:50,  7.58s/it]

 83%|████████▎ | 3181/3844 [6:33:34<1:22:47,  7.49s/it]

 83%|████████▎ | 3182/3844 [6:33:40<1:18:50,  7.15s/it]
{'loss': 1.2446, 'grad_norm': 0.3939170854930067, 'learning_rate': 1.5161467187864642e-06, 'epoch': 0.83}


 83%|████████▎ | 3184/3844 [6:33:54<1:16:28,  6.95s/it]
{'loss': 0.9861, 'grad_norm': 0.4135196849305123, 'learning_rate': 1.5072365955332346e-06, 'epoch': 0.83}


 83%|████████▎ | 3186/3844 [6:34:08<1:16:47,  7.00s/it]

 83%|████████▎ | 3187/3844 [6:34:14<1:13:04,  6.67s/it]

 83%|████████▎ | 3188/3844 [6:34:21<1:11:49,  6.57s/it]

 83%|████████▎ | 3189/3844 [6:34:29<1:18:00,  7.15s/it]
{'loss': 1.0724, 'grad_norm': 0.38690026513694953, 'learning_rate': 1.4850668869275497e-06, 'epoch': 0.83}


 83%|████████▎ | 3191/3844 [6:34:43<1:17:49,  7.15s/it]
{'loss': 1.1453, 'grad_norm': 0.39537353141797044, 'learning_rate': 1.4762413094199267e-06, 'epoch': 0.83}

 83%|████████▎ | 3192/3844 [6:34:49<1:14:34,  6.86s/it]

 83%|████████▎ | 3193/3844 [6:34:57<1:17:40,  7.16s/it]

 83%|████████▎ | 3194/3844 [6:35:06<1:21:54,  7.56s/it]


 83%|████████▎ | 3196/3844 [6:35:19<1:16:30,  7.08s/it]

 83%|████████▎ | 3197/3844 [6:35:25<1:13:39,  6.83s/it]
{'loss': 1.1306, 'grad_norm': 0.40097705657698784, 'learning_rate': 1.4499099516112358e-06, 'epoch': 0.83}


 83%|████████▎ | 3199/3844 [6:35:42<1:23:06,  7.73s/it]
{'loss': 1.0906, 'grad_norm': 0.4042598332777441, 'learning_rate': 1.441181373686923e-06, 'epoch': 0.83}


 83%|████████▎ | 3201/3844 [6:35:55<1:15:37,  7.06s/it]

 83%|████████▎ | 3202/3844 [6:36:02<1:16:55,  7.19s/it]
{'loss': 1.1439, 'grad_norm': 0.37827203910420726, 'learning_rate': 1.4281340996308223e-06, 'epoch': 0.83}

 83%|████████▎ | 3203/3844 [6:36:12<1:23:10,  7.79s/it]


 83%|████████▎ | 3205/3844 [6:36:25<1:17:53,  7.31s/it]

 83%|████████▎ | 3206/3844 [6:36:31<1:12:40,  6.83s/it]
{'loss': 1.2475, 'grad_norm': 0.3920167429682558, 'learning_rate': 1.4108229704652422e-06, 'epoch': 0.83}

 83%|████████▎ | 3207/3844 [6:36:38<1:12:04,  6.79s/it]

 83%|████████▎ | 3208/3844 [6:36:44<1:10:50,  6.68s/it]


 84%|████████▎ | 3210/3844 [6:36:57<1:09:21,  6.56s/it]
{'loss': 1.2053, 'grad_norm': 0.42443576286819035, 'learning_rate': 1.3936094343884377e-06, 'epoch': 0.83}

 84%|████████▎ | 3211/3844 [6:37:04<1:10:32,  6.69s/it]

 84%|████████▎ | 3212/3844 [6:37:09<1:07:27,  6.40s/it]

 84%|████████▎ | 3213/3844 [6:37:18<1:13:26,  6.98s/it]


 84%|████████▎ | 3215/3844 [6:37:35<1:22:07,  7.83s/it]

 84%|████████▎ | 3216/3844 [6:37:42<1:19:40,  7.61s/it]

 84%|████████▎ | 3217/3844 [6:37:49<1:17:00,  7.37s/it]
{'loss': 1.0975, 'grad_norm': 0.42790018168046007, 'learning_rate': 1.363721167283133e-06, 'epoch': 0.84}


 84%|████████▎ | 3219/3844 [6:38:04<1:16:37,  7.36s/it]
{'loss': 1.3083, 'grad_norm': 0.40612840591986865, 'learning_rate': 1.3552368142217743e-06, 'epoch': 0.84}


 84%|████████▍ | 3221/3844 [6:38:23<1:25:09,  8.20s/it]

 84%|████████▍ | 3222/3844 [6:38:31<1:24:19,  8.13s/it]

 84%|████████▍ | 3223/3844 [6:38:37<1:16:02,  7.35s/it]

 84%|████████▍ | 3224/3844 [6:38:43<1:13:44,  7.14s/it]

 84%|████████▍ | 3225/3844 [6:38:49<1:08:57,  6.68s/it]
{'loss': 1.2914, 'grad_norm': 0.3641860371960051, 'learning_rate': 1.3299311882113851e-06, 'epoch': 0.84}


 84%|████████▍ | 3227/3844 [6:39:05<1:16:16,  7.42s/it]

 84%|████████▍ | 3228/3844 [6:39:13<1:17:25,  7.54s/it]

 84%|████████▍ | 3229/3844 [6:39:19<1:14:42,  7.29s/it]
{'loss': 1.215, 'grad_norm': 0.39485656678979975, 'learning_rate': 1.3131838712837586e-06, 'epoch': 0.84}


 84%|████████▍ | 3231/3844 [6:39:35<1:17:30,  7.59s/it]
{'loss': 1.1874, 'grad_norm': 0.3804377348422691, 'learning_rate': 1.3048472144069624e-06, 'epoch': 0.84}

 84%|████████▍ | 3232/3844 [6:39:44<1:21:42,  8.01s/it]

 84%|████████▍ | 3233/3844 [6:39:50<1:16:15,  7.49s/it]


 84%|████████▍ | 3235/3844 [6:40:05<1:17:23,  7.62s/it]

 84%|████████▍ | 3236/3844 [6:40:11<1:11:54,  7.10s/it]

 84%|████████▍ | 3237/3844 [6:40:21<1:19:48,  7.89s/it]

 84%|████████▍ | 3238/3844 [6:40:27<1:14:08,  7.34s/it]
{'loss': 1.102, 'grad_norm': 0.4079697657690782, 'learning_rate': 1.2758635772775418e-06, 'epoch': 0.84}


 84%|████████▍ | 3240/3844 [6:40:41<1:12:31,  7.20s/it]

 84%|████████▍ | 3241/3844 [6:40:47<1:07:51,  6.75s/it]
{'loss': 1.1704, 'grad_norm': 0.42071263502083633, 'learning_rate': 1.2635348910259826e-06, 'epoch': 0.84}

 84%|████████▍ | 3242/3844 [6:40:56<1:14:52,  7.46s/it]

 84%|████████▍ | 3243/3844 [6:41:04<1:16:12,  7.61s/it]

 84%|████████▍ | 3244/3844 [6:41:10<1:11:44,  7.17s/it]


 84%|████████▍ | 3246/3844 [6:41:25<1:12:41,  7.29s/it]

 84%|████████▍ | 3247/3844 [6:41:33<1:15:10,  7.55s/it]

 84%|████████▍ | 3248/3844 [6:41:45<1:28:54,  8.95s/it]

 85%|████████▍ | 3249/3844 [6:41:51<1:18:16,  7.89s/it]

 85%|████████▍ | 3250/3844 [6:41:57<1:12:07,  7.28s/it]

 85%|████████▍ | 3251/3844 [6:42:05<1:13:41,  7.46s/it]

 85%|████████▍ | 3252/3844 [6:42:14<1:18:14,  7.93s/it]

 85%|████████▍ | 3253/3844 [6:42:21<1:15:32,  7.67s/it]
{'loss': 1.231, 'grad_norm': 0.4223520016943703, 'learning_rate': 1.2147793036255273e-06, 'epoch': 0.85}


 85%|████████▍ | 3255/3844 [6:42:35<1:13:01,  7.44s/it]
{'loss': 1.1296, 'grad_norm': 0.41336932602094023, 'learning_rate': 1.2067405814523691e-06, 'epoch': 0.85}


 85%|████████▍ | 3257/3844 [6:42:49<1:12:03,  7.37s/it]
{'loss': 1.1597, 'grad_norm': 0.4005187566262855, 'learning_rate': 1.1987268372821548e-06, 'epoch': 0.85}

 85%|████████▍ | 3258/3844 [6:42:56<1:10:02,  7.17s/it]


 85%|████████▍ | 3260/3844 [6:43:11<1:11:39,  7.36s/it]

 85%|████████▍ | 3261/3844 [6:43:17<1:08:38,  7.06s/it]

 85%|████████▍ | 3262/3844 [6:43:25<1:12:01,  7.42s/it]
{'loss': 1.0908, 'grad_norm': 0.42561041254917653, 'learning_rate': 1.1788019048316457e-06, 'epoch': 0.85}

 85%|████████▍ | 3263/3844 [6:43:32<1:09:59,  7.23s/it]

 85%|████████▍ | 3264/3844 [6:43:40<1:13:13,  7.57s/it]


 85%|████████▍ | 3266/3844 [6:43:55<1:12:20,  7.51s/it]

 85%|████████▍ | 3267/3844 [6:44:01<1:06:42,  6.94s/it]

 85%|████████▌ | 3268/3844 [6:44:07<1:03:39,  6.63s/it]

 85%|████████▌ | 3269/3844 [6:44:14<1:04:15,  6.70s/it]
{'loss': 0.9845, 'grad_norm': 0.4142960324489717, 'learning_rate': 1.1511701801164055e-06, 'epoch': 0.85}

 85%|████████▌ | 3270/3844 [6:44:20<1:03:38,  6.65s/it]

 85%|████████▌ | 3271/3844 [6:44:28<1:08:01,  7.12s/it]


 85%|████████▌ | 3273/3844 [6:44:45<1:12:08,  7.58s/it]

 85%|████████▌ | 3274/3844 [6:44:57<1:24:07,  8.86s/it]
{'loss': 1.1178, 'grad_norm': 0.4069614996760866, 'learning_rate': 1.131621696664007e-06, 'epoch': 0.85}


 85%|████████▌ | 3276/3844 [6:45:12<1:16:19,  8.06s/it]

 85%|████████▌ | 3277/3844 [6:45:20<1:16:41,  8.12s/it]

 85%|████████▌ | 3278/3844 [6:45:29<1:19:47,  8.46s/it]
[2024-05-26 17:52:01,862] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 3279/3844 [6:45:36<1:15:08,  7.98s/it]

 85%|████████▌ | 3280/3844 [6:45:44<1:14:41,  7.95s/it]
{'loss': 1.0963, 'grad_norm': 0.3970264106496661, 'learning_rate': 1.108371375529561e-06, 'epoch': 0.85}

 85%|████████▌ | 3281/3844 [6:45:51<1:11:27,  7.61s/it]


 85%|████████▌ | 3283/3844 [6:46:05<1:09:16,  7.41s/it]

 85%|████████▌ | 3284/3844 [6:46:13<1:11:03,  7.61s/it]

 85%|████████▌ | 3285/3844 [6:46:19<1:06:32,  7.14s/it]

 85%|████████▌ | 3286/3844 [6:46:27<1:08:16,  7.34s/it]

 86%|████████▌ | 3287/3844 [6:46:33<1:04:34,  6.96s/it]

 86%|████████▌ | 3288/3844 [6:46:40<1:03:23,  6.84s/it]
{'loss': 1.1488, 'grad_norm': 0.4031217468023202, 'learning_rate': 1.077724652586466e-06, 'epoch': 0.86}


 86%|████████▌ | 3290/3844 [6:46:53<1:01:25,  6.65s/it]

 86%|████████▌ | 3291/3844 [6:47:00<1:01:50,  6.71s/it]

 86%|████████▌ | 3292/3844 [6:47:07<1:04:46,  7.04s/it]

 86%|████████▌ | 3293/3844 [6:47:17<1:11:47,  7.82s/it]

 86%|████████▌ | 3294/3844 [6:47:24<1:08:52,  7.51s/it]

 86%|████████▌ | 3295/3844 [6:47:31<1:07:15,  7.35s/it]

 86%|████████▌ | 3296/3844 [6:47:37<1:03:29,  6.95s/it]
{'loss': 1.1503, 'grad_norm': 0.385973542877558, 'learning_rate': 1.047483439942495e-06, 'epoch': 0.86}


 86%|████████▌ | 3298/3844 [6:47:51<1:03:03,  6.93s/it]
{'loss': 1.1423, 'grad_norm': 0.3816102366700452, 'learning_rate': 1.0399866592386642e-06, 'epoch': 0.86}


 86%|████████▌ | 3300/3844 [6:48:06<1:06:00,  7.28s/it]
 86%|████████▌ | 3300/3844 [6:48:06<1:06:00,  7.28s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 86%|████████▌ | 3301/3844 [6:49:14<3:50:32, 25.47s/it]
[2024-05-26 17:55:46,750] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8878, 'grad_norm': 0.3900895922716395, 'learning_rate': 1.0287892167210544e-06, 'epoch': 0.86}


 86%|████████▌ | 3303/3844 [6:49:27<2:23:11, 15.88s/it]

 86%|████████▌ | 3304/3844 [6:49:37<2:06:57, 14.11s/it]
{'loss': 1.1741, 'grad_norm': 0.3647278361603695, 'learning_rate': 1.0176491120366482e-06, 'epoch': 0.86}

 86%|████████▌ | 3305/3844 [6:49:44<1:47:38, 11.98s/it]

 86%|████████▌ | 3306/3844 [6:49:56<1:47:56, 12.04s/it]

 86%|████████▌ | 3307/3844 [6:50:03<1:32:23, 10.32s/it]


 86%|████████▌ | 3309/3844 [6:50:16<1:14:41,  8.38s/it]
{'loss': 1.1852, 'grad_norm': 0.3768907320416351, 'learning_rate': 9.99209881519494e-07, 'epoch': 0.86}

 86%|████████▌ | 3310/3844 [6:50:24<1:14:25,  8.36s/it]

 86%|████████▌ | 3311/3844 [6:50:33<1:13:45,  8.30s/it]


 86%|████████▌ | 3313/3844 [6:50:51<1:20:51,  9.14s/it]
[2024-05-26 17:57:24,182] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 3314/3844 [6:50:57<1:12:30,  8.21s/it]

 86%|████████▌ | 3315/3844 [6:51:04<1:07:12,  7.62s/it]

 86%|████████▋ | 3316/3844 [6:51:09<1:01:15,  6.96s/it]

 86%|████████▋ | 3317/3844 [6:51:16<1:00:11,  6.85s/it]

 86%|████████▋ | 3318/3844 [6:51:22<59:24,  6.78s/it]

 86%|████████▋ | 3319/3844 [6:51:30<1:01:56,  7.08s/it]
{'loss': 1.1322, 'grad_norm': 0.4003348285075477, 'learning_rate': 9.628111352580805e-07, 'epoch': 0.86}


 86%|████████▋ | 3321/3844 [6:51:43<59:30,  6.83s/it]
{'loss': 1.2571, 'grad_norm': 0.38704314827871467, 'learning_rate': 9.55608316370703e-07, 'epoch': 0.86}


 86%|████████▋ | 3323/3844 [6:52:01<1:08:59,  7.95s/it]

 86%|████████▋ | 3324/3844 [6:52:07<1:03:35,  7.34s/it]
{'loss': 1.1979, 'grad_norm': 0.41091040341942253, 'learning_rate': 9.448522657239045e-07, 'epoch': 0.86}

 86%|████████▋ | 3325/3844 [6:52:16<1:08:16,  7.89s/it]

 87%|████████▋ | 3326/3844 [6:52:25<1:10:01,  8.11s/it]


 87%|████████▋ | 3328/3844 [6:52:36<58:50,  6.84s/it]

 87%|████████▋ | 3329/3844 [6:52:46<1:06:35,  7.76s/it]
{'loss': 1.0888, 'grad_norm': 0.39895318816298736, 'learning_rate': 9.270541579838632e-07, 'epoch': 0.87}

 87%|████████▋ | 3330/3844 [6:52:53<1:03:18,  7.39s/it]


 87%|████████▋ | 3332/3844 [6:53:05<58:10,  6.82s/it]

 87%|████████▋ | 3333/3844 [6:53:12<57:12,  6.72s/it]

 87%|████████▋ | 3334/3844 [6:53:18<54:55,  6.46s/it]

 87%|████████▋ | 3335/3844 [6:53:28<1:04:50,  7.64s/it]
{'loss': 1.2643, 'grad_norm': 0.39639550552883146, 'learning_rate': 9.059090789642333e-07, 'epoch': 0.87}

 87%|████████▋ | 3336/3844 [6:53:35<1:02:49,  7.42s/it]

 87%|████████▋ | 3337/3844 [6:53:45<1:09:08,  8.18s/it]

 87%|████████▋ | 3338/3844 [6:53:51<1:03:44,  7.56s/it]

 87%|████████▋ | 3339/3844 [6:53:57<58:36,  6.96s/it]


 87%|████████▋ | 3341/3844 [6:54:10<57:14,  6.83s/it]
{'loss': 1.076, 'grad_norm': 0.4150426832005781, 'learning_rate': 8.84996492368555e-07, 'epoch': 0.87}


 87%|████████▋ | 3343/3844 [6:54:22<52:39,  6.31s/it]

 87%|████████▋ | 3344/3844 [6:54:28<50:48,  6.10s/it]

 87%|████████▋ | 3345/3844 [6:54:36<55:45,  6.70s/it]
{'loss': 1.1268, 'grad_norm': 0.39723142480943113, 'learning_rate': 8.711841946914068e-07, 'epoch': 0.87}

 87%|████████▋ | 3346/3844 [6:54:45<1:01:47,  7.44s/it]


 87%|████████▋ | 3348/3844 [6:55:02<1:06:48,  8.08s/it]

 87%|████████▋ | 3349/3844 [6:55:08<1:00:47,  7.37s/it]
{'loss': 1.1859, 'grad_norm': 0.4108928331088292, 'learning_rate': 8.574756216584523e-07, 'epoch': 0.87}

 87%|████████▋ | 3350/3844 [6:55:15<59:50,  7.27s/it]


 87%|████████▋ | 3352/3844 [6:55:30<1:01:17,  7.47s/it]

 87%|████████▋ | 3353/3844 [6:55:36<57:10,  6.99s/it]
{'loss': 1.1692, 'grad_norm': 0.38805074613755575, 'learning_rate': 8.438709290310631e-07, 'epoch': 0.87}


 87%|████████▋ | 3355/3844 [6:55:51<1:01:02,  7.49s/it]

 87%|████████▋ | 3356/3844 [6:55:58<59:19,  7.29s/it]

 87%|████████▋ | 3357/3844 [6:56:08<1:05:51,  8.11s/it]
{'loss': 1.2226, 'grad_norm': 0.3717765282332437, 'learning_rate': 8.303702713902872e-07, 'epoch': 0.87}

 87%|████████▋ | 3358/3844 [6:56:15<1:02:51,  7.76s/it]


 87%|████████▋ | 3360/3844 [6:56:29<59:29,  7.38s/it]

 87%|████████▋ | 3361/3844 [6:56:36<56:59,  7.08s/it]

 87%|████████▋ | 3362/3844 [6:56:44<58:24,  7.27s/it]

 87%|████████▋ | 3363/3844 [6:56:50<57:25,  7.16s/it]
{'loss': 1.1491, 'grad_norm': 0.43975952917558325, 'learning_rate': 8.103146857656019e-07, 'epoch': 0.87}


 88%|████████▊ | 3365/3844 [6:57:05<57:03,  7.15s/it]

 88%|████████▊ | 3366/3844 [6:57:12<56:57,  7.15s/it]

 88%|████████▊ | 3367/3844 [6:57:20<1:00:00,  7.55s/it]

 88%|████████▊ | 3368/3844 [6:57:27<57:08,  7.20s/it]

 88%|████████▊ | 3369/3844 [6:57:35<59:19,  7.49s/it]

 88%|████████▊ | 3370/3844 [6:57:42<58:52,  7.45s/it]
{'loss': 1.2701, 'grad_norm': 0.42245385963642423, 'learning_rate': 7.872134716033975e-07, 'epoch': 0.88}

 88%|████████▊ | 3371/3844 [6:57:49<58:18,  7.40s/it]

 88%|████████▊ | 3372/3844 [6:57:55<54:48,  6.97s/it]


 88%|████████▊ | 3374/3844 [6:58:10<56:21,  7.19s/it]

 88%|████████▊ | 3375/3844 [6:58:18<58:03,  7.43s/it]

 88%|████████▊ | 3376/3844 [6:58:24<55:05,  7.06s/it]

 88%|████████▊ | 3377/3844 [6:58:31<54:45,  7.04s/it]
{'loss': 1.2319, 'grad_norm': 0.3970606482981023, 'learning_rate': 7.644328354793639e-07, 'epoch': 0.88}


 88%|████████▊ | 3379/3844 [6:58:48<1:00:26,  7.80s/it]

 88%|████████▊ | 3380/3844 [6:58:56<1:00:28,  7.82s/it]

 88%|████████▊ | 3381/3844 [6:59:02<57:08,  7.41s/it]

 88%|████████▊ | 3382/3844 [6:59:08<53:05,  6.90s/it]

 88%|████████▊ | 3383/3844 [6:59:16<57:11,  7.44s/it]
{'loss': 1.1352, 'grad_norm': 0.3787334746861677, 'learning_rate': 7.451623310759182e-07, 'epoch': 0.88}

 88%|████████▊ | 3384/3844 [6:59:23<56:02,  7.31s/it]


 88%|████████▊ | 3386/3844 [6:59:38<57:04,  7.48s/it]
{'loss': 1.2052, 'grad_norm': 0.4105524878864907, 'learning_rate': 7.356157740505599e-07, 'epoch': 0.88}

 88%|████████▊ | 3387/3844 [6:59:45<56:12,  7.38s/it]


 88%|████████▊ | 3389/3844 [6:59:58<52:05,  6.87s/it]

 88%|████████▊ | 3390/3844 [7:00:04<49:39,  6.56s/it]

 88%|████████▊ | 3391/3844 [7:00:10<49:22,  6.54s/it]

 88%|████████▊ | 3392/3844 [7:00:16<48:03,  6.38s/it]
{'loss': 1.1883, 'grad_norm': 0.41914284043537503, 'learning_rate': 7.167003554274454e-07, 'epoch': 0.88}


 88%|████████▊ | 3394/3844 [7:00:31<50:54,  6.79s/it]

 88%|████████▊ | 3395/3844 [7:00:37<49:21,  6.60s/it]
{'loss': 1.0986, 'grad_norm': 0.42089519862894514, 'learning_rate': 7.073316147240872e-07, 'epoch': 0.88}
[2024-05-26 18:07:22,155] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 3397/3844 [7:00:56<58:14,  7.82s/it]
{'loss': 1.2242, 'grad_norm': 0.3899413777407619, 'learning_rate': 7.011187796690155e-07, 'epoch': 0.88}


 88%|████████▊ | 3399/3844 [7:01:11<57:56,  7.81s/it]

 88%|████████▊ | 3400/3844 [7:01:24<1:10:24,  9.51s/it]

 88%|████████▊ | 3401/3844 [7:01:32<1:05:38,  8.89s/it]

 89%|████████▊ | 3402/3844 [7:01:39<1:01:28,  8.34s/it]

 89%|████████▊ | 3403/3844 [7:01:44<54:49,  7.46s/it]
{'loss': 1.1874, 'grad_norm': 0.4738070099933058, 'learning_rate': 6.826388303095721e-07, 'epoch': 0.89}

 89%|████████▊ | 3404/3844 [7:01:50<50:11,  6.84s/it]


 89%|████████▊ | 3406/3844 [7:02:04<50:19,  6.89s/it]

 89%|████████▊ | 3407/3844 [7:02:14<58:17,  8.00s/it]
[2024-05-26 18:08:47,273] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.2143, 'grad_norm': 0.36913039907801753, 'learning_rate': 6.704511686998194e-07, 'epoch': 0.89}

 89%|████████▊ | 3408/3844 [7:02:21<55:22,  7.62s/it]


 89%|████████▊ | 3410/3844 [7:02:39<1:00:50,  8.41s/it]

 89%|████████▊ | 3411/3844 [7:02:45<56:03,  7.77s/it]

 89%|████████▉ | 3412/3844 [7:02:53<55:52,  7.76s/it]
{'loss': 1.0121, 'grad_norm': 0.380210374250591, 'learning_rate': 6.553656779506468e-07, 'epoch': 0.89}


 89%|████████▉ | 3414/3844 [7:03:17<1:10:24,  9.82s/it]
{'loss': 1.1015, 'grad_norm': 0.47234624631698824, 'learning_rate': 6.493779190972371e-07, 'epoch': 0.89}

 89%|████████▉ | 3415/3844 [7:03:25<1:08:00,  9.51s/it]


 89%|████████▉ | 3417/3844 [7:03:38<56:34,  7.95s/it]

 89%|████████▉ | 3418/3844 [7:03:44<52:29,  7.39s/it]
{'loss': 1.194, 'grad_norm': 0.3821475615537468, 'learning_rate': 6.374821020409994e-07, 'epoch': 0.89}

 89%|████████▉ | 3419/3844 [7:03:53<55:43,  7.87s/it]

 89%|████████▉ | 3420/3844 [7:03:59<52:09,  7.38s/it]

 89%|████████▉ | 3421/3844 [7:04:06<50:00,  7.09s/it]

 89%|████████▉ | 3422/3844 [7:04:18<1:00:22,  8.59s/it]


 89%|████████▉ | 3424/3844 [7:04:33<56:58,  8.14s/it]

 89%|████████▉ | 3425/3844 [7:04:39<52:19,  7.49s/it]
{'loss': 1.1692, 'grad_norm': 0.4030020731564982, 'learning_rate': 6.16920479804377e-07, 'epoch': 0.89}

 89%|████████▉ | 3426/3844 [7:04:45<50:39,  7.27s/it]

 89%|████████▉ | 3427/3844 [7:04:55<55:59,  8.06s/it]


 89%|████████▉ | 3429/3844 [7:05:07<47:46,  6.91s/it]
{'loss': 1.169, 'grad_norm': 0.3805451543254514, 'learning_rate': 6.053175294748193e-07, 'epoch': 0.89}


 89%|████████▉ | 3431/3844 [7:05:19<44:28,  6.46s/it]

 89%|████████▉ | 3432/3844 [7:05:25<42:23,  6.17s/it]
{'loss': 1.2172, 'grad_norm': 0.3911987825045861, 'learning_rate': 5.966853613042001e-07, 'epoch': 0.89}

 89%|████████▉ | 3433/3844 [7:05:31<43:12,  6.31s/it]


 89%|████████▉ | 3435/3844 [7:05:46<46:17,  6.79s/it]
{'loss': 1.0466, 'grad_norm': 0.40683327484205606, 'learning_rate': 5.881132926785948e-07, 'epoch': 0.89}


 89%|████████▉ | 3437/3844 [7:06:01<48:09,  7.10s/it]
{'loss': 1.1989, 'grad_norm': 0.41734227682125524, 'learning_rate': 5.824319959686676e-07, 'epoch': 0.89}


 89%|████████▉ | 3439/3844 [7:06:15<46:49,  6.94s/it]
{'loss': 1.1881, 'grad_norm': 0.3818484294634992, 'learning_rate': 5.767774506634361e-07, 'epoch': 0.89}


 90%|████████▉ | 3441/3844 [7:06:28<45:11,  6.73s/it]

 90%|████████▉ | 3442/3844 [7:06:34<43:57,  6.56s/it]

 90%|████████▉ | 3443/3844 [7:06:40<42:51,  6.41s/it]

 90%|████████▉ | 3444/3844 [7:06:51<51:19,  7.70s/it]

 90%|████████▉ | 3445/3844 [7:06:59<51:30,  7.75s/it]
{'loss': 1.0144, 'grad_norm': 0.3868380026751606, 'learning_rate': 5.599744834178944e-07, 'epoch': 0.9}

 90%|████████▉ | 3446/3844 [7:07:08<53:51,  8.12s/it]


 90%|████████▉ | 3448/3844 [7:07:19<44:48,  6.79s/it]

 90%|████████▉ | 3449/3844 [7:07:25<43:40,  6.63s/it]

 90%|████████▉ | 3450/3844 [7:07:31<41:45,  6.36s/it]
{'loss': 1.2443, 'grad_norm': 0.4308265851260108, 'learning_rate': 5.461563467998676e-07, 'epoch': 0.9}


 90%|████████▉ | 3452/3844 [7:07:47<46:05,  7.06s/it]

 90%|████████▉ | 3453/3844 [7:07:53<44:45,  6.87s/it]

 90%|████████▉ | 3454/3844 [7:07:59<42:32,  6.54s/it]

 90%|████████▉ | 3455/3844 [7:08:05<41:35,  6.42s/it]

 90%|████████▉ | 3456/3844 [7:08:11<39:59,  6.18s/it]

 90%|████████▉ | 3457/3844 [7:08:18<42:57,  6.66s/it]
{'loss': 1.1701, 'grad_norm': 0.37221659106115107, 'learning_rate': 5.270929812812254e-07, 'epoch': 0.9}


 90%|████████▉ | 3459/3844 [7:08:33<44:55,  7.00s/it]

 90%|█████████ | 3460/3844 [7:08:39<42:42,  6.67s/it]

 90%|█████████ | 3461/3844 [7:08:49<49:16,  7.72s/it]

 90%|█████████ | 3462/3844 [7:08:57<48:45,  7.66s/it]

 90%|█████████ | 3463/3844 [7:09:03<46:02,  7.25s/it]

 90%|█████████ | 3464/3844 [7:09:09<44:36,  7.04s/it]
{'loss': 1.2101, 'grad_norm': 0.38318523482136996, 'learning_rate': 5.083592452333752e-07, 'epoch': 0.9}


 90%|█████████ | 3466/3844 [7:09:26<48:30,  7.70s/it]
{'loss': 1.1474, 'grad_norm': 0.3941504883985285, 'learning_rate': 5.030673849203082e-07, 'epoch': 0.9}

 90%|█████████ | 3467/3844 [7:09:32<44:59,  7.16s/it]


 90%|█████████ | 3469/3844 [7:09:50<48:57,  7.83s/it]

 90%|█████████ | 3470/3844 [7:09:58<49:08,  7.88s/it]

 90%|█████████ | 3471/3844 [7:10:06<50:53,  8.19s/it]
{'loss': 1.0858, 'grad_norm': 0.37419180539427277, 'learning_rate': 4.899557905355623e-07, 'epoch': 0.9}


 90%|█████████ | 3473/3844 [7:10:22<48:46,  7.89s/it]
{'loss': 1.0685, 'grad_norm': 0.4128876430830052, 'learning_rate': 4.847584144476624e-07, 'epoch': 0.9}


 90%|█████████ | 3475/3844 [7:10:35<44:01,  7.16s/it]

 90%|█████████ | 3476/3844 [7:10:41<42:58,  7.01s/it]

 90%|█████████ | 3477/3844 [7:10:48<41:42,  6.82s/it]

 90%|█████████ | 3478/3844 [7:10:53<39:12,  6.43s/it]

 91%|█████████ | 3479/3844 [7:11:00<40:46,  6.70s/it]

 91%|█████████ | 3480/3844 [7:11:09<43:46,  7.21s/it]
{'loss': 1.2131, 'grad_norm': 0.4023132016583415, 'learning_rate': 4.6678054656497216e-07, 'epoch': 0.91}

 91%|█████████ | 3481/3844 [7:11:16<43:07,  7.13s/it]

 91%|█████████ | 3482/3844 [7:11:24<45:43,  7.58s/it]


 91%|█████████ | 3484/3844 [7:11:36<40:51,  6.81s/it]

 91%|█████████ | 3485/3844 [7:11:45<44:38,  7.46s/it]
{'loss': 0.9961, 'grad_norm': 0.4052987325083846, 'learning_rate': 4.5414227522062594e-07, 'epoch': 0.91}


 91%|█████████ | 3487/3844 [7:11:59<41:48,  7.03s/it]

 91%|█████████ | 3488/3844 [7:12:05<40:28,  6.82s/it]

 91%|█████████ | 3489/3844 [7:12:11<38:25,  6.49s/it]
{'loss': 1.0363, 'grad_norm': 0.4363296685343292, 'learning_rate': 4.441536685261938e-07, 'epoch': 0.91}


 91%|█████████ | 3491/3844 [7:12:27<44:15,  7.52s/it]
{'loss': 0.8973, 'grad_norm': 0.38724209883764105, 'learning_rate': 4.392000743989822e-07, 'epoch': 0.91}

 91%|█████████ | 3492/3844 [7:12:34<43:30,  7.42s/it]


 91%|█████████ | 3494/3844 [7:12:51<45:53,  7.87s/it]
{'loss': 1.0791, 'grad_norm': 0.395845904456351, 'learning_rate': 4.318206093352584e-07, 'epoch': 0.91}


 91%|█████████ | 3496/3844 [7:13:03<40:46,  7.03s/it]

 91%|█████████ | 3497/3844 [7:13:12<43:23,  7.50s/it]

 91%|█████████ | 3498/3844 [7:13:20<44:20,  7.69s/it]

 91%|█████████ | 3499/3844 [7:13:27<42:50,  7.45s/it]

 91%|█████████ | 3500/3844 [7:13:35<45:13,  7.89s/it]

 91%|█████████ | 3501/3844 [7:13:43<44:53,  7.85s/it]

 91%|█████████ | 3502/3844 [7:13:51<44:58,  7.89s/it]

 91%|█████████ | 3503/3844 [7:13:57<41:31,  7.31s/it]

 91%|█████████ | 3504/3844 [7:14:03<38:17,  6.76s/it]
{'loss': 1.2027, 'grad_norm': 0.3842340760557441, 'learning_rate': 4.0766431680802965e-07, 'epoch': 0.91}


 91%|█████████ | 3506/3844 [7:14:17<39:28,  7.01s/it]

 91%|█████████ | 3507/3844 [7:14:29<46:37,  8.30s/it]

 91%|█████████▏| 3508/3844 [7:14:35<42:50,  7.65s/it]
{'loss': 1.2658, 'grad_norm': 0.39880980741996946, 'learning_rate': 3.9819243920328764e-07, 'epoch': 0.91}


 91%|█████████▏| 3510/3844 [7:14:47<39:11,  7.04s/it]

 91%|█████████▏| 3511/3844 [7:14:59<47:15,  8.51s/it]

 91%|█████████▏| 3512/3844 [7:15:08<46:40,  8.43s/it]
{'loss': 1.1251, 'grad_norm': 0.3758548763202632, 'learning_rate': 3.888296605325237e-07, 'epoch': 0.91}

 91%|█████████▏| 3513/3844 [7:15:15<44:09,  8.00s/it]


 91%|█████████▏| 3515/3844 [7:15:29<42:15,  7.71s/it]
{'loss': 0.9896, 'grad_norm': 0.4063804819232731, 'learning_rate': 3.8187923674754544e-07, 'epoch': 0.91}


 91%|█████████▏| 3517/3844 [7:15:46<42:57,  7.88s/it]

 92%|█████████▏| 3518/3844 [7:15:53<42:19,  7.79s/it]

 92%|█████████▏| 3519/3844 [7:15:59<38:41,  7.14s/it]

 92%|█████████▏| 3520/3844 [7:16:06<37:42,  6.98s/it]

 92%|█████████▏| 3521/3844 [7:16:13<38:33,  7.16s/it]

 92%|█████████▏| 3522/3844 [7:16:19<36:32,  6.81s/it]

 92%|█████████▏| 3523/3844 [7:16:26<36:12,  6.77s/it]
{'loss': 1.2043, 'grad_norm': 0.4111755486063945, 'learning_rate': 3.6364542465119446e-07, 'epoch': 0.92}

 92%|█████████▏| 3524/3844 [7:16:34<38:29,  7.22s/it]


 92%|█████████▏| 3526/3844 [7:16:48<36:36,  6.91s/it]
{'loss': 1.1626, 'grad_norm': 0.40227649748454, 'learning_rate': 3.5692061399487667e-07, 'epoch': 0.92}

 92%|█████████▏| 3527/3844 [7:16:55<36:07,  6.84s/it]


 92%|█████████▏| 3529/3844 [7:17:07<34:30,  6.57s/it]

 92%|█████████▏| 3530/3844 [7:17:15<36:50,  7.04s/it]

 92%|█████████▏| 3531/3844 [7:17:22<36:06,  6.92s/it]
{'loss': 1.0967, 'grad_norm': 0.4087398423852825, 'learning_rate': 3.4584957718540645e-07, 'epoch': 0.92}


 92%|█████████▏| 3533/3844 [7:17:34<33:21,  6.44s/it]
{'loss': 1.1924, 'grad_norm': 0.41518475513374664, 'learning_rate': 3.4146914251246167e-07, 'epoch': 0.92}


 92%|█████████▏| 3535/3844 [7:17:46<31:37,  6.14s/it]
{'loss': 1.2691, 'grad_norm': 0.4008392298168253, 'learning_rate': 3.3711614371974123e-07, 'epoch': 0.92}

 92%|█████████▏| 3536/3844 [7:17:52<31:49,  6.20s/it]

 92%|█████████▏| 3537/3844 [7:17:59<32:25,  6.34s/it]


 92%|█████████▏| 3539/3844 [7:18:13<33:59,  6.69s/it]
{'loss': 0.9118, 'grad_norm': 0.4178019267405566, 'learning_rate': 3.284925031572594e-07, 'epoch': 0.92}


 92%|█████████▏| 3541/3844 [7:18:25<32:29,  6.43s/it]

 92%|█████████▏| 3542/3844 [7:18:35<37:05,  7.37s/it]
{'loss': 1.0417, 'grad_norm': 0.38159056480211806, 'learning_rate': 3.2209688331895304e-07, 'epoch': 0.92}


 92%|█████████▏| 3544/3844 [7:18:47<34:21,  6.87s/it]
{'loss': 1.1423, 'grad_norm': 0.358648966769184, 'learning_rate': 3.1786749787891537e-07, 'epoch': 0.92}


 92%|█████████▏| 3546/3844 [7:19:03<37:50,  7.62s/it]
{'loss': 0.924, 'grad_norm': 0.35501728239716906, 'learning_rate': 3.136656153615847e-07, 'epoch': 0.92}


 92%|█████████▏| 3548/3844 [7:19:16<34:13,  6.94s/it]

 92%|█████████▏| 3549/3844 [7:19:22<32:42,  6.65s/it]
{'loss': 1.1068, 'grad_norm': 0.4283743419923367, 'learning_rate': 3.0741438565386914e-07, 'epoch': 0.92}

 92%|█████████▏| 3550/3844 [7:19:29<32:36,  6.65s/it]


 92%|█████████▏| 3552/3844 [7:19:40<29:37,  6.09s/it]

 92%|█████████▏| 3553/3844 [7:19:46<29:22,  6.06s/it]

 92%|█████████▏| 3554/3844 [7:19:52<29:43,  6.15s/it]

 92%|█████████▏| 3555/3844 [7:19:58<29:20,  6.09s/it]
{'loss': 1.0474, 'grad_norm': 0.4058174186560114, 'learning_rate': 2.9509781089682164e-07, 'epoch': 0.92}

 93%|█████████▎| 3556/3844 [7:20:05<29:52,  6.22s/it]

 93%|█████████▎| 3557/3844 [7:20:11<29:46,  6.23s/it]


 93%|█████████▎| 3559/3844 [7:20:26<33:30,  7.05s/it]

 93%|█████████▎| 3560/3844 [7:20:35<36:33,  7.73s/it]

 93%|█████████▎| 3561/3844 [7:20:41<34:03,  7.22s/it]
{'loss': 1.1444, 'grad_norm': 0.3904144930052026, 'learning_rate': 2.8302934408742324e-07, 'epoch': 0.93}

 93%|█████████▎| 3562/3844 [7:20:49<34:27,  7.33s/it]


 93%|█████████▎| 3564/3844 [7:21:08<39:31,  8.47s/it]
{'loss': 1.0997, 'grad_norm': 0.36995782202284416, 'learning_rate': 2.7708824782764155e-07, 'epoch': 0.93}


 93%|█████████▎| 3566/3844 [7:21:22<36:06,  7.79s/it]
{'loss': 1.1671, 'grad_norm': 0.34911575766010344, 'learning_rate': 2.7316203809955144e-07, 'epoch': 0.93}

 93%|█████████▎| 3567/3844 [7:21:29<35:03,  7.59s/it]


 93%|█████████▎| 3569/3844 [7:21:44<34:59,  7.63s/it]

 93%|█████████▎| 3570/3844 [7:21:56<40:41,  8.91s/it]
{'loss': 1.0452, 'grad_norm': 0.3799449959325233, 'learning_rate': 2.653925194547513e-07, 'epoch': 0.93}

 93%|█████████▎| 3571/3844 [7:22:09<45:42, 10.05s/it]


 93%|█████████▎| 3573/3844 [7:22:24<39:09,  8.67s/it]
{'loss': 1.1839, 'grad_norm': 0.3995045863352541, 'learning_rate': 2.5963796209272695e-07, 'epoch': 0.93}


 93%|█████████▎| 3575/3844 [7:22:37<34:08,  7.61s/it]

 93%|█████████▎| 3576/3844 [7:22:45<34:25,  7.71s/it]

 93%|█████████▎| 3577/3844 [7:22:53<34:33,  7.77s/it]

 93%|█████████▎| 3578/3844 [7:23:00<33:00,  7.44s/it]
{'loss': 1.2182, 'grad_norm': 0.4149732326449675, 'learning_rate': 2.501853927408793e-07, 'epoch': 0.93}

 93%|█████████▎| 3579/3844 [7:23:07<31:43,  7.18s/it]


 93%|█████████▎| 3581/3844 [7:23:20<30:39,  7.00s/it]

 93%|█████████▎| 3582/3844 [7:23:28<32:02,  7.34s/it]

 93%|█████████▎| 3583/3844 [7:23:34<29:58,  6.89s/it]

 93%|█████████▎| 3584/3844 [7:23:39<28:03,  6.48s/it]
{'loss': 1.1777, 'grad_norm': 0.3839015752101138, 'learning_rate': 2.390708090171823e-07, 'epoch': 0.93}

 93%|█████████▎| 3585/3844 [7:23:47<29:09,  6.75s/it]

 93%|█████████▎| 3586/3844 [7:23:53<28:18,  6.58s/it]

 93%|█████████▎| 3587/3844 [7:23:59<27:30,  6.42s/it]

 93%|█████████▎| 3588/3844 [7:24:07<29:02,  6.80s/it]

 93%|█████████▎| 3589/3844 [7:24:13<27:59,  6.59s/it]


 93%|█████████▎| 3591/3844 [7:24:27<28:33,  6.77s/it]
{'loss': 1.2059, 'grad_norm': 0.39578502442310975, 'learning_rate': 2.2641920547009978e-07, 'epoch': 0.93}


 93%|█████████▎| 3593/3844 [7:24:41<27:42,  6.62s/it]
{'loss': 1.2184, 'grad_norm': 0.40485906781084313, 'learning_rate': 2.2286690852742933e-07, 'epoch': 0.93}


 94%|█████████▎| 3595/3844 [7:24:54<27:24,  6.60s/it]

 94%|█████████▎| 3596/3844 [7:25:01<27:31,  6.66s/it]
{'loss': 1.1668, 'grad_norm': 0.39406265093525467, 'learning_rate': 2.1759054020762837e-07, 'epoch': 0.94}


 94%|█████████▎| 3598/3844 [7:25:12<25:47,  6.29s/it]

 94%|█████████▎| 3599/3844 [7:25:19<25:27,  6.23s/it]

 94%|█████████▎| 3600/3844 [7:25:24<24:30,  6.03s/it]
 94%|█████████▎| 3600/3844 [7:25:24<24:30,  6.03s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 94%|█████████▎| 3601/3844 [7:25:58<58:52, 14.54s/it]
{'loss': 1.1759, 'grad_norm': 0.41486431272871266, 'learning_rate': 2.0893554827903117e-07, 'epoch': 0.94}


 94%|█████████▎| 3603/3844 [7:26:12<42:17, 10.53s/it]

 94%|█████████▍| 3604/3844 [7:26:18<36:30,  9.13s/it]
{'loss': 1.1665, 'grad_norm': 0.42102871546774123, 'learning_rate': 2.0382598037203217e-07, 'epoch': 0.94}


 94%|█████████▍| 3606/3844 [7:26:32<31:23,  7.91s/it]
{'loss': 1.1282, 'grad_norm': 0.39466818041385276, 'learning_rate': 2.004543833418826e-07, 'epoch': 0.94}


 94%|█████████▍| 3608/3844 [7:26:48<31:26,  8.00s/it]

 94%|█████████▍| 3609/3844 [7:26:54<29:11,  7.45s/it]
{'loss': 1.1597, 'grad_norm': 0.37659489160490917, 'learning_rate': 1.9544918410169922e-07, 'epoch': 0.94}


 94%|█████████▍| 3611/3844 [7:27:10<29:50,  7.69s/it]

 94%|█████████▍| 3612/3844 [7:27:16<27:30,  7.11s/it]

 94%|█████████▍| 3613/3844 [7:27:22<26:01,  6.76s/it]

 94%|█████████▍| 3614/3844 [7:27:28<24:38,  6.43s/it]
{'loss': 1.2089, 'grad_norm': 0.3964335360041921, 'learning_rate': 1.8724645425481137e-07, 'epoch': 0.94}

 94%|█████████▍| 3615/3844 [7:27:37<27:39,  7.25s/it]


 94%|█████████▍| 3617/3844 [7:27:53<28:20,  7.49s/it]

 94%|█████████▍| 3618/3844 [7:28:00<27:43,  7.36s/it]

 94%|█████████▍| 3619/3844 [7:28:06<26:31,  7.07s/it]

 94%|█████████▍| 3620/3844 [7:28:12<25:33,  6.84s/it]

 94%|█████████▍| 3621/3844 [7:28:21<27:00,  7.27s/it]

 94%|█████████▍| 3622/3844 [7:28:26<24:40,  6.67s/it]
{'loss': 1.1739, 'grad_norm': 0.4008093794373745, 'learning_rate': 1.7448450699083296e-07, 'epoch': 0.94}


 94%|█████████▍| 3624/3844 [7:28:37<22:02,  6.01s/it]

 94%|█████████▍| 3625/3844 [7:28:42<21:17,  5.83s/it]

 94%|█████████▍| 3626/3844 [7:28:52<25:22,  6.98s/it]

 94%|█████████▍| 3627/3844 [7:28:58<24:23,  6.74s/it]
{'loss': 1.203, 'grad_norm': 0.40392749964281166, 'learning_rate': 1.6673502366442272e-07, 'epoch': 0.94}

 94%|█████████▍| 3628/3844 [7:29:05<25:09,  6.99s/it]


 94%|█████████▍| 3630/3844 [7:29:17<22:33,  6.33s/it]

 94%|█████████▍| 3631/3844 [7:29:23<22:05,  6.22s/it]

 94%|█████████▍| 3632/3844 [7:29:30<22:52,  6.48s/it]

 95%|█████████▍| 3633/3844 [7:29:36<22:55,  6.52s/it]

 95%|█████████▍| 3634/3844 [7:29:42<22:02,  6.30s/it]

 95%|█████████▍| 3635/3844 [7:29:53<26:16,  7.54s/it]

 95%|█████████▍| 3636/3844 [7:30:00<26:11,  7.56s/it]

 95%|█████████▍| 3637/3844 [7:30:07<24:42,  7.16s/it]

 95%|█████████▍| 3638/3844 [7:30:12<23:13,  6.76s/it]
{'loss': 1.2433, 'grad_norm': 0.39018509194007145, 'learning_rate': 1.503008577175824e-07, 'epoch': 0.95}


 95%|█████████▍| 3640/3844 [7:30:25<21:42,  6.38s/it]
{'loss': 1.2267, 'grad_norm': 0.40303765485306164, 'learning_rate': 1.4740371815755937e-07, 'epoch': 0.95}

 95%|█████████▍| 3641/3844 [7:30:33<23:41,  7.00s/it]


 95%|█████████▍| 3643/3844 [7:30:46<22:22,  6.68s/it]

 95%|█████████▍| 3644/3844 [7:30:52<21:53,  6.57s/it]

 95%|█████████▍| 3645/3844 [7:31:01<23:54,  7.21s/it]

 95%|█████████▍| 3646/3844 [7:31:06<22:10,  6.72s/it]

 95%|█████████▍| 3647/3844 [7:31:13<21:37,  6.59s/it]

 95%|█████████▍| 3648/3844 [7:31:18<20:35,  6.31s/it]

 95%|█████████▍| 3649/3844 [7:31:24<20:13,  6.22s/it]

 95%|█████████▍| 3650/3844 [7:31:30<19:49,  6.13s/it]

 95%|█████████▍| 3651/3844 [7:31:37<20:12,  6.28s/it]

 95%|█████████▌| 3652/3844 [7:31:43<19:27,  6.08s/it]

 95%|█████████▌| 3653/3844 [7:31:48<18:48,  5.91s/it]
{'loss': 1.132, 'grad_norm': 0.42568981088037045, 'learning_rate': 1.292548577529118e-07, 'epoch': 0.95}


 95%|█████████▌| 3655/3844 [7:32:00<18:58,  6.03s/it]
{'loss': 1.0279, 'grad_norm': 0.3966289921255544, 'learning_rate': 1.2656781684309837e-07, 'epoch': 0.95}


 95%|█████████▌| 3657/3844 [7:32:17<22:42,  7.29s/it]

 95%|█████████▌| 3658/3844 [7:32:24<22:42,  7.32s/it]

 95%|█████████▌| 3659/3844 [7:32:32<23:08,  7.50s/it]

 95%|█████████▌| 3660/3844 [7:32:38<21:58,  7.17s/it]

 95%|█████████▌| 3661/3844 [7:32:45<21:15,  6.97s/it]
{'loss': 1.1883, 'grad_norm': 0.38755482566296484, 'learning_rate': 1.1867500220092199e-07, 'epoch': 0.95}


 95%|█████████▌| 3663/3844 [7:32:58<20:25,  6.77s/it]
{'loss': 1.0748, 'grad_norm': 0.41168574775806055, 'learning_rate': 1.1610019159396635e-07, 'epoch': 0.95}

 95%|█████████▌| 3664/3844 [7:33:05<20:46,  6.93s/it]


 95%|█████████▌| 3666/3844 [7:33:17<18:36,  6.27s/it]

 95%|█████████▌| 3667/3844 [7:33:27<21:33,  7.31s/it]
{'loss': 1.3575, 'grad_norm': 0.3891923524060545, 'learning_rate': 1.1103480579409354e-07, 'epoch': 0.95}


 95%|█████████▌| 3669/3844 [7:33:41<21:25,  7.35s/it]
{'loss': 1.1628, 'grad_norm': 0.37928598065917885, 'learning_rate': 1.0854424498983551e-07, 'epoch': 0.95}

 95%|█████████▌| 3670/3844 [7:33:47<20:22,  7.02s/it]


 96%|█████████▌| 3672/3844 [7:34:05<22:00,  7.68s/it]

 96%|█████████▌| 3673/3844 [7:34:11<20:47,  7.30s/it]
{'loss': 1.2097, 'grad_norm': 0.3874098597295076, 'learning_rate': 1.0364742294590723e-07, 'epoch': 0.96}


 96%|█████████▌| 3675/3844 [7:34:28<22:14,  7.89s/it]

 96%|█████████▌| 3676/3844 [7:34:34<20:43,  7.40s/it]
{'loss': 1.1771, 'grad_norm': 0.4299355802510203, 'learning_rate': 1.0004859587767645e-07, 'epoch': 0.96}


 96%|█████████▌| 3678/3844 [7:34:46<18:31,  6.70s/it]

 96%|█████████▌| 3679/3844 [7:34:54<19:38,  7.14s/it]
{'loss': 1.17, 'grad_norm': 0.39041197829599167, 'learning_rate': 9.651304251642179e-08, 'epoch': 0.96}

 96%|█████████▌| 3680/3844 [7:35:02<19:37,  7.18s/it]

 96%|█████████▌| 3681/3844 [7:35:09<20:04,  7.39s/it]


 96%|█████████▌| 3683/3844 [7:35:22<18:23,  6.86s/it]
{'loss': 1.2093, 'grad_norm': 0.39608071605681533, 'learning_rate': 9.189743613248203e-08, 'epoch': 0.96}


 96%|█████████▌| 3685/3844 [7:35:36<18:26,  6.96s/it]
{'loss': 1.2774, 'grad_norm': 0.38758755574913034, 'learning_rate': 8.963184689763316e-08, 'epoch': 0.96}

 96%|█████████▌| 3686/3844 [7:35:42<17:09,  6.51s/it]

 96%|█████████▌| 3687/3844 [7:35:50<18:09,  6.94s/it]

 96%|█████████▌| 3688/3844 [7:35:55<17:05,  6.58s/it]

 96%|█████████▌| 3689/3844 [7:36:02<16:36,  6.43s/it]

 96%|█████████▌| 3690/3844 [7:36:12<19:28,  7.58s/it]


 96%|█████████▌| 3692/3844 [7:36:29<20:23,  8.05s/it]
{'loss': 1.1738, 'grad_norm': 0.372433253032531, 'learning_rate': 8.192401714621101e-08, 'epoch': 0.96}


 96%|█████████▌| 3694/3844 [7:36:43<18:43,  7.49s/it]

 96%|█████████▌| 3695/3844 [7:36:49<17:34,  7.08s/it]

 96%|█████████▌| 3696/3844 [7:36:55<16:37,  6.74s/it]

 96%|█████████▌| 3697/3844 [7:37:01<16:28,  6.72s/it]

 96%|█████████▌| 3698/3844 [7:37:07<15:31,  6.38s/it]

 96%|█████████▌| 3699/3844 [7:37:14<16:12,  6.71s/it]

 96%|█████████▋| 3700/3844 [7:37:21<16:19,  6.80s/it]

 96%|█████████▋| 3701/3844 [7:37:31<18:33,  7.78s/it]

 96%|█████████▋| 3702/3844 [7:37:38<17:44,  7.49s/it]
{'loss': 1.1094, 'grad_norm': 0.40628183442921245, 'learning_rate': 7.151157333896153e-08, 'epoch': 0.96}


 96%|█████████▋| 3704/3844 [7:37:52<17:11,  7.37s/it]

 96%|█████████▋| 3705/3844 [7:37:58<16:09,  6.97s/it]

 96%|█████████▋| 3706/3844 [7:38:04<15:15,  6.64s/it]

 96%|█████████▋| 3707/3844 [7:38:10<14:48,  6.49s/it]
{'loss': 1.1416, 'grad_norm': 0.43076189867431486, 'learning_rate': 6.656970615254943e-08, 'epoch': 0.96}

 96%|█████████▋| 3708/3844 [7:38:18<15:20,  6.77s/it]


 97%|█████████▋| 3710/3844 [7:38:33<15:50,  7.09s/it]

 97%|█████████▋| 3711/3844 [7:38:41<16:31,  7.46s/it]
{'loss': 1.1612, 'grad_norm': 0.4267283334052455, 'learning_rate': 6.274318353925223e-08, 'epoch': 0.97}


 97%|█████████▋| 3713/3844 [7:38:56<16:44,  7.67s/it]

 97%|█████████▋| 3714/3844 [7:39:09<19:42,  9.10s/it]
{'loss': 1.0586, 'grad_norm': 0.38792436462141855, 'learning_rate': 5.994738671298184e-08, 'epoch': 0.97}


 97%|█████████▋| 3716/3844 [7:39:22<16:45,  7.85s/it]
{'loss': 1.0637, 'grad_norm': 0.3836718770054612, 'learning_rate': 5.811881552384768e-08, 'epoch': 0.97}

 97%|█████████▋| 3717/3844 [7:39:32<17:37,  8.33s/it]


 97%|█████████▋| 3719/3844 [7:39:48<17:03,  8.19s/it]
{'loss': 1.1096, 'grad_norm': 0.3928166278192521, 'learning_rate': 5.542891176824161e-08, 'epoch': 0.97}


 97%|█████████▋| 3721/3844 [7:40:04<16:15,  7.93s/it]
{'loss': 1.2219, 'grad_norm': 0.4255760868497317, 'learning_rate': 5.367095204522277e-08, 'epoch': 0.97}

 97%|█████████▋| 3722/3844 [7:40:12<16:00,  7.87s/it]


 97%|█████████▋| 3724/3844 [7:40:29<16:34,  8.29s/it]
{'loss': 1.0502, 'grad_norm': 0.3999544805721899, 'learning_rate': 5.108698911585386e-08, 'epoch': 0.97}

 97%|█████████▋| 3725/3844 [7:40:38<16:50,  8.49s/it]

 97%|█████████▋| 3726/3844 [7:40:46<16:46,  8.53s/it]

 97%|█████████▋| 3727/3844 [7:40:52<14:52,  7.63s/it]

 97%|█████████▋| 3728/3844 [7:40:58<14:06,  7.29s/it]


 97%|█████████▋| 3730/3844 [7:41:15<14:58,  7.88s/it]
{'loss': 1.2165, 'grad_norm': 0.3567803024205719, 'learning_rate': 4.610983927370471e-08, 'epoch': 0.97}


 97%|█████████▋| 3732/3844 [7:41:29<13:37,  7.30s/it]
{'loss': 1.303, 'grad_norm': 0.38681492620257546, 'learning_rate': 4.4507332870059594e-08, 'epoch': 0.97}

 97%|█████████▋| 3733/3844 [7:41:34<12:30,  6.76s/it]


 97%|█████████▋| 3735/3844 [7:41:51<13:39,  7.52s/it]
{'loss': 1.0914, 'grad_norm': 0.42784339902884905, 'learning_rate': 4.2156598588066754e-08, 'epoch': 0.97}

 97%|█████████▋| 3736/3844 [7:42:00<14:46,  8.20s/it]

 97%|█████████▋| 3737/3844 [7:42:06<13:11,  7.40s/it]


 97%|█████████▋| 3739/3844 [7:42:19<12:02,  6.88s/it]

 97%|█████████▋| 3740/3844 [7:42:25<11:43,  6.76s/it]
{'loss': 1.1993, 'grad_norm': 0.41314157227470155, 'learning_rate': 3.838014582659311e-08, 'epoch': 0.97}


 97%|█████████▋| 3742/3844 [7:42:42<13:02,  7.67s/it]

 97%|█████████▋| 3743/3844 [7:42:49<12:36,  7.49s/it]
{'loss': 1.1579, 'grad_norm': 0.43068962376509023, 'learning_rate': 3.619916039494964e-08, 'epoch': 0.97}

 97%|█████████▋| 3744/3844 [7:42:54<11:28,  6.89s/it]


 97%|█████████▋| 3746/3844 [7:43:06<10:15,  6.28s/it]

 97%|█████████▋| 3747/3844 [7:43:15<11:33,  7.15s/it]
{'loss': 1.1739, 'grad_norm': 0.3692841081971173, 'learning_rate': 3.33902427304722e-08, 'epoch': 0.97}

 98%|█████████▊| 3748/3844 [7:43:24<12:35,  7.87s/it]


 98%|█████████▊| 3750/3844 [7:43:39<12:08,  7.75s/it]

 98%|█████████▊| 3751/3844 [7:43:49<12:51,  8.30s/it]

 98%|█████████▊| 3752/3844 [7:43:55<12:00,  7.83s/it]

 98%|█████████▊| 3753/3844 [7:44:03<11:42,  7.72s/it]
{'loss': 1.2329, 'grad_norm': 0.413787879332075, 'learning_rate': 2.9389208239348766e-08, 'epoch': 0.98}

 98%|█████████▊| 3754/3844 [7:44:10<11:18,  7.54s/it]


 98%|█████████▊| 3756/3844 [7:44:25<11:09,  7.60s/it]
{'loss': 1.1747, 'grad_norm': 0.39365880217760135, 'learning_rate': 2.7484272672680946e-08, 'epoch': 0.98}


 98%|█████████▊| 3758/3844 [7:44:37<09:43,  6.78s/it]

 98%|█████████▊| 3759/3844 [7:44:47<10:58,  7.74s/it]

 98%|█████████▊| 3760/3844 [7:44:55<10:58,  7.84s/it]
{'loss': 1.1292, 'grad_norm': 0.37446253620786135, 'learning_rate': 2.5043507833649728e-08, 'epoch': 0.98}


 98%|█████████▊| 3762/3844 [7:45:08<09:49,  7.19s/it]
{'loss': 1.2543, 'grad_norm': 0.39783093574125616, 'learning_rate': 2.386562577227136e-08, 'epoch': 0.98}


 98%|█████████▊| 3764/3844 [7:45:25<10:28,  7.85s/it]

 98%|█████████▊| 3765/3844 [7:45:31<09:56,  7.55s/it]

 98%|█████████▊| 3766/3844 [7:45:39<09:55,  7.63s/it]

 98%|█████████▊| 3767/3844 [7:45:45<08:58,  7.00s/it]

 98%|█████████▊| 3768/3844 [7:45:52<08:54,  7.03s/it]
{'loss': 1.2472, 'grad_norm': 0.4048541286834581, 'learning_rate': 2.050202092966247e-08, 'epoch': 0.98}


 98%|█████████▊| 3770/3844 [7:46:07<08:58,  7.28s/it]

 98%|█████████▊| 3771/3844 [7:46:17<09:48,  8.05s/it]

 98%|█████████▊| 3772/3844 [7:46:24<09:11,  7.66s/it]
{'loss': 1.1648, 'grad_norm': 0.37534884655697787, 'learning_rate': 1.8401350466592526e-08, 'epoch': 0.98}

 98%|█████████▊| 3773/3844 [7:46:34<09:56,  8.41s/it]

 98%|█████████▊| 3774/3844 [7:46:41<09:07,  7.83s/it]


 98%|█████████▊| 3776/3844 [7:46:56<08:33,  7.56s/it]
{'loss': 1.1127, 'grad_norm': 0.42281382639680726, 'learning_rate': 1.6414094250286616e-08, 'epoch': 0.98}

 98%|█████████▊| 3777/3844 [7:47:04<08:46,  7.85s/it]

 98%|█████████▊| 3778/3844 [7:47:11<08:09,  7.41s/it]

 98%|█████████▊| 3779/3844 [7:47:17<07:30,  6.93s/it]


 98%|█████████▊| 3781/3844 [7:47:32<07:37,  7.26s/it]
{'loss': 1.2618, 'grad_norm': 0.3840576800934162, 'learning_rate': 1.4089547040714302e-08, 'epoch': 0.98}


 98%|█████████▊| 3783/3844 [7:47:46<07:14,  7.13s/it]

 98%|█████████▊| 3784/3844 [7:47:51<06:43,  6.73s/it]

 98%|█████████▊| 3785/3844 [7:47:57<06:20,  6.44s/it]
{'loss': 1.2136, 'grad_norm': 0.4009840189653524, 'learning_rate': 1.2357553473674177e-08, 'epoch': 0.98}


 99%|█████████▊| 3787/3844 [7:48:14<07:13,  7.61s/it]

 99%|█████████▊| 3788/3844 [7:48:19<06:26,  6.90s/it]

 99%|█████████▊| 3789/3844 [7:48:25<06:06,  6.67s/it]

 99%|█████████▊| 3790/3844 [7:48:32<06:07,  6.81s/it]

 99%|█████████▊| 3791/3844 [7:48:38<05:43,  6.49s/it]
{'loss': 1.2464, 'grad_norm': 0.3996144489665335, 'learning_rate': 9.972349392378678e-09, 'epoch': 0.99}


 99%|█████████▊| 3793/3844 [7:48:52<05:41,  6.70s/it]

 99%|█████████▊| 3794/3844 [7:48:59<05:44,  6.89s/it]
{'loss': 1.0951, 'grad_norm': 0.3748501877607893, 'learning_rate': 8.87551776154072e-09, 'epoch': 0.99}

 99%|█████████▊| 3795/3844 [7:49:05<05:19,  6.52s/it]

 99%|█████████▉| 3796/3844 [7:49:10<05:03,  6.33s/it]


 99%|█████████▉| 3798/3844 [7:49:25<05:15,  6.86s/it]

 99%|█████████▉| 3799/3844 [7:49:32<05:11,  6.92s/it]
{'loss': 1.2853, 'grad_norm': 0.401664447468556, 'learning_rate': 7.189371482398955e-09, 'epoch': 0.99}


 99%|█████████▉| 3801/3844 [7:49:49<05:27,  7.61s/it]
{'loss': 1.1344, 'grad_norm': 0.4224642427223298, 'learning_rate': 6.564585841674076e-09, 'epoch': 0.99}


 99%|█████████▉| 3803/3844 [7:50:06<05:30,  8.05s/it]
{'loss': 1.1484, 'grad_norm': 0.4101744354800891, 'learning_rate': 5.96818740611349e-09, 'epoch': 0.99}

 99%|█████████▉| 3804/3844 [7:50:13<05:06,  7.67s/it]


 99%|█████████▉| 3806/3844 [7:50:39<06:36, 10.44s/it]
{'loss': 1.0371, 'grad_norm': 0.3643902758523211, 'learning_rate': 5.126819446297227e-09, 'epoch': 0.99}


 99%|█████████▉| 3808/3844 [7:50:50<04:46,  7.95s/it]

 99%|█████████▉| 3809/3844 [7:51:02<05:19,  9.14s/it]
{'loss': 1.1243, 'grad_norm': 0.38590282117264446, 'learning_rate': 4.349331868421791e-09, 'epoch': 0.99}

 99%|█████████▉| 3810/3844 [7:51:11<05:13,  9.21s/it]

 99%|█████████▉| 3811/3844 [7:51:20<04:55,  8.96s/it]

 99%|█████████▉| 3812/3844 [7:51:28<04:37,  8.68s/it]
{'loss': 1.1358, 'grad_norm': 0.39288045306680286, 'learning_rate': 3.635729641654484e-09, 'epoch': 0.99}

 99%|█████████▉| 3813/3844 [7:51:35<04:14,  8.20s/it]

 99%|█████████▉| 3814/3844 [7:51:41<03:48,  7.60s/it]

 99%|█████████▉| 3815/3844 [7:51:47<03:28,  7.20s/it]

 99%|█████████▉| 3816/3844 [7:51:53<03:08,  6.74s/it]


 99%|█████████▉| 3818/3844 [7:52:06<02:52,  6.65s/it]
{'loss': 1.3386, 'grad_norm': 0.3909214507311771, 'learning_rate': 2.4001990765321057e-09, 'epoch': 0.99}

 99%|█████████▉| 3819/3844 [7:52:13<02:49,  6.77s/it]

 99%|█████████▉| 3820/3844 [7:52:19<02:33,  6.38s/it]

 99%|█████████▉| 3821/3844 [7:52:25<02:27,  6.42s/it]


 99%|█████████▉| 3823/3844 [7:52:42<02:40,  7.65s/it]
{'loss': 1.1163, 'grad_norm': 0.4168765058163999, 'learning_rate': 1.5658321203804172e-09, 'epoch': 0.99}

 99%|█████████▉| 3824/3844 [7:52:49<02:26,  7.34s/it]


100%|█████████▉| 3826/3844 [7:53:04<02:16,  7.60s/it]

100%|█████████▉| 3827/3844 [7:53:14<02:23,  8.47s/it]
{'loss': 1.0384, 'grad_norm': 0.36706423497029733, 'learning_rate': 1.0261441119685522e-09, 'epoch': 1.0}

100%|█████████▉| 3828/3844 [7:53:21<02:05,  7.85s/it]

100%|█████████▉| 3829/3844 [7:53:27<01:49,  7.30s/it]


100%|█████████▉| 3831/3844 [7:53:38<01:24,  6.53s/it]
{'loss': 1.0595, 'grad_norm': 0.41637363671875227, 'learning_rate': 6.00067773199875e-10, 'epoch': 1.0}

100%|█████████▉| 3832/3844 [7:53:45<01:19,  6.59s/it]


100%|█████████▉| 3834/3844 [7:54:00<01:10,  7.07s/it]

100%|█████████▉| 3835/3844 [7:54:06<01:01,  6.79s/it]
{'loss': 1.0498, 'grad_norm': 0.40117654472426606, 'learning_rate': 2.8760794529625324e-10, 'epoch': 1.0}

100%|█████████▉| 3836/3844 [7:54:17<01:04,  8.03s/it]

100%|█████████▉| 3837/3844 [7:54:26<00:56,  8.14s/it]

100%|█████████▉| 3838/3844 [7:54:32<00:44,  7.48s/it]

100%|█████████▉| 3839/3844 [7:54:37<00:34,  6.97s/it]


100%|█████████▉| 3841/3844 [7:54:57<00:24,  8.13s/it]

100%|█████████▉| 3842/3844 [7:55:04<00:15,  7.90s/it]

100%|█████████▉| 3843/3844 [7:55:10<00:07,  7.40s/it]
{'loss': 1.1604, 'grad_norm': 0.41081416862753684, 'learning_rate': 3.550732183832395e-12, 'epoch': 1.0}

100%|██████████| 3844/3844 [7:55:22<00:00,  7.42s/it]
{'train_runtime': 28539.2305, 'train_samples_per_second': 17.242, 'train_steps_per_second': 0.135, 'train_loss': 1.1877578113782419, 'epoch': 1.0}
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-1.8B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(