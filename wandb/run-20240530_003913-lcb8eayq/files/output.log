/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
  0%|          | 0/3107 [00:00<?, ?it/s]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3107 [00:37<32:25:28, 37.58s/it]

  0%|          | 2/3107 [00:54<21:41:38, 25.15s/it]

  0%|          | 3/3107 [01:07<17:14:25, 20.00s/it]

  0%|          | 4/3107 [01:27<17:14:04, 19.99s/it]

  0%|          | 5/3107 [01:50<18:01:22, 20.92s/it]

  0%|          | 6/3107 [02:04<15:59:21, 18.56s/it]

  0%|          | 7/3107 [02:21<15:37:24, 18.14s/it]

  0%|          | 8/3107 [02:44<16:51:47, 19.59s/it]

  0%|          | 9/3107 [03:04<17:06:20, 19.88s/it]

  0%|          | 10/3107 [03:34<19:41:25, 22.89s/it]
{'loss': 2.0103, 'grad_norm': 1.4363579915099363, 'learning_rate': 2.1276595744680852e-05, 'epoch': 0.0}


  0%|          | 12/3107 [04:10<17:14:23, 20.05s/it]

  0%|          | 13/3107 [04:26<16:17:31, 18.96s/it]

  0%|          | 14/3107 [04:42<15:30:15, 18.05s/it]

  0%|          | 15/3107 [04:53<13:43:31, 15.98s/it]

  1%|          | 16/3107 [05:14<15:01:02, 17.49s/it]

  1%|          | 17/3107 [05:30<14:32:44, 16.95s/it]

  1%|          | 18/3107 [05:50<15:11:20, 17.70s/it]

  1%|          | 19/3107 [06:02<13:50:46, 16.14s/it]

  1%|          | 20/3107 [06:18<13:39:58, 15.94s/it]

  1%|          | 21/3107 [06:35<13:58:10, 16.30s/it]

  1%|          | 22/3107 [06:52<14:09:05, 16.51s/it]

  1%|          | 23/3107 [07:11<14:46:01, 17.24s/it]
{'loss': 1.3644, 'grad_norm': 0.37898087305792116, 'learning_rate': 4.893617021276596e-05, 'epoch': 0.01}


  1%|          | 25/3107 [07:50<15:09:53, 17.71s/it]

  1%|          | 26/3107 [08:04<14:16:41, 16.68s/it]

  1%|          | 27/3107 [08:21<14:21:11, 16.78s/it]

  1%|          | 28/3107 [08:38<14:31:27, 16.98s/it]

  1%|          | 29/3107 [09:06<17:14:34, 20.17s/it]

  1%|          | 30/3107 [09:25<16:58:28, 19.86s/it]

  1%|          | 31/3107 [09:46<17:14:20, 20.18s/it]

  1%|          | 32/3107 [10:06<17:13:59, 20.18s/it]

  1%|          | 33/3107 [10:24<16:41:47, 19.55s/it]
{'loss': 1.1657, 'grad_norm': 0.3156524355181483, 'learning_rate': 7.021276595744681e-05, 'epoch': 0.01}

  1%|          | 34/3107 [10:46<17:08:04, 20.07s/it]


  1%|          | 36/3107 [11:20<15:53:23, 18.63s/it]
{'loss': 1.137, 'grad_norm': 0.25025448505665426, 'learning_rate': 7.659574468085106e-05, 'epoch': 0.01}


  1%|          | 38/3107 [11:57<16:01:30, 18.80s/it]

  1%|▏         | 39/3107 [12:11<14:55:12, 17.51s/it]
{'loss': 1.2173, 'grad_norm': 0.23814822370554756, 'learning_rate': 8.297872340425533e-05, 'epoch': 0.01}


  1%|▏         | 41/3107 [12:58<17:24:17, 20.44s/it]
{'loss': 1.1516, 'grad_norm': 0.16563183251840038, 'learning_rate': 8.723404255319149e-05, 'epoch': 0.01}


  1%|▏         | 43/3107 [13:28<14:57:19, 17.57s/it]

  1%|▏         | 44/3107 [13:40<13:25:57, 15.79s/it]

  1%|▏         | 45/3107 [14:01<14:44:50, 17.34s/it]
{'loss': 1.1349, 'grad_norm': 0.14431472173192028, 'learning_rate': 9.574468085106384e-05, 'epoch': 0.01}


  2%|▏         | 47/3107 [14:42<16:08:57, 19.00s/it]

  2%|▏         | 48/3107 [15:01<16:19:20, 19.21s/it]

  2%|▏         | 49/3107 [15:17<15:26:21, 18.18s/it]

  2%|▏         | 50/3107 [15:44<17:40:46, 20.82s/it]

  2%|▏         | 51/3107 [15:57<15:37:39, 18.41s/it]

  2%|▏         | 52/3107 [16:19<16:40:32, 19.65s/it]

  2%|▏         | 53/3107 [16:35<15:42:27, 18.52s/it]

  2%|▏         | 54/3107 [16:58<16:39:00, 19.63s/it]

  2%|▏         | 55/3107 [17:09<14:39:52, 17.30s/it]
{'loss': 1.031, 'grad_norm': 0.1456890201835932, 'learning_rate': 0.00011702127659574468, 'epoch': 0.02}


  2%|▏         | 57/3107 [17:49<16:06:05, 19.00s/it]

  2%|▏         | 58/3107 [18:09<16:17:45, 19.24s/it]

  2%|▏         | 59/3107 [18:27<16:02:44, 18.95s/it]

  2%|▏         | 60/3107 [18:38<13:56:40, 16.48s/it]

  2%|▏         | 61/3107 [18:52<13:21:58, 15.80s/it]

  2%|▏         | 62/3107 [19:08<13:17:02, 15.71s/it]

  2%|▏         | 63/3107 [19:25<13:38:30, 16.13s/it]

  2%|▏         | 64/3107 [19:52<16:27:10, 19.46s/it]

  2%|▏         | 65/3107 [20:05<14:39:19, 17.34s/it]

  2%|▏         | 66/3107 [20:18<13:41:15, 16.20s/it]

  2%|▏         | 67/3107 [20:41<15:24:11, 18.24s/it]

  2%|▏         | 68/3107 [21:01<15:45:16, 18.66s/it]

  2%|▏         | 69/3107 [21:15<14:35:50, 17.30s/it]

  2%|▏         | 70/3107 [21:29<13:43:06, 16.26s/it]

  2%|▏         | 71/3107 [21:45<13:40:33, 16.22s/it]

  2%|▏         | 72/3107 [22:11<16:04:32, 19.07s/it]

  2%|▏         | 73/3107 [22:36<17:41:08, 20.98s/it]

  2%|▏         | 74/3107 [23:12<21:29:42, 25.51s/it]

  2%|▏         | 75/3107 [23:33<20:18:14, 24.11s/it]

  2%|▏         | 76/3107 [23:49<18:10:39, 21.59s/it]

  2%|▏         | 77/3107 [24:03<16:23:50, 19.48s/it]

  3%|▎         | 78/3107 [24:16<14:49:46, 17.63s/it]

  3%|▎         | 79/3107 [24:29<13:28:04, 16.01s/it]

  3%|▎         | 80/3107 [24:46<13:51:15, 16.48s/it]

  3%|▎         | 81/3107 [25:07<14:59:34, 17.84s/it]

  3%|▎         | 82/3107 [25:21<13:53:59, 16.54s/it]

  3%|▎         | 83/3107 [25:36<13:36:06, 16.19s/it]

  3%|▎         | 84/3107 [25:50<13:02:38, 15.53s/it]

  3%|▎         | 85/3107 [26:06<13:05:03, 15.59s/it]

  3%|▎         | 86/3107 [26:30<15:13:32, 18.14s/it]

  3%|▎         | 87/3107 [26:44<14:13:51, 16.96s/it]

  3%|▎         | 88/3107 [27:03<14:38:28, 17.46s/it]

  3%|▎         | 89/3107 [27:17<13:43:32, 16.37s/it]

  3%|▎         | 90/3107 [27:39<15:07:15, 18.04s/it]

  3%|▎         | 91/3107 [27:51<13:48:46, 16.49s/it]

  3%|▎         | 92/3107 [28:18<16:17:28, 19.45s/it]

  3%|▎         | 93/3107 [28:30<14:31:23, 17.35s/it]

  3%|▎         | 94/3107 [28:54<16:07:18, 19.26s/it]

  3%|▎         | 95/3107 [29:13<15:55:28, 19.03s/it]

  3%|▎         | 96/3107 [29:25<14:21:48, 17.17s/it]

  3%|▎         | 97/3107 [29:39<13:27:47, 16.10s/it]

  3%|▎         | 98/3107 [30:00<14:39:49, 17.54s/it]

  3%|▎         | 99/3107 [30:30<17:46:18, 21.27s/it]

  3%|▎         | 100/3107 [30:51<17:40:33, 21.16s/it]

  3%|▎         | 101/3107 [31:04<15:37:01, 18.70s/it]

  3%|▎         | 102/3107 [31:20<14:55:16, 17.88s/it]

  3%|▎         | 103/3107 [31:36<14:28:53, 17.35s/it]

  3%|▎         | 104/3107 [31:52<14:07:42, 16.94s/it]

  3%|▎         | 105/3107 [32:10<14:21:11, 17.21s/it]

  3%|▎         | 106/3107 [32:26<14:09:35, 16.99s/it]

  3%|▎         | 107/3107 [32:43<14:01:24, 16.83s/it]

  3%|▎         | 108/3107 [33:03<14:51:09, 17.83s/it]

  4%|▎         | 109/3107 [33:31<17:33:05, 21.08s/it]

  4%|▎         | 110/3107 [33:47<16:14:00, 19.50s/it]

  4%|▎         | 111/3107 [34:03<15:25:38, 18.54s/it]

  4%|▎         | 112/3107 [34:21<15:16:26, 18.36s/it]

  4%|▎         | 113/3107 [34:41<15:28:29, 18.61s/it]

  4%|▎         | 114/3107 [34:54<14:09:25, 17.03s/it]

  4%|▎         | 115/3107 [35:12<14:27:36, 17.40s/it]

  4%|▎         | 116/3107 [35:30<14:27:36, 17.40s/it]

  4%|▍         | 117/3107 [35:51<15:23:44, 18.54s/it]

  4%|▍         | 118/3107 [36:09<15:24:33, 18.56s/it]

  4%|▍         | 119/3107 [36:24<14:29:03, 17.45s/it]

  4%|▍         | 120/3107 [36:35<12:45:03, 15.37s/it]

  4%|▍         | 121/3107 [36:53<13:31:46, 16.31s/it]

  4%|▍         | 122/3107 [37:05<12:18:41, 14.85s/it]

  4%|▍         | 123/3107 [37:22<12:59:30, 15.67s/it]

  4%|▍         | 124/3107 [37:46<14:54:06, 17.98s/it]

  4%|▍         | 125/3107 [37:58<13:32:31, 16.35s/it]

  4%|▍         | 126/3107 [38:16<13:47:03, 16.65s/it]

  4%|▍         | 127/3107 [38:38<15:08:43, 18.30s/it]

  4%|▍         | 128/3107 [39:01<16:17:43, 19.69s/it]

  4%|▍         | 129/3107 [39:17<15:25:03, 18.64s/it]

  4%|▍         | 130/3107 [39:30<14:05:05, 17.03s/it]

  4%|▍         | 131/3107 [39:48<14:23:12, 17.40s/it]

  4%|▍         | 132/3107 [40:02<13:20:12, 16.14s/it]

  4%|▍         | 133/3107 [40:20<14:00:06, 16.95s/it]

  4%|▍         | 134/3107 [40:39<14:21:34, 17.39s/it]

  4%|▍         | 135/3107 [40:55<14:05:42, 17.07s/it]

  4%|▍         | 136/3107 [41:21<16:19:37, 19.78s/it]

  4%|▍         | 137/3107 [41:40<15:56:55, 19.33s/it]

  4%|▍         | 138/3107 [41:58<15:48:35, 19.17s/it]

  4%|▍         | 139/3107 [42:13<14:38:40, 17.76s/it]

  5%|▍         | 140/3107 [42:28<14:00:20, 16.99s/it]

  5%|▍         | 141/3107 [42:54<16:12:22, 19.67s/it]

  5%|▍         | 142/3107 [43:06<14:18:03, 17.36s/it]

  5%|▍         | 143/3107 [43:23<14:06:58, 17.15s/it]

  5%|▍         | 144/3107 [43:39<13:49:37, 16.80s/it]

  5%|▍         | 145/3107 [43:56<14:00:19, 17.02s/it]

  5%|▍         | 146/3107 [44:15<14:25:38, 17.54s/it]

  5%|▍         | 147/3107 [44:31<14:03:47, 17.10s/it]

  5%|▍         | 148/3107 [44:55<15:49:21, 19.25s/it]

  5%|▍         | 149/3107 [45:12<15:09:45, 18.45s/it]

  5%|▍         | 150/3107 [45:31<15:19:29, 18.66s/it]

  5%|▍         | 151/3107 [45:46<14:31:44, 17.69s/it]

  5%|▍         | 152/3107 [46:04<14:31:24, 17.69s/it]

  5%|▍         | 153/3107 [46:18<13:36:45, 16.59s/it]

  5%|▍         | 154/3107 [46:35<13:45:28, 16.77s/it]

  5%|▍         | 155/3107 [46:55<14:28:31, 17.65s/it]

  5%|▌         | 156/3107 [47:14<14:42:21, 17.94s/it]

  5%|▌         | 157/3107 [47:27<13:34:17, 16.56s/it]

  5%|▌         | 158/3107 [47:43<13:20:43, 16.29s/it]

  5%|▌         | 159/3107 [47:56<12:44:00, 15.55s/it]

  5%|▌         | 160/3107 [48:15<13:22:34, 16.34s/it]

  5%|▌         | 161/3107 [48:28<12:45:12, 15.58s/it]

  5%|▌         | 162/3107 [48:43<12:23:49, 15.15s/it]

  5%|▌         | 163/3107 [49:00<12:55:02, 15.80s/it]

  5%|▌         | 164/3107 [49:14<12:29:11, 15.27s/it]

  5%|▌         | 165/3107 [49:28<12:17:58, 15.05s/it]

  5%|▌         | 166/3107 [49:47<13:15:27, 16.23s/it]

  5%|▌         | 167/3107 [50:17<16:29:10, 20.19s/it]

  5%|▌         | 168/3107 [50:38<16:38:47, 20.39s/it]

  5%|▌         | 169/3107 [50:58<16:33:49, 20.30s/it]

  5%|▌         | 170/3107 [51:20<17:00:51, 20.85s/it]

  6%|▌         | 171/3107 [51:40<16:52:08, 20.68s/it]

  6%|▌         | 172/3107 [51:54<15:13:50, 18.68s/it]

  6%|▌         | 173/3107 [52:17<16:19:28, 20.03s/it]

  6%|▌         | 174/3107 [52:39<16:45:48, 20.58s/it]

  6%|▌         | 175/3107 [53:00<16:53:35, 20.74s/it]

  6%|▌         | 176/3107 [53:26<18:03:00, 22.17s/it]

  6%|▌         | 177/3107 [53:39<15:50:36, 19.47s/it]

  6%|▌         | 178/3107 [53:53<14:35:05, 17.93s/it]

  6%|▌         | 179/3107 [54:06<13:21:29, 16.42s/it]

  6%|▌         | 180/3107 [54:28<14:34:54, 17.93s/it]

  6%|▌         | 181/3107 [54:46<14:42:01, 18.09s/it]

  6%|▌         | 182/3107 [54:56<12:45:13, 15.70s/it]

  6%|▌         | 183/3107 [55:14<13:19:17, 16.40s/it]

  6%|▌         | 184/3107 [55:35<14:23:45, 17.73s/it]

  6%|▌         | 185/3107 [55:56<15:13:04, 18.75s/it]

  6%|▌         | 186/3107 [56:11<14:11:57, 17.50s/it]

  6%|▌         | 187/3107 [56:24<13:02:36, 16.08s/it]

  6%|▌         | 188/3107 [56:41<13:20:19, 16.45s/it]

  6%|▌         | 189/3107 [56:58<13:27:21, 16.60s/it]

  6%|▌         | 190/3107 [57:12<12:45:51, 15.75s/it]

  6%|▌         | 191/3107 [57:37<15:09:01, 18.70s/it]

  6%|▌         | 192/3107 [57:50<13:34:42, 16.77s/it]

  6%|▌         | 193/3107 [58:04<12:56:11, 15.98s/it]

  6%|▌         | 194/3107 [58:23<13:38:27, 16.86s/it]

  6%|▋         | 195/3107 [58:47<15:25:33, 19.07s/it]

  6%|▋         | 196/3107 [59:08<15:51:03, 19.60s/it]

  6%|▋         | 197/3107 [59:24<14:56:51, 18.49s/it]

  6%|▋         | 198/3107 [59:44<15:19:48, 18.97s/it]

  6%|▋         | 199/3107 [1:00:01<14:53:23, 18.43s/it]

  6%|▋         | 200/3107 [1:00:20<14:58:17, 18.54s/it]

  6%|▋         | 201/3107 [1:00:36<14:28:37, 17.93s/it]

  7%|▋         | 202/3107 [1:00:50<13:32:42, 16.79s/it]

  7%|▋         | 203/3107 [1:01:14<15:13:35, 18.88s/it]

  7%|▋         | 204/3107 [1:01:27<13:49:55, 17.15s/it]

  7%|▋         | 205/3107 [1:01:51<15:25:48, 19.14s/it]

  7%|▋         | 206/3107 [1:02:10<15:18:57, 19.01s/it]

  7%|▋         | 207/3107 [1:02:29<15:27:29, 19.19s/it]

  7%|▋         | 208/3107 [1:02:41<13:33:13, 16.83s/it]

  7%|▋         | 209/3107 [1:02:59<14:01:58, 17.43s/it]

  7%|▋         | 210/3107 [1:03:16<13:48:42, 17.16s/it]

  7%|▋         | 211/3107 [1:03:34<14:07:52, 17.57s/it]

  7%|▋         | 212/3107 [1:03:50<13:41:00, 17.02s/it]

  7%|▋         | 213/3107 [1:04:02<12:29:31, 15.54s/it]

  7%|▋         | 214/3107 [1:04:21<13:09:15, 16.37s/it]

  7%|▋         | 215/3107 [1:04:41<14:01:20, 17.46s/it]

  7%|▋         | 216/3107 [1:05:03<15:13:23, 18.96s/it]

  7%|▋         | 217/3107 [1:05:16<13:48:02, 17.19s/it]

  7%|▋         | 218/3107 [1:05:42<15:59:12, 19.92s/it]

  7%|▋         | 219/3107 [1:06:02<15:57:49, 19.90s/it]

  7%|▋         | 220/3107 [1:06:17<14:45:22, 18.40s/it]

  7%|▋         | 221/3107 [1:06:30<13:26:34, 16.77s/it]

  7%|▋         | 222/3107 [1:06:43<12:25:22, 15.50s/it]

  7%|▋         | 223/3107 [1:06:56<11:59:31, 14.97s/it]

  7%|▋         | 224/3107 [1:07:13<12:22:07, 15.44s/it]

  7%|▋         | 225/3107 [1:07:31<12:52:59, 16.09s/it]

  7%|▋         | 226/3107 [1:07:51<13:56:02, 17.41s/it]

  7%|▋         | 227/3107 [1:08:13<14:58:10, 18.71s/it]

  7%|▋         | 228/3107 [1:08:33<15:16:28, 19.10s/it]

  7%|▋         | 229/3107 [1:08:57<16:34:57, 20.74s/it]

  7%|▋         | 230/3107 [1:09:11<14:58:49, 18.75s/it]

  7%|▋         | 231/3107 [1:09:36<16:17:09, 20.39s/it]

  7%|▋         | 232/3107 [1:09:54<15:46:27, 19.75s/it]

  7%|▋         | 233/3107 [1:10:08<14:31:02, 18.18s/it]

  8%|▊         | 234/3107 [1:10:24<13:52:58, 17.40s/it]

  8%|▊         | 235/3107 [1:10:43<14:14:45, 17.86s/it]

  8%|▊         | 236/3107 [1:10:58<13:36:07, 17.06s/it]

  8%|▊         | 237/3107 [1:11:11<12:39:34, 15.88s/it]

  8%|▊         | 238/3107 [1:11:35<14:25:59, 18.11s/it]

  8%|▊         | 239/3107 [1:11:51<14:02:08, 17.62s/it]

  8%|▊         | 240/3107 [1:12:12<14:49:58, 18.63s/it]

  8%|▊         | 241/3107 [1:12:35<15:45:43, 19.80s/it]

  8%|▊         | 242/3107 [1:12:50<14:37:35, 18.38s/it]

  8%|▊         | 243/3107 [1:13:13<15:48:19, 19.87s/it]

  8%|▊         | 244/3107 [1:13:39<17:12:35, 21.64s/it]

  8%|▊         | 245/3107 [1:14:02<17:29:04, 21.99s/it]

  8%|▊         | 246/3107 [1:14:27<18:10:56, 22.88s/it]

  8%|▊         | 247/3107 [1:14:47<17:42:04, 22.28s/it]

  8%|▊         | 248/3107 [1:15:08<17:23:12, 21.89s/it]

  8%|▊         | 249/3107 [1:15:26<16:14:58, 20.47s/it]

  8%|▊         | 250/3107 [1:15:47<16:29:11, 20.77s/it]

  8%|▊         | 251/3107 [1:15:59<14:27:37, 18.23s/it]

  8%|▊         | 252/3107 [1:16:21<15:20:58, 19.35s/it]

  8%|▊         | 253/3107 [1:16:39<15:01:23, 18.95s/it]

  8%|▊         | 254/3107 [1:16:59<15:13:34, 19.21s/it]

  8%|▊         | 255/3107 [1:17:19<15:19:12, 19.34s/it]

  8%|▊         | 256/3107 [1:17:44<16:47:59, 21.21s/it]

  8%|▊         | 257/3107 [1:18:04<16:18:58, 20.61s/it]

  8%|▊         | 258/3107 [1:18:20<15:23:24, 19.45s/it]

  8%|▊         | 259/3107 [1:18:43<16:03:03, 20.29s/it]

  8%|▊         | 260/3107 [1:18:56<14:24:56, 18.23s/it]

  8%|▊         | 261/3107 [1:19:09<13:10:45, 16.67s/it]

  8%|▊         | 262/3107 [1:19:22<12:23:23, 15.68s/it]

  8%|▊         | 263/3107 [1:19:48<14:43:41, 18.64s/it]

  8%|▊         | 264/3107 [1:20:01<13:21:19, 16.91s/it]

  9%|▊         | 265/3107 [1:20:19<13:39:23, 17.30s/it]

  9%|▊         | 266/3107 [1:20:42<14:55:41, 18.92s/it]

  9%|▊         | 267/3107 [1:21:03<15:31:44, 19.68s/it]

  9%|▊         | 268/3107 [1:21:21<15:04:07, 19.11s/it]

  9%|▊         | 269/3107 [1:21:36<14:00:59, 17.78s/it]

  9%|▊         | 270/3107 [1:21:53<14:01:51, 17.80s/it]

  9%|▊         | 271/3107 [1:22:10<13:47:37, 17.51s/it]

  9%|▉         | 272/3107 [1:22:28<13:46:05, 17.48s/it]

  9%|▉         | 273/3107 [1:22:52<15:29:07, 19.67s/it]

  9%|▉         | 274/3107 [1:23:26<18:42:27, 23.77s/it]

  9%|▉         | 275/3107 [1:23:45<17:43:24, 22.53s/it]

  9%|▉         | 276/3107 [1:24:07<17:27:25, 22.20s/it]

  9%|▉         | 277/3107 [1:24:27<16:55:42, 21.53s/it]

  9%|▉         | 278/3107 [1:24:40<14:56:44, 19.02s/it]

  9%|▉         | 279/3107 [1:25:02<15:34:56, 19.84s/it]

  9%|▉         | 280/3107 [1:25:21<15:21:07, 19.55s/it]

  9%|▉         | 281/3107 [1:25:40<15:23:13, 19.60s/it]

  9%|▉         | 282/3107 [1:26:03<16:01:03, 20.41s/it]

  9%|▉         | 283/3107 [1:26:19<15:03:42, 19.20s/it]

  9%|▉         | 284/3107 [1:26:34<14:09:24, 18.05s/it]

  9%|▉         | 285/3107 [1:26:55<14:41:25, 18.74s/it]

  9%|▉         | 286/3107 [1:27:08<13:27:58, 17.18s/it]

  9%|▉         | 287/3107 [1:27:26<13:39:50, 17.44s/it]

  9%|▉         | 288/3107 [1:27:50<15:11:12, 19.39s/it]

  9%|▉         | 289/3107 [1:28:09<15:05:45, 19.29s/it]

  9%|▉         | 290/3107 [1:28:31<15:39:13, 20.00s/it]

  9%|▉         | 291/3107 [1:28:59<17:33:08, 22.44s/it]

  9%|▉         | 292/3107 [1:29:30<19:36:23, 25.07s/it]

  9%|▉         | 293/3107 [1:29:49<18:08:45, 23.21s/it]

  9%|▉         | 294/3107 [1:29:58<14:49:59, 18.98s/it]

  9%|▉         | 295/3107 [1:30:18<14:57:10, 19.14s/it]

 10%|▉         | 296/3107 [1:30:32<13:50:40, 17.73s/it]

 10%|▉         | 297/3107 [1:30:48<13:15:50, 16.99s/it]

 10%|▉         | 298/3107 [1:31:10<14:26:17, 18.50s/it]

 10%|▉         | 299/3107 [1:31:26<13:56:51, 17.88s/it]

 10%|▉         | 300/3107 [1:31:41<13:10:25, 16.90s/it]
 10%|▉         | 300/3107 [1:31:41<13:10:25, 16.90s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.0321, 'grad_norm': 0.12330668540386276, 'learning_rate': 0.00019767979921075866, 'epoch': 0.1}

 10%|▉         | 302/3107 [1:32:54<20:02:54, 25.73s/it]

 10%|▉         | 303/3107 [1:33:14<18:49:13, 24.16s/it]

 10%|▉         | 304/3107 [1:33:36<18:18:26, 23.51s/it]

 10%|▉         | 305/3107 [1:33:54<17:01:38, 21.88s/it]

 10%|▉         | 306/3107 [1:34:08<15:08:08, 19.45s/it]

 10%|▉         | 307/3107 [1:34:30<15:40:36, 20.16s/it]

 10%|▉         | 308/3107 [1:34:48<15:11:51, 19.55s/it]

 10%|▉         | 309/3107 [1:35:06<14:50:16, 19.09s/it]

 10%|▉         | 310/3107 [1:35:30<16:02:51, 20.65s/it]
{'loss': 0.8807, 'grad_norm': 0.1362734439309235, 'learning_rate': 0.00019747452852019893, 'epoch': 0.1}


 10%|█         | 312/3107 [1:36:00<13:32:03, 17.43s/it]

 10%|█         | 313/3107 [1:36:18<13:28:43, 17.37s/it]

 10%|█         | 314/3107 [1:36:31<12:30:17, 16.12s/it]
{'loss': 0.9658, 'grad_norm': 0.143419490920608, 'learning_rate': 0.00019738054073893807, 'epoch': 0.1}


 10%|█         | 316/3107 [1:37:08<13:27:21, 17.36s/it]

 10%|█         | 317/3107 [1:37:25<13:24:58, 17.31s/it]

 10%|█         | 318/3107 [1:37:41<13:00:28, 16.79s/it]

 10%|█         | 319/3107 [1:37:58<13:03:59, 16.87s/it]

 10%|█         | 320/3107 [1:38:16<13:19:23, 17.21s/it]

 10%|█         | 321/3107 [1:38:35<13:37:28, 17.61s/it]
{'loss': 0.9893, 'grad_norm': 0.1296318664123529, 'learning_rate': 0.00019721198712570922, 'epoch': 0.1}


 10%|█         | 323/3107 [1:39:10<13:30:43, 17.47s/it]

 10%|█         | 324/3107 [1:39:25<12:55:27, 16.72s/it]

 10%|█         | 325/3107 [1:39:44<13:28:15, 17.43s/it]

 10%|█         | 326/3107 [1:40:12<15:56:55, 20.65s/it]

 11%|█         | 327/3107 [1:40:27<14:36:41, 18.92s/it]

 11%|█         | 328/3107 [1:40:41<13:32:41, 17.55s/it]

 11%|█         | 329/3107 [1:40:54<12:25:31, 16.10s/it]

 11%|█         | 330/3107 [1:41:13<13:05:54, 16.98s/it]

 11%|█         | 331/3107 [1:41:34<14:02:59, 18.22s/it]

 11%|█         | 332/3107 [1:41:52<13:53:12, 18.02s/it]

 11%|█         | 333/3107 [1:42:08<13:38:16, 17.70s/it]

 11%|█         | 334/3107 [1:42:21<12:30:12, 16.23s/it]

 11%|█         | 335/3107 [1:42:41<13:13:13, 17.17s/it]

 11%|█         | 336/3107 [1:43:00<13:38:33, 17.72s/it]

 11%|█         | 337/3107 [1:43:18<13:51:19, 18.01s/it]

 11%|█         | 338/3107 [1:43:31<12:41:06, 16.49s/it]

 11%|█         | 339/3107 [1:43:50<13:05:49, 17.03s/it]

 11%|█         | 340/3107 [1:44:08<13:26:43, 17.49s/it]

 11%|█         | 341/3107 [1:44:21<12:17:20, 15.99s/it]

 11%|█         | 342/3107 [1:44:43<13:41:28, 17.83s/it]

 11%|█         | 343/3107 [1:45:08<15:23:41, 20.05s/it]

 11%|█         | 344/3107 [1:45:23<14:08:12, 18.42s/it]

 11%|█         | 345/3107 [1:45:41<14:08:49, 18.44s/it]

 11%|█         | 346/3107 [1:45:53<12:35:19, 16.41s/it]

 11%|█         | 347/3107 [1:46:07<11:57:54, 15.61s/it]

 11%|█         | 348/3107 [1:46:26<12:52:10, 16.79s/it]

 11%|█         | 349/3107 [1:46:52<14:59:33, 19.57s/it]
[2024-05-30 02:26:20,354] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 11%|█▏        | 350/3107 [1:47:09<14:21:48, 18.76s/it]

 11%|█▏        | 351/3107 [1:47:26<14:03:47, 18.37s/it]

 11%|█▏        | 352/3107 [1:47:44<13:52:13, 18.12s/it]

 11%|█▏        | 353/3107 [1:48:02<13:50:07, 18.09s/it]

 11%|█▏        | 354/3107 [1:48:21<14:02:51, 18.37s/it]

 11%|█▏        | 355/3107 [1:48:36<13:10:56, 17.24s/it]

 11%|█▏        | 356/3107 [1:48:52<12:51:37, 16.83s/it]

 11%|█▏        | 357/3107 [1:49:17<14:50:35, 19.43s/it]

 12%|█▏        | 358/3107 [1:49:33<14:03:54, 18.42s/it]

 12%|█▏        | 359/3107 [1:49:46<12:54:31, 16.91s/it]

 12%|█▏        | 360/3107 [1:50:01<12:19:31, 16.15s/it]

 12%|█▏        | 361/3107 [1:50:26<14:27:44, 18.96s/it]

 12%|█▏        | 362/3107 [1:50:59<17:30:25, 22.96s/it]

 12%|█▏        | 363/3107 [1:51:23<17:46:24, 23.32s/it]

 12%|█▏        | 364/3107 [1:51:42<16:51:57, 22.14s/it]

 12%|█▏        | 365/3107 [1:51:55<14:39:09, 19.24s/it]

 12%|█▏        | 366/3107 [1:52:19<15:49:25, 20.78s/it]

 12%|█▏        | 367/3107 [1:52:30<13:28:45, 17.71s/it]

 12%|█▏        | 368/3107 [1:52:45<12:56:44, 17.02s/it]

 12%|█▏        | 369/3107 [1:53:01<12:48:12, 16.83s/it]

 12%|█▏        | 370/3107 [1:53:20<13:15:16, 17.43s/it]

 12%|█▏        | 371/3107 [1:53:45<14:50:13, 19.52s/it]

 12%|█▏        | 372/3107 [1:54:07<15:26:16, 20.32s/it]

 12%|█▏        | 373/3107 [1:54:24<14:36:54, 19.24s/it]

 12%|█▏        | 374/3107 [1:54:43<14:39:08, 19.30s/it]

 12%|█▏        | 375/3107 [1:54:56<13:16:21, 17.49s/it]

 12%|█▏        | 376/3107 [1:55:09<12:09:23, 16.02s/it]

 12%|█▏        | 377/3107 [1:55:34<14:08:57, 18.66s/it]

 12%|█▏        | 378/3107 [1:55:51<13:49:11, 18.23s/it]

 12%|█▏        | 379/3107 [1:56:18<15:50:47, 20.91s/it]

 12%|█▏        | 380/3107 [1:56:33<14:24:42, 19.03s/it]

 12%|█▏        | 381/3107 [1:56:53<14:43:56, 19.46s/it]

 12%|█▏        | 382/3107 [1:57:15<15:16:32, 20.18s/it]

 12%|█▏        | 383/3107 [1:57:32<14:29:28, 19.15s/it]

 12%|█▏        | 384/3107 [1:57:55<15:27:16, 20.43s/it]

 12%|█▏        | 385/3107 [1:58:14<15:06:09, 19.97s/it]

 12%|█▏        | 386/3107 [1:58:27<13:24:37, 17.74s/it]

 12%|█▏        | 387/3107 [1:58:45<13:31:27, 17.90s/it]

 12%|█▏        | 388/3107 [1:58:59<12:33:39, 16.63s/it]

 13%|█▎        | 389/3107 [1:59:12<11:46:28, 15.60s/it]

 13%|█▎        | 390/3107 [1:59:26<11:23:12, 15.09s/it]

 13%|█▎        | 391/3107 [1:59:41<11:31:44, 15.28s/it]

 13%|█▎        | 392/3107 [2:00:06<13:36:43, 18.05s/it]

 13%|█▎        | 393/3107 [2:00:21<13:01:03, 17.27s/it]

 13%|█▎        | 394/3107 [2:00:42<13:43:13, 18.21s/it]

 13%|█▎        | 395/3107 [2:00:58<13:16:10, 17.61s/it]

 13%|█▎        | 396/3107 [2:01:10<12:04:09, 16.03s/it]

 13%|█▎        | 397/3107 [2:01:32<13:27:13, 17.87s/it]
{'loss': 0.8391, 'grad_norm': 0.130540036207607, 'learning_rate': 0.0001950507272949906, 'epoch': 0.13}


 13%|█▎        | 399/3107 [2:02:12<14:17:00, 18.99s/it]

 13%|█▎        | 400/3107 [2:02:26<13:07:58, 17.47s/it]
{'loss': 0.8869, 'grad_norm': 0.1526619123571811, 'learning_rate': 0.0001949530735791607, 'epoch': 0.13}


 13%|█▎        | 402/3107 [2:03:03<13:22:00, 17.79s/it]

 13%|█▎        | 403/3107 [2:03:14<11:58:59, 15.95s/it]

 13%|█▎        | 404/3107 [2:03:38<13:50:25, 18.43s/it]
{'loss': 0.8417, 'grad_norm': 0.13329947089880984, 'learning_rate': 0.00019482142355711023, 'epoch': 0.13}

 13%|█▎        | 405/3107 [2:03:55<13:27:14, 17.93s/it]

 13%|█▎        | 406/3107 [2:04:11<13:01:38, 17.36s/it]


 13%|█▎        | 408/3107 [2:04:52<14:20:04, 19.12s/it]

 13%|█▎        | 409/3107 [2:05:09<13:46:46, 18.39s/it]

 13%|█▎        | 410/3107 [2:05:19<11:53:41, 15.88s/it]

 13%|█▎        | 411/3107 [2:05:37<12:22:23, 16.52s/it]

 13%|█▎        | 412/3107 [2:05:56<13:05:21, 17.48s/it]

 13%|█▎        | 413/3107 [2:06:12<12:33:38, 16.78s/it]

 13%|█▎        | 414/3107 [2:06:27<12:13:31, 16.34s/it]
{'loss': 0.8688, 'grad_norm': 0.1377664739828433, 'learning_rate': 0.00019448508743516186, 'epoch': 0.13}


 13%|█▎        | 416/3107 [2:07:01<12:45:51, 17.08s/it]

 13%|█▎        | 417/3107 [2:07:18<12:53:05, 17.24s/it]

 13%|█▎        | 418/3107 [2:07:38<13:18:10, 17.81s/it]

 13%|█▎        | 419/3107 [2:07:57<13:35:05, 18.19s/it]

 14%|█▎        | 420/3107 [2:08:20<14:40:44, 19.67s/it]

 14%|█▎        | 421/3107 [2:08:36<13:53:33, 18.62s/it]

 14%|█▎        | 422/3107 [2:08:51<13:04:41, 17.54s/it]

 14%|█▎        | 423/3107 [2:09:10<13:26:08, 18.02s/it]

 14%|█▎        | 424/3107 [2:09:21<11:51:56, 15.92s/it]

 14%|█▎        | 425/3107 [2:09:43<13:15:12, 17.79s/it]

 14%|█▎        | 426/3107 [2:10:01<13:16:32, 17.83s/it]

 14%|█▎        | 427/3107 [2:10:18<12:59:26, 17.45s/it]

 14%|█▍        | 428/3107 [2:10:43<14:46:39, 19.86s/it]

 14%|█▍        | 429/3107 [2:11:06<15:30:34, 20.85s/it]

 14%|█▍        | 430/3107 [2:11:21<14:11:59, 19.10s/it]

 14%|█▍        | 431/3107 [2:11:45<15:06:33, 20.33s/it]

 14%|█▍        | 432/3107 [2:11:58<13:37:03, 18.33s/it]

 14%|█▍        | 433/3107 [2:12:17<13:42:06, 18.45s/it]

 14%|█▍        | 434/3107 [2:12:38<14:19:55, 19.30s/it]

 14%|█▍        | 435/3107 [2:12:59<14:37:33, 19.71s/it]

 14%|█▍        | 436/3107 [2:13:13<13:21:16, 18.00s/it]
{'loss': 0.8111, 'grad_norm': 0.1347916416623431, 'learning_rate': 0.0001937090431784334, 'epoch': 0.14}


 14%|█▍        | 438/3107 [2:13:56<14:28:29, 19.52s/it]

 14%|█▍        | 439/3107 [2:14:17<14:46:04, 19.93s/it]

 14%|█▍        | 440/3107 [2:14:31<13:30:46, 18.24s/it]

 14%|█▍        | 441/3107 [2:14:43<12:03:19, 16.28s/it]

 14%|█▍        | 442/3107 [2:14:55<11:10:42, 15.10s/it]

 14%|█▍        | 443/3107 [2:15:17<12:34:23, 16.99s/it]

 14%|█▍        | 444/3107 [2:15:36<12:59:59, 17.57s/it]

 14%|█▍        | 445/3107 [2:15:51<12:29:34, 16.89s/it]
{'loss': 0.8578, 'grad_norm': 0.14406121265400565, 'learning_rate': 0.00019337733501482437, 'epoch': 0.14}


 14%|█▍        | 447/3107 [2:16:25<12:32:22, 16.97s/it]

 14%|█▍        | 448/3107 [2:16:41<12:20:07, 16.70s/it]

 14%|█▍        | 449/3107 [2:17:09<14:50:54, 20.11s/it]

 14%|█▍        | 450/3107 [2:17:27<14:23:56, 19.51s/it]

 15%|█▍        | 451/3107 [2:17:43<13:27:17, 18.24s/it]

 15%|█▍        | 452/3107 [2:17:56<12:29:20, 16.93s/it]

 15%|█▍        | 453/3107 [2:18:19<13:40:10, 18.54s/it]

 15%|█▍        | 454/3107 [2:18:43<15:00:21, 20.36s/it]

 15%|█▍        | 455/3107 [2:18:59<13:52:29, 18.83s/it]

 15%|█▍        | 456/3107 [2:19:13<12:58:50, 17.63s/it]
{'loss': 0.9914, 'grad_norm': 0.13184932334374846, 'learning_rate': 0.0001929607500275159, 'epoch': 0.15}

 15%|█▍        | 457/3107 [2:19:32<13:12:16, 17.94s/it]

 15%|█▍        | 458/3107 [2:19:46<12:22:44, 16.82s/it]


 15%|█▍        | 460/3107 [2:20:17<12:02:11, 16.37s/it]

 15%|█▍        | 461/3107 [2:20:32<11:34:19, 15.74s/it]

 15%|█▍        | 462/3107 [2:20:46<11:20:24, 15.43s/it]

 15%|█▍        | 463/3107 [2:21:01<11:12:13, 15.25s/it]

 15%|█▍        | 464/3107 [2:21:27<13:25:02, 18.28s/it]

 15%|█▍        | 465/3107 [2:21:43<13:01:32, 17.75s/it]

 15%|█▍        | 466/3107 [2:21:55<11:45:18, 16.02s/it]

 15%|█▌        | 467/3107 [2:22:22<14:02:23, 19.15s/it]

 15%|█▌        | 468/3107 [2:22:41<14:06:43, 19.25s/it]

 15%|█▌        | 469/3107 [2:22:57<13:21:23, 18.23s/it]
{'loss': 0.9693, 'grad_norm': 0.1310695392924019, 'learning_rate': 0.00019245266071843596, 'epoch': 0.15}


 15%|█▌        | 471/3107 [2:23:34<13:28:05, 18.39s/it]

 15%|█▌        | 472/3107 [2:23:54<13:51:41, 18.94s/it]

 15%|█▌        | 473/3107 [2:24:13<13:58:23, 19.10s/it]

 15%|█▌        | 474/3107 [2:24:31<13:43:44, 18.77s/it]

 15%|█▌        | 475/3107 [2:24:54<14:33:08, 19.90s/it]

 15%|█▌        | 476/3107 [2:25:14<14:33:07, 19.91s/it]

 15%|█▌        | 477/3107 [2:25:29<13:30:45, 18.50s/it]

 15%|█▌        | 478/3107 [2:25:46<13:14:03, 18.12s/it]
{'loss': 0.8273, 'grad_norm': 0.11812810955879176, 'learning_rate': 0.0001920909502422833, 'epoch': 0.15}


 15%|█▌        | 480/3107 [2:26:34<14:52:59, 20.40s/it]

 15%|█▌        | 481/3107 [2:26:48<13:30:50, 18.53s/it]

 16%|█▌        | 482/3107 [2:27:04<12:58:41, 17.80s/it]
{'loss': 1.0235, 'grad_norm': 0.11983426061542242, 'learning_rate': 0.00019192758502481678, 'epoch': 0.16}


 16%|█▌        | 484/3107 [2:27:34<11:57:49, 16.42s/it]

 16%|█▌        | 485/3107 [2:28:01<14:20:15, 19.69s/it]
{'loss': 0.8663, 'grad_norm': 0.12647444441309885, 'learning_rate': 0.00019180401156748396, 'epoch': 0.16}


 16%|█▌        | 487/3107 [2:28:41<14:38:20, 20.11s/it]

 16%|█▌        | 488/3107 [2:28:58<13:50:24, 19.02s/it]

 16%|█▌        | 489/3107 [2:29:16<13:38:33, 18.76s/it]

 16%|█▌        | 490/3107 [2:29:34<13:28:34, 18.54s/it]

 16%|█▌        | 491/3107 [2:29:56<14:18:22, 19.69s/it]

 16%|█▌        | 492/3107 [2:30:14<13:55:06, 19.16s/it]

 16%|█▌        | 493/3107 [2:30:30<13:11:36, 18.17s/it]

 16%|█▌        | 494/3107 [2:30:51<13:50:33, 19.07s/it]
{'loss': 0.8802, 'grad_norm': 0.1313461494524585, 'learning_rate': 0.00019142790647573902, 'epoch': 0.16}


 16%|█▌        | 496/3107 [2:31:46<16:52:26, 23.27s/it]

 16%|█▌        | 497/3107 [2:32:03<15:33:53, 21.47s/it]

 16%|█▌        | 498/3107 [2:32:20<14:32:18, 20.06s/it]

 16%|█▌        | 499/3107 [2:32:34<13:17:08, 18.34s/it]

 16%|█▌        | 500/3107 [2:32:54<13:32:06, 18.69s/it]

 16%|█▌        | 501/3107 [2:33:07<12:21:22, 17.07s/it]

 16%|█▌        | 502/3107 [2:33:27<12:51:21, 17.77s/it]

 16%|█▌        | 503/3107 [2:33:44<12:46:36, 17.66s/it]

 16%|█▌        | 504/3107 [2:34:00<12:17:29, 17.00s/it]

 16%|█▋        | 505/3107 [2:34:14<11:43:34, 16.22s/it]

 16%|█▋        | 506/3107 [2:34:32<12:12:59, 16.91s/it]
{'loss': 0.8886, 'grad_norm': 0.12513980303194366, 'learning_rate': 0.0001909139147153287, 'epoch': 0.16}


 16%|█▋        | 508/3107 [2:35:10<12:49:49, 17.77s/it]

 16%|█▋        | 509/3107 [2:35:36<14:41:30, 20.36s/it]

 16%|█▋        | 510/3107 [2:35:51<13:35:57, 18.85s/it]

 16%|█▋        | 511/3107 [2:36:11<13:39:53, 18.95s/it]

 16%|█▋        | 512/3107 [2:36:23<12:20:15, 17.12s/it]

 17%|█▋        | 513/3107 [2:36:42<12:40:18, 17.59s/it]

 17%|█▋        | 514/3107 [2:37:00<12:39:46, 17.58s/it]
{'loss': 0.8934, 'grad_norm': 0.13102126286678098, 'learning_rate': 0.00019056334140229777, 'epoch': 0.17}


 17%|█▋        | 516/3107 [2:37:52<15:55:18, 22.12s/it]

 17%|█▋        | 517/3107 [2:38:23<17:44:14, 24.65s/it]

 17%|█▋        | 518/3107 [2:38:44<16:53:34, 23.49s/it]
{'loss': 0.8876, 'grad_norm': 0.14219095049343583, 'learning_rate': 0.00019038569020996617, 'epoch': 0.17}


 17%|█▋        | 520/3107 [2:39:28<16:37:41, 23.14s/it]

 17%|█▋        | 521/3107 [2:39:53<16:57:28, 23.61s/it]
[2024-05-30 03:19:20,746] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 522/3107 [2:40:13<16:16:36, 22.67s/it]

 17%|█▋        | 523/3107 [2:40:30<15:00:38, 20.91s/it]

 17%|█▋        | 524/3107 [2:40:53<15:30:39, 21.62s/it]

 17%|█▋        | 525/3107 [2:41:09<14:13:04, 19.82s/it]

 17%|█▋        | 526/3107 [2:41:28<14:10:55, 19.78s/it]

 17%|█▋        | 527/3107 [2:41:48<14:05:28, 19.66s/it]
{'loss': 0.8786, 'grad_norm': 0.1312355346424879, 'learning_rate': 0.0001899802312545949, 'epoch': 0.17}


 17%|█▋        | 529/3107 [2:42:28<14:23:55, 20.11s/it]

 17%|█▋        | 530/3107 [2:42:53<15:30:07, 21.66s/it]

 17%|█▋        | 531/3107 [2:43:15<15:25:41, 21.56s/it]
{'loss': 0.9093, 'grad_norm': 0.14299782061871738, 'learning_rate': 0.0001897974817027588, 'epoch': 0.17}


 17%|█▋        | 533/3107 [2:43:53<14:24:16, 20.15s/it]

 17%|█▋        | 534/3107 [2:44:08<13:10:08, 18.43s/it]

 17%|█▋        | 535/3107 [2:44:26<13:04:36, 18.30s/it]

 17%|█▋        | 536/3107 [2:44:47<13:40:04, 19.14s/it]

 17%|█▋        | 537/3107 [2:45:00<12:24:29, 17.38s/it]

 17%|█▋        | 538/3107 [2:45:20<12:59:47, 18.21s/it]

 17%|█▋        | 539/3107 [2:45:38<12:45:13, 17.88s/it]

 17%|█▋        | 540/3107 [2:46:03<14:15:42, 20.00s/it]

 17%|█▋        | 541/3107 [2:46:21<14:00:00, 19.64s/it]

 17%|█▋        | 542/3107 [2:46:48<15:24:11, 21.62s/it]

 17%|█▋        | 543/3107 [2:47:03<14:03:49, 19.75s/it]

 18%|█▊        | 544/3107 [2:47:17<12:50:04, 18.03s/it]

 18%|█▊        | 545/3107 [2:47:37<13:16:08, 18.64s/it]

 18%|█▊        | 546/3107 [2:48:01<14:26:52, 20.31s/it]

 18%|█▊        | 547/3107 [2:48:25<15:14:09, 21.43s/it]

 18%|█▊        | 548/3107 [2:48:37<13:16:25, 18.67s/it]

 18%|█▊        | 549/3107 [2:48:52<12:18:23, 17.32s/it]

 18%|█▊        | 550/3107 [2:49:08<12:01:28, 16.93s/it]

 18%|█▊        | 551/3107 [2:49:33<13:42:27, 19.31s/it]
{'loss': 0.9048, 'grad_norm': 0.1386039916716719, 'learning_rate': 0.0001888603682576276, 'epoch': 0.18}


 18%|█▊        | 553/3107 [2:50:30<17:32:24, 24.72s/it]
{'loss': 0.9578, 'grad_norm': 0.12798923121006342, 'learning_rate': 0.00018876452476120084, 'epoch': 0.18}


 18%|█▊        | 555/3107 [2:51:04<14:41:05, 20.72s/it]

 18%|█▊        | 556/3107 [2:51:21<13:49:28, 19.51s/it]

 18%|█▊        | 557/3107 [2:51:39<13:39:50, 19.29s/it]

 18%|█▊        | 558/3107 [2:51:59<13:41:07, 19.33s/it]

 18%|█▊        | 559/3107 [2:52:13<12:34:37, 17.77s/it]

 18%|█▊        | 560/3107 [2:52:33<13:07:32, 18.55s/it]
{'loss': 0.7835, 'grad_norm': 0.12774780310968062, 'learning_rate': 0.00018842603542997926, 'epoch': 0.18}


 18%|█▊        | 562/3107 [2:53:06<12:07:08, 17.14s/it]

 18%|█▊        | 563/3107 [2:53:25<12:28:01, 17.64s/it]

 18%|█▊        | 564/3107 [2:53:50<14:03:54, 19.91s/it]

 18%|█▊        | 565/3107 [2:54:08<13:37:07, 19.29s/it]

 18%|█▊        | 566/3107 [2:54:23<12:48:40, 18.15s/it]

 18%|█▊        | 567/3107 [2:54:44<13:20:12, 18.90s/it]

 18%|█▊        | 568/3107 [2:55:07<14:10:13, 20.09s/it]

 18%|█▊        | 569/3107 [2:55:21<12:58:45, 18.41s/it]

 18%|█▊        | 570/3107 [2:55:39<12:45:32, 18.11s/it]

 18%|█▊        | 571/3107 [2:55:58<12:55:51, 18.36s/it]

 18%|█▊        | 572/3107 [2:56:11<11:49:46, 16.80s/it]

 18%|█▊        | 573/3107 [2:56:23<10:50:34, 15.40s/it]

 18%|█▊        | 574/3107 [2:56:42<11:36:21, 16.49s/it]

 19%|█▊        | 575/3107 [2:57:00<11:54:15, 16.93s/it]

 19%|█▊        | 576/3107 [2:57:17<11:58:32, 17.03s/it]

 19%|█▊        | 577/3107 [2:57:32<11:25:39, 16.26s/it]

 19%|█▊        | 578/3107 [2:57:52<12:11:53, 17.36s/it]

 19%|█▊        | 579/3107 [2:58:09<12:10:30, 17.34s/it]

 19%|█▊        | 580/3107 [2:58:24<11:39:52, 16.62s/it]

 19%|█▊        | 581/3107 [2:58:36<10:48:29, 15.40s/it]

 19%|█▊        | 582/3107 [2:58:59<12:19:32, 17.57s/it]
{'loss': 0.9385, 'grad_norm': 0.12616684982478948, 'learning_rate': 0.0001873316302310514, 'epoch': 0.19}

 19%|█▉        | 583/3107 [2:59:13<11:30:17, 16.41s/it]

 19%|█▉        | 584/3107 [2:59:35<12:39:45, 18.07s/it]


 19%|█▉        | 586/3107 [3:00:02<11:16:00, 16.09s/it]

 19%|█▉        | 587/3107 [3:00:24<12:27:39, 17.80s/it]

 19%|█▉        | 588/3107 [3:00:44<12:53:06, 18.41s/it]

 19%|█▉        | 589/3107 [3:01:04<13:10:07, 18.83s/it]

 19%|█▉        | 590/3107 [3:01:21<12:43:11, 18.19s/it]

 19%|█▉        | 591/3107 [3:01:40<13:01:44, 18.64s/it]

 19%|█▉        | 592/3107 [3:01:56<12:28:40, 17.86s/it]

 19%|█▉        | 593/3107 [3:02:17<13:09:01, 18.83s/it]
{'loss': 0.7804, 'grad_norm': 0.13658914890697735, 'learning_rate': 0.00018676715889355322, 'epoch': 0.19}


 19%|█▉        | 595/3107 [3:02:56<12:59:40, 18.62s/it]

 19%|█▉        | 596/3107 [3:03:08<11:41:24, 16.76s/it]

 19%|█▉        | 597/3107 [3:03:22<11:06:14, 15.93s/it]

 19%|█▉        | 598/3107 [3:03:38<11:11:00, 16.05s/it]

 19%|█▉        | 599/3107 [3:03:54<10:58:06, 15.74s/it]

 19%|█▉        | 600/3107 [3:04:10<11:08:42, 16.00s/it]
 19%|█▉        | 600/3107 [3:04:10<11:08:42, 16.00s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 19%|█▉        | 601/3107 [3:04:57<17:41:22, 25.41s/it]

 19%|█▉        | 602/3107 [3:05:12<15:30:35, 22.29s/it]

 19%|█▉        | 603/3107 [3:05:32<14:49:51, 21.32s/it]

 19%|█▉        | 604/3107 [3:05:54<15:06:02, 21.72s/it]

 19%|█▉        | 605/3107 [3:06:13<14:32:56, 20.93s/it]

 20%|█▉        | 606/3107 [3:06:27<13:08:17, 18.91s/it]

 20%|█▉        | 607/3107 [3:06:48<13:22:19, 19.26s/it]

 20%|█▉        | 608/3107 [3:07:12<14:26:06, 20.79s/it]
{'loss': 0.6744, 'grad_norm': 0.11594885746801914, 'learning_rate': 0.00018597904555752587, 'epoch': 0.2}


 20%|█▉        | 610/3107 [3:07:52<14:14:49, 20.54s/it]

 20%|█▉        | 611/3107 [3:08:15<14:56:10, 21.54s/it]

 20%|█▉        | 612/3107 [3:08:36<14:40:32, 21.18s/it]

 20%|█▉        | 613/3107 [3:08:50<13:16:59, 19.17s/it]

 20%|█▉        | 614/3107 [3:09:14<14:07:40, 20.40s/it]

 20%|█▉        | 615/3107 [3:09:29<13:04:19, 18.88s/it]

 20%|█▉        | 616/3107 [3:09:44<12:17:14, 17.76s/it]
{'loss': 0.8103, 'grad_norm': 0.14640958696565076, 'learning_rate': 0.0001855501064778312, 'epoch': 0.2}


 20%|█▉        | 618/3107 [3:10:21<12:30:49, 18.10s/it]

 20%|█▉        | 619/3107 [3:10:34<11:29:49, 16.64s/it]

 20%|█▉        | 620/3107 [3:10:47<10:43:23, 15.52s/it]

 20%|█▉        | 621/3107 [3:11:02<10:39:31, 15.44s/it]

 20%|██        | 622/3107 [3:11:22<11:42:47, 16.97s/it]

 20%|██        | 623/3107 [3:11:40<11:55:05, 17.27s/it]

 20%|██        | 624/3107 [3:12:01<12:35:09, 18.25s/it]

 20%|██        | 625/3107 [3:12:18<12:22:48, 17.96s/it]

 20%|██        | 626/3107 [3:12:37<12:30:46, 18.16s/it]

 20%|██        | 627/3107 [3:13:00<13:36:18, 19.75s/it]

 20%|██        | 628/3107 [3:13:31<15:50:28, 23.00s/it]

 20%|██        | 629/3107 [3:13:57<16:26:13, 23.88s/it]

 20%|██        | 630/3107 [3:14:15<15:18:27, 22.25s/it]

 20%|██        | 631/3107 [3:14:36<15:02:03, 21.86s/it]

 20%|██        | 632/3107 [3:14:51<13:36:14, 19.79s/it]

 20%|██        | 633/3107 [3:15:11<13:41:19, 19.92s/it]
{'loss': 0.735, 'grad_norm': 0.12892261984031989, 'learning_rate': 0.00018461888446182472, 'epoch': 0.2}


 20%|██        | 635/3107 [3:15:53<13:49:36, 20.14s/it]

 20%|██        | 636/3107 [3:16:08<12:53:32, 18.78s/it]

 21%|██        | 637/3107 [3:16:27<12:49:41, 18.70s/it]

 21%|██        | 638/3107 [3:16:43<12:12:34, 17.80s/it]

 21%|██        | 639/3107 [3:16:55<11:07:33, 16.23s/it]

 21%|██        | 640/3107 [3:17:15<11:58:54, 17.48s/it]
{'loss': 0.9386, 'grad_norm': 0.1312591298896534, 'learning_rate': 0.00018422769568105467, 'epoch': 0.21}


 21%|██        | 642/3107 [3:17:45<10:58:29, 16.03s/it]

 21%|██        | 643/3107 [3:18:01<10:58:21, 16.03s/it]

 21%|██        | 644/3107 [3:18:12<9:57:04, 14.55s/it]
{'loss': 0.996, 'grad_norm': 0.1524965614882762, 'learning_rate': 0.00018400214334256227, 'epoch': 0.21}


 21%|██        | 646/3107 [3:18:42<10:01:12, 14.66s/it]
{'loss': 0.8483, 'grad_norm': 0.1287005920561609, 'learning_rate': 0.000183888818975165, 'epoch': 0.21}


 21%|██        | 648/3107 [3:19:17<11:07:32, 16.29s/it]

 21%|██        | 649/3107 [3:19:37<12:01:30, 17.61s/it]

 21%|██        | 650/3107 [3:20:11<15:16:02, 22.37s/it]
[2024-05-30 03:59:39,191] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 651/3107 [3:20:39<16:21:05, 23.97s/it]

 21%|██        | 652/3107 [3:20:54<14:38:12, 21.46s/it]

 21%|██        | 653/3107 [3:21:09<13:16:39, 19.48s/it]

 21%|██        | 654/3107 [3:21:37<14:56:03, 21.92s/it]

 21%|██        | 655/3107 [3:21:52<13:31:08, 19.85s/it]

 21%|██        | 656/3107 [3:22:13<13:48:19, 20.28s/it]

 21%|██        | 657/3107 [3:22:38<14:41:41, 21.59s/it]

 21%|██        | 658/3107 [3:22:52<13:08:15, 19.31s/it]

 21%|██        | 659/3107 [3:23:09<12:47:59, 18.82s/it]

 21%|██        | 660/3107 [3:23:29<12:59:22, 19.11s/it]

 21%|██▏       | 661/3107 [3:23:42<11:39:58, 17.17s/it]

 21%|██▏       | 662/3107 [3:24:11<14:11:22, 20.89s/it]
{'loss': 0.7892, 'grad_norm': 0.13046942776302467, 'learning_rate': 0.00018296913264559468, 'epoch': 0.21}


 21%|██▏       | 664/3107 [3:24:48<13:27:31, 19.83s/it]

 21%|██▏       | 665/3107 [3:25:09<13:40:12, 20.15s/it]

 21%|██▏       | 666/3107 [3:25:25<12:48:13, 18.88s/it]

 21%|██▏       | 667/3107 [3:25:39<11:48:16, 17.42s/it]

 21%|██▏       | 668/3107 [3:25:56<11:46:06, 17.37s/it]

 22%|██▏       | 669/3107 [3:26:22<13:27:48, 19.88s/it]

 22%|██▏       | 670/3107 [3:26:38<12:43:50, 18.81s/it]

 22%|██▏       | 671/3107 [3:27:01<13:35:43, 20.09s/it]

 22%|██▏       | 672/3107 [3:27:20<13:19:47, 19.71s/it]

 22%|██▏       | 673/3107 [3:27:40<13:26:31, 19.88s/it]

 22%|██▏       | 674/3107 [3:28:04<14:14:35, 21.07s/it]

 22%|██▏       | 675/3107 [3:28:15<12:16:49, 18.18s/it]

 22%|██▏       | 676/3107 [3:28:40<13:33:03, 20.07s/it]
{'loss': 0.7925, 'grad_norm': 0.12570044875623868, 'learning_rate': 0.00018214545613831414, 'epoch': 0.22}


 22%|██▏       | 678/3107 [3:29:08<11:22:13, 16.85s/it]

 22%|██▏       | 679/3107 [3:29:29<12:12:30, 18.10s/it]

 22%|██▏       | 680/3107 [3:29:50<12:46:54, 18.96s/it]
{'loss': 0.808, 'grad_norm': 0.13707629231228566, 'learning_rate': 0.00018190689723299378, 'epoch': 0.22}


 22%|██▏       | 682/3107 [3:30:23<11:45:33, 17.46s/it]

 22%|██▏       | 683/3107 [3:30:40<11:38:49, 17.30s/it]

 22%|██▏       | 684/3107 [3:30:56<11:25:55, 16.99s/it]

 22%|██▏       | 685/3107 [3:31:15<11:46:52, 17.51s/it]

 22%|██▏       | 686/3107 [3:31:31<11:33:13, 17.18s/it]

 22%|██▏       | 687/3107 [3:31:49<11:41:00, 17.38s/it]

 22%|██▏       | 688/3107 [3:32:08<11:57:12, 17.79s/it]

 22%|██▏       | 689/3107 [3:32:29<12:35:53, 18.76s/it]
{'loss': 0.8493, 'grad_norm': 0.12452262785404573, 'learning_rate': 0.00018136493678403686, 'epoch': 0.22}


 22%|██▏       | 691/3107 [3:33:20<14:51:52, 22.15s/it]
{'loss': 0.9356, 'grad_norm': 0.13376731815082055, 'learning_rate': 0.00018124352640144572, 'epoch': 0.22}

 22%|██▏       | 692/3107 [3:33:37<13:47:25, 20.56s/it]


 22%|██▏       | 694/3107 [3:34:06<11:54:29, 17.77s/it]

 22%|██▏       | 695/3107 [3:34:19<10:53:04, 16.25s/it]

 22%|██▏       | 696/3107 [3:34:34<10:42:56, 16.00s/it]

 22%|██▏       | 697/3107 [3:34:53<11:13:40, 16.77s/it]

 22%|██▏       | 698/3107 [3:35:05<10:24:24, 15.55s/it]

 22%|██▏       | 699/3107 [3:35:17<9:40:21, 14.46s/it]

 23%|██▎       | 700/3107 [3:35:28<8:55:04, 13.34s/it]

 23%|██▎       | 701/3107 [3:35:42<9:05:34, 13.61s/it]

 23%|██▎       | 702/3107 [3:36:02<10:14:23, 15.33s/it]
{'loss': 0.7204, 'grad_norm': 0.13665233248041406, 'learning_rate': 0.00018056946819959698, 'epoch': 0.23}


 23%|██▎       | 704/3107 [3:36:44<12:42:45, 19.05s/it]
{'loss': 0.9302, 'grad_norm': 0.1402996226537881, 'learning_rate': 0.00018044577083910758, 'epoch': 0.23}


 23%|██▎       | 706/3107 [3:37:26<13:14:14, 19.85s/it]
{'loss': 0.8842, 'grad_norm': 0.1349019281682765, 'learning_rate': 0.0001803217236425901, 'epoch': 0.23}


 23%|██▎       | 708/3107 [3:38:00<12:21:11, 18.54s/it]

 23%|██▎       | 709/3107 [3:38:23<13:05:16, 19.65s/it]

 23%|██▎       | 710/3107 [3:38:45<13:33:14, 20.36s/it]

 23%|██▎       | 711/3107 [3:39:00<12:38:08, 18.99s/it]

 23%|██▎       | 712/3107 [3:39:21<12:56:34, 19.45s/it]

 23%|██▎       | 713/3107 [3:39:37<12:12:14, 18.35s/it]

 23%|██▎       | 714/3107 [3:39:57<12:31:01, 18.83s/it]

 23%|██▎       | 715/3107 [3:40:14<12:17:39, 18.50s/it]

 23%|██▎       | 716/3107 [3:40:30<11:39:51, 17.56s/it]

 23%|██▎       | 717/3107 [3:40:54<12:54:00, 19.43s/it]
{'loss': 0.7914, 'grad_norm': 0.12168106891551178, 'learning_rate': 0.00017963323492553438, 'epoch': 0.23}


 23%|██▎       | 719/3107 [3:41:32<12:56:33, 19.51s/it]

 23%|██▎       | 720/3107 [3:42:00<14:30:55, 21.89s/it]
{'loss': 0.8077, 'grad_norm': 0.11759476893694845, 'learning_rate': 0.00017944364336277353, 'epoch': 0.23}


 23%|██▎       | 722/3107 [3:42:50<15:40:30, 23.66s/it]
{'loss': 0.8504, 'grad_norm': 0.13166252444182344, 'learning_rate': 0.000179316817025399, 'epoch': 0.23}


 23%|██▎       | 724/3107 [3:43:24<13:29:07, 20.37s/it]

 23%|██▎       | 725/3107 [3:43:41<12:38:44, 19.11s/it]

 23%|██▎       | 726/3107 [3:44:02<13:09:18, 19.89s/it]

 23%|██▎       | 727/3107 [3:44:34<15:29:59, 23.45s/it]

 23%|██▎       | 728/3107 [3:44:49<13:48:00, 20.88s/it]
{'loss': 0.9507, 'grad_norm': 0.14204685378916287, 'learning_rate': 0.00017893427066774836, 'epoch': 0.23}


 23%|██▎       | 730/3107 [3:45:33<14:14:50, 21.58s/it]

 24%|██▎       | 731/3107 [3:45:57<14:42:45, 22.29s/it]

 24%|██▎       | 732/3107 [3:46:15<13:52:44, 21.04s/it]

 24%|██▎       | 733/3107 [3:46:29<12:26:43, 18.87s/it]

 24%|██▎       | 734/3107 [3:46:43<11:37:08, 17.63s/it]

 24%|██▎       | 735/3107 [3:47:03<12:03:27, 18.30s/it]

 24%|██▎       | 736/3107 [3:47:27<13:03:14, 19.82s/it]

 24%|██▎       | 737/3107 [3:47:47<13:13:54, 20.10s/it]

 24%|██▍       | 738/3107 [3:48:09<13:27:32, 20.45s/it]

 24%|██▍       | 739/3107 [3:48:30<13:39:45, 20.77s/it]

 24%|██▍       | 740/3107 [3:48:45<12:33:23, 19.10s/it]

 24%|██▍       | 741/3107 [3:49:01<11:53:27, 18.09s/it]

 24%|██▍       | 742/3107 [3:49:26<13:18:03, 20.25s/it]

 24%|██▍       | 743/3107 [3:49:39<11:43:58, 17.87s/it]

 24%|██▍       | 744/3107 [3:49:56<11:38:41, 17.74s/it]

 24%|██▍       | 745/3107 [3:50:20<12:43:52, 19.40s/it]
{'loss': 0.7438, 'grad_norm': 0.11917468716303464, 'learning_rate': 0.00017783366256598972, 'epoch': 0.24}


 24%|██▍       | 747/3107 [3:50:49<11:05:44, 16.93s/it]
{'loss': 0.8395, 'grad_norm': 0.14081128215134578, 'learning_rate': 0.0001777025647715141, 'epoch': 0.24}

 24%|██▍       | 748/3107 [3:51:06<11:01:43, 16.83s/it]


 24%|██▍       | 750/3107 [3:51:45<12:01:42, 18.37s/it]

 24%|██▍       | 751/3107 [3:52:00<11:17:36, 17.26s/it]

 24%|██▍       | 752/3107 [3:52:13<10:30:52, 16.07s/it]

 24%|██▍       | 753/3107 [3:52:33<11:22:38, 17.40s/it]

 24%|██▍       | 754/3107 [3:52:57<12:29:33, 19.11s/it]
{'loss': 0.7895, 'grad_norm': 0.13735363701588152, 'learning_rate': 0.00017724106523098486, 'epoch': 0.24}


 24%|██▍       | 756/3107 [3:53:24<10:46:57, 16.51s/it]
{'loss': 0.9081, 'grad_norm': 0.13936533535970522, 'learning_rate': 0.0001771084513676139, 'epoch': 0.24}


 24%|██▍       | 758/3107 [3:54:01<11:47:05, 18.06s/it]

 24%|██▍       | 759/3107 [3:54:18<11:35:36, 17.78s/it]

 24%|██▍       | 760/3107 [3:54:36<11:44:05, 18.00s/it]

 24%|██▍       | 761/3107 [3:55:00<12:55:39, 19.84s/it]

 25%|██▍       | 762/3107 [3:55:21<13:00:56, 19.98s/it]

 25%|██▍       | 763/3107 [3:55:43<13:32:28, 20.80s/it]

 25%|██▍       | 764/3107 [3:56:01<12:59:10, 19.95s/it]

 25%|██▍       | 765/3107 [3:56:16<11:53:11, 18.27s/it]

 25%|██▍       | 766/3107 [3:56:35<12:07:01, 18.63s/it]
{'loss': 0.9455, 'grad_norm': 0.12806098119745063, 'learning_rate': 0.00017644036379128522, 'epoch': 0.25}


 25%|██▍       | 768/3107 [3:57:14<12:36:15, 19.40s/it]

 25%|██▍       | 769/3107 [3:57:44<14:40:38, 22.60s/it]
{'loss': 0.7458, 'grad_norm': 0.13047949501636752, 'learning_rate': 0.0001762383136680022, 'epoch': 0.25}

 25%|██▍       | 770/3107 [3:58:01<13:30:56, 20.82s/it]

 25%|██▍       | 771/3107 [3:58:25<14:06:06, 21.73s/it]


 25%|██▍       | 773/3107 [3:58:56<12:01:52, 18.56s/it]

 25%|██▍       | 774/3107 [3:59:07<10:38:37, 16.42s/it]

 25%|██▍       | 775/3107 [3:59:26<11:02:44, 17.05s/it]

 25%|██▍       | 776/3107 [3:59:39<10:24:30, 16.07s/it]

 25%|██▌       | 777/3107 [4:00:01<11:29:25, 17.75s/it]
{'loss': 0.9116, 'grad_norm': 0.12428014190282588, 'learning_rate': 0.00017569587180481694, 'epoch': 0.25}


 25%|██▌       | 779/3107 [4:00:36<11:28:41, 17.75s/it]

 25%|██▌       | 780/3107 [4:00:48<10:26:57, 16.17s/it]
{'loss': 0.8074, 'grad_norm': 0.14373129483746175, 'learning_rate': 0.00017549109620687784, 'epoch': 0.25}

 25%|██▌       | 781/3107 [4:01:00<9:38:42, 14.93s/it]


 25%|██▌       | 783/3107 [4:01:42<11:50:45, 18.35s/it]
{'loss': 0.8278, 'grad_norm': 0.12096276374748613, 'learning_rate': 0.00017528558195780425, 'epoch': 0.25}


 25%|██▌       | 785/3107 [4:02:14<11:15:18, 17.45s/it]

 25%|██▌       | 786/3107 [4:02:30<10:57:23, 16.99s/it]

 25%|██▌       | 787/3107 [4:02:54<12:12:58, 18.96s/it]

 25%|██▌       | 788/3107 [4:03:14<12:24:40, 19.27s/it]
{'loss': 0.8518, 'grad_norm': 0.12316729214890192, 'learning_rate': 0.00017494142222728825, 'epoch': 0.25}

 25%|██▌       | 789/3107 [4:03:41<13:59:40, 21.73s/it]


 25%|██▌       | 791/3107 [4:04:18<12:41:21, 19.72s/it]

 25%|██▌       | 792/3107 [4:04:44<13:43:17, 21.34s/it]
{'loss': 0.8516, 'grad_norm': 0.13668426342750672, 'learning_rate': 0.0001746646274485689, 'epoch': 0.25}

 26%|██▌       | 793/3107 [4:05:05<13:47:48, 21.46s/it]


 26%|██▌       | 795/3107 [4:05:38<12:04:13, 18.79s/it]

 26%|██▌       | 796/3107 [4:05:57<11:59:12, 18.67s/it]

 26%|██▌       | 797/3107 [4:06:12<11:19:56, 17.66s/it]

 26%|██▌       | 798/3107 [4:06:38<13:01:32, 20.31s/it]
{'loss': 0.7575, 'grad_norm': 0.11908069066674606, 'learning_rate': 0.00017424700157798138, 'epoch': 0.26}


 26%|██▌       | 800/3107 [4:07:07<10:57:43, 17.11s/it]

 26%|██▌       | 801/3107 [4:07:24<11:00:13, 17.18s/it]

 26%|██▌       | 802/3107 [4:07:38<10:23:23, 16.23s/it]

 26%|██▌       | 803/3107 [4:07:58<11:04:42, 17.31s/it]
{'loss': 0.9761, 'grad_norm': 0.1306640005955253, 'learning_rate': 0.00017389675953315495, 'epoch': 0.26}

 26%|██▌       | 804/3107 [4:08:13<10:41:05, 16.70s/it]


 26%|██▌       | 806/3107 [4:08:55<11:49:09, 18.49s/it]
{'loss': 0.8135, 'grad_norm': 0.13455732348044233, 'learning_rate': 0.00017368564962791857, 'epoch': 0.26}


 26%|██▌       | 808/3107 [4:09:26<10:58:17, 17.18s/it]

 26%|██▌       | 809/3107 [4:09:48<11:55:07, 18.67s/it]

 26%|██▌       | 810/3107 [4:10:00<10:38:18, 16.67s/it]

 26%|██▌       | 811/3107 [4:10:20<11:11:46, 17.56s/it]

 26%|██▌       | 812/3107 [4:10:44<12:25:14, 19.48s/it]

 26%|██▌       | 813/3107 [4:11:06<12:55:22, 20.28s/it]

 26%|██▌       | 814/3107 [4:11:21<11:46:55, 18.50s/it]

 26%|██▌       | 815/3107 [4:11:46<13:10:47, 20.70s/it]

 26%|██▋       | 816/3107 [4:12:13<14:18:29, 22.48s/it]
{'loss': 0.8452, 'grad_norm': 0.12925072080684177, 'learning_rate': 0.0001729767544820342, 'epoch': 0.26}

 26%|██▋       | 817/3107 [4:12:39<15:02:02, 23.63s/it]


 26%|██▋       | 819/3107 [4:13:22<14:28:16, 22.77s/it]

 26%|██▋       | 820/3107 [4:13:37<12:55:03, 20.33s/it]

 26%|██▋       | 821/3107 [4:14:01<13:35:45, 21.41s/it]
{'loss': 0.8867, 'grad_norm': 0.12462333639183824, 'learning_rate': 0.00017261932687770754, 'epoch': 0.26}


 26%|██▋       | 823/3107 [4:14:38<12:37:24, 19.90s/it]

 27%|██▋       | 824/3107 [4:14:58<12:38:48, 19.94s/it]
{'loss': 0.8504, 'grad_norm': 0.12793956745273888, 'learning_rate': 0.00017240392228986518, 'epoch': 0.27}


 27%|██▋       | 826/3107 [4:15:31<11:38:14, 18.37s/it]
{'loss': 0.836, 'grad_norm': 0.13066614846823482, 'learning_rate': 0.00017225992552073093, 'epoch': 0.27}

 27%|██▋       | 827/3107 [4:15:52<12:06:48, 19.13s/it]


 27%|██▋       | 829/3107 [4:16:29<11:45:55, 18.59s/it]
{'loss': 0.9486, 'grad_norm': 0.13912349999455312, 'learning_rate': 0.00017204334136667365, 'epoch': 0.27}


 27%|██▋       | 831/3107 [4:17:10<12:11:17, 19.28s/it]
{'loss': 0.7541, 'grad_norm': 0.1346295231654648, 'learning_rate': 0.0001718985601794321, 'epoch': 0.27}

 27%|██▋       | 832/3107 [4:17:26<11:25:07, 18.07s/it]

 27%|██▋       | 833/3107 [4:17:49<12:31:35, 19.83s/it]


 27%|██▋       | 835/3107 [4:18:27<12:04:50, 19.14s/it]
{'loss': 0.9974, 'grad_norm': 0.12663356410759666, 'learning_rate': 0.00017160806043608183, 'epoch': 0.27}

 27%|██▋       | 836/3107 [4:18:48<12:27:54, 19.76s/it]

 27%|██▋       | 837/3107 [4:19:04<11:45:02, 18.64s/it]

 27%|██▋       | 838/3107 [4:19:20<11:08:28, 17.68s/it]


 27%|██▋       | 840/3107 [4:19:49<10:06:43, 16.06s/it]
{'loss': 0.8927, 'grad_norm': 0.11888178796652162, 'learning_rate': 0.0001712431847094073, 'epoch': 0.27}

 27%|██▋       | 841/3107 [4:20:12<11:27:54, 18.21s/it]


 27%|██▋       | 843/3107 [4:20:53<12:17:09, 19.54s/it]

 27%|██▋       | 844/3107 [4:21:11<11:58:50, 19.06s/it]

 27%|██▋       | 845/3107 [4:21:29<11:45:23, 18.71s/it]

 27%|██▋       | 846/3107 [4:21:51<12:17:31, 19.57s/it]

 27%|██▋       | 847/3107 [4:22:25<15:02:45, 23.97s/it]

 27%|██▋       | 848/3107 [4:22:42<13:41:13, 21.81s/it]

 27%|██▋       | 849/3107 [4:23:13<15:27:21, 24.64s/it]
[2024-05-30 05:02:41,147] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7259, 'grad_norm': 0.1377456105068618, 'learning_rate': 0.0001705815354976135, 'epoch': 0.27}

 27%|██▋       | 850/3107 [4:23:26<13:21:44, 21.31s/it]


 27%|██▋       | 852/3107 [4:24:01<11:51:48, 18.94s/it]
{'loss': 0.953, 'grad_norm': 0.13029619428261413, 'learning_rate': 0.00017035960165026666, 'epoch': 0.27}

 27%|██▋       | 853/3107 [4:24:24<12:41:55, 20.28s/it]


 28%|██▊       | 855/3107 [4:25:14<13:42:43, 21.92s/it]
{'loss': 0.9489, 'grad_norm': 0.12755375561934373, 'learning_rate': 0.00017013697936147187, 'epoch': 0.28}

 28%|██▊       | 856/3107 [4:25:32<13:01:26, 20.83s/it]


 28%|██▊       | 858/3107 [4:26:07<12:03:46, 19.31s/it]

 28%|██▊       | 859/3107 [4:26:30<12:38:18, 20.24s/it]

 28%|██▊       | 860/3107 [4:26:53<13:16:30, 21.27s/it]
{'loss': 0.8677, 'grad_norm': 0.1405719180189319, 'learning_rate': 0.0001697644182629332, 'epoch': 0.28}


 28%|██▊       | 862/3107 [4:27:34<13:00:47, 20.87s/it]
{'loss': 0.9865, 'grad_norm': 0.12770876610717222, 'learning_rate': 0.00016961486233053403, 'epoch': 0.28}

 28%|██▊       | 863/3107 [4:27:50<12:12:04, 19.57s/it]


 28%|██▊       | 865/3107 [4:28:26<11:27:57, 18.41s/it]

 28%|██▊       | 866/3107 [4:28:39<10:32:29, 16.93s/it]
{'loss': 0.8052, 'grad_norm': 0.13978851966963396, 'learning_rate': 0.00016931484291106793, 'epoch': 0.28}

 28%|██▊       | 867/3107 [4:28:55<10:14:22, 16.46s/it]


 28%|██▊       | 869/3107 [4:29:27<10:12:44, 16.43s/it]
{'loss': 0.8724, 'grad_norm': 0.13820643389073364, 'learning_rate': 0.0001690890368054082, 'epoch': 0.28}


 28%|██▊       | 871/3107 [4:30:26<14:18:59, 23.05s/it]

 28%|██▊       | 872/3107 [4:30:48<14:08:13, 22.77s/it]

 28%|██▊       | 873/3107 [4:31:06<13:14:46, 21.35s/it]
{'loss': 0.8782, 'grad_norm': 0.13856411861359594, 'learning_rate': 0.00016878691081041502, 'epoch': 0.28}


 28%|██▊       | 875/3107 [4:31:57<14:09:31, 22.84s/it]
{'loss': 0.8457, 'grad_norm': 0.13415655994033937, 'learning_rate': 0.00016863539878173914, 'epoch': 0.28}


 28%|██▊       | 877/3107 [4:32:33<12:31:39, 20.22s/it]
{'loss': 1.0193, 'grad_norm': 0.13118177363578276, 'learning_rate': 0.0001684835882770211, 'epoch': 0.28}

 28%|██▊       | 878/3107 [4:32:48<11:34:33, 18.70s/it]


 28%|██▊       | 880/3107 [4:33:24<11:03:45, 17.88s/it]

 28%|██▊       | 881/3107 [4:33:44<11:22:49, 18.40s/it]
{'loss': 0.9042, 'grad_norm': 0.13065915804194494, 'learning_rate': 0.00016817907448147819, 'epoch': 0.28}


 28%|██▊       | 883/3107 [4:34:14<10:15:06, 16.59s/it]

 28%|██▊       | 884/3107 [4:34:38<11:44:58, 19.03s/it]

 28%|██▊       | 885/3107 [4:35:00<12:14:54, 19.84s/it]
{'loss': 0.8324, 'grad_norm': 0.12615344519574243, 'learning_rate': 0.0001678733747207599, 'epoch': 0.28}


 29%|██▊       | 887/3107 [4:35:46<13:15:27, 21.50s/it]
{'loss': 0.7882, 'grad_norm': 0.141967744240709, 'learning_rate': 0.0001677200817644077, 'epoch': 0.29}
[2024-05-30 05:15:37,411] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 29%|██▊       | 889/3107 [4:36:24<12:16:37, 19.93s/it]
{'loss': 0.9272, 'grad_norm': 0.1389855215814908, 'learning_rate': 0.00016756649431246953, 'epoch': 0.29}


 29%|██▊       | 891/3107 [4:36:51<10:17:12, 16.71s/it]
{'loss': 0.8232, 'grad_norm': 0.13309141467975633, 'learning_rate': 0.00016741261303285397, 'epoch': 0.29}


 29%|██▊       | 893/3107 [4:37:20<9:44:42, 15.85s/it]

 29%|██▉       | 894/3107 [4:37:40<10:27:54, 17.02s/it]
{'loss': 0.8891, 'grad_norm': 0.1415278144408751, 'learning_rate': 0.00016718124165072953, 'epoch': 0.29}


 29%|██▉       | 896/3107 [4:38:18<11:01:30, 17.95s/it]
{'loss': 0.8221, 'grad_norm': 0.1253066891359295, 'learning_rate': 0.00016702662873239907, 'epoch': 0.29}


 29%|██▉       | 898/3107 [4:38:58<11:34:02, 18.85s/it]

 29%|██▉       | 899/3107 [4:39:11<10:26:35, 17.03s/it]
{'loss': 0.8944, 'grad_norm': 0.13989946599681163, 'learning_rate': 0.0001667941630404517, 'epoch': 0.29}

 29%|██▉       | 900/3107 [4:39:35<11:48:08, 19.25s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 29%|██▉       | 901/3107 [4:40:18<16:12:21, 26.45s/it]
{'loss': 0.8281, 'grad_norm': 0.1332826397904487, 'learning_rate': 0.0001666388226856989, 'epoch': 0.29}

 29%|██▉       | 902/3107 [4:40:35<14:24:04, 23.51s/it]


 29%|██▉       | 904/3107 [4:41:16<13:20:18, 21.80s/it]

 29%|██▉       | 905/3107 [4:41:30<11:57:09, 19.54s/it]
{'loss': 1.0081, 'grad_norm': 0.14593683535579977, 'learning_rate': 0.00016632727327250182, 'epoch': 0.29}


 29%|██▉       | 907/3107 [4:42:08<11:46:53, 19.28s/it]
{'loss': 0.7115, 'grad_norm': 0.13173102973474313, 'learning_rate': 0.0001661710655688983, 'epoch': 0.29}

 29%|██▉       | 908/3107 [4:42:25<11:25:27, 18.70s/it]

 29%|██▉       | 909/3107 [4:42:39<10:33:44, 17.30s/it]


 29%|██▉       | 911/3107 [4:43:18<11:24:21, 18.70s/it]

 29%|██▉       | 912/3107 [4:43:32<10:29:33, 17.21s/it]

 29%|██▉       | 913/3107 [4:43:44<9:36:30, 15.77s/it]

 29%|██▉       | 914/3107 [4:44:01<9:43:22, 15.96s/it]
{'loss': 0.812, 'grad_norm': 0.1384625326149524, 'learning_rate': 0.0001656220769706146, 'epoch': 0.29}


 29%|██▉       | 916/3107 [4:44:47<11:46:09, 19.34s/it]
{'loss': 0.831, 'grad_norm': 0.12215044715128257, 'learning_rate': 0.00016546457971748898, 'epoch': 0.29}


 30%|██▉       | 918/3107 [4:45:32<13:03:53, 21.49s/it]

 30%|██▉       | 919/3107 [4:45:53<12:52:31, 21.18s/it]

 30%|██▉       | 920/3107 [4:46:06<11:27:53, 18.87s/it]
{'loss': 0.8637, 'grad_norm': 0.12882129001960885, 'learning_rate': 0.00016514873183626078, 'epoch': 0.3}


 30%|██▉       | 922/3107 [4:46:43<11:09:53, 18.40s/it]

 30%|██▉       | 923/3107 [4:46:58<10:39:24, 17.57s/it]

 30%|██▉       | 924/3107 [4:47:17<10:47:31, 17.80s/it]

 30%|██▉       | 925/3107 [4:47:36<11:06:54, 18.34s/it]
{'loss': 0.9002, 'grad_norm': 0.13598950901451026, 'learning_rate': 0.00016475232899369454, 'epoch': 0.3}


 30%|██▉       | 927/3107 [4:48:08<10:25:12, 17.21s/it]

 30%|██▉       | 928/3107 [4:48:23<9:57:17, 16.45s/it]

 30%|██▉       | 929/3107 [4:48:35<9:11:36, 15.20s/it]
{'loss': 0.857, 'grad_norm': 0.14925827231776218, 'learning_rate': 0.00016443393904980242, 'epoch': 0.3}

 30%|██▉       | 930/3107 [4:48:57<10:26:26, 17.27s/it]

 30%|██▉       | 931/3107 [4:49:14<10:17:19, 17.02s/it]

 30%|██▉       | 932/3107 [4:49:26<9:25:26, 15.60s/it]


 30%|███       | 934/3107 [4:49:55<8:54:47, 14.77s/it]

 30%|███       | 935/3107 [4:50:09<8:53:59, 14.75s/it]

 30%|███       | 936/3107 [4:50:29<9:45:41, 16.19s/it]

 30%|███       | 937/3107 [4:50:47<10:10:51, 16.89s/it]

 30%|███       | 938/3107 [4:51:07<10:45:20, 17.85s/it]
{'loss': 0.8113, 'grad_norm': 0.13687953457676785, 'learning_rate': 0.00016371347215014432, 'epoch': 0.3}


 30%|███       | 940/3107 [4:51:41<10:22:05, 17.22s/it]

 30%|███       | 941/3107 [4:51:57<10:14:40, 17.03s/it]
{'loss': 0.9267, 'grad_norm': 0.14479924715712325, 'learning_rate': 0.000163472066554395, 'epoch': 0.3}


 30%|███       | 943/3107 [4:52:21<8:39:19, 14.40s/it]
{'loss': 0.9328, 'grad_norm': 0.1254007257650541, 'learning_rate': 0.00016331078431695842, 'epoch': 0.3}

 30%|███       | 944/3107 [4:52:40<9:35:40, 15.97s/it]

 30%|███       | 945/3107 [4:53:00<10:11:58, 16.98s/it]

 30%|███       | 946/3107 [4:53:24<11:27:31, 19.09s/it]


 31%|███       | 948/3107 [4:53:59<10:50:20, 18.07s/it]

 31%|███       | 949/3107 [4:54:21<11:32:52, 19.26s/it]

 31%|███       | 950/3107 [4:54:36<10:42:42, 17.88s/it]
{'loss': 0.8515, 'grad_norm': 0.13400583029049265, 'learning_rate': 0.00016274413294823875, 'epoch': 0.31}

 31%|███       | 951/3107 [4:54:58<11:31:14, 19.24s/it]


 31%|███       | 953/3107 [4:55:28<10:10:30, 17.01s/it]

 31%|███       | 954/3107 [4:55:49<10:57:22, 18.32s/it]

 31%|███       | 955/3107 [4:56:07<10:54:51, 18.26s/it]

 31%|███       | 956/3107 [4:56:30<11:40:29, 19.54s/it]

 31%|███       | 957/3107 [4:56:44<10:42:29, 17.93s/it]
{'loss': 0.926, 'grad_norm': 0.12648892439024678, 'learning_rate': 0.00016217413909998845, 'epoch': 0.31}

 31%|███       | 958/3107 [4:57:02<10:43:25, 17.96s/it]

 31%|███       | 959/3107 [4:57:24<11:27:44, 19.21s/it]

 31%|███       | 960/3107 [4:57:45<11:41:16, 19.60s/it]


 31%|███       | 962/3107 [4:58:30<12:38:01, 21.20s/it]

 31%|███       | 963/3107 [4:58:41<10:54:41, 18.32s/it]
{'loss': 0.7903, 'grad_norm': 0.14522627971486518, 'learning_rate': 0.0001616829356155123, 'epoch': 0.31}

 31%|███       | 964/3107 [4:59:02<11:21:52, 19.09s/it]


 31%|███       | 966/3107 [4:59:39<11:04:39, 18.63s/it]

 31%|███       | 967/3107 [4:59:56<10:44:09, 18.06s/it]

 31%|███       | 968/3107 [5:00:10<10:03:17, 16.92s/it]
{'loss': 1.0035, 'grad_norm': 0.14202487062846028, 'learning_rate': 0.00016127175440314596, 'epoch': 0.31}

 31%|███       | 969/3107 [5:00:30<10:38:05, 17.91s/it]


 31%|███▏      | 971/3107 [5:01:14<11:38:33, 19.62s/it]
{'loss': 0.9706, 'grad_norm': 0.1251244492627668, 'learning_rate': 0.00016102424559924473, 'epoch': 0.31}


 31%|███▏      | 973/3107 [5:01:50<11:05:34, 18.71s/it]

 31%|███▏      | 974/3107 [5:02:02<9:57:41, 16.81s/it]
{'loss': 0.8069, 'grad_norm': 0.13688655836424043, 'learning_rate': 0.0001607761396967384, 'epoch': 0.31}


 31%|███▏      | 976/3107 [5:02:33<9:23:33, 15.87s/it]

 31%|███▏      | 977/3107 [5:02:54<10:17:38, 17.40s/it]
{'loss': 0.7793, 'grad_norm': 0.13279863423919758, 'learning_rate': 0.0001605274391232471, 'epoch': 0.31}


 32%|███▏      | 979/3107 [5:03:37<11:27:27, 19.38s/it]

 32%|███▏      | 980/3107 [5:03:58<11:38:07, 19.69s/it]

 32%|███▏      | 981/3107 [5:04:16<11:21:57, 19.25s/it]
{'loss': 0.9645, 'grad_norm': 0.12522222489743887, 'learning_rate': 0.00016019491752161647, 'epoch': 0.32}


 32%|███▏      | 983/3107 [5:04:43<9:45:26, 16.54s/it]

 32%|███▏      | 984/3107 [5:05:04<10:31:05, 17.84s/it]
{'loss': 0.8377, 'grad_norm': 0.1322759080572629, 'learning_rate': 0.00015994483885587902, 'epoch': 0.32}


 32%|███▏      | 986/3107 [5:05:40<10:54:14, 18.51s/it]
{'loss': 0.9454, 'grad_norm': 0.12645542362324447, 'learning_rate': 0.00015977779374020176, 'epoch': 0.32}

 32%|███▏      | 987/3107 [5:05:55<10:14:02, 17.38s/it]

 32%|███▏      | 988/3107 [5:06:05<9:00:52, 15.32s/it]

 32%|███▏      | 989/3107 [5:06:17<8:23:58, 14.28s/it]


 32%|███▏      | 991/3107 [5:06:57<10:23:12, 17.67s/it]
{'loss': 0.8943, 'grad_norm': 0.12892602516091778, 'learning_rate': 0.0001593590452318187, 'epoch': 0.32}


 32%|███▏      | 993/3107 [5:07:38<11:30:05, 19.59s/it]
{'loss': 0.8543, 'grad_norm': 0.12349062447422772, 'learning_rate': 0.00015919109345286005, 'epoch': 0.32}

 32%|███▏      | 994/3107 [5:07:53<10:35:10, 18.04s/it]

 32%|███▏      | 995/3107 [5:08:09<10:18:18, 17.57s/it]

 32%|███▏      | 996/3107 [5:08:31<11:01:04, 18.79s/it]


 32%|███▏      | 998/3107 [5:09:08<11:04:42, 18.91s/it]
{'loss': 0.8795, 'grad_norm': 0.12780506124582194, 'learning_rate': 0.00015877008945722215, 'epoch': 0.32}


 32%|███▏      | 1000/3107 [5:09:47<11:10:39, 19.10s/it]

 32%|███▏      | 1001/3107 [5:10:01<10:17:02, 17.58s/it]
{'loss': 1.0395, 'grad_norm': 0.14079753526632552, 'learning_rate': 0.00015851671960339753, 'epoch': 0.32}


 32%|███▏      | 1003/3107 [5:10:34<10:04:25, 17.24s/it]
{'loss': 0.85, 'grad_norm': 0.1341729323808701, 'learning_rate': 0.00015834748812345418, 'epoch': 0.32}


 32%|███▏      | 1005/3107 [5:11:13<10:32:00, 18.04s/it]
{'loss': 0.8127, 'grad_norm': 0.14431019121089397, 'learning_rate': 0.0001581780029066981, 'epoch': 0.32}

 32%|███▏      | 1006/3107 [5:11:29<10:13:39, 17.52s/it]

 32%|███▏      | 1007/3107 [5:11:53<11:22:31, 19.50s/it]

 32%|███▏      | 1008/3107 [5:12:07<10:25:05, 17.87s/it]


 33%|███▎      | 1010/3107 [5:12:34<9:06:32, 15.64s/it]

 33%|███▎      | 1011/3107 [5:12:51<9:14:03, 15.86s/it]
{'loss': 0.824, 'grad_norm': 0.12713422561703963, 'learning_rate': 0.00015766803221148673, 'epoch': 0.33}

 33%|███▎      | 1012/3107 [5:13:07<9:23:04, 16.13s/it]


 33%|███▎      | 1014/3107 [5:13:50<11:00:04, 18.92s/it]

 33%|███▎      | 1015/3107 [5:14:12<11:35:51, 19.96s/it]

 33%|███▎      | 1016/3107 [5:14:41<13:02:02, 22.44s/it]
{'loss': 0.8499, 'grad_norm': 0.12838920457514433, 'learning_rate': 0.00015724133166084397, 'epoch': 0.33}

 33%|███▎      | 1017/3107 [5:15:06<13:30:07, 23.26s/it]


 33%|███▎      | 1019/3107 [5:15:39<11:39:42, 20.11s/it]
{'loss': 0.8831, 'grad_norm': 0.1355061227540793, 'learning_rate': 0.00015698456380852915, 'epoch': 0.33}

 33%|███▎      | 1020/3107 [5:16:07<13:07:37, 22.64s/it]


 33%|███▎      | 1022/3107 [5:16:44<11:53:50, 20.54s/it]
{'loss': 1.0481, 'grad_norm': 0.13912322388344583, 'learning_rate': 0.00015672723838433078, 'epoch': 0.33}

 33%|███▎      | 1023/3107 [5:16:57<10:36:21, 18.32s/it]


 33%|███▎      | 1025/3107 [5:17:21<8:43:06, 15.08s/it]

 33%|███▎      | 1026/3107 [5:17:40<9:29:06, 16.41s/it]
{'loss': 0.913, 'grad_norm': 0.1294355392963894, 'learning_rate': 0.00015638327483727887, 'epoch': 0.33}

 33%|███▎      | 1027/3107 [5:18:01<10:16:52, 17.79s/it]

 33%|███▎      | 1028/3107 [5:18:20<10:21:11, 17.93s/it]

 33%|███▎      | 1029/3107 [5:18:34<9:43:50, 16.86s/it]

 33%|███▎      | 1030/3107 [5:18:54<10:17:00, 17.82s/it]


 33%|███▎      | 1032/3107 [5:19:25<9:34:42, 16.62s/it]
{'loss': 0.8272, 'grad_norm': 0.14100505875958994, 'learning_rate': 0.00015586549243060428, 'epoch': 0.33}


 33%|███▎      | 1034/3107 [5:19:55<9:07:29, 15.85s/it]

 33%|███▎      | 1035/3107 [5:20:14<9:41:06, 16.83s/it]

 33%|███▎      | 1036/3107 [5:20:27<8:56:20, 15.54s/it]

 33%|███▎      | 1037/3107 [5:20:46<9:37:34, 16.74s/it]

 33%|███▎      | 1038/3107 [5:21:07<10:14:37, 17.82s/it]
{'loss': 0.8411, 'grad_norm': 0.13755974007703328, 'learning_rate': 0.0001553455235404638, 'epoch': 0.33}

 33%|███▎      | 1039/3107 [5:21:24<10:07:08, 17.62s/it]


 34%|███▎      | 1041/3107 [5:21:55<9:29:09, 16.53s/it]

 34%|███▎      | 1042/3107 [5:22:11<9:19:15, 16.25s/it]
{'loss': 0.8652, 'grad_norm': 0.1403978473067407, 'learning_rate': 0.00015499767294691093, 'epoch': 0.34}

 34%|███▎      | 1043/3107 [5:22:30<9:47:15, 17.07s/it]

 34%|███▎      | 1044/3107 [5:22:50<10:18:24, 17.99s/it]


 34%|███▎      | 1046/3107 [5:23:22<9:50:24, 17.19s/it]
{'loss': 0.8582, 'grad_norm': 0.13549903862554688, 'learning_rate': 0.0001546488656767628, 'epoch': 0.34}

 34%|███▎      | 1047/3107 [5:23:50<11:34:12, 20.22s/it]

 34%|███▎      | 1048/3107 [5:24:04<10:35:19, 18.51s/it]


 34%|███▍      | 1050/3107 [5:24:31<8:57:33, 15.68s/it]

 34%|███▍      | 1051/3107 [5:24:54<10:10:11, 17.81s/it]
{'loss': 0.7035, 'grad_norm': 0.1254287618845561, 'learning_rate': 0.00015421152050757573, 'epoch': 0.34}


 34%|███▍      | 1053/3107 [5:25:33<10:34:14, 18.53s/it]
{'loss': 0.8137, 'grad_norm': 0.12591092058645584, 'learning_rate': 0.00015403616921003336, 'epoch': 0.34}


 34%|███▍      | 1055/3107 [5:26:17<11:32:06, 20.24s/it]

 34%|███▍      | 1056/3107 [5:26:36<11:18:22, 19.85s/it]
{'loss': 0.9287, 'grad_norm': 0.12986127750342044, 'learning_rate': 0.00015377270189961058, 'epoch': 0.34}


 34%|███▍      | 1058/3107 [5:27:13<10:49:39, 19.02s/it]

 34%|███▍      | 1059/3107 [5:27:31<10:37:04, 18.66s/it]

 34%|███▍      | 1060/3107 [5:27:49<10:26:58, 18.38s/it]

 34%|███▍      | 1061/3107 [5:28:05<10:07:01, 17.80s/it]

 34%|███▍      | 1062/3107 [5:28:20<9:31:31, 16.77s/it]

 34%|███▍      | 1063/3107 [5:28:36<9:24:31, 16.57s/it]

 34%|███▍      | 1064/3107 [5:29:01<10:55:57, 19.26s/it]
{'loss': 0.678, 'grad_norm': 0.13195849401753051, 'learning_rate': 0.00015306755715849293, 'epoch': 0.34}


 34%|███▍      | 1066/3107 [5:29:42<11:12:26, 19.77s/it]

 34%|███▍      | 1067/3107 [5:29:57<10:32:22, 18.60s/it]

 34%|███▍      | 1068/3107 [5:30:11<9:40:47, 17.09s/it]
{'loss': 0.7623, 'grad_norm': 0.1363870967500187, 'learning_rate': 0.0001527135970636564, 'epoch': 0.34}

 34%|███▍      | 1069/3107 [5:30:26<9:23:13, 16.58s/it]

 34%|███▍      | 1070/3107 [5:30:43<9:20:29, 16.51s/it]

 34%|███▍      | 1071/3107 [5:31:08<10:51:00, 19.18s/it]

 35%|███▍      | 1072/3107 [5:31:20<9:37:23, 17.02s/it]

 35%|███▍      | 1073/3107 [5:31:35<9:12:32, 16.30s/it]


 35%|███▍      | 1075/3107 [5:32:01<8:24:57, 14.91s/it]
{'loss': 0.7987, 'grad_norm': 0.13930115517322203, 'learning_rate': 0.00015209196421063465, 'epoch': 0.35}

 35%|███▍      | 1076/3107 [5:32:13<7:47:46, 13.82s/it]

 35%|███▍      | 1077/3107 [5:32:28<8:07:39, 14.41s/it]


 35%|███▍      | 1079/3107 [5:33:14<10:49:24, 19.21s/it]

 35%|███▍      | 1080/3107 [5:33:34<11:02:49, 19.62s/it]

 35%|███▍      | 1081/3107 [5:33:51<10:36:58, 18.86s/it]

 35%|███▍      | 1082/3107 [5:34:06<9:50:43, 17.50s/it]

 35%|███▍      | 1083/3107 [5:34:17<8:51:00, 15.74s/it]

 35%|███▍      | 1084/3107 [5:34:31<8:32:39, 15.21s/it]

 35%|███▍      | 1085/3107 [5:35:00<10:50:22, 19.30s/it]
{'loss': 0.8527, 'grad_norm': 0.12179871414167563, 'learning_rate': 0.00015119911170264624, 'epoch': 0.35}


 35%|███▍      | 1087/3107 [5:35:52<12:13:13, 21.78s/it]

 35%|███▌      | 1088/3107 [5:36:10<11:39:04, 20.78s/it]
{'loss': 0.8207, 'grad_norm': 0.13066485327677657, 'learning_rate': 0.000150930166105802, 'epoch': 0.35}

 35%|███▌      | 1089/3107 [5:36:33<11:57:52, 21.34s/it]


 35%|███▌      | 1091/3107 [5:37:16<12:12:13, 21.79s/it]

 35%|███▌      | 1092/3107 [5:37:38<12:12:29, 21.81s/it]
{'loss': 0.775, 'grad_norm': 0.1252954187458712, 'learning_rate': 0.0001505707972491822, 'epoch': 0.35}


 35%|███▌      | 1094/3107 [5:38:13<11:06:31, 19.87s/it]
{'loss': 0.8148, 'grad_norm': 0.12591748386953402, 'learning_rate': 0.00015039078255267628, 'epoch': 0.35}

 35%|███▌      | 1095/3107 [5:38:29<10:23:07, 18.58s/it]


 35%|███▌      | 1097/3107 [5:39:06<10:14:03, 18.33s/it]
{'loss': 0.8894, 'grad_norm': 0.11996282385477845, 'learning_rate': 0.0001501203498740539, 'epoch': 0.35}

 35%|███▌      | 1098/3107 [5:39:19<9:21:57, 16.78s/it]


 35%|███▌      | 1100/3107 [5:39:43<8:00:02, 14.35s/it]

 35%|███▌      | 1101/3107 [5:40:16<11:05:29, 19.91s/it]
{'loss': 0.7889, 'grad_norm': 0.12235416944499872, 'learning_rate': 0.00014975901057004854, 'epoch': 0.35}

 35%|███▌      | 1102/3107 [5:40:29<9:51:08, 17.69s/it]


 36%|███▌      | 1104/3107 [5:41:17<11:57:21, 21.49s/it]

 36%|███▌      | 1105/3107 [5:41:40<12:18:45, 22.14s/it]
{'loss': 0.7228, 'grad_norm': 0.13436011586180757, 'learning_rate': 0.00014939680571522502, 'epoch': 0.36}

 36%|███▌      | 1106/3107 [5:41:53<10:48:52, 19.46s/it]


 36%|███▌      | 1108/3107 [5:42:40<12:22:22, 22.28s/it]

 36%|███▌      | 1109/3107 [5:42:58<11:38:30, 20.98s/it]
{'loss': 0.9563, 'grad_norm': 0.13908383372368846, 'learning_rate': 0.00014903374161008464, 'epoch': 0.36}


 36%|███▌      | 1111/3107 [5:43:43<11:56:52, 21.55s/it]
{'loss': 0.8869, 'grad_norm': 0.14991084097602853, 'learning_rate': 0.00014885188931164215, 'epoch': 0.36}

 36%|███▌      | 1112/3107 [5:43:59<11:03:29, 19.95s/it]

 36%|███▌      | 1113/3107 [5:44:14<10:10:18, 18.36s/it]

 36%|███▌      | 1114/3107 [5:44:37<10:58:55, 19.84s/it]


 36%|███▌      | 1116/3107 [5:45:21<11:39:13, 21.07s/it]
{'loss': 0.8303, 'grad_norm': 0.14465238436354916, 'learning_rate': 0.00014839633085906716, 'epoch': 0.36}

 36%|███▌      | 1117/3107 [5:45:38<11:00:32, 19.92s/it]

 36%|███▌      | 1118/3107 [5:45:53<10:13:27, 18.51s/it]

 36%|███▌      | 1119/3107 [5:46:08<9:34:43, 17.35s/it]

 36%|███▌      | 1120/3107 [5:46:32<10:36:28, 19.22s/it]

 36%|███▌      | 1121/3107 [5:46:52<10:44:35, 19.47s/it]

 36%|███▌      | 1122/3107 [5:47:08<10:10:51, 18.46s/it]


 36%|███▌      | 1124/3107 [5:47:43<9:56:32, 18.05s/it]
{'loss': 0.8176, 'grad_norm': 0.13140826711601994, 'learning_rate': 0.00014766470649800904, 'epoch': 0.36}


 36%|███▌      | 1126/3107 [5:48:15<9:22:12, 17.03s/it]
{'loss': 0.9069, 'grad_norm': 0.1339130127469802, 'learning_rate': 0.00014748128021595543, 'epoch': 0.36}


 36%|███▋      | 1128/3107 [5:48:59<10:40:32, 19.42s/it]
{'loss': 0.9164, 'grad_norm': 0.1450692495268039, 'learning_rate': 0.00014729764745117062, 'epoch': 0.36}

 36%|███▋      | 1129/3107 [5:49:16<10:12:09, 18.57s/it]

 36%|███▋      | 1130/3107 [5:49:30<9:30:29, 17.31s/it]


 36%|███▋      | 1132/3107 [5:50:01<9:04:28, 16.54s/it]

 36%|███▋      | 1133/3107 [5:50:23<9:54:41, 18.08s/it]
{'loss': 0.7482, 'grad_norm': 0.11558734087802253, 'learning_rate': 0.00014683766742005746, 'epoch': 0.36}

 36%|███▋      | 1134/3107 [5:50:47<10:50:20, 19.78s/it]


 37%|███▋      | 1136/3107 [5:51:25<10:30:30, 19.19s/it]

 37%|███▋      | 1137/3107 [5:51:39<9:44:46, 17.81s/it]
{'loss': 0.8954, 'grad_norm': 0.12791321676063347, 'learning_rate': 0.0001464687662183686, 'epoch': 0.37}

 37%|███▋      | 1138/3107 [5:51:57<9:40:00, 17.67s/it]


 37%|███▋      | 1140/3107 [5:52:35<9:58:43, 18.26s/it]

 37%|███▋      | 1141/3107 [5:52:51<9:38:11, 17.65s/it]
{'loss': 0.8578, 'grad_norm': 0.14695556353372316, 'learning_rate': 0.00014609905669918792, 'epoch': 0.37}

 37%|███▋      | 1142/3107 [5:53:08<9:26:31, 17.30s/it]

 37%|███▋      | 1143/3107 [5:53:20<8:35:14, 15.74s/it]

 37%|███▋      | 1144/3107 [5:53:43<9:42:44, 17.81s/it]

 37%|███▋      | 1145/3107 [5:54:04<10:21:32, 19.01s/it]


 37%|███▋      | 1147/3107 [5:54:37<9:34:13, 17.58s/it]
{'loss': 0.8429, 'grad_norm': 0.15350143238952868, 'learning_rate': 0.00014554299089684523, 'epoch': 0.37}

 37%|███▋      | 1148/3107 [5:55:00<10:30:43, 19.32s/it]


 37%|███▋      | 1150/3107 [5:55:47<11:43:37, 21.57s/it]

 37%|███▋      | 1151/3107 [5:56:03<10:48:44, 19.90s/it]
{'loss': 0.8887, 'grad_norm': 0.15039710886251476, 'learning_rate': 0.0001451712887502338, 'epoch': 0.37}


 37%|███▋      | 1153/3107 [5:56:53<12:20:17, 22.73s/it]
{'loss': 0.8913, 'grad_norm': 0.12805335807031118, 'learning_rate': 0.00014498514261676446, 'epoch': 0.37}

 37%|███▋      | 1154/3107 [5:57:12<11:41:11, 21.54s/it]

 37%|███▋      | 1155/3107 [5:57:32<11:23:22, 21.01s/it]

 37%|███▋      | 1156/3107 [5:57:50<10:54:14, 20.12s/it]

 37%|███▋      | 1157/3107 [5:58:08<10:35:39, 19.56s/it]


 37%|███▋      | 1159/3107 [5:58:48<10:38:31, 19.67s/it]

 37%|███▋      | 1160/3107 [5:59:03<9:56:53, 18.39s/it]
{'loss': 0.8914, 'grad_norm': 0.12488624870247504, 'learning_rate': 0.0001443320959012327, 'epoch': 0.37}

 37%|███▋      | 1161/3107 [5:59:19<9:28:39, 17.53s/it]

 37%|███▋      | 1162/3107 [5:59:37<9:33:40, 17.70s/it]


 37%|███▋      | 1164/3107 [6:00:18<9:57:51, 18.46s/it]
{'loss': 0.9111, 'grad_norm': 0.13697180758284586, 'learning_rate': 0.00014395786378428033, 'epoch': 0.37}


 38%|███▊      | 1166/3107 [6:00:58<10:37:50, 19.72s/it]
[2024-05-30 06:40:25,938] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 1167/3107 [6:01:21<11:16:29, 20.92s/it]
[2024-05-30 06:40:49,684] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7733, 'grad_norm': 0.13153228652249377, 'learning_rate': 0.00014367668754460176, 'epoch': 0.38}


 38%|███▊      | 1169/3107 [6:01:58<10:42:10, 19.88s/it]

 38%|███▊      | 1170/3107 [6:02:20<11:05:56, 20.63s/it]
{'loss': 0.9361, 'grad_norm': 0.13256843597345136, 'learning_rate': 0.00014339508394545401, 'epoch': 0.38}


 38%|███▊      | 1172/3107 [6:02:51<9:46:48, 18.20s/it]
{'loss': 0.8541, 'grad_norm': 0.13818266348630123, 'learning_rate': 0.00014320711215127277, 'epoch': 0.38}

 38%|███▊      | 1173/3107 [6:03:17<11:01:29, 20.52s/it]
{'loss': 0.874, 'grad_norm': 0.1399809319265049, 'learning_rate': 0.00014311305574221916, 'epoch': 0.38}


 38%|███▊      | 1175/3107 [6:04:02<11:25:51, 21.30s/it]
{'loss': 0.884, 'grad_norm': 0.1234244189082843, 'learning_rate': 0.00014292480241146716, 'epoch': 0.38}

 38%|███▊      | 1176/3107 [6:04:17<10:24:54, 19.42s/it]


 38%|███▊      | 1178/3107 [6:04:54<10:01:57, 18.72s/it]
{'loss': 0.6902, 'grad_norm': 0.13043236205887218, 'learning_rate': 0.00014264207266909712, 'epoch': 0.38}

 38%|███▊      | 1179/3107 [6:05:15<10:19:46, 19.29s/it]

 38%|███▊      | 1180/3107 [6:05:37<10:52:17, 20.31s/it]


 38%|███▊      | 1182/3107 [6:06:24<11:43:17, 21.92s/it]
{'loss': 0.8645, 'grad_norm': 0.1276120399653435, 'learning_rate': 0.00014226445112397042, 'epoch': 0.38}


 38%|███▊      | 1184/3107 [6:07:00<10:29:23, 19.64s/it]

 38%|███▊      | 1185/3107 [6:07:20<10:28:13, 19.61s/it]

 38%|███▊      | 1186/3107 [6:07:36<9:55:58, 18.61s/it]
{'loss': 0.9112, 'grad_norm': 0.1300513186521633, 'learning_rate': 0.00014188609439480708, 'epoch': 0.38}

 38%|███▊      | 1187/3107 [6:07:55<10:03:45, 18.87s/it]


 38%|███▊      | 1189/3107 [6:08:30<9:47:32, 18.38s/it]
{'loss': 0.7697, 'grad_norm': 0.1293504482272521, 'learning_rate': 0.00014160184834209296, 'epoch': 0.38}

 38%|███▊      | 1190/3107 [6:08:46<9:19:59, 17.53s/it]

 38%|███▊      | 1191/3107 [6:09:17<11:34:02, 21.73s/it]

 38%|███▊      | 1192/3107 [6:09:37<11:16:16, 21.19s/it]

 38%|███▊      | 1193/3107 [6:10:00<11:29:26, 21.61s/it]

 38%|███▊      | 1194/3107 [6:10:22<11:30:56, 21.67s/it]

 38%|███▊      | 1195/3107 [6:10:35<10:09:39, 19.13s/it]

 38%|███▊      | 1196/3107 [6:10:51<9:46:10, 18.40s/it]


 39%|███▊      | 1198/3107 [6:11:29<9:39:47, 18.22s/it]

 39%|███▊      | 1199/3107 [6:11:54<10:48:15, 20.39s/it]

 39%|███▊      | 1200/3107 [6:12:09<9:52:44, 18.65s/it]
 39%|███▊      | 1200/3107 [6:12:09<9:52:44, 18.65s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.7611, 'grad_norm': 0.12423209371724127, 'learning_rate': 0.00014046082142347863, 'epoch': 0.39}

 39%|███▊      | 1202/3107 [6:13:15<12:54:56, 24.41s/it]

 39%|███▊      | 1203/3107 [6:13:32<11:47:29, 22.30s/it]

 39%|███▉      | 1204/3107 [6:13:52<11:27:05, 21.66s/it]

 39%|███▉      | 1205/3107 [6:14:10<10:50:06, 20.51s/it]
{'loss': 0.918, 'grad_norm': 0.13562694802734312, 'learning_rate': 0.00014007906266321615, 'epoch': 0.39}

 39%|███▉      | 1206/3107 [6:14:26<10:04:39, 19.08s/it]


 39%|███▉      | 1208/3107 [6:15:10<10:47:49, 20.47s/it]

 39%|███▉      | 1209/3107 [6:15:26<10:06:28, 19.17s/it]

 39%|███▉      | 1210/3107 [6:15:36<8:37:46, 16.38s/it]

 39%|███▉      | 1211/3107 [6:15:55<8:59:27, 17.07s/it]
{'loss': 0.8921, 'grad_norm': 0.12813145393509423, 'learning_rate': 0.0001395051194084925, 'epoch': 0.39}


 39%|███▉      | 1213/3107 [6:16:20<7:51:18, 14.93s/it]
{'loss': 0.8493, 'grad_norm': 0.1434980254343791, 'learning_rate': 0.00013931346028689908, 'epoch': 0.39}


 39%|███▉      | 1215/3107 [6:16:55<8:18:30, 15.81s/it]
{'loss': 0.7377, 'grad_norm': 0.1380802181513781, 'learning_rate': 0.00013912163020212575, 'epoch': 0.39}

 39%|███▉      | 1216/3107 [6:17:11<8:26:26, 16.07s/it]


 39%|███▉      | 1218/3107 [6:17:39<7:39:11, 14.59s/it]
{'loss': 1.0072, 'grad_norm': 0.14279421158529823, 'learning_rate': 0.00013883356634405124, 'epoch': 0.39}
[2024-05-30 06:57:32,389] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 1219/3107 [6:18:04<9:23:25, 17.91s/it]

 39%|███▉      | 1220/3107 [6:18:20<9:07:17, 17.40s/it]

 39%|███▉      | 1221/3107 [6:18:45<10:13:24, 19.51s/it]

 39%|███▉      | 1222/3107 [6:19:02<9:54:54, 18.94s/it]

 39%|███▉      | 1223/3107 [6:19:17<9:10:30, 17.53s/it]
{'loss': 0.7823, 'grad_norm': 0.1450397324339709, 'learning_rate': 0.00013835261692661044, 'epoch': 0.39}

 39%|███▉      | 1224/3107 [6:19:30<8:33:31, 16.36s/it]

 39%|███▉      | 1225/3107 [6:19:44<8:06:22, 15.51s/it]


 39%|███▉      | 1227/3107 [6:20:15<7:58:05, 15.26s/it]
{'loss': 0.827, 'grad_norm': 0.1336808142463761, 'learning_rate': 0.00013796710623433696, 'epoch': 0.39}

 40%|███▉      | 1228/3107 [6:20:39<9:23:46, 18.00s/it]

 40%|███▉      | 1229/3107 [6:21:02<10:07:29, 19.41s/it]


 40%|███▉      | 1231/3107 [6:21:35<9:06:46, 17.49s/it]
{'loss': 0.764, 'grad_norm': 0.1450066718876173, 'learning_rate': 0.00013758093510972194, 'epoch': 0.4}

 40%|███▉      | 1232/3107 [6:21:48<8:25:32, 16.18s/it]

 40%|███▉      | 1233/3107 [6:22:15<10:00:06, 19.21s/it]

 40%|███▉      | 1234/3107 [6:22:34<10:00:08, 19.23s/it]

 40%|███▉      | 1235/3107 [6:22:50<9:30:32, 18.29s/it]


 40%|███▉      | 1237/3107 [6:23:19<8:31:13, 16.40s/it]

 40%|███▉      | 1238/3107 [6:23:33<8:09:56, 15.73s/it]
{'loss': 0.8611, 'grad_norm': 0.133147179903951, 'learning_rate': 0.00013690356668739293, 'epoch': 0.4}

 40%|███▉      | 1239/3107 [6:23:51<8:26:00, 16.25s/it]


 40%|███▉      | 1241/3107 [6:24:23<8:24:05, 16.21s/it]
{'loss': 0.9539, 'grad_norm': 0.13805097304502748, 'learning_rate': 0.00013661266201752417, 'epoch': 0.4}

 40%|███▉      | 1242/3107 [6:24:40<8:26:43, 16.30s/it]


 40%|████      | 1244/3107 [6:25:13<8:30:03, 16.43s/it]

 40%|████      | 1245/3107 [6:25:31<8:39:23, 16.74s/it]
{'loss': 0.7989, 'grad_norm': 0.12851285181368094, 'learning_rate': 0.00013622423235350428, 'epoch': 0.4}


 40%|████      | 1247/3107 [6:26:05<8:37:10, 16.68s/it]

 40%|████      | 1248/3107 [6:26:23<8:51:32, 17.16s/it]
{'loss': 0.9231, 'grad_norm': 0.14508743906775712, 'learning_rate': 0.00013593249622240365, 'epoch': 0.4}

 40%|████      | 1249/3107 [6:26:43<9:11:59, 17.83s/it]

 40%|████      | 1250/3107 [6:27:01<9:14:05, 17.90s/it]

 40%|████      | 1251/3107 [6:27:18<9:08:43, 17.74s/it]


 40%|████      | 1253/3107 [6:27:53<9:12:36, 17.88s/it]

 40%|████      | 1254/3107 [6:28:10<8:55:37, 17.34s/it]
{'loss': 0.9216, 'grad_norm': 0.14592168933135757, 'learning_rate': 0.000135347972061497, 'epoch': 0.4}

 40%|████      | 1255/3107 [6:28:29<9:12:04, 17.89s/it]


 40%|████      | 1257/3107 [6:29:04<9:06:36, 17.73s/it]

 40%|████      | 1258/3107 [6:29:15<8:09:14, 15.88s/it]
{'loss': 0.8611, 'grad_norm': 0.13384026535742716, 'learning_rate': 0.00013495751928343386, 'epoch': 0.4}

 41%|████      | 1259/3107 [6:29:32<8:19:57, 16.23s/it]


 41%|████      | 1261/3107 [6:29:59<7:36:09, 14.83s/it]
{'loss': 0.9977, 'grad_norm': 0.14049569463548858, 'learning_rate': 0.00013466428027489433, 'epoch': 0.41}

 41%|████      | 1262/3107 [6:30:13<7:24:37, 14.46s/it]

 41%|████      | 1263/3107 [6:30:33<8:13:43, 16.06s/it]

 41%|████      | 1264/3107 [6:31:00<9:57:50, 19.46s/it]

 41%|████      | 1265/3107 [6:31:21<10:07:48, 19.80s/it]

 41%|████      | 1266/3107 [6:31:38<9:44:07, 19.04s/it]

 41%|████      | 1267/3107 [6:31:52<8:58:51, 17.57s/it]

 41%|████      | 1268/3107 [6:32:18<10:17:30, 20.15s/it]


 41%|████      | 1270/3107 [6:32:50<9:08:34, 17.92s/it]
{'loss': 0.8546, 'grad_norm': 0.149962003968924, 'learning_rate': 0.0001337825396834312, 'epoch': 0.41}

 41%|████      | 1271/3107 [6:33:09<9:22:17, 18.38s/it]


 41%|████      | 1273/3107 [6:33:42<8:43:52, 17.14s/it]

 41%|████      | 1274/3107 [6:34:00<8:55:21, 17.52s/it]
{'loss': 0.85, 'grad_norm': 0.14294452938591068, 'learning_rate': 0.00013338969543772892, 'epoch': 0.41}

 41%|████      | 1275/3107 [6:34:18<9:02:51, 17.78s/it]

 41%|████      | 1276/3107 [6:34:36<9:05:01, 17.86s/it]

 41%|████      | 1277/3107 [6:34:56<9:24:40, 18.51s/it]

 41%|████      | 1278/3107 [6:35:15<9:22:15, 18.44s/it]


 41%|████      | 1280/3107 [6:35:58<10:16:34, 20.25s/it]
{'loss': 0.7287, 'grad_norm': 0.14325099321314916, 'learning_rate': 0.00013279934219063714, 'epoch': 0.41}

 41%|████      | 1281/3107 [6:36:25<11:16:28, 22.23s/it]

 41%|████▏     | 1282/3107 [6:36:37<9:43:19, 19.18s/it]

 41%|████▏     | 1283/3107 [6:36:52<9:09:15, 18.07s/it]


 41%|████▏     | 1285/3107 [6:37:28<8:51:46, 17.51s/it]

 41%|████▏     | 1286/3107 [6:38:00<11:04:51, 21.91s/it]

 41%|████▏     | 1287/3107 [6:38:16<10:06:27, 19.99s/it]
{'loss': 0.8172, 'grad_norm': 0.13263676041120598, 'learning_rate': 0.000132108975891168, 'epoch': 0.41}

 41%|████▏     | 1288/3107 [6:38:39<10:35:15, 20.95s/it]

 41%|████▏     | 1289/3107 [6:38:51<9:10:52, 18.18s/it]

 42%|████▏     | 1290/3107 [6:39:05<8:37:35, 17.09s/it]

 42%|████▏     | 1291/3107 [6:39:25<8:57:06, 17.75s/it]

 42%|████▏     | 1292/3107 [6:39:55<10:49:54, 21.48s/it]

 42%|████▏     | 1293/3107 [6:40:13<10:23:50, 20.63s/it]

 42%|████▏     | 1294/3107 [6:40:31<9:53:02, 19.63s/it]

 42%|████▏     | 1295/3107 [6:40:47<9:22:03, 18.61s/it]

 42%|████▏     | 1296/3107 [6:41:01<8:40:45, 17.25s/it]


 42%|████▏     | 1298/3107 [6:41:43<9:27:00, 18.81s/it]
{'loss': 0.9342, 'grad_norm': 0.12310104899612043, 'learning_rate': 0.00013102067284772836, 'epoch': 0.42}

 42%|████▏     | 1299/3107 [6:41:56<8:34:14, 17.07s/it]


 42%|████▏     | 1301/3107 [6:42:34<9:09:59, 18.27s/it]

 42%|████▏     | 1302/3107 [6:42:54<9:25:02, 18.78s/it]
{'loss': 0.847, 'grad_norm': 0.13149074855942708, 'learning_rate': 0.0001306239069997235, 'epoch': 0.42}

 42%|████▏     | 1303/3107 [6:43:11<9:08:01, 18.23s/it]


 42%|████▏     | 1305/3107 [6:43:52<9:40:20, 19.32s/it]

 42%|████▏     | 1306/3107 [6:44:10<9:29:11, 18.96s/it]
{'loss': 0.8665, 'grad_norm': 0.15133457994919144, 'learning_rate': 0.0001302266084532695, 'epoch': 0.42}

 42%|████▏     | 1307/3107 [6:44:34<10:08:53, 20.30s/it]


 42%|████▏     | 1309/3107 [6:45:17<10:16:46, 20.58s/it]
{'loss': 0.9527, 'grad_norm': 0.13525431326391799, 'learning_rate': 0.0001299282891170958, 'epoch': 0.42}


 42%|████▏     | 1311/3107 [6:45:57<10:22:20, 20.79s/it]
{'loss': 0.8618, 'grad_norm': 0.13651339501762866, 'learning_rate': 0.00012972924669228246, 'epoch': 0.42}

 42%|████▏     | 1312/3107 [6:46:18<10:24:42, 20.88s/it]

 42%|████▏     | 1313/3107 [6:46:35<9:51:17, 19.78s/it]


 42%|████▏     | 1315/3107 [6:46:59<7:49:53, 15.73s/it]
{'loss': 0.9867, 'grad_norm': 0.1518120772704603, 'learning_rate': 0.0001293307748564044, 'epoch': 0.42}

 42%|████▏     | 1316/3107 [6:47:16<8:03:09, 16.19s/it]

 42%|████▏     | 1317/3107 [6:47:37<8:49:38, 17.75s/it]


 42%|████▏     | 1319/3107 [6:48:11<8:29:08, 17.09s/it]
{'loss': 0.9218, 'grad_norm': 0.13534217482505903, 'learning_rate': 0.00012893179281592453, 'epoch': 0.42}

 42%|████▏     | 1320/3107 [6:48:23<7:48:34, 15.73s/it]

 43%|████▎     | 1321/3107 [6:48:46<8:48:58, 17.77s/it]

 43%|████▎     | 1322/3107 [6:48:58<7:59:36, 16.12s/it]

 43%|████▎     | 1323/3107 [6:49:16<8:09:43, 16.47s/it]

 43%|████▎     | 1324/3107 [6:49:37<8:57:35, 18.09s/it]

 43%|████▎     | 1325/3107 [6:49:52<8:27:55, 17.10s/it]

 43%|████▎     | 1326/3107 [6:50:10<8:32:29, 17.27s/it]

 43%|████▎     | 1327/3107 [6:50:22<7:47:42, 15.77s/it]

 43%|████▎     | 1328/3107 [6:50:39<7:59:30, 16.17s/it]

 43%|████▎     | 1329/3107 [6:50:55<7:59:06, 16.17s/it]

 43%|████▎     | 1330/3107 [6:51:17<8:49:18, 17.87s/it]

 43%|████▎     | 1331/3107 [6:51:30<8:03:35, 16.34s/it]


 43%|████▎     | 1333/3107 [6:52:11<9:09:07, 18.57s/it]
{'loss': 0.8421, 'grad_norm': 0.13248852489917176, 'learning_rate': 0.0001275314380913159, 'epoch': 0.43}


 43%|████▎     | 1335/3107 [6:52:59<10:25:51, 21.19s/it]

 43%|████▎     | 1336/3107 [6:53:15<9:41:37, 19.70s/it]
{'loss': 0.8258, 'grad_norm': 0.13543486729297924, 'learning_rate': 0.00012723058864433118, 'epoch': 0.43}

 43%|████▎     | 1337/3107 [6:53:26<8:25:10, 17.12s/it]

 43%|████▎     | 1338/3107 [6:53:44<8:28:53, 17.26s/it]

 43%|████▎     | 1339/3107 [6:54:00<8:21:32, 17.02s/it]


 43%|████▎     | 1341/3107 [6:54:43<9:25:51, 19.23s/it]
{'loss': 0.8277, 'grad_norm': 0.13327857744945942, 'learning_rate': 0.0001267285822636323, 'epoch': 0.43}


 43%|████▎     | 1343/3107 [6:55:27<10:03:54, 20.54s/it]
{'loss': 0.8355, 'grad_norm': 0.1359234163936707, 'learning_rate': 0.0001265275755355945, 'epoch': 0.43}

 43%|████▎     | 1344/3107 [6:55:46<9:52:29, 20.16s/it]

 43%|████▎     | 1345/3107 [6:56:00<8:52:11, 18.12s/it]

 43%|████▎     | 1346/3107 [6:56:13<8:04:41, 16.51s/it]

 43%|████▎     | 1347/3107 [6:56:34<8:45:30, 17.92s/it]

 43%|████▎     | 1348/3107 [6:56:52<8:48:02, 18.01s/it]


 43%|████▎     | 1350/3107 [6:57:25<8:20:44, 17.10s/it]

 43%|████▎     | 1351/3107 [6:57:41<8:11:21, 16.79s/it]
{'loss': 0.8996, 'grad_norm': 0.13606589788553325, 'learning_rate': 0.0001257224037625246, 'epoch': 0.43}


 44%|████▎     | 1353/3107 [6:58:21<8:45:01, 17.96s/it]
{'loss': 0.803, 'grad_norm': 0.1489720149923508, 'learning_rate': 0.0001255208289808613, 'epoch': 0.44}

 44%|████▎     | 1354/3107 [6:58:39<8:42:14, 17.87s/it]

 44%|████▎     | 1355/3107 [6:59:12<10:58:31, 22.55s/it]


 44%|████▎     | 1357/3107 [7:00:01<11:24:37, 23.47s/it]
{'loss': 0.8075, 'grad_norm': 0.13309000302474747, 'learning_rate': 0.0001251173473458906, 'epoch': 0.44}


 44%|████▎     | 1359/3107 [7:00:33<9:24:30, 19.38s/it]
{'loss': 0.8363, 'grad_norm': 0.1386027646621672, 'learning_rate': 0.00012491544224721136, 'epoch': 0.44}

 44%|████▍     | 1360/3107 [7:00:56<9:52:49, 20.36s/it]


 44%|████▍     | 1362/3107 [7:01:35<9:50:20, 20.30s/it]

 44%|████▍     | 1363/3107 [7:01:56<9:52:45, 20.39s/it]

 44%|████▍     | 1364/3107 [7:02:08<8:41:45, 17.96s/it]
{'loss': 0.8039, 'grad_norm': 0.15191215191167548, 'learning_rate': 0.00012441020739001698, 'epoch': 0.44}


 44%|████▍     | 1366/3107 [7:02:38<8:03:06, 16.65s/it]
{'loss': 0.8585, 'grad_norm': 0.13041251942664134, 'learning_rate': 0.00012420792690992133, 'epoch': 0.44}
[2024-05-30 07:42:33,434] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 44%|████▍     | 1368/3107 [7:03:30<10:18:42, 21.35s/it]

 44%|████▍     | 1369/3107 [7:03:48<9:51:27, 20.42s/it]
{'loss': 0.8521, 'grad_norm': 0.12771275576374408, 'learning_rate': 0.00012390430907705134, 'epoch': 0.44}

 44%|████▍     | 1370/3107 [7:04:11<10:11:38, 21.13s/it]

 44%|████▍     | 1371/3107 [7:04:27<9:24:54, 19.52s/it]


 44%|████▍     | 1373/3107 [7:05:04<9:12:45, 19.13s/it]

 44%|████▍     | 1374/3107 [7:05:32<10:27:29, 21.72s/it]
{'loss': 0.7691, 'grad_norm': 0.13015051057873525, 'learning_rate': 0.00012339776105834744, 'epoch': 0.44}

 44%|████▍     | 1375/3107 [7:05:59<11:10:30, 23.23s/it]

 44%|████▍     | 1376/3107 [7:06:17<10:28:31, 21.79s/it]

 44%|████▍     | 1377/3107 [7:06:35<9:51:20, 20.51s/it]

 44%|████▍     | 1378/3107 [7:06:49<9:00:49, 18.77s/it]


 44%|████▍     | 1380/3107 [7:07:24<8:40:53, 18.10s/it]
{'loss': 0.8023, 'grad_norm': 0.12952595583658194, 'learning_rate': 0.00012278906521045888, 'epoch': 0.44}

 44%|████▍     | 1381/3107 [7:07:47<9:23:22, 19.58s/it]


 45%|████▍     | 1383/3107 [7:08:16<8:03:46, 16.84s/it]

 45%|████▍     | 1384/3107 [7:08:36<8:31:54, 17.83s/it]

 45%|████▍     | 1385/3107 [7:08:54<8:30:37, 17.79s/it]

 45%|████▍     | 1386/3107 [7:09:15<8:58:47, 18.78s/it]
{'loss': 0.878, 'grad_norm': 0.12907290509874528, 'learning_rate': 0.0001221794774360497, 'epoch': 0.45}


 45%|████▍     | 1388/3107 [7:09:52<9:05:34, 19.04s/it]

 45%|████▍     | 1389/3107 [7:10:09<8:47:19, 18.42s/it]

 45%|████▍     | 1390/3107 [7:10:26<8:39:45, 18.16s/it]

 45%|████▍     | 1391/3107 [7:10:39<7:51:27, 16.48s/it]
{'loss': 0.9793, 'grad_norm': 0.15043992187383018, 'learning_rate': 0.00012167082350281369, 'epoch': 0.45}

 45%|████▍     | 1392/3107 [7:10:57<8:07:36, 17.06s/it]

 45%|████▍     | 1393/3107 [7:11:15<8:17:30, 17.42s/it]


 45%|████▍     | 1395/3107 [7:11:42<7:14:24, 15.22s/it]

 45%|████▍     | 1396/3107 [7:12:06<8:27:33, 17.80s/it]

 45%|████▍     | 1397/3107 [7:12:27<8:48:14, 18.53s/it]

 45%|████▍     | 1398/3107 [7:12:40<8:05:13, 17.04s/it]
{'loss': 0.9268, 'grad_norm': 0.15045474807178216, 'learning_rate': 0.00012095772157469476, 'epoch': 0.45}


 45%|████▌     | 1400/3107 [7:13:22<8:44:32, 18.44s/it]

 45%|████▌     | 1401/3107 [7:13:39<8:30:55, 17.97s/it]

 45%|████▌     | 1402/3107 [7:13:53<7:58:25, 16.84s/it]
{'loss': 0.9155, 'grad_norm': 0.14019866921634924, 'learning_rate': 0.00012054973105584497, 'epoch': 0.45}


 45%|████▌     | 1404/3107 [7:14:33<8:49:00, 18.64s/it]
{'loss': 0.8272, 'grad_norm': 0.1588265521612425, 'learning_rate': 0.0001203456013052634, 'epoch': 0.45}

 45%|████▌     | 1405/3107 [7:14:55<9:23:42, 19.87s/it]

 45%|████▌     | 1406/3107 [7:15:14<9:12:41, 19.50s/it]


 45%|████▌     | 1408/3107 [7:15:41<7:42:23, 16.33s/it]
{'loss': 0.8522, 'grad_norm': 0.1317277360848841, 'learning_rate': 0.00011993707726029719, 'epoch': 0.45}

 45%|████▌     | 1409/3107 [7:16:01<8:18:23, 17.61s/it]

 45%|████▌     | 1410/3107 [7:16:14<7:38:46, 16.22s/it]

 45%|████▌     | 1411/3107 [7:16:29<7:21:42, 15.63s/it]

 45%|████▌     | 1412/3107 [7:16:54<8:45:04, 18.59s/it]


 46%|████▌     | 1414/3107 [7:17:33<9:06:17, 19.36s/it]

 46%|████▌     | 1415/3107 [7:17:49<8:40:24, 18.45s/it]
{'loss': 0.9218, 'grad_norm': 0.14577218419076174, 'learning_rate': 0.00011922132996612345, 'epoch': 0.46}

 46%|████▌     | 1416/3107 [7:18:01<7:48:05, 16.61s/it]

 46%|████▌     | 1417/3107 [7:18:30<9:28:33, 20.19s/it]


 46%|████▌     | 1419/3107 [7:18:59<8:15:55, 17.63s/it]

 46%|████▌     | 1420/3107 [7:19:13<7:49:55, 16.71s/it]

 46%|████▌     | 1421/3107 [7:19:29<7:41:34, 16.43s/it]
{'loss': 0.8845, 'grad_norm': 0.13427804791123876, 'learning_rate': 0.00011860701585758271, 'epoch': 0.46}


 46%|████▌     | 1423/3107 [7:19:59<7:20:43, 15.70s/it]
{'loss': 0.8057, 'grad_norm': 0.14106343807169996, 'learning_rate': 0.00011840208146703191, 'epoch': 0.46}

 46%|████▌     | 1424/3107 [7:20:15<7:18:54, 15.65s/it]


 46%|████▌     | 1426/3107 [7:20:55<8:24:38, 18.01s/it]
{'loss': 0.7083, 'grad_norm': 0.13220759300999801, 'learning_rate': 0.00011809453011203444, 'epoch': 0.46}

 46%|████▌     | 1427/3107 [7:21:10<7:56:28, 17.02s/it]

 46%|████▌     | 1428/3107 [7:21:24<7:36:59, 16.33s/it]

 46%|████▌     | 1429/3107 [7:21:40<7:33:41, 16.22s/it]


 46%|████▌     | 1431/3107 [7:22:25<8:44:15, 18.77s/it]

 46%|████▌     | 1432/3107 [7:22:45<8:54:08, 19.13s/it]
{'loss': 0.7458, 'grad_norm': 0.12217795913625366, 'learning_rate': 0.00011747889926913838, 'epoch': 0.46}

 46%|████▌     | 1433/3107 [7:23:05<8:56:29, 19.23s/it]

 46%|████▌     | 1434/3107 [7:23:24<8:59:57, 19.36s/it]


 46%|████▌     | 1436/3107 [7:23:56<8:15:14, 17.78s/it]
{'loss': 0.8461, 'grad_norm': 0.14030545253351928, 'learning_rate': 0.00011706809716578475, 'epoch': 0.46}


 46%|████▋     | 1438/3107 [7:24:35<8:48:35, 19.00s/it]
{'loss': 0.8157, 'grad_norm': 0.14074084919675994, 'learning_rate': 0.00011686258433088055, 'epoch': 0.46}

 46%|████▋     | 1439/3107 [7:24:51<8:21:36, 18.04s/it]

 46%|████▋     | 1440/3107 [7:25:06<7:58:29, 17.22s/it]


 46%|████▋     | 1442/3107 [7:25:42<7:58:00, 17.23s/it]
{'loss': 0.7305, 'grad_norm': 0.1304156018087372, 'learning_rate': 0.00011645133956320114, 'epoch': 0.46}

 46%|████▋     | 1443/3107 [7:25:56<7:35:55, 16.44s/it]


 47%|████▋     | 1445/3107 [7:26:34<8:02:13, 17.41s/it]

 47%|████▋     | 1446/3107 [7:26:58<8:56:51, 19.39s/it]
{'loss': 0.7136, 'grad_norm': 0.1519583946488932, 'learning_rate': 0.00011603980862684081, 'epoch': 0.47}

 47%|████▋     | 1447/3107 [7:27:18<9:08:14, 19.82s/it]

 47%|████▋     | 1448/3107 [7:27:44<9:59:46, 21.69s/it]

 47%|████▋     | 1449/3107 [7:28:01<9:15:15, 20.09s/it]


 47%|████▋     | 1451/3107 [7:28:52<10:31:55, 22.90s/it]

 47%|████▋     | 1452/3107 [7:29:06<9:16:13, 20.17s/it]

 47%|████▋     | 1453/3107 [7:29:24<8:56:38, 19.47s/it]
{'loss': 0.8593, 'grad_norm': 0.1439490869309221, 'learning_rate': 0.00011531896242905233, 'epoch': 0.47}


 47%|████▋     | 1455/3107 [7:29:56<8:17:00, 18.05s/it]
{'loss': 0.8319, 'grad_norm': 0.1270375053990104, 'learning_rate': 0.00011511285480253971, 'epoch': 0.47}

 47%|████▋     | 1456/3107 [7:30:16<8:35:56, 18.75s/it]

 47%|████▋     | 1457/3107 [7:30:30<7:57:04, 17.35s/it]


 47%|████▋     | 1459/3107 [7:31:02<7:37:12, 16.65s/it]
{'loss': 0.7513, 'grad_norm': 0.1472279119579957, 'learning_rate': 0.00011470044328144143, 'epoch': 0.47}

 47%|████▋     | 1460/3107 [7:31:28<8:59:05, 19.64s/it]


 47%|████▋     | 1462/3107 [7:32:02<8:31:18, 18.65s/it]
{'loss': 0.8095, 'grad_norm': 0.14452410073178368, 'learning_rate': 0.00011439096643706935, 'epoch': 0.47}

 47%|████▋     | 1463/3107 [7:32:20<8:19:46, 18.24s/it]

 47%|████▋     | 1464/3107 [7:32:39<8:30:22, 18.64s/it]

 47%|████▋     | 1465/3107 [7:32:59<8:37:21, 18.90s/it]

 47%|████▋     | 1466/3107 [7:33:22<9:10:31, 20.13s/it]


 47%|████▋     | 1468/3107 [7:33:58<8:40:49, 19.07s/it]

 47%|████▋     | 1469/3107 [7:34:12<7:55:43, 17.43s/it]

 47%|████▋     | 1470/3107 [7:34:26<7:32:09, 16.57s/it]
{'loss': 0.7763, 'grad_norm': 0.13794324103082184, 'learning_rate': 0.00011356501467632785, 'epoch': 0.47}

 47%|████▋     | 1471/3107 [7:34:45<7:48:11, 17.17s/it]

 47%|████▋     | 1472/3107 [7:35:01<7:39:21, 16.86s/it]

 47%|████▋     | 1473/3107 [7:35:18<7:37:13, 16.79s/it]


 47%|████▋     | 1475/3107 [7:35:46<6:59:51, 15.44s/it]
{'loss': 0.8203, 'grad_norm': 0.13709663898979957, 'learning_rate': 0.00011304831188048581, 'epoch': 0.47}

 48%|████▊     | 1476/3107 [7:36:01<6:50:07, 15.09s/it]

 48%|████▊     | 1477/3107 [7:36:27<8:22:17, 18.49s/it]

 48%|████▊     | 1478/3107 [7:36:45<8:17:44, 18.33s/it]

 48%|████▊     | 1479/3107 [7:37:00<7:46:45, 17.20s/it]


 48%|████▊     | 1481/3107 [7:37:32<7:38:44, 16.93s/it]

 48%|████▊     | 1482/3107 [7:37:51<7:50:07, 17.36s/it]
{'loss': 0.9139, 'grad_norm': 0.13809347244578699, 'learning_rate': 0.00011232433530897388, 'epoch': 0.48}

 48%|████▊     | 1483/3107 [7:38:10<8:06:13, 17.96s/it]

 48%|████▊     | 1484/3107 [7:38:31<8:33:56, 19.00s/it]


 48%|████▊     | 1486/3107 [7:39:12<8:43:51, 19.39s/it]

 48%|████▊     | 1487/3107 [7:39:30<8:34:06, 19.04s/it]
{'loss': 0.8596, 'grad_norm': 0.15265510041359873, 'learning_rate': 0.0001118068049748668, 'epoch': 0.48}

 48%|████▊     | 1488/3107 [7:39:45<8:01:11, 17.83s/it]


 48%|████▊     | 1490/3107 [7:40:20<8:09:50, 18.18s/it]
{'loss': 0.8899, 'grad_norm': 0.14457157465084883, 'learning_rate': 0.00011149613184067171, 'epoch': 0.48}

 48%|████▊     | 1491/3107 [7:40:41<8:31:22, 18.99s/it]

 48%|████▊     | 1492/3107 [7:40:59<8:20:48, 18.61s/it]


 48%|████▊     | 1494/3107 [7:41:35<8:09:46, 18.22s/it]
{'loss': 0.8001, 'grad_norm': 0.13684682642597432, 'learning_rate': 0.00011108172654366467, 'epoch': 0.48}

 48%|████▊     | 1495/3107 [7:41:50<7:43:35, 17.26s/it]

 48%|████▊     | 1496/3107 [7:42:04<7:19:53, 16.38s/it]

 48%|████▊     | 1497/3107 [7:42:23<7:41:32, 17.20s/it]


 48%|████▊     | 1499/3107 [7:43:04<8:12:29, 18.38s/it]
{'loss': 0.9837, 'grad_norm': 0.133294987129435, 'learning_rate': 0.00011056344969164317, 'epoch': 0.48}

 48%|████▊     | 1500/3107 [7:43:21<8:01:05, 17.96s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 48%|████▊     | 1501/3107 [7:44:11<12:11:00, 27.31s/it]

 48%|████▊     | 1502/3107 [7:44:25<10:23:54, 23.32s/it]
{'loss': 0.8506, 'grad_norm': 0.13526626200815436, 'learning_rate': 0.00011025234486641533, 'epoch': 0.48}


 48%|████▊     | 1504/3107 [7:44:59<9:05:10, 20.41s/it]
{'loss': 0.7699, 'grad_norm': 0.13083687216724227, 'learning_rate': 0.0001100448857309728, 'epoch': 0.48}

 48%|████▊     | 1505/3107 [7:45:16<8:36:58, 19.36s/it]

 48%|████▊     | 1506/3107 [7:45:32<8:07:11, 18.26s/it]

 49%|████▊     | 1507/3107 [7:45:50<8:04:32, 18.17s/it]

 49%|████▊     | 1508/3107 [7:46:06<7:48:23, 17.58s/it]

 49%|████▊     | 1509/3107 [7:46:18<7:06:14, 16.00s/it]

 49%|████▊     | 1510/3107 [7:46:34<7:08:13, 16.09s/it]


 49%|████▊     | 1512/3107 [7:47:01<6:29:47, 14.66s/it]

 49%|████▊     | 1513/3107 [7:47:21<7:09:56, 16.18s/it]

 49%|████▊     | 1514/3107 [7:47:49<8:45:36, 19.80s/it]
{'loss': 0.838, 'grad_norm': 0.13628762989509, 'learning_rate': 0.00010900695286821843, 'epoch': 0.49}

 49%|████▉     | 1515/3107 [7:48:06<8:20:41, 18.87s/it]

 49%|████▉     | 1516/3107 [7:48:20<7:42:45, 17.45s/it]

 49%|████▉     | 1517/3107 [7:48:36<7:35:42, 17.20s/it]


 49%|████▉     | 1519/3107 [7:49:11<7:43:42, 17.52s/it]
{'loss': 0.7962, 'grad_norm': 0.14415094643385226, 'learning_rate': 0.00010848761217656856, 'epoch': 0.49}

 49%|████▉     | 1520/3107 [7:49:34<8:22:17, 18.99s/it]


 49%|████▉     | 1522/3107 [7:50:07<7:47:43, 17.71s/it]
{'loss': 0.9636, 'grad_norm': 0.1543061377220778, 'learning_rate': 0.0001081758961274257, 'epoch': 0.49}

 49%|████▉     | 1523/3107 [7:50:26<7:58:39, 18.13s/it]

 49%|████▉     | 1524/3107 [7:50:52<9:00:35, 20.49s/it]

 49%|████▉     | 1525/3107 [7:51:16<9:25:34, 21.45s/it]

 49%|████▉     | 1526/3107 [7:51:36<9:12:16, 20.96s/it]

 49%|████▉     | 1527/3107 [7:51:52<8:31:50, 19.44s/it]


 49%|████▉     | 1529/3107 [7:52:33<9:00:51, 20.57s/it]
{'loss': 0.8523, 'grad_norm': 0.1362308920428015, 'learning_rate': 0.0001074482528492861, 'epoch': 0.49}

 49%|████▉     | 1530/3107 [7:52:46<8:00:56, 18.30s/it]


 49%|████▉     | 1532/3107 [7:53:17<7:22:11, 16.85s/it]
{'loss': 0.9254, 'grad_norm': 0.14170913503832677, 'learning_rate': 0.00010713628200575417, 'epoch': 0.49}


 49%|████▉     | 1534/3107 [7:53:53<7:33:00, 17.28s/it]

 49%|████▉     | 1535/3107 [7:54:07<7:08:11, 16.34s/it]
{'loss': 0.8351, 'grad_norm': 0.14374702055810243, 'learning_rate': 0.0001068242413364671, 'epoch': 0.49}

 49%|████▉     | 1536/3107 [7:54:34<8:31:11, 19.52s/it]

 49%|████▉     | 1537/3107 [7:54:52<8:20:30, 19.13s/it]

 50%|████▉     | 1538/3107 [7:55:11<8:13:56, 18.89s/it]

 50%|████▉     | 1539/3107 [7:55:22<7:15:09, 16.65s/it]

 50%|████▉     | 1540/3107 [7:55:47<8:18:32, 19.09s/it]


 50%|████▉     | 1542/3107 [7:56:28<8:19:36, 19.15s/it]
{'loss': 0.8494, 'grad_norm': 0.1272392474633275, 'learning_rate': 0.00010609589204875518, 'epoch': 0.5}


 50%|████▉     | 1544/3107 [7:57:06<8:31:50, 19.65s/it]
{'loss': 0.7467, 'grad_norm': 0.13066326997908578, 'learning_rate': 0.00010588773090928268, 'epoch': 0.5}


 50%|████▉     | 1546/3107 [7:57:47<8:46:06, 20.22s/it]
{'loss': 0.7637, 'grad_norm': 0.13493006832770385, 'learning_rate': 0.00010567954416572465, 'epoch': 0.5}


 50%|████▉     | 1548/3107 [7:58:25<8:38:01, 19.94s/it]

 50%|████▉     | 1549/3107 [7:58:54<9:44:53, 22.52s/it]

 50%|████▉     | 1550/3107 [7:59:06<8:20:52, 19.30s/it]
{'loss': 0.8394, 'grad_norm': 0.13195766153605318, 'learning_rate': 0.00010526309748784184, 'epoch': 0.5}

 50%|████▉     | 1551/3107 [7:59:22<7:58:04, 18.43s/it]

 50%|████▉     | 1552/3107 [7:59:38<7:38:36, 17.70s/it]

 50%|████▉     | 1553/3107 [8:00:05<8:46:44, 20.34s/it]

 50%|█████     | 1554/3107 [8:00:24<8:42:06, 20.17s/it]


 50%|█████     | 1556/3107 [8:00:56<7:49:33, 18.16s/it]
{'loss': 0.7019, 'grad_norm': 0.13862595775910969, 'learning_rate': 0.00010463825807742398, 'epoch': 0.5}

 50%|█████     | 1557/3107 [8:01:09<7:11:46, 16.71s/it]

 50%|█████     | 1558/3107 [8:01:27<7:19:58, 17.04s/it]


 50%|█████     | 1560/3107 [8:02:02<7:29:11, 17.42s/it]

 50%|█████     | 1561/3107 [8:02:22<7:49:52, 18.24s/it]
{'loss': 0.7931, 'grad_norm': 0.1347601429450173, 'learning_rate': 0.00010411741885904083, 'epoch': 0.5}

 50%|█████     | 1562/3107 [8:02:43<8:08:32, 18.97s/it]


 50%|█████     | 1564/3107 [8:03:28<8:58:57, 20.96s/it]
{'loss': 0.8153, 'grad_norm': 0.13286594305396446, 'learning_rate': 0.00010380486070550135, 'epoch': 0.5}

 50%|█████     | 1565/3107 [8:03:47<8:45:25, 20.44s/it]

 50%|█████     | 1566/3107 [8:04:04<8:18:46, 19.42s/it]


 50%|█████     | 1568/3107 [8:04:42<8:06:42, 18.97s/it]

 50%|█████     | 1569/3107 [8:04:58<7:44:07, 18.11s/it]
{'loss': 0.7203, 'grad_norm': 0.14032641653882763, 'learning_rate': 0.00010328384922871307, 'epoch': 0.5}


 51%|█████     | 1571/3107 [8:05:36<7:49:27, 18.34s/it]
{'loss': 0.8801, 'grad_norm': 0.13368552348684715, 'learning_rate': 0.00010307541885401591, 'epoch': 0.51}

 51%|█████     | 1572/3107 [8:05:51<7:23:58, 17.35s/it]

 51%|█████     | 1573/3107 [8:06:15<8:13:54, 19.32s/it]

 51%|█████     | 1574/3107 [8:06:28<7:23:17, 17.35s/it]


 51%|█████     | 1576/3107 [8:07:00<7:04:32, 16.64s/it]

 51%|█████     | 1577/3107 [8:07:14<6:40:41, 15.71s/it]
{'loss': 0.891, 'grad_norm': 0.13834210234727348, 'learning_rate': 0.00010245005111104257, 'epoch': 0.51}


 51%|█████     | 1579/3107 [8:07:54<7:31:33, 17.73s/it]
{'loss': 0.8342, 'grad_norm': 0.13289753811289068, 'learning_rate': 0.00010224157267881176, 'epoch': 0.51}

 51%|█████     | 1580/3107 [8:08:05<6:40:00, 15.72s/it]

 51%|█████     | 1581/3107 [8:08:23<6:53:02, 16.24s/it]

 51%|█████     | 1582/3107 [8:08:37<6:36:28, 15.60s/it]


 51%|█████     | 1584/3107 [8:09:14<7:04:49, 16.74s/it]

 51%|█████     | 1585/3107 [8:09:29<6:48:27, 16.10s/it]
{'loss': 0.8311, 'grad_norm': 0.13389026990142647, 'learning_rate': 0.00010161608252096938, 'epoch': 0.51}

 51%|█████     | 1586/3107 [8:09:40<6:12:11, 14.68s/it]

 51%|█████     | 1587/3107 [8:09:54<6:05:49, 14.44s/it]

 51%|█████     | 1588/3107 [8:10:15<7:01:07, 16.63s/it]


 51%|█████     | 1590/3107 [8:10:53<7:28:13, 17.73s/it]
{'loss': 0.7043, 'grad_norm': 0.11213655324737903, 'learning_rate': 0.00010109479136717773, 'epoch': 0.51}

 51%|█████     | 1591/3107 [8:11:12<7:38:46, 18.16s/it]

 51%|█████     | 1592/3107 [8:11:31<7:49:02, 18.58s/it]

 51%|█████▏    | 1593/3107 [8:11:48<7:33:25, 17.97s/it]


 51%|█████▏    | 1595/3107 [8:12:24<7:32:55, 17.97s/it]
{'loss': 0.8941, 'grad_norm': 0.13788026471470624, 'learning_rate': 0.00010057347045756958, 'epoch': 0.51}

 51%|█████▏    | 1596/3107 [8:12:44<7:44:29, 18.44s/it]


 51%|█████▏    | 1598/3107 [8:13:14<7:01:46, 16.77s/it]

 51%|█████▏    | 1599/3107 [8:13:28<6:41:50, 15.99s/it]
{'loss': 0.8682, 'grad_norm': 0.1373868887353496, 'learning_rate': 0.00010015640182738733, 'epoch': 0.51}

 51%|█████▏    | 1600/3107 [8:13:51<7:32:12, 18.00s/it]

 52%|█████▏    | 1601/3107 [8:14:09<7:34:25, 18.10s/it]


 52%|█████▏    | 1603/3107 [8:14:47<7:29:13, 17.92s/it]
{'loss': 0.8492, 'grad_norm': 0.14586398754233706, 'learning_rate': 9.973933047661777e-05, 'epoch': 0.52}

 52%|█████▏    | 1604/3107 [8:15:00<6:55:20, 16.58s/it]


 52%|█████▏    | 1606/3107 [8:15:28<6:20:16, 15.20s/it]
{'loss': 0.8296, 'grad_norm': 0.13611805533686164, 'learning_rate': 9.942652954243047e-05, 'epoch': 0.52}

 52%|█████▏    | 1607/3107 [8:15:47<6:47:52, 16.32s/it]

 52%|█████▏    | 1608/3107 [8:16:01<6:28:11, 15.54s/it]

 52%|█████▏    | 1609/3107 [8:16:15<6:17:52, 15.14s/it]

 52%|█████▏    | 1610/3107 [8:16:26<5:41:28, 13.69s/it]


 52%|█████▏    | 1612/3107 [8:17:05<6:55:25, 16.67s/it]

 52%|█████▏    | 1613/3107 [8:17:25<7:19:08, 17.64s/it]
{'loss': 1.036, 'grad_norm': 0.14645916444395682, 'learning_rate': 9.86966878071549e-05, 'epoch': 0.52}


 52%|█████▏    | 1615/3107 [8:18:01<7:10:58, 17.33s/it]
{'loss': 0.8304, 'grad_norm': 0.13973828892260193, 'learning_rate': 9.848817264922561e-05, 'epoch': 0.52}

 52%|█████▏    | 1616/3107 [8:18:18<7:08:30, 17.24s/it]

 52%|█████▏    | 1617/3107 [8:18:46<8:31:17, 20.59s/it]


 52%|█████▏    | 1619/3107 [8:19:29<8:51:28, 21.43s/it]
{'loss': 0.7693, 'grad_norm': 0.13708830377186929, 'learning_rate': 9.807116296364783e-05, 'epoch': 0.52}


 52%|█████▏    | 1621/3107 [8:20:17<9:34:13, 23.19s/it]
{'loss': 0.7855, 'grad_norm': 0.1387300852744257, 'learning_rate': 9.786267024945714e-05, 'epoch': 0.52}


 52%|█████▏    | 1623/3107 [8:20:59<9:07:51, 22.15s/it]
{'loss': 0.76, 'grad_norm': 0.1356660869481153, 'learning_rate': 9.765418682991231e-05, 'epoch': 0.52}


 52%|█████▏    | 1625/3107 [8:21:41<8:45:25, 21.27s/it]
{'loss': 0.7933, 'grad_norm': 0.1390157318573047, 'learning_rate': 9.744571361164906e-05, 'epoch': 0.52}

 52%|█████▏    | 1626/3107 [8:21:56<7:54:00, 19.20s/it]


 52%|█████▏    | 1628/3107 [8:22:31<7:24:58, 18.05s/it]

 52%|█████▏    | 1629/3107 [8:22:47<7:11:16, 17.51s/it]

 52%|█████▏    | 1630/3107 [8:23:03<6:59:09, 17.03s/it]
{'loss': 0.8714, 'grad_norm': 0.1354193069771475, 'learning_rate': 9.692458114598413e-05, 'epoch': 0.52}

 52%|█████▏    | 1631/3107 [8:23:16<6:24:55, 15.65s/it]

 53%|█████▎    | 1632/3107 [8:23:32<6:29:04, 15.83s/it]
[2024-05-30 09:03:32,188] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1633/3107 [8:24:04<8:28:31, 20.70s/it]

 53%|█████▎    | 1634/3107 [8:24:22<8:07:20, 19.85s/it]

 53%|█████▎    | 1635/3107 [8:24:34<7:08:24, 17.46s/it]

 53%|█████▎    | 1636/3107 [8:24:46<6:31:05, 15.95s/it]

 53%|█████▎    | 1637/3107 [8:25:10<7:26:56, 18.24s/it]


 53%|█████▎    | 1639/3107 [8:25:55<8:22:16, 20.53s/it]

 53%|█████▎    | 1640/3107 [8:26:21<9:04:41, 22.28s/it]
[2024-05-30 09:05:49,704] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7543, 'grad_norm': 0.12280135830490736, 'learning_rate': 9.588258114095918e-05, 'epoch': 0.53}


 53%|█████▎    | 1642/3107 [8:26:59<8:22:29, 20.58s/it]

 53%|█████▎    | 1643/3107 [8:27:24<8:48:15, 21.65s/it]
{'loss': 0.8358, 'grad_norm': 0.13132226970837668, 'learning_rate': 9.557006327476837e-05, 'epoch': 0.53}


 53%|█████▎    | 1645/3107 [8:28:12<9:22:13, 23.07s/it]
{'loss': 0.8402, 'grad_norm': 0.15087393062970733, 'learning_rate': 9.536174192257603e-05, 'epoch': 0.53}

 53%|█████▎    | 1646/3107 [8:28:29<8:37:09, 21.24s/it]


 53%|█████▎    | 1648/3107 [8:29:03<7:46:28, 19.18s/it]
{'loss': 0.8505, 'grad_norm': 0.1357585467327823, 'learning_rate': 9.504929799701638e-05, 'epoch': 0.53}

 53%|█████▎    | 1649/3107 [8:29:24<7:56:45, 19.62s/it]


 53%|█████▎    | 1651/3107 [8:30:10<8:24:03, 20.77s/it]

 53%|█████▎    | 1652/3107 [8:30:26<7:49:07, 19.35s/it]

 53%|█████▎    | 1653/3107 [8:30:44<7:38:47, 18.93s/it]

 53%|█████▎    | 1654/3107 [8:31:06<7:58:58, 19.78s/it]

 53%|█████▎    | 1655/3107 [8:31:22<7:32:49, 18.71s/it]

 53%|█████▎    | 1656/3107 [8:31:35<6:54:37, 17.15s/it]
{'loss': 0.8734, 'grad_norm': 0.14228722326139198, 'learning_rate': 9.421635931856723e-05, 'epoch': 0.53}

 53%|█████▎    | 1657/3107 [8:31:50<6:38:09, 16.48s/it]

 53%|█████▎    | 1658/3107 [8:32:05<6:24:56, 15.94s/it]


 53%|█████▎    | 1660/3107 [8:32:48<7:41:00, 19.12s/it]
{'loss': 0.8943, 'grad_norm': 0.14097505179434552, 'learning_rate': 9.380003726593081e-05, 'epoch': 0.53}

 53%|█████▎    | 1661/3107 [8:33:01<6:59:49, 17.42s/it]

 53%|█████▎    | 1662/3107 [8:33:19<7:01:48, 17.51s/it]


 54%|█████▎    | 1664/3107 [8:33:56<7:01:47, 17.54s/it]

 54%|█████▎    | 1665/3107 [8:34:10<6:39:21, 16.62s/it]

 54%|█████▎    | 1666/3107 [8:34:34<7:33:10, 18.87s/it]

 54%|█████▎    | 1667/3107 [8:34:56<7:53:22, 19.72s/it]
{'loss': 0.8615, 'grad_norm': 0.13739907096707235, 'learning_rate': 9.307173753714186e-05, 'epoch': 0.54}


 54%|█████▎    | 1669/3107 [8:35:30<7:26:13, 18.62s/it]
{'loss': 0.75, 'grad_norm': 0.13845939556367742, 'learning_rate': 9.286371799424584e-05, 'epoch': 0.54}

 54%|█████▎    | 1670/3107 [8:35:41<6:31:51, 16.36s/it]


 54%|█████▍    | 1672/3107 [8:36:20<7:06:33, 17.84s/it]

 54%|█████▍    | 1673/3107 [8:36:40<7:25:24, 18.64s/it]

 54%|█████▍    | 1674/3107 [8:36:54<6:52:13, 17.26s/it]

 54%|█████▍    | 1675/3107 [8:37:10<6:43:57, 16.93s/it]
{'loss': 0.7945, 'grad_norm': 0.1334900752308851, 'learning_rate': 9.223984918545101e-05, 'epoch': 0.54}

 54%|█████▍    | 1676/3107 [8:37:25<6:23:31, 16.08s/it]

 54%|█████▍    | 1677/3107 [8:37:44<6:51:04, 17.25s/it]

 54%|█████▍    | 1678/3107 [8:38:05<7:12:01, 18.14s/it]


 54%|█████▍    | 1680/3107 [8:38:40<6:57:42, 17.56s/it]

 54%|█████▍    | 1681/3107 [8:39:14<8:51:36, 22.37s/it]
[2024-05-30 09:18:42,166] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.692, 'grad_norm': 0.12748299962173518, 'learning_rate': 9.161628409619236e-05, 'epoch': 0.54}

 54%|█████▍    | 1682/3107 [8:39:33<8:28:49, 21.42s/it]


 54%|█████▍    | 1684/3107 [8:40:00<6:47:50, 17.20s/it]

 54%|█████▍    | 1685/3107 [8:40:16<6:36:04, 16.71s/it]

 54%|█████▍    | 1686/3107 [8:40:28<6:04:13, 15.38s/it]
{'loss': 0.806, 'grad_norm': 0.15094911421623797, 'learning_rate': 9.109689613739573e-05, 'epoch': 0.54}

 54%|█████▍    | 1687/3107 [8:40:40<5:35:41, 14.18s/it]


 54%|█████▍    | 1689/3107 [8:41:13<6:01:06, 15.28s/it]
{'loss': 0.776, 'grad_norm': 0.14283832334343266, 'learning_rate': 9.078537860998155e-05, 'epoch': 0.54}

 54%|█████▍    | 1690/3107 [8:41:26<5:45:27, 14.63s/it]
[2024-05-30 09:21:18,079] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 54%|█████▍    | 1692/3107 [8:42:13<7:30:24, 19.10s/it]
[2024-05-30 09:21:40,977] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8346, 'grad_norm': 0.14273767880085567, 'learning_rate': 9.047395124406831e-05, 'epoch': 0.54}
[2024-05-30 09:22:05,115] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 54%|█████▍    | 1693/3107 [8:42:37<8:05:44, 20.61s/it]


 55%|█████▍    | 1695/3107 [8:43:13<7:32:31, 19.23s/it]
{'loss': 0.9505, 'grad_norm': 0.13623241729973798, 'learning_rate': 9.01626170868522e-05, 'epoch': 0.55}

 55%|█████▍    | 1696/3107 [8:43:32<7:32:15, 19.23s/it]

 55%|█████▍    | 1697/3107 [8:43:48<7:13:00, 18.43s/it]


 55%|█████▍    | 1699/3107 [8:44:23<6:54:33, 17.67s/it]
{'loss': 0.8934, 'grad_norm': 0.1411566710482285, 'learning_rate': 8.974765513358466e-05, 'epoch': 0.55}

 55%|█████▍    | 1700/3107 [8:44:44<7:16:20, 18.61s/it]

 55%|█████▍    | 1701/3107 [8:45:02<7:12:24, 18.45s/it]


 55%|█████▍    | 1703/3107 [8:45:35<6:42:40, 17.21s/it]
{'loss': 0.7781, 'grad_norm': 0.1389704091759394, 'learning_rate': 8.933287151837868e-05, 'epoch': 0.55}


 55%|█████▍    | 1705/3107 [8:46:11<7:03:20, 18.12s/it]
{'loss': 0.7712, 'grad_norm': 0.1269312863261449, 'learning_rate': 8.912554884239246e-05, 'epoch': 0.55}

 55%|█████▍    | 1706/3107 [8:46:26<6:39:48, 17.12s/it]

 55%|█████▍    | 1707/3107 [8:46:41<6:23:46, 16.45s/it]

 55%|█████▍    | 1708/3107 [8:47:04<7:14:42, 18.64s/it]

 55%|█████▌    | 1709/3107 [8:47:18<6:39:24, 17.14s/it]

 55%|█████▌    | 1710/3107 [8:47:32<6:19:42, 16.31s/it]

 55%|█████▌    | 1711/3107 [8:47:46<6:01:05, 15.52s/it]

 55%|█████▌    | 1712/3107 [8:48:01<5:58:15, 15.41s/it]

 55%|█████▌    | 1713/3107 [8:48:16<5:55:56, 15.32s/it]

 55%|█████▌    | 1714/3107 [8:48:35<6:16:59, 16.24s/it]

 55%|█████▌    | 1715/3107 [8:48:49<6:05:00, 15.73s/it]


 55%|█████▌    | 1717/3107 [8:49:33<7:18:33, 18.93s/it]
{'loss': 0.7408, 'grad_norm': 0.14384979649226426, 'learning_rate': 8.788263741595138e-05, 'epoch': 0.55}

 55%|█████▌    | 1718/3107 [8:49:58<8:01:44, 20.81s/it]


 55%|█████▌    | 1720/3107 [8:50:39<7:48:41, 20.27s/it]
{'loss': 0.7764, 'grad_norm': 0.13515112307266633, 'learning_rate': 8.75721983704687e-05, 'epoch': 0.55}

 55%|█████▌    | 1721/3107 [8:50:56<7:22:07, 19.14s/it]

 55%|█████▌    | 1722/3107 [8:51:14<7:14:18, 18.81s/it]

 55%|█████▌    | 1723/3107 [8:51:30<6:56:09, 18.04s/it]

 55%|█████▌    | 1724/3107 [8:51:54<7:40:24, 19.97s/it]

 56%|█████▌    | 1725/3107 [8:52:24<8:46:51, 22.87s/it]

 56%|█████▌    | 1726/3107 [8:52:38<7:42:00, 20.07s/it]

 56%|█████▌    | 1727/3107 [8:52:56<7:31:26, 19.63s/it]

 56%|█████▌    | 1728/3107 [8:53:18<7:48:03, 20.36s/it]

 56%|█████▌    | 1729/3107 [8:53:47<8:46:28, 22.92s/it]

 56%|█████▌    | 1730/3107 [8:54:03<7:54:27, 20.67s/it]

 56%|█████▌    | 1731/3107 [8:54:22<7:44:55, 20.27s/it]

 56%|█████▌    | 1732/3107 [8:54:39<7:20:29, 19.22s/it]

 56%|█████▌    | 1733/3107 [8:54:54<6:52:22, 18.01s/it]

 56%|█████▌    | 1734/3107 [8:55:10<6:39:59, 17.48s/it]

 56%|█████▌    | 1735/3107 [8:55:26<6:28:38, 17.00s/it]

 56%|█████▌    | 1736/3107 [8:55:42<6:22:05, 16.72s/it]


 56%|█████▌    | 1738/3107 [8:56:16<6:26:13, 16.93s/it]
{'loss': 0.8813, 'grad_norm': 0.14442339119732214, 'learning_rate': 8.57122239517493e-05, 'epoch': 0.56}

 56%|█████▌    | 1739/3107 [8:56:44<7:44:00, 20.35s/it]

 56%|█████▌    | 1740/3107 [8:57:04<7:42:23, 20.30s/it]

 56%|█████▌    | 1741/3107 [8:57:21<7:16:34, 19.18s/it]

 56%|█████▌    | 1742/3107 [8:57:39<7:08:53, 18.85s/it]

 56%|█████▌    | 1743/3107 [8:57:55<6:50:20, 18.05s/it]


 56%|█████▌    | 1745/3107 [8:58:22<6:02:35, 15.97s/it]
{'loss': 0.9605, 'grad_norm': 0.14882029034114944, 'learning_rate': 8.499022371232975e-05, 'epoch': 0.56}

 56%|█████▌    | 1746/3107 [8:58:39<6:14:03, 16.49s/it]

 56%|█████▌    | 1747/3107 [8:59:03<7:00:06, 18.53s/it]

 56%|█████▋    | 1748/3107 [8:59:17<6:31:31, 17.29s/it]

 56%|█████▋    | 1749/3107 [8:59:41<7:14:05, 19.18s/it]

 56%|█████▋    | 1750/3107 [8:59:57<6:57:02, 18.44s/it]

 56%|█████▋    | 1751/3107 [9:00:19<7:16:19, 19.31s/it]

 56%|█████▋    | 1752/3107 [9:00:36<7:02:34, 18.71s/it]

 56%|█████▋    | 1753/3107 [9:00:58<7:21:49, 19.58s/it]

 56%|█████▋    | 1754/3107 [9:01:18<7:28:13, 19.88s/it]

 56%|█████▋    | 1755/3107 [9:01:38<7:24:27, 19.72s/it]

 57%|█████▋    | 1756/3107 [9:01:52<6:45:42, 18.02s/it]

 57%|█████▋    | 1757/3107 [9:02:13<7:09:50, 19.10s/it]

 57%|█████▋    | 1758/3107 [9:02:37<7:40:09, 20.47s/it]

 57%|█████▋    | 1759/3107 [9:02:53<7:10:33, 19.16s/it]

 57%|█████▋    | 1760/3107 [9:03:08<6:44:27, 18.02s/it]

 57%|█████▋    | 1761/3107 [9:03:28<6:57:15, 18.60s/it]

 57%|█████▋    | 1762/3107 [9:03:45<6:47:38, 18.18s/it]

 57%|█████▋    | 1763/3107 [9:04:01<6:32:18, 17.51s/it]


 57%|█████▋    | 1765/3107 [9:04:38<6:43:12, 18.03s/it]
{'loss': 0.8571, 'grad_norm': 0.1477772014845417, 'learning_rate': 8.293190283421528e-05, 'epoch': 0.57}

 57%|█████▋    | 1766/3107 [9:04:55<6:35:17, 17.69s/it]

 57%|█████▋    | 1767/3107 [9:05:10<6:14:55, 16.79s/it]

 57%|█████▋    | 1768/3107 [9:05:23<5:50:05, 15.69s/it]

 57%|█████▋    | 1769/3107 [9:05:36<5:32:52, 14.93s/it]

 57%|█████▋    | 1770/3107 [9:05:51<5:32:29, 14.92s/it]

 57%|█████▋    | 1771/3107 [9:06:08<5:44:16, 15.46s/it]

 57%|█████▋    | 1772/3107 [9:06:20<5:23:31, 14.54s/it]

 57%|█████▋    | 1773/3107 [9:06:36<5:30:35, 14.87s/it]


 57%|█████▋    | 1775/3107 [9:07:08<5:43:56, 15.49s/it]
{'loss': 0.8495, 'grad_norm': 0.13834713170752017, 'learning_rate': 8.19054698879656e-05, 'epoch': 0.57}


 57%|█████▋    | 1777/3107 [9:07:42<5:56:43, 16.09s/it]
{'loss': 0.789, 'grad_norm': 0.14057199373558504, 'learning_rate': 8.170041579351073e-05, 'epoch': 0.57}

 57%|█████▋    | 1778/3107 [9:08:02<6:18:16, 17.08s/it]

 57%|█████▋    | 1779/3107 [9:08:30<7:32:42, 20.45s/it]

 57%|█████▋    | 1780/3107 [9:08:45<6:55:06, 18.77s/it]

 57%|█████▋    | 1781/3107 [9:09:04<6:57:00, 18.87s/it]

 57%|█████▋    | 1782/3107 [9:09:25<7:14:44, 19.69s/it]

 57%|█████▋    | 1783/3107 [9:09:49<7:37:47, 20.75s/it]

 57%|█████▋    | 1784/3107 [9:10:11<7:48:12, 21.23s/it]

 57%|█████▋    | 1785/3107 [9:10:30<7:30:33, 20.45s/it]

 57%|█████▋    | 1786/3107 [9:10:47<7:07:43, 19.43s/it]

 58%|█████▊    | 1787/3107 [9:11:16<8:11:42, 22.35s/it]


 58%|█████▊    | 1789/3107 [9:11:52<7:23:25, 20.19s/it]
{'loss': 0.8335, 'grad_norm': 0.14059766037864843, 'learning_rate': 8.047179358725487e-05, 'epoch': 0.58}

 58%|█████▊    | 1790/3107 [9:12:13<7:28:01, 20.41s/it]

 58%|█████▊    | 1791/3107 [9:12:34<7:30:40, 20.55s/it]

 58%|█████▊    | 1792/3107 [9:12:58<7:51:56, 21.53s/it]

 58%|█████▊    | 1793/3107 [9:13:21<8:01:05, 21.97s/it]

 58%|█████▊    | 1794/3107 [9:13:37<7:20:08, 20.11s/it]

 58%|█████▊    | 1795/3107 [9:13:54<7:00:38, 19.24s/it]

 58%|█████▊    | 1796/3107 [9:14:12<6:52:51, 18.89s/it]

 58%|█████▊    | 1797/3107 [9:14:28<6:34:40, 18.08s/it]


 58%|█████▊    | 1799/3107 [9:14:57<5:54:19, 16.25s/it]

 58%|█████▊    | 1800/3107 [9:15:15<6:06:12, 16.81s/it]
 58%|█████▊    | 1800/3107 [9:15:15<6:06:12, 16.81s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 58%|█████▊    | 1801/3107 [9:16:03<9:29:06, 26.15s/it]
{'loss': 0.6923, 'grad_norm': 0.1415575693725724, 'learning_rate': 7.924622855857316e-05, 'epoch': 0.58}

 58%|█████▊    | 1802/3107 [9:16:19<8:24:41, 23.20s/it]

 58%|█████▊    | 1803/3107 [9:16:49<9:11:18, 25.37s/it]

 58%|█████▊    | 1804/3107 [9:17:13<8:56:19, 24.70s/it]

 58%|█████▊    | 1805/3107 [9:17:26<7:44:28, 21.40s/it]

 58%|█████▊    | 1806/3107 [9:17:38<6:40:47, 18.48s/it]


 58%|█████▊    | 1808/3107 [9:18:11<6:18:12, 17.47s/it]
{'loss': 0.849, 'grad_norm': 0.15477458502174596, 'learning_rate': 7.853280376545691e-05, 'epoch': 0.58}

 58%|█████▊    | 1809/3107 [9:18:24<5:53:23, 16.34s/it]

 58%|█████▊    | 1810/3107 [9:18:41<5:57:05, 16.52s/it]

 58%|█████▊    | 1811/3107 [9:18:56<5:44:12, 15.94s/it]
[2024-05-30 09:58:48,349] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 58%|█████▊    | 1812/3107 [9:19:20<6:37:04, 18.40s/it]

 58%|█████▊    | 1813/3107 [9:19:43<7:03:49, 19.65s/it]

 58%|█████▊    | 1814/3107 [9:19:56<6:24:46, 17.85s/it]

 58%|█████▊    | 1815/3107 [9:20:13<6:15:20, 17.43s/it]

 58%|█████▊    | 1816/3107 [9:20:31<6:19:21, 17.63s/it]

 58%|█████▊    | 1817/3107 [9:20:47<6:08:40, 17.15s/it]

 59%|█████▊    | 1818/3107 [9:21:06<6:23:11, 17.84s/it]

 59%|█████▊    | 1819/3107 [9:21:26<6:31:18, 18.23s/it]

 59%|█████▊    | 1820/3107 [9:21:48<6:55:25, 19.37s/it]

 59%|█████▊    | 1821/3107 [9:22:03<6:27:44, 18.09s/it]

 59%|█████▊    | 1822/3107 [9:22:21<6:29:27, 18.19s/it]

 59%|█████▊    | 1823/3107 [9:22:35<5:59:39, 16.81s/it]

 59%|█████▊    | 1824/3107 [9:22:50<5:50:11, 16.38s/it]


 59%|█████▉    | 1826/3107 [9:23:15<5:06:37, 14.36s/it]
{'loss': 0.9648, 'grad_norm': 0.15013431241239664, 'learning_rate': 7.670362529883173e-05, 'epoch': 0.59}

 59%|█████▉    | 1827/3107 [9:23:37<5:53:07, 16.55s/it]

 59%|█████▉    | 1828/3107 [9:23:51<5:35:28, 15.74s/it]

 59%|█████▉    | 1829/3107 [9:24:03<5:10:53, 14.60s/it]

 59%|█████▉    | 1830/3107 [9:24:18<5:16:14, 14.86s/it]

 59%|█████▉    | 1831/3107 [9:24:35<5:32:01, 15.61s/it]

 59%|█████▉    | 1832/3107 [9:24:50<5:23:02, 15.20s/it]

 59%|█████▉    | 1833/3107 [9:25:09<5:49:56, 16.48s/it]

 59%|█████▉    | 1834/3107 [9:25:33<6:36:11, 18.67s/it]

 59%|█████▉    | 1835/3107 [9:25:50<6:23:29, 18.09s/it]

 59%|█████▉    | 1836/3107 [9:26:04<5:58:24, 16.92s/it]

 59%|█████▉    | 1837/3107 [9:26:19<5:45:57, 16.34s/it]


 59%|█████▉    | 1839/3107 [9:26:50<5:29:11, 15.58s/it]
{'loss': 0.9227, 'grad_norm': 0.1472965147278098, 'learning_rate': 7.538761828301276e-05, 'epoch': 0.59}

 59%|█████▉    | 1840/3107 [9:27:15<6:29:33, 18.45s/it]


 59%|█████▉    | 1842/3107 [9:27:56<6:59:45, 19.91s/it]
{'loss': 0.7952, 'grad_norm': 0.13341448011735196, 'learning_rate': 7.508455775278867e-05, 'epoch': 0.59}

 59%|█████▉    | 1843/3107 [9:28:15<6:57:44, 19.83s/it]

 59%|█████▉    | 1844/3107 [9:28:33<6:41:11, 19.06s/it]

 59%|█████▉    | 1845/3107 [9:28:50<6:33:28, 18.71s/it]

 59%|█████▉    | 1846/3107 [9:29:05<6:09:19, 17.57s/it]

 59%|█████▉    | 1847/3107 [9:29:21<5:56:20, 16.97s/it]

 59%|█████▉    | 1848/3107 [9:29:39<6:01:38, 17.24s/it]


 60%|█████▉    | 1850/3107 [9:30:24<6:57:01, 19.91s/it]
{'loss': 0.7704, 'grad_norm': 0.14207799571544455, 'learning_rate': 7.427759623747541e-05, 'epoch': 0.6}

 60%|█████▉    | 1851/3107 [9:30:43<6:51:39, 19.66s/it]

 60%|█████▉    | 1852/3107 [9:31:05<7:07:35, 20.44s/it]

 60%|█████▉    | 1853/3107 [9:31:25<7:01:48, 20.18s/it]

 60%|█████▉    | 1854/3107 [9:31:39<6:22:47, 18.33s/it]

 60%|█████▉    | 1855/3107 [9:31:50<5:40:01, 16.30s/it]

 60%|█████▉    | 1856/3107 [9:32:10<6:03:47, 17.45s/it]

 60%|█████▉    | 1857/3107 [9:32:35<6:50:37, 19.71s/it]

 60%|█████▉    | 1858/3107 [9:32:53<6:34:57, 18.97s/it]


 60%|█████▉    | 1860/3107 [9:33:30<6:39:39, 19.23s/it]
{'loss': 0.8398, 'grad_norm': 0.12732905669049804, 'learning_rate': 7.327141773636772e-05, 'epoch': 0.6}

 60%|█████▉    | 1861/3107 [9:33:47<6:25:57, 18.59s/it]

 60%|█████▉    | 1862/3107 [9:34:07<6:36:47, 19.12s/it]

 60%|█████▉    | 1863/3107 [9:34:22<6:09:43, 17.83s/it]

 60%|█████▉    | 1864/3107 [9:34:34<5:29:52, 15.92s/it]

 60%|██████    | 1865/3107 [9:34:47<5:15:22, 15.24s/it]

 60%|██████    | 1866/3107 [9:35:01<5:05:04, 14.75s/it]

 60%|██████    | 1867/3107 [9:35:17<5:12:50, 15.14s/it]


 60%|██████    | 1869/3107 [9:35:40<4:36:05, 13.38s/it]

 60%|██████    | 1870/3107 [9:35:58<5:05:17, 14.81s/it]
{'loss': 0.8344, 'grad_norm': 0.13832126447176457, 'learning_rate': 7.226814508712472e-05, 'epoch': 0.6}


 60%|██████    | 1872/3107 [9:36:26<5:00:24, 14.60s/it]
{'loss': 0.6833, 'grad_norm': 0.13652770656437394, 'learning_rate': 7.206784886360139e-05, 'epoch': 0.6}

 60%|██████    | 1873/3107 [9:36:39<4:47:22, 13.97s/it]

 60%|██████    | 1874/3107 [9:36:56<5:06:15, 14.90s/it]

 60%|██████    | 1875/3107 [9:37:05<4:34:24, 13.36s/it]

 60%|██████    | 1876/3107 [9:37:25<5:12:27, 15.23s/it]

 60%|██████    | 1877/3107 [9:37:45<5:39:43, 16.57s/it]

 60%|██████    | 1878/3107 [9:38:07<6:17:16, 18.42s/it]

 60%|██████    | 1879/3107 [9:38:21<5:47:28, 16.98s/it]

 61%|██████    | 1880/3107 [9:38:47<6:42:37, 19.69s/it]

 61%|██████    | 1881/3107 [9:39:02<6:11:08, 18.16s/it]

 61%|██████    | 1882/3107 [9:39:29<7:10:05, 21.07s/it]

 61%|██████    | 1883/3107 [9:39:50<7:03:47, 20.77s/it]

 61%|██████    | 1884/3107 [9:40:13<7:20:40, 21.62s/it]


 61%|██████    | 1886/3107 [9:40:44<6:24:10, 18.88s/it]
{'loss': 0.8, 'grad_norm': 0.1367456188284143, 'learning_rate': 7.066922514359564e-05, 'epoch': 0.61}

 61%|██████    | 1887/3107 [9:40:58<5:50:18, 17.23s/it]


 61%|██████    | 1889/3107 [9:41:38<6:14:21, 18.44s/it]
{'loss': 0.747, 'grad_norm': 0.13084765844266774, 'learning_rate': 7.037032305579409e-05, 'epoch': 0.61}

 61%|██████    | 1890/3107 [9:42:04<6:58:39, 20.64s/it]

 61%|██████    | 1891/3107 [9:42:18<6:16:56, 18.60s/it]

 61%|██████    | 1892/3107 [9:42:43<6:53:35, 20.42s/it]

 61%|██████    | 1893/3107 [9:43:00<6:33:25, 19.44s/it]

 61%|██████    | 1894/3107 [9:43:17<6:22:15, 18.91s/it]

 61%|██████    | 1895/3107 [9:43:30<5:43:43, 17.02s/it]

 61%|██████    | 1896/3107 [9:43:47<5:43:23, 17.01s/it]

 61%|██████    | 1897/3107 [9:44:00<5:17:16, 15.73s/it]

 61%|██████    | 1898/3107 [9:44:18<5:32:29, 16.50s/it]

 61%|██████    | 1899/3107 [9:44:34<5:30:53, 16.44s/it]

 61%|██████    | 1900/3107 [9:44:48<5:15:28, 15.68s/it]

 61%|██████    | 1901/3107 [9:45:15<6:23:40, 19.09s/it]

 61%|██████    | 1902/3107 [9:45:30<5:56:33, 17.75s/it]

 61%|██████    | 1903/3107 [9:45:48<5:58:58, 17.89s/it]

 61%|██████▏   | 1904/3107 [9:46:04<5:45:56, 17.25s/it]

 61%|██████▏   | 1905/3107 [9:46:24<6:00:50, 18.01s/it]


 61%|██████▏   | 1907/3107 [9:46:59<6:02:50, 18.14s/it]

 61%|██████▏   | 1908/3107 [9:47:19<6:12:04, 18.62s/it]
{'loss': 0.8583, 'grad_norm': 0.14377738150389655, 'learning_rate': 6.848412946285292e-05, 'epoch': 0.61}

 61%|██████▏   | 1909/3107 [9:47:30<5:31:07, 16.58s/it]

 61%|██████▏   | 1910/3107 [9:47:48<5:36:24, 16.86s/it]


 62%|██████▏   | 1912/3107 [9:48:19<5:13:15, 15.73s/it]

 62%|██████▏   | 1913/3107 [9:48:39<5:40:03, 17.09s/it]

 62%|██████▏   | 1914/3107 [9:48:55<5:32:29, 16.72s/it]

 62%|██████▏   | 1915/3107 [9:49:13<5:39:24, 17.08s/it]

 62%|██████▏   | 1916/3107 [9:49:33<5:57:07, 17.99s/it]
{'loss': 0.8235, 'grad_norm': 0.12765142010274938, 'learning_rate': 6.769360044423547e-05, 'epoch': 0.62}

 62%|██████▏   | 1917/3107 [9:49:45<5:24:53, 16.38s/it]

 62%|██████▏   | 1918/3107 [9:50:02<5:26:28, 16.48s/it]

 62%|██████▏   | 1919/3107 [9:50:16<5:10:15, 15.67s/it]


 62%|██████▏   | 1921/3107 [9:50:45<4:58:50, 15.12s/it]
{'loss': 0.6881, 'grad_norm': 0.13956730748557747, 'learning_rate': 6.720065780936286e-05, 'epoch': 0.62}

 62%|██████▏   | 1922/3107 [9:51:04<5:23:11, 16.36s/it]

 62%|██████▏   | 1923/3107 [9:51:16<4:53:29, 14.87s/it]

 62%|██████▏   | 1924/3107 [9:51:35<5:17:43, 16.11s/it]

 62%|██████▏   | 1925/3107 [9:51:57<5:51:33, 17.85s/it]

 62%|██████▏   | 1926/3107 [9:52:24<6:45:59, 20.63s/it]


 62%|██████▏   | 1928/3107 [9:53:01<6:22:35, 19.47s/it]
{'loss': 0.8442, 'grad_norm': 0.1413107976749751, 'learning_rate': 6.651203878290139e-05, 'epoch': 0.62}

 62%|██████▏   | 1929/3107 [9:53:20<6:20:01, 19.36s/it]


 62%|██████▏   | 1931/3107 [9:54:01<6:39:34, 20.39s/it]
{'loss': 0.8783, 'grad_norm': 0.13386951064690675, 'learning_rate': 6.62174603165688e-05, 'epoch': 0.62}

 62%|██████▏   | 1932/3107 [9:54:22<6:42:21, 20.55s/it]

 62%|██████▏   | 1933/3107 [9:54:41<6:30:44, 19.97s/it]

 62%|██████▏   | 1934/3107 [9:55:06<7:03:37, 21.67s/it]

 62%|██████▏   | 1935/3107 [9:55:20<6:15:04, 19.20s/it]

 62%|██████▏   | 1936/3107 [9:55:39<6:12:08, 19.07s/it]


 62%|██████▏   | 1938/3107 [9:56:17<6:18:39, 19.43s/it]
{'loss': 0.8877, 'grad_norm': 0.14348363761764907, 'learning_rate': 6.553140100613473e-05, 'epoch': 0.62}

 62%|██████▏   | 1939/3107 [9:56:29<5:32:23, 17.07s/it]

 62%|██████▏   | 1940/3107 [9:56:48<5:43:13, 17.65s/it]

 62%|██████▏   | 1941/3107 [9:57:08<5:56:49, 18.36s/it]

 63%|██████▎   | 1942/3107 [9:57:24<5:43:42, 17.70s/it]

 63%|██████▎   | 1943/3107 [9:57:37<5:16:44, 16.33s/it]


 63%|██████▎   | 1945/3107 [9:58:08<5:08:04, 15.91s/it]
{'loss': 0.8252, 'grad_norm': 0.14547697438632365, 'learning_rate': 6.484717789266719e-05, 'epoch': 0.63}

 63%|██████▎   | 1946/3107 [9:58:22<4:57:52, 15.39s/it]

 63%|██████▎   | 1947/3107 [9:58:40<5:16:41, 16.38s/it]

 63%|██████▎   | 1948/3107 [9:58:59<5:27:04, 16.93s/it]

 63%|██████▎   | 1949/3107 [9:59:14<5:16:19, 16.39s/it]


 63%|██████▎   | 1951/3107 [9:59:50<5:31:00, 17.18s/it]
{'loss': 0.7527, 'grad_norm': 0.13405014843390037, 'learning_rate': 6.426219003331993e-05, 'epoch': 0.63}

 63%|██████▎   | 1952/3107 [10:00:03<5:09:48, 16.09s/it]
[2024-05-30 10:39:55,663] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1953/3107 [10:00:27<5:57:19, 18.58s/it]
[2024-05-30 10:40:23,284] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1954/3107 [10:00:55<6:49:08, 21.29s/it]

 63%|██████▎   | 1955/3107 [10:01:11<6:17:52, 19.68s/it]

 63%|██████▎   | 1956/3107 [10:01:28<6:04:53, 19.02s/it]

 63%|██████▎   | 1957/3107 [10:01:42<5:34:51, 17.47s/it]

 63%|██████▎   | 1958/3107 [10:01:58<5:27:00, 17.08s/it]

 63%|██████▎   | 1959/3107 [10:02:13<5:09:43, 16.19s/it]

 63%|██████▎   | 1960/3107 [10:02:33<5:36:23, 17.60s/it]

 63%|██████▎   | 1961/3107 [10:02:55<5:57:34, 18.72s/it]

 63%|██████▎   | 1962/3107 [10:03:13<5:55:59, 18.65s/it]

 63%|██████▎   | 1963/3107 [10:03:27<5:28:50, 17.25s/it]

 63%|██████▎   | 1964/3107 [10:03:45<5:29:07, 17.28s/it]

 63%|██████▎   | 1965/3107 [10:03:57<5:03:26, 15.94s/it]

 63%|██████▎   | 1966/3107 [10:04:16<5:20:20, 16.85s/it]

 63%|██████▎   | 1967/3107 [10:04:31<5:07:43, 16.20s/it]

 63%|██████▎   | 1968/3107 [10:04:49<5:19:26, 16.83s/it]

 63%|██████▎   | 1969/3107 [10:05:07<5:21:40, 16.96s/it]

 63%|██████▎   | 1970/3107 [10:05:29<5:50:12, 18.48s/it]

 63%|██████▎   | 1971/3107 [10:05:44<5:33:57, 17.64s/it]

 63%|██████▎   | 1972/3107 [10:05:59<5:16:35, 16.74s/it]


 64%|██████▎   | 1974/3107 [10:06:36<5:35:51, 17.79s/it]

 64%|██████▎   | 1975/3107 [10:06:50<5:15:52, 16.74s/it]
{'loss': 0.7895, 'grad_norm': 0.13955283078449363, 'learning_rate': 6.193645391491611e-05, 'epoch': 0.64}

 64%|██████▎   | 1976/3107 [10:07:11<5:36:20, 17.84s/it]

 64%|██████▎   | 1977/3107 [10:07:23<5:03:47, 16.13s/it]

 64%|██████▎   | 1978/3107 [10:07:37<4:53:05, 15.58s/it]

 64%|██████▎   | 1979/3107 [10:07:58<5:20:53, 17.07s/it]


 64%|██████▍   | 1981/3107 [10:08:40<5:58:59, 19.13s/it]
{'loss': 0.7909, 'grad_norm': 0.14506953172219977, 'learning_rate': 6.13586874973636e-05, 'epoch': 0.64}

 64%|██████▍   | 1982/3107 [10:09:06<6:34:49, 21.06s/it]

 64%|██████▍   | 1983/3107 [10:09:20<5:54:31, 18.93s/it]

 64%|██████▍   | 1984/3107 [10:09:51<7:01:07, 22.50s/it]

 64%|██████▍   | 1985/3107 [10:10:10<6:42:01, 21.50s/it]

 64%|██████▍   | 1986/3107 [10:10:27<6:16:17, 20.14s/it]

 64%|██████▍   | 1987/3107 [10:10:55<7:01:39, 22.59s/it]


 64%|██████▍   | 1989/3107 [10:11:30<6:14:27, 20.10s/it]
{'loss': 0.834, 'grad_norm': 0.15845812298604997, 'learning_rate': 6.0590688729797295e-05, 'epoch': 0.64}

 64%|██████▍   | 1990/3107 [10:11:42<5:25:24, 17.48s/it]

 64%|██████▍   | 1991/3107 [10:12:06<6:04:37, 19.60s/it]

 64%|██████▍   | 1992/3107 [10:12:25<6:00:20, 19.39s/it]

 64%|██████▍   | 1993/3107 [10:12:40<5:33:55, 17.99s/it]

 64%|██████▍   | 1994/3107 [10:13:01<5:52:34, 19.01s/it]

 64%|██████▍   | 1995/3107 [10:13:21<5:58:54, 19.37s/it]

 64%|██████▍   | 1996/3107 [10:13:35<5:26:05, 17.61s/it]

 64%|██████▍   | 1997/3107 [10:13:47<4:58:21, 16.13s/it]

 64%|██████▍   | 1998/3107 [10:14:06<5:11:55, 16.88s/it]

 64%|██████▍   | 1999/3107 [10:14:18<4:44:09, 15.39s/it]


 64%|██████▍   | 2001/3107 [10:15:01<5:44:03, 18.67s/it]
{'loss': 0.7551, 'grad_norm': 0.15470068007774243, 'learning_rate': 5.94438485976611e-05, 'epoch': 0.64}

 64%|██████▍   | 2002/3107 [10:15:22<5:57:30, 19.41s/it]

 64%|██████▍   | 2003/3107 [10:15:50<6:47:41, 22.16s/it]


 65%|██████▍   | 2005/3107 [10:16:27<6:07:19, 20.00s/it]
{'loss': 0.8844, 'grad_norm': 0.15308641119875144, 'learning_rate': 5.9062970636263616e-05, 'epoch': 0.65}

 65%|██████▍   | 2006/3107 [10:16:42<5:39:11, 18.48s/it]

 65%|██████▍   | 2007/3107 [10:17:02<5:49:48, 19.08s/it]


 65%|██████▍   | 2009/3107 [10:17:39<5:34:45, 18.29s/it]

 65%|██████▍   | 2010/3107 [10:17:57<5:33:08, 18.22s/it]
{'loss': 0.8151, 'grad_norm': 0.13676778210009138, 'learning_rate': 5.858787534154607e-05, 'epoch': 0.65}

 65%|██████▍   | 2011/3107 [10:18:11<5:14:06, 17.20s/it]

 65%|██████▍   | 2012/3107 [10:18:24<4:48:41, 15.82s/it]

 65%|██████▍   | 2013/3107 [10:18:39<4:44:36, 15.61s/it]

 65%|██████▍   | 2014/3107 [10:18:53<4:36:06, 15.16s/it]

 65%|██████▍   | 2015/3107 [10:19:14<5:06:05, 16.82s/it]

 65%|██████▍   | 2016/3107 [10:19:40<5:53:42, 19.45s/it]

 65%|██████▍   | 2017/3107 [10:19:57<5:43:49, 18.93s/it]

 65%|██████▍   | 2018/3107 [10:20:14<5:32:02, 18.29s/it]

 65%|██████▍   | 2019/3107 [10:20:29<5:15:26, 17.40s/it]

 65%|██████▌   | 2020/3107 [10:20:45<5:03:15, 16.74s/it]

 65%|██████▌   | 2021/3107 [10:21:05<5:23:11, 17.86s/it]

 65%|██████▌   | 2022/3107 [10:21:28<5:49:00, 19.30s/it]

 65%|██████▌   | 2023/3107 [10:21:49<5:58:44, 19.86s/it]

 65%|██████▌   | 2024/3107 [10:22:08<5:57:08, 19.79s/it]

 65%|██████▌   | 2025/3107 [10:22:27<5:47:37, 19.28s/it]

 65%|██████▌   | 2026/3107 [10:22:44<5:36:10, 18.66s/it]

 65%|██████▌   | 2027/3107 [10:22:59<5:19:34, 17.75s/it]

 65%|██████▌   | 2028/3107 [10:23:14<5:00:47, 16.73s/it]

 65%|██████▌   | 2029/3107 [10:23:30<4:55:52, 16.47s/it]

 65%|██████▌   | 2030/3107 [10:23:48<5:05:38, 17.03s/it]

 65%|██████▌   | 2031/3107 [10:24:01<4:44:03, 15.84s/it]

 65%|██████▌   | 2032/3107 [10:24:12<4:17:53, 14.39s/it]

 65%|██████▌   | 2033/3107 [10:24:30<4:37:11, 15.49s/it]

 65%|██████▌   | 2034/3107 [10:24:48<4:50:35, 16.25s/it]

 65%|██████▌   | 2035/3107 [10:25:02<4:35:07, 15.40s/it]

 66%|██████▌   | 2036/3107 [10:25:21<4:54:34, 16.50s/it]

 66%|██████▌   | 2037/3107 [10:25:41<5:13:40, 17.59s/it]

 66%|██████▌   | 2038/3107 [10:25:56<5:02:30, 16.98s/it]

 66%|██████▌   | 2039/3107 [10:26:16<5:16:02, 17.75s/it]

 66%|██████▌   | 2040/3107 [10:26:29<4:50:16, 16.32s/it]

 66%|██████▌   | 2041/3107 [10:26:48<5:04:45, 17.15s/it]

 66%|██████▌   | 2042/3107 [10:27:04<4:57:48, 16.78s/it]

 66%|██████▌   | 2043/3107 [10:27:24<5:14:57, 17.76s/it]

 66%|██████▌   | 2044/3107 [10:27:43<5:23:30, 18.26s/it]

 66%|██████▌   | 2045/3107 [10:28:07<5:50:22, 19.80s/it]

 66%|██████▌   | 2046/3107 [10:28:28<5:56:40, 20.17s/it]

 66%|██████▌   | 2047/3107 [10:28:41<5:19:01, 18.06s/it]

 66%|██████▌   | 2048/3107 [10:29:12<6:30:07, 22.10s/it]

 66%|██████▌   | 2049/3107 [10:29:27<5:51:22, 19.93s/it]

 66%|██████▌   | 2050/3107 [10:29:43<5:28:57, 18.67s/it]

 66%|██████▌   | 2051/3107 [10:30:05<5:48:35, 19.81s/it]

 66%|██████▌   | 2052/3107 [10:30:19<5:15:48, 17.96s/it]

 66%|██████▌   | 2053/3107 [10:30:39<5:24:31, 18.47s/it]

 66%|██████▌   | 2054/3107 [10:30:52<4:59:05, 17.04s/it]

 66%|██████▌   | 2055/3107 [10:31:14<5:23:03, 18.42s/it]

 66%|██████▌   | 2056/3107 [10:31:33<5:26:52, 18.66s/it]

 66%|██████▌   | 2057/3107 [10:31:56<5:46:21, 19.79s/it]

 66%|██████▌   | 2058/3107 [10:32:14<5:37:59, 19.33s/it]

 66%|██████▋   | 2059/3107 [10:32:32<5:29:55, 18.89s/it]

 66%|██████▋   | 2060/3107 [10:32:58<6:07:47, 21.08s/it]

 66%|██████▋   | 2061/3107 [10:33:13<5:33:09, 19.11s/it]

 66%|██████▋   | 2062/3107 [10:33:33<5:40:35, 19.56s/it]

 66%|██████▋   | 2063/3107 [10:33:55<5:54:28, 20.37s/it]

 66%|██████▋   | 2064/3107 [10:34:19<6:09:13, 21.24s/it]

 66%|██████▋   | 2065/3107 [10:34:38<6:01:08, 20.80s/it]

 66%|██████▋   | 2066/3107 [10:34:57<5:49:17, 20.13s/it]

 67%|██████▋   | 2067/3107 [10:35:13<5:29:24, 19.00s/it]

 67%|██████▋   | 2068/3107 [10:35:35<5:40:05, 19.64s/it]

 67%|██████▋   | 2069/3107 [10:35:56<5:50:55, 20.28s/it]

 67%|██████▋   | 2070/3107 [10:36:11<5:19:47, 18.50s/it]

 67%|██████▋   | 2071/3107 [10:36:30<5:21:47, 18.64s/it]

 67%|██████▋   | 2072/3107 [10:36:42<4:49:10, 16.76s/it]

 67%|██████▋   | 2073/3107 [10:37:00<4:54:48, 17.11s/it]
[2024-05-30 11:16:55,846] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 2074/3107 [10:37:28<5:49:05, 20.28s/it]

 67%|██████▋   | 2075/3107 [10:37:46<5:36:44, 19.58s/it]

 67%|██████▋   | 2076/3107 [10:37:59<5:06:27, 17.83s/it]

 67%|██████▋   | 2077/3107 [10:38:18<5:11:43, 18.16s/it]

 67%|██████▋   | 2078/3107 [10:38:35<5:01:42, 17.59s/it]

 67%|██████▋   | 2079/3107 [10:39:05<6:07:21, 21.44s/it]

 67%|██████▋   | 2080/3107 [10:39:24<5:54:33, 20.71s/it]

 67%|██████▋   | 2081/3107 [10:39:43<5:44:22, 20.14s/it]

 67%|██████▋   | 2082/3107 [10:40:01<5:31:56, 19.43s/it]

 67%|██████▋   | 2083/3107 [10:40:18<5:21:04, 18.81s/it]

 67%|██████▋   | 2084/3107 [10:40:35<5:13:08, 18.37s/it]

 67%|██████▋   | 2085/3107 [10:40:53<5:08:35, 18.12s/it]

 67%|██████▋   | 2086/3107 [10:41:11<5:10:18, 18.24s/it]

 67%|██████▋   | 2087/3107 [10:41:25<4:49:00, 17.00s/it]

 67%|██████▋   | 2088/3107 [10:41:39<4:31:59, 16.02s/it]

 67%|██████▋   | 2089/3107 [10:41:52<4:15:16, 15.05s/it]

 67%|██████▋   | 2090/3107 [10:42:14<4:52:32, 17.26s/it]

 67%|██████▋   | 2091/3107 [10:42:37<5:21:50, 19.01s/it]

 67%|██████▋   | 2092/3107 [10:42:54<5:07:36, 18.18s/it]

 67%|██████▋   | 2093/3107 [10:43:08<4:45:38, 16.90s/it]

 67%|██████▋   | 2094/3107 [10:43:23<4:36:20, 16.37s/it]


 67%|██████▋   | 2096/3107 [10:44:04<5:12:31, 18.55s/it]

 67%|██████▋   | 2097/3107 [10:44:29<5:46:43, 20.60s/it]

 68%|██████▊   | 2098/3107 [10:44:51<5:55:36, 21.15s/it]

 68%|██████▊   | 2099/3107 [10:45:04<5:11:27, 18.54s/it]

 68%|██████▊   | 2100/3107 [10:45:20<4:57:57, 17.75s/it]
 68%|██████▊   | 2100/3107 [10:45:20<4:57:57, 17.75s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 68%|██████▊   | 2101/3107 [10:46:03<7:06:37, 25.44s/it]

 68%|██████▊   | 2102/3107 [10:46:22<6:31:28, 23.37s/it]

 68%|██████▊   | 2103/3107 [10:46:44<6:25:28, 23.04s/it]

 68%|██████▊   | 2104/3107 [10:47:11<6:43:28, 24.14s/it]

 68%|██████▊   | 2105/3107 [10:47:26<5:57:10, 21.39s/it]

 68%|██████▊   | 2106/3107 [10:47:57<6:47:39, 24.44s/it]

 68%|██████▊   | 2107/3107 [10:48:16<6:17:09, 22.63s/it]

 68%|██████▊   | 2108/3107 [10:48:36<6:07:46, 22.09s/it]
{'loss': 0.7802, 'grad_norm': 0.1232234300853606, 'learning_rate': 4.9519182658204044e-05, 'epoch': 0.68}


 68%|██████▊   | 2110/3107 [10:49:18<5:55:45, 21.41s/it]

 68%|██████▊   | 2111/3107 [10:49:41<6:03:43, 21.91s/it]

 68%|██████▊   | 2112/3107 [10:50:05<6:11:24, 22.40s/it]

 68%|██████▊   | 2113/3107 [10:50:24<5:51:48, 21.24s/it]
{'loss': 0.9693, 'grad_norm': 0.14474339866812982, 'learning_rate': 4.9069833894197984e-05, 'epoch': 0.68}


 68%|██████▊   | 2115/3107 [10:50:56<5:08:03, 18.63s/it]

 68%|██████▊   | 2116/3107 [10:51:18<5:25:30, 19.71s/it]

 68%|██████▊   | 2117/3107 [10:51:35<5:10:24, 18.81s/it]

 68%|██████▊   | 2118/3107 [10:51:49<4:47:18, 17.43s/it]

 68%|██████▊   | 2119/3107 [10:52:03<4:30:56, 16.45s/it]

 68%|██████▊   | 2120/3107 [10:52:25<4:58:34, 18.15s/it]

 68%|██████▊   | 2121/3107 [10:52:42<4:53:35, 17.87s/it]

 68%|██████▊   | 2122/3107 [10:52:58<4:43:56, 17.30s/it]

 68%|██████▊   | 2123/3107 [10:53:15<4:43:04, 17.26s/it]
{'loss': 0.9462, 'grad_norm': 0.13443514057795047, 'learning_rate': 4.817530130181785e-05, 'epoch': 0.68}


 68%|██████▊   | 2125/3107 [10:54:00<5:32:55, 20.34s/it]

 68%|██████▊   | 2126/3107 [10:54:22<5:37:04, 20.62s/it]

 68%|██████▊   | 2127/3107 [10:54:45<5:49:38, 21.41s/it]

 68%|██████▊   | 2128/3107 [10:55:06<5:49:58, 21.45s/it]

 69%|██████▊   | 2129/3107 [10:55:28<5:49:26, 21.44s/it]

 69%|██████▊   | 2130/3107 [10:55:49<5:45:58, 21.25s/it]

 69%|██████▊   | 2131/3107 [10:56:03<5:09:59, 19.06s/it]

 69%|██████▊   | 2132/3107 [10:56:13<4:25:10, 16.32s/it]

 69%|██████▊   | 2133/3107 [10:56:25<4:07:25, 15.24s/it]

 69%|██████▊   | 2134/3107 [10:56:49<4:46:41, 17.68s/it]

 69%|██████▊   | 2135/3107 [10:57:09<5:00:06, 18.53s/it]

 69%|██████▊   | 2136/3107 [10:57:27<4:58:19, 18.43s/it]

 69%|██████▉   | 2137/3107 [10:57:41<4:33:19, 16.91s/it]

 69%|██████▉   | 2138/3107 [10:58:02<4:54:03, 18.21s/it]

 69%|██████▉   | 2139/3107 [10:58:35<6:06:00, 22.69s/it]

 69%|██████▉   | 2140/3107 [10:58:56<5:56:16, 22.11s/it]

 69%|██████▉   | 2141/3107 [10:59:20<6:05:31, 22.70s/it]

 69%|██████▉   | 2142/3107 [10:59:31<5:08:41, 19.19s/it]

 69%|██████▉   | 2143/3107 [10:59:50<5:08:41, 19.21s/it]

 69%|██████▉   | 2144/3107 [11:00:09<5:05:02, 19.01s/it]
{'loss': 0.8947, 'grad_norm': 0.1435910871706873, 'learning_rate': 4.631523758577475e-05, 'epoch': 0.69}


 69%|██████▉   | 2146/3107 [11:00:46<4:58:06, 18.61s/it]

 69%|██████▉   | 2147/3107 [11:00:57<4:23:13, 16.45s/it]

 69%|██████▉   | 2148/3107 [11:01:10<4:05:20, 15.35s/it]
{'loss': 0.9069, 'grad_norm': 0.16108257171478965, 'learning_rate': 4.59638307899667e-05, 'epoch': 0.69}


 69%|██████▉   | 2150/3107 [11:01:43<4:10:39, 15.72s/it]

 69%|██████▉   | 2151/3107 [11:02:05<4:43:49, 17.81s/it]

 69%|██████▉   | 2152/3107 [11:02:23<4:41:37, 17.69s/it]

 69%|██████▉   | 2153/3107 [11:02:37<4:24:46, 16.65s/it]

 69%|██████▉   | 2154/3107 [11:02:56<4:35:37, 17.35s/it]

 69%|██████▉   | 2155/3107 [11:03:11<4:23:42, 16.62s/it]
{'loss': 0.9273, 'grad_norm': 0.1482360110855091, 'learning_rate': 4.535113432323723e-05, 'epoch': 0.69}


 69%|██████▉   | 2157/3107 [11:03:51<4:45:28, 18.03s/it]

 69%|██████▉   | 2158/3107 [11:04:09<4:47:47, 18.20s/it]

 69%|██████▉   | 2159/3107 [11:04:28<4:51:03, 18.42s/it]

 70%|██████▉   | 2160/3107 [11:04:40<4:20:48, 16.52s/it]

 70%|██████▉   | 2161/3107 [11:05:10<5:21:40, 20.40s/it]

 70%|██████▉   | 2162/3107 [11:05:38<5:55:37, 22.58s/it]

 70%|██████▉   | 2163/3107 [11:05:59<5:49:08, 22.19s/it]

 70%|██████▉   | 2164/3107 [11:06:20<5:44:57, 21.95s/it]

 70%|██████▉   | 2165/3107 [11:06:44<5:55:10, 22.62s/it]

 70%|██████▉   | 2166/3107 [11:07:01<5:24:44, 20.71s/it]

 70%|██████▉   | 2167/3107 [11:07:14<4:48:41, 18.43s/it]

 70%|██████▉   | 2168/3107 [11:07:29<4:34:24, 17.53s/it]

 70%|██████▉   | 2169/3107 [11:07:41<4:07:36, 15.84s/it]

 70%|██████▉   | 2170/3107 [11:07:54<3:53:47, 14.97s/it]

 70%|██████▉   | 2171/3107 [11:08:08<3:49:10, 14.69s/it]

 70%|██████▉   | 2172/3107 [11:08:23<3:49:45, 14.74s/it]
{'loss': 0.8509, 'grad_norm': 0.13286580918839727, 'learning_rate': 4.3875341787038236e-05, 'epoch': 0.7}


 70%|██████▉   | 2174/3107 [11:08:59<4:11:52, 16.20s/it]

 70%|███████   | 2175/3107 [11:09:11<3:55:53, 15.19s/it]

 70%|███████   | 2176/3107 [11:09:28<4:02:36, 15.64s/it]

 70%|███████   | 2177/3107 [11:09:56<4:57:48, 19.21s/it]

 70%|███████   | 2178/3107 [11:10:16<5:01:39, 19.48s/it]

 70%|███████   | 2179/3107 [11:10:38<5:14:28, 20.33s/it]

 70%|███████   | 2180/3107 [11:10:52<4:44:10, 18.39s/it]

 70%|███████   | 2181/3107 [11:11:12<4:53:12, 19.00s/it]

 70%|███████   | 2182/3107 [11:11:30<4:46:30, 18.58s/it]

 70%|███████   | 2183/3107 [11:11:45<4:31:42, 17.64s/it]

 70%|███████   | 2184/3107 [11:12:07<4:49:26, 18.82s/it]

 70%|███████   | 2185/3107 [11:12:25<4:44:07, 18.49s/it]

 70%|███████   | 2186/3107 [11:12:40<4:29:55, 17.58s/it]

 70%|███████   | 2187/3107 [11:12:55<4:15:55, 16.69s/it]

 70%|███████   | 2188/3107 [11:13:15<4:32:17, 17.78s/it]

 70%|███████   | 2189/3107 [11:13:35<4:40:54, 18.36s/it]

 70%|███████   | 2190/3107 [11:13:54<4:42:42, 18.50s/it]

 71%|███████   | 2191/3107 [11:14:23<5:31:04, 21.69s/it]

 71%|███████   | 2192/3107 [11:14:41<5:16:54, 20.78s/it]

 71%|███████   | 2193/3107 [11:15:00<5:03:59, 19.96s/it]

 71%|███████   | 2194/3107 [11:15:21<5:10:23, 20.40s/it]

 71%|███████   | 2195/3107 [11:15:42<5:12:58, 20.59s/it]

 71%|███████   | 2196/3107 [11:16:02<5:09:11, 20.36s/it]

 71%|███████   | 2197/3107 [11:16:20<5:00:37, 19.82s/it]

 71%|███████   | 2198/3107 [11:16:41<5:01:56, 19.93s/it]

 71%|███████   | 2199/3107 [11:16:53<4:26:06, 17.58s/it]

 71%|███████   | 2200/3107 [11:17:08<4:15:20, 16.89s/it]

 71%|███████   | 2201/3107 [11:17:27<4:26:27, 17.65s/it]

 71%|███████   | 2202/3107 [11:17:40<4:01:39, 16.02s/it]

 71%|███████   | 2203/3107 [11:17:56<4:03:49, 16.18s/it]

 71%|███████   | 2204/3107 [11:18:08<3:43:43, 14.87s/it]

 71%|███████   | 2205/3107 [11:18:30<4:16:56, 17.09s/it]

 71%|███████   | 2206/3107 [11:18:48<4:18:16, 17.20s/it]

 71%|███████   | 2207/3107 [11:19:03<4:08:41, 16.58s/it]

 71%|███████   | 2208/3107 [11:19:21<4:16:51, 17.14s/it]

 71%|███████   | 2209/3107 [11:19:34<3:57:17, 15.85s/it]

 71%|███████   | 2210/3107 [11:19:50<3:57:10, 15.86s/it]

 71%|███████   | 2211/3107 [11:20:11<4:22:08, 17.55s/it]

 71%|███████   | 2212/3107 [11:20:29<4:19:36, 17.40s/it]

 71%|███████   | 2213/3107 [11:20:41<3:59:26, 16.07s/it]

 71%|███████▏  | 2214/3107 [11:20:58<4:01:16, 16.21s/it]

 71%|███████▏  | 2215/3107 [11:21:12<3:49:03, 15.41s/it]

 71%|███████▏  | 2216/3107 [11:21:26<3:46:29, 15.25s/it]

 71%|███████▏  | 2217/3107 [11:21:44<3:57:26, 16.01s/it]

 71%|███████▏  | 2218/3107 [11:22:02<4:04:20, 16.49s/it]

 71%|███████▏  | 2219/3107 [11:22:15<3:47:12, 15.35s/it]

 71%|███████▏  | 2220/3107 [11:22:28<3:37:56, 14.74s/it]

 71%|███████▏  | 2221/3107 [11:22:43<3:39:16, 14.85s/it]

 72%|███████▏  | 2222/3107 [11:22:59<3:46:01, 15.32s/it]

 72%|███████▏  | 2223/3107 [11:23:16<3:52:24, 15.77s/it]

 72%|███████▏  | 2224/3107 [11:23:32<3:54:11, 15.91s/it]

 72%|███████▏  | 2225/3107 [11:23:52<4:07:56, 16.87s/it]

 72%|███████▏  | 2226/3107 [11:24:11<4:18:59, 17.64s/it]

 72%|███████▏  | 2227/3107 [11:24:31<4:28:15, 18.29s/it]

 72%|███████▏  | 2228/3107 [11:24:44<4:05:44, 16.77s/it]

 72%|███████▏  | 2229/3107 [11:24:58<3:53:24, 15.95s/it]

 72%|███████▏  | 2230/3107 [11:25:22<4:28:41, 18.38s/it]

 72%|███████▏  | 2231/3107 [11:25:46<4:50:33, 19.90s/it]

 72%|███████▏  | 2232/3107 [11:26:07<4:55:34, 20.27s/it]

 72%|███████▏  | 2233/3107 [11:26:31<5:13:47, 21.54s/it]

 72%|███████▏  | 2234/3107 [11:26:45<4:37:33, 19.08s/it]

 72%|███████▏  | 2235/3107 [11:27:06<4:48:03, 19.82s/it]

 72%|███████▏  | 2236/3107 [11:27:27<4:50:26, 20.01s/it]

 72%|███████▏  | 2237/3107 [11:27:45<4:43:44, 19.57s/it]

 72%|███████▏  | 2238/3107 [11:27:57<4:12:21, 17.42s/it]

 72%|███████▏  | 2239/3107 [11:28:12<3:57:40, 16.43s/it]

 72%|███████▏  | 2240/3107 [11:28:25<3:44:51, 15.56s/it]

 72%|███████▏  | 2241/3107 [11:28:57<4:56:25, 20.54s/it]

 72%|███████▏  | 2242/3107 [11:29:17<4:51:52, 20.25s/it]

 72%|███████▏  | 2243/3107 [11:29:45<5:24:07, 22.51s/it]

 72%|███████▏  | 2244/3107 [11:30:01<4:57:41, 20.70s/it]

 72%|███████▏  | 2245/3107 [11:30:17<4:38:07, 19.36s/it]
{'loss': 0.9039, 'grad_norm': 0.15418315462296808, 'learning_rate': 3.7744229714722514e-05, 'epoch': 0.72}


 72%|███████▏  | 2247/3107 [11:30:46<3:57:04, 16.54s/it]

 72%|███████▏  | 2248/3107 [11:31:13<4:43:36, 19.81s/it]

 72%|███████▏  | 2249/3107 [11:31:31<4:34:52, 19.22s/it]

 72%|███████▏  | 2250/3107 [11:31:46<4:16:40, 17.97s/it]

 72%|███████▏  | 2251/3107 [11:32:07<4:30:04, 18.93s/it]

 72%|███████▏  | 2252/3107 [11:32:32<4:55:20, 20.73s/it]
[2024-05-30 12:12:00,384] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 2253/3107 [11:32:48<4:35:06, 19.33s/it]

 73%|███████▎  | 2254/3107 [11:33:07<4:31:52, 19.12s/it]

 73%|███████▎  | 2255/3107 [11:33:29<4:43:04, 19.94s/it]

 73%|███████▎  | 2256/3107 [11:33:43<4:18:26, 18.22s/it]

 73%|███████▎  | 2257/3107 [11:33:56<3:58:03, 16.80s/it]

 73%|███████▎  | 2258/3107 [11:34:15<4:05:56, 17.38s/it]

 73%|███████▎  | 2259/3107 [11:34:36<4:20:29, 18.43s/it]

 73%|███████▎  | 2260/3107 [11:35:02<4:52:16, 20.70s/it]

 73%|███████▎  | 2261/3107 [11:35:15<4:18:28, 18.33s/it]

 73%|███████▎  | 2262/3107 [11:35:32<4:13:08, 17.97s/it]

 73%|███████▎  | 2263/3107 [11:35:47<3:59:01, 16.99s/it]

 73%|███████▎  | 2264/3107 [11:36:02<3:50:41, 16.42s/it]

 73%|███████▎  | 2265/3107 [11:36:23<4:09:03, 17.75s/it]

 73%|███████▎  | 2266/3107 [11:36:42<4:16:51, 18.33s/it]

 73%|███████▎  | 2267/3107 [11:37:03<4:25:04, 18.93s/it]

 73%|███████▎  | 2268/3107 [11:37:24<4:34:27, 19.63s/it]

 73%|███████▎  | 2269/3107 [11:37:39<4:16:22, 18.36s/it]

 73%|███████▎  | 2270/3107 [11:37:55<4:04:45, 17.55s/it]

 73%|███████▎  | 2271/3107 [11:38:19<4:30:22, 19.40s/it]

 73%|███████▎  | 2272/3107 [11:38:38<4:27:52, 19.25s/it]

 73%|███████▎  | 2273/3107 [11:38:54<4:14:36, 18.32s/it]

 73%|███████▎  | 2274/3107 [11:39:18<4:38:39, 20.07s/it]

 73%|███████▎  | 2275/3107 [11:39:36<4:31:25, 19.57s/it]

 73%|███████▎  | 2276/3107 [11:39:52<4:14:24, 18.37s/it]

 73%|███████▎  | 2277/3107 [11:40:16<4:39:38, 20.21s/it]

 73%|███████▎  | 2278/3107 [11:40:34<4:27:16, 19.34s/it]

 73%|███████▎  | 2279/3107 [11:40:49<4:11:13, 18.20s/it]

 73%|███████▎  | 2280/3107 [11:41:17<4:52:22, 21.21s/it]

 73%|███████▎  | 2281/3107 [11:41:41<5:00:34, 21.83s/it]

 73%|███████▎  | 2282/3107 [11:42:03<5:02:25, 21.99s/it]

 73%|███████▎  | 2283/3107 [11:42:21<4:43:56, 20.68s/it]

 74%|███████▎  | 2284/3107 [11:42:42<4:46:41, 20.90s/it]

 74%|███████▎  | 2285/3107 [11:42:56<4:17:11, 18.77s/it]

 74%|███████▎  | 2286/3107 [11:43:15<4:19:06, 18.94s/it]

 74%|███████▎  | 2287/3107 [11:43:28<3:53:50, 17.11s/it]

 74%|███████▎  | 2288/3107 [11:43:42<3:42:28, 16.30s/it]

 74%|███████▎  | 2289/3107 [11:44:00<3:45:37, 16.55s/it]

 74%|███████▎  | 2290/3107 [11:44:19<3:56:43, 17.39s/it]

 74%|███████▎  | 2291/3107 [11:44:33<3:44:13, 16.49s/it]

 74%|███████▍  | 2292/3107 [11:44:55<4:03:32, 17.93s/it]

 74%|███████▍  | 2293/3107 [11:45:12<4:00:38, 17.74s/it]

 74%|███████▍  | 2294/3107 [11:45:33<4:13:38, 18.72s/it]

 74%|███████▍  | 2295/3107 [11:45:53<4:17:24, 19.02s/it]

 74%|███████▍  | 2296/3107 [11:46:15<4:30:16, 20.00s/it]

 74%|███████▍  | 2297/3107 [11:46:38<4:40:34, 20.78s/it]

 74%|███████▍  | 2298/3107 [11:46:58<4:40:16, 20.79s/it]

 74%|███████▍  | 2299/3107 [11:47:20<4:41:59, 20.94s/it]

 74%|███████▍  | 2300/3107 [11:47:33<4:10:14, 18.61s/it]

 74%|███████▍  | 2301/3107 [11:47:47<3:52:14, 17.29s/it]

 74%|███████▍  | 2302/3107 [11:48:07<4:02:45, 18.09s/it]

 74%|███████▍  | 2303/3107 [11:48:31<4:25:30, 19.81s/it]

 74%|███████▍  | 2304/3107 [11:48:54<4:37:23, 20.73s/it]

 74%|███████▍  | 2305/3107 [11:49:06<4:04:38, 18.30s/it]

 74%|███████▍  | 2306/3107 [11:49:31<4:30:10, 20.24s/it]

 74%|███████▍  | 2307/3107 [11:49:50<4:24:47, 19.86s/it]

 74%|███████▍  | 2308/3107 [11:50:17<4:53:11, 22.02s/it]

 74%|███████▍  | 2309/3107 [11:50:34<4:31:30, 20.41s/it]

 74%|███████▍  | 2310/3107 [11:50:52<4:21:24, 19.68s/it]

 74%|███████▍  | 2311/3107 [11:51:08<4:07:55, 18.69s/it]

 74%|███████▍  | 2312/3107 [11:51:24<3:55:54, 17.80s/it]

 74%|███████▍  | 2313/3107 [11:51:41<3:52:08, 17.54s/it]

 74%|███████▍  | 2314/3107 [11:51:55<3:39:34, 16.61s/it]

 75%|███████▍  | 2315/3107 [11:52:06<3:16:56, 14.92s/it]

 75%|███████▍  | 2316/3107 [11:52:26<3:35:59, 16.38s/it]

 75%|███████▍  | 2317/3107 [11:52:45<3:47:05, 17.25s/it]

 75%|███████▍  | 2318/3107 [11:53:00<3:36:18, 16.45s/it]

 75%|███████▍  | 2319/3107 [11:53:20<3:49:10, 17.45s/it]

 75%|███████▍  | 2320/3107 [11:53:33<3:32:49, 16.23s/it]

 75%|███████▍  | 2321/3107 [11:53:49<3:29:48, 16.02s/it]

 75%|███████▍  | 2322/3107 [11:54:14<4:08:20, 18.98s/it]

 75%|███████▍  | 2323/3107 [11:54:31<3:59:30, 18.33s/it]

 75%|███████▍  | 2324/3107 [11:54:47<3:50:29, 17.66s/it]

 75%|███████▍  | 2325/3107 [11:55:10<4:09:05, 19.11s/it]

 75%|███████▍  | 2326/3107 [11:55:30<4:12:47, 19.42s/it]

 75%|███████▍  | 2327/3107 [11:55:45<3:54:47, 18.06s/it]

 75%|███████▍  | 2328/3107 [11:55:58<3:36:50, 16.70s/it]

 75%|███████▍  | 2329/3107 [11:56:29<4:28:43, 20.72s/it]
[2024-05-30 12:35:56,752] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 2330/3107 [11:56:42<3:58:22, 18.41s/it]

 75%|███████▌  | 2331/3107 [11:56:55<3:38:15, 16.88s/it]

 75%|███████▌  | 2332/3107 [11:57:25<4:30:24, 20.94s/it]
{'loss': 0.8174, 'grad_norm': 0.13268630925237043, 'learning_rate': 3.0910963194591835e-05, 'epoch': 0.75}


 75%|███████▌  | 2334/3107 [11:57:57<3:51:33, 17.97s/it]

 75%|███████▌  | 2335/3107 [11:58:09<3:28:39, 16.22s/it]
{'loss': 0.9315, 'grad_norm': 0.14335022251784607, 'learning_rate': 3.068515708893206e-05, 'epoch': 0.75}


 75%|███████▌  | 2337/3107 [11:58:57<4:21:54, 20.41s/it]

 75%|███████▌  | 2338/3107 [11:59:11<3:57:11, 18.51s/it]

 75%|███████▌  | 2339/3107 [11:59:25<3:37:54, 17.02s/it]

 75%|███████▌  | 2340/3107 [11:59:39<3:28:05, 16.28s/it]

 75%|███████▌  | 2341/3107 [11:59:56<3:31:11, 16.54s/it]

 75%|███████▌  | 2342/3107 [12:00:13<3:31:31, 16.59s/it]

 75%|███████▌  | 2343/3107 [12:00:32<3:40:46, 17.34s/it]

 75%|███████▌  | 2344/3107 [12:00:48<3:36:10, 17.00s/it]
{'loss': 0.8924, 'grad_norm': 0.13578562223225388, 'learning_rate': 3.001181688978203e-05, 'epoch': 0.75}


 76%|███████▌  | 2346/3107 [12:01:23<3:33:37, 16.84s/it]

 76%|███████▌  | 2347/3107 [12:01:38<3:29:23, 16.53s/it]

 76%|███████▌  | 2348/3107 [12:01:51<3:15:01, 15.42s/it]
{'loss': 0.9688, 'grad_norm': 0.15388166625354502, 'learning_rate': 2.971452939326802e-05, 'epoch': 0.76}


 76%|███████▌  | 2350/3107 [12:02:31<3:43:30, 17.72s/it]

 76%|███████▌  | 2351/3107 [12:02:45<3:30:35, 16.71s/it]

 76%|███████▌  | 2352/3107 [12:03:07<3:49:40, 18.25s/it]
{'loss': 0.8056, 'grad_norm': 0.12923124835743005, 'learning_rate': 2.94184645023865e-05, 'epoch': 0.76}


 76%|███████▌  | 2354/3107 [12:03:40<3:40:24, 17.56s/it]
{'loss': 0.7827, 'grad_norm': 0.1338858092498712, 'learning_rate': 2.9270892144267993e-05, 'epoch': 0.76}

 76%|███████▌  | 2355/3107 [12:03:58<3:40:37, 17.60s/it]


 76%|███████▌  | 2357/3107 [12:04:33<3:45:49, 18.07s/it]

 76%|███████▌  | 2358/3107 [12:04:59<4:16:59, 20.59s/it]

 76%|███████▌  | 2359/3107 [12:05:17<4:05:31, 19.69s/it]

 76%|███████▌  | 2360/3107 [12:05:29<3:36:13, 17.37s/it]

 76%|███████▌  | 2361/3107 [12:05:55<4:08:09, 19.96s/it]

 76%|███████▌  | 2362/3107 [12:06:18<4:18:06, 20.79s/it]

 76%|███████▌  | 2363/3107 [12:06:37<4:11:30, 20.28s/it]

 76%|███████▌  | 2364/3107 [12:06:50<3:44:06, 18.10s/it]

 76%|███████▌  | 2365/3107 [12:07:04<3:27:53, 16.81s/it]

 76%|███████▌  | 2366/3107 [12:07:19<3:21:45, 16.34s/it]
{'loss': 0.8789, 'grad_norm': 0.14661575762199303, 'learning_rate': 2.83919395639182e-05, 'epoch': 0.76}


 76%|███████▌  | 2368/3107 [12:07:53<3:24:36, 16.61s/it]

 76%|███████▌  | 2369/3107 [12:08:10<3:26:10, 16.76s/it]

 76%|███████▋  | 2370/3107 [12:08:31<3:41:11, 18.01s/it]

 76%|███████▋  | 2371/3107 [12:08:45<3:27:19, 16.90s/it]

 76%|███████▋  | 2372/3107 [12:09:02<3:26:51, 16.89s/it]

 76%|███████▋  | 2373/3107 [12:09:19<3:27:08, 16.93s/it]
{'loss': 0.8796, 'grad_norm': 0.13545305342762895, 'learning_rate': 2.7884385486492715e-05, 'epoch': 0.76}


 76%|███████▋  | 2375/3107 [12:10:00<3:47:10, 18.62s/it]
{'loss': 0.7659, 'grad_norm': 0.13669076974241195, 'learning_rate': 2.7740074479269085e-05, 'epoch': 0.76}


 77%|███████▋  | 2377/3107 [12:10:30<3:26:24, 16.96s/it]
{'loss': 0.6448, 'grad_norm': 0.13508406435703993, 'learning_rate': 2.7596077710134847e-05, 'epoch': 0.77}


 77%|███████▋  | 2379/3107 [12:11:07<3:29:03, 17.23s/it]

 77%|███████▋  | 2380/3107 [12:11:21<3:16:38, 16.23s/it]

 77%|███████▋  | 2381/3107 [12:11:37<3:15:44, 16.18s/it]

 77%|███████▋  | 2382/3107 [12:12:01<3:45:42, 18.68s/it]

 77%|███████▋  | 2383/3107 [12:12:13<3:21:56, 16.74s/it]
{'loss': 0.789, 'grad_norm': 0.15116031479673675, 'learning_rate': 2.7165979086434077e-05, 'epoch': 0.77}


 77%|███████▋  | 2385/3107 [12:12:44<3:13:28, 16.08s/it]

 77%|███████▋  | 2386/3107 [12:13:05<3:30:24, 17.51s/it]

 77%|███████▋  | 2387/3107 [12:13:27<3:45:43, 18.81s/it]

 77%|███████▋  | 2388/3107 [12:13:52<4:08:10, 20.71s/it]

 77%|███████▋  | 2389/3107 [12:14:07<3:47:49, 19.04s/it]
{'loss': 0.8144, 'grad_norm': 0.14874245054875207, 'learning_rate': 2.6738731066488075e-05, 'epoch': 0.77}


 77%|███████▋  | 2391/3107 [12:14:40<3:30:08, 17.61s/it]

 77%|███████▋  | 2392/3107 [12:14:58<3:32:23, 17.82s/it]
{'loss': 0.8539, 'grad_norm': 0.13845300320163614, 'learning_rate': 2.6526181262856863e-05, 'epoch': 0.77}


 77%|███████▋  | 2394/3107 [12:15:40<3:47:53, 19.18s/it]

 77%|███████▋  | 2395/3107 [12:15:56<3:38:25, 18.41s/it]

 77%|███████▋  | 2396/3107 [12:16:09<3:19:30, 16.84s/it]

 77%|███████▋  | 2397/3107 [12:16:30<3:33:14, 18.02s/it]
{'loss': 0.8159, 'grad_norm': 0.1303194335206614, 'learning_rate': 2.617353019809956e-05, 'epoch': 0.77}

 77%|███████▋  | 2398/3107 [12:16:47<3:28:20, 17.63s/it]


 77%|███████▋  | 2400/3107 [12:17:19<3:24:24, 17.35s/it]
 77%|███████▋  | 2400/3107 [12:17:19<3:24:24, 17.35s/it]/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/petrelfs/liaoyusheng/oss/download_models/Qwen1.5-7B-Chat - will assume that the vocabulary was not modified.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/mnt/petrelfs/liaoyusheng/miniconda3/envs/ming/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 77%|███████▋  | 2401/3107 [12:17:57<4:36:54, 23.53s/it]

 77%|███████▋  | 2402/3107 [12:18:11<4:03:10, 20.70s/it]

 77%|███████▋  | 2403/3107 [12:18:29<3:50:03, 19.61s/it]

 77%|███████▋  | 2404/3107 [12:18:42<3:28:32, 17.80s/it]
{'loss': 0.8733, 'grad_norm': 0.16338942457283231, 'learning_rate': 2.5683191868429247e-05, 'epoch': 0.77}


 77%|███████▋  | 2406/3107 [12:19:28<3:51:21, 19.80s/it]

 77%|███████▋  | 2407/3107 [12:19:47<3:47:51, 19.53s/it]
{'loss': 0.6702, 'grad_norm': 0.14307939377606416, 'learning_rate': 2.5474257285262624e-05, 'epoch': 0.77}

 78%|███████▊  | 2408/3107 [12:20:09<3:54:52, 20.16s/it]


 78%|███████▊  | 2410/3107 [12:20:46<3:40:46, 19.01s/it]

 78%|███████▊  | 2411/3107 [12:21:02<3:32:02, 18.28s/it]

 78%|███████▊  | 2412/3107 [12:21:19<3:26:59, 17.87s/it]
{'loss': 0.8184, 'grad_norm': 0.1462449344875537, 'learning_rate': 2.51276544430293e-05, 'epoch': 0.78}


 78%|███████▊  | 2414/3107 [12:22:03<3:46:07, 19.58s/it]

 78%|███████▊  | 2415/3107 [12:22:27<4:00:55, 20.89s/it]

 78%|███████▊  | 2416/3107 [12:22:48<4:02:33, 21.06s/it]

 78%|███████▊  | 2417/3107 [12:23:08<4:00:23, 20.90s/it]

 78%|███████▊  | 2418/3107 [12:23:24<3:41:50, 19.32s/it]

 78%|███████▊  | 2419/3107 [12:23:54<4:18:35, 22.55s/it]
{'loss': 0.7155, 'grad_norm': 0.13195642749754177, 'learning_rate': 2.4645831344136037e-05, 'epoch': 0.78}


 78%|███████▊  | 2421/3107 [12:24:30<3:57:29, 20.77s/it]

 78%|███████▊  | 2422/3107 [12:24:53<4:02:25, 21.23s/it]

 78%|███████▊  | 2423/3107 [12:25:15<4:03:38, 21.37s/it]

 78%|███████▊  | 2424/3107 [12:25:34<3:57:15, 20.84s/it]

 78%|███████▊  | 2425/3107 [12:25:59<4:10:59, 22.08s/it]

 78%|███████▊  | 2426/3107 [12:26:19<4:02:23, 21.36s/it]

 78%|███████▊  | 2427/3107 [12:26:34<3:40:20, 19.44s/it]

 78%|███████▊  | 2428/3107 [12:26:51<3:31:15, 18.67s/it]
{'loss': 0.6837, 'grad_norm': 0.135402432839478, 'learning_rate': 2.4032246539528548e-05, 'epoch': 0.78}


 78%|███████▊  | 2430/3107 [12:27:27<3:29:15, 18.55s/it]
{'loss': 0.935, 'grad_norm': 0.15076455386031523, 'learning_rate': 2.389680096006076e-05, 'epoch': 0.78}


 78%|███████▊  | 2432/3107 [12:28:09<3:46:00, 20.09s/it]
{'loss': 0.7508, 'grad_norm': 0.13674600312418303, 'learning_rate': 2.3761686331997855e-05, 'epoch': 0.78}


 78%|███████▊  | 2434/3107 [12:28:37<3:11:23, 17.06s/it]

 78%|███████▊  | 2435/3107 [12:28:59<3:28:05, 18.58s/it]

 78%|███████▊  | 2436/3107 [12:29:24<3:49:42, 20.54s/it]
{'loss': 0.897, 'grad_norm': 0.14818596744216267, 'learning_rate': 2.3492452278946765e-05, 'epoch': 0.78}

 78%|███████▊  | 2437/3107 [12:29:42<3:38:13, 19.54s/it]


 79%|███████▊  | 2439/3107 [12:30:24<3:49:05, 20.58s/it]

 79%|███████▊  | 2440/3107 [12:30:43<3:44:06, 20.16s/it]

 79%|███████▊  | 2441/3107 [12:31:06<3:52:49, 20.98s/it]

 79%|███████▊  | 2442/3107 [12:31:22<3:36:25, 19.53s/it]

 79%|███████▊  | 2443/3107 [12:31:40<3:30:06, 18.99s/it]

 79%|███████▊  | 2444/3107 [12:32:00<3:34:25, 19.40s/it]

 79%|███████▊  | 2445/3107 [12:32:25<3:50:37, 20.90s/it]

 79%|███████▊  | 2446/3107 [12:32:39<3:29:14, 18.99s/it]
{'loss': 0.7977, 'grad_norm': 0.1432478386392642, 'learning_rate': 2.282519974925209e-05, 'epoch': 0.79}


 79%|███████▉  | 2448/3107 [12:33:24<3:47:37, 20.73s/it]

 79%|███████▉  | 2449/3107 [12:33:43<3:41:14, 20.17s/it]
{'loss': 0.6737, 'grad_norm': 0.15019832586749285, 'learning_rate': 2.2626656805309e-05, 'epoch': 0.79}

 79%|███████▉  | 2450/3107 [12:34:00<3:29:40, 19.15s/it]


 79%|███████▉  | 2452/3107 [12:34:31<3:06:21, 17.07s/it]

 79%|███████▉  | 2453/3107 [12:34:56<3:33:28, 19.59s/it]
{'loss': 0.7964, 'grad_norm': 0.13823352722361712, 'learning_rate': 2.2363110876386562e-05, 'epoch': 0.79}


 79%|███████▉  | 2455/3107 [12:35:23<2:57:32, 16.34s/it]
{'loss': 0.762, 'grad_norm': 0.1405179877781172, 'learning_rate': 2.2231844057257367e-05, 'epoch': 0.79}


 79%|███████▉  | 2457/3107 [12:35:54<2:50:51, 15.77s/it]

 79%|███████▉  | 2458/3107 [12:36:15<3:06:38, 17.25s/it]

 79%|███████▉  | 2459/3107 [12:36:27<2:47:06, 15.47s/it]

 79%|███████▉  | 2460/3107 [12:36:48<3:07:53, 17.42s/it]

 79%|███████▉  | 2461/3107 [12:37:03<2:57:01, 16.44s/it]

 79%|███████▉  | 2462/3107 [12:37:29<3:29:50, 19.52s/it]

 79%|███████▉  | 2463/3107 [12:37:45<3:18:14, 18.47s/it]

 79%|███████▉  | 2464/3107 [12:38:07<3:29:35, 19.56s/it]

 79%|███████▉  | 2465/3107 [12:38:26<3:25:08, 19.17s/it]

 79%|███████▉  | 2466/3107 [12:38:53<3:50:34, 21.58s/it]

 79%|███████▉  | 2467/3107 [12:39:06<3:21:27, 18.89s/it]

 79%|███████▉  | 2468/3107 [12:39:30<3:38:36, 20.53s/it]

 79%|███████▉  | 2469/3107 [12:39:55<3:52:29, 21.86s/it]
{'loss': 0.8255, 'grad_norm': 0.13903209409170217, 'learning_rate': 2.1322477476265367e-05, 'epoch': 0.79}


 80%|███████▉  | 2471/3107 [12:40:25<3:16:08, 18.50s/it]

 80%|███████▉  | 2472/3107 [12:40:51<3:40:14, 20.81s/it]

 80%|███████▉  | 2473/3107 [12:41:06<3:21:46, 19.10s/it]

 80%|███████▉  | 2474/3107 [12:41:25<3:20:08, 18.97s/it]
{'loss': 0.9192, 'grad_norm': 0.14313170395416078, 'learning_rate': 2.1001756661419093e-05, 'epoch': 0.8}

 80%|███████▉  | 2475/3107 [12:41:41<3:09:32, 17.99s/it]

 80%|███████▉  | 2476/3107 [12:42:02<3:21:22, 19.15s/it]


 80%|███████▉  | 2478/3107 [12:42:46<3:34:54, 20.50s/it]

 80%|███████▉  | 2479/3107 [12:43:05<3:31:24, 20.20s/it]

 80%|███████▉  | 2480/3107 [12:43:23<3:23:53, 19.51s/it]
{'loss': 0.8829, 'grad_norm': 0.13449713417497433, 'learning_rate': 2.061972665559213e-05, 'epoch': 0.8}


 80%|███████▉  | 2482/3107 [12:44:08<3:35:07, 20.65s/it]

 80%|███████▉  | 2483/3107 [12:44:26<3:27:33, 19.96s/it]

 80%|███████▉  | 2484/3107 [12:44:45<3:24:10, 19.66s/it]

 80%|███████▉  | 2485/3107 [12:45:06<3:26:06, 19.88s/it]
{'loss': 0.7916, 'grad_norm': 0.13991359159588218, 'learning_rate': 2.030374094667973e-05, 'epoch': 0.8}


 80%|████████  | 2487/3107 [12:45:32<2:51:40, 16.61s/it]

 80%|████████  | 2488/3107 [12:45:46<2:43:12, 15.82s/it]

 80%|████████  | 2489/3107 [12:46:06<2:56:17, 17.12s/it]
{'loss': 0.7345, 'grad_norm': 0.12666843551669973, 'learning_rate': 2.005251156107426e-05, 'epoch': 0.8}


 80%|████████  | 2491/3107 [12:46:36<2:43:25, 15.92s/it]

 80%|████████  | 2492/3107 [12:46:59<3:04:48, 18.03s/it]

 80%|████████  | 2493/3107 [12:47:22<3:18:08, 19.36s/it]

 80%|████████  | 2494/3107 [12:47:40<3:15:56, 19.18s/it]

 80%|████████  | 2495/3107 [12:48:01<3:20:51, 19.69s/it]

 80%|████████  | 2496/3107 [12:48:22<3:24:40, 20.10s/it]

 80%|████████  | 2497/3107 [12:48:44<3:28:20, 20.49s/it]

 80%|████████  | 2498/3107 [12:48:56<3:01:47, 17.91s/it]
{'loss': 0.8985, 'grad_norm': 0.1476833593002505, 'learning_rate': 1.9492336717488235e-05, 'epoch': 0.8}


 80%|████████  | 2500/3107 [12:49:25<2:43:17, 16.14s/it]

 80%|████████  | 2501/3107 [12:49:40<2:40:03, 15.85s/it]

 81%|████████  | 2502/3107 [12:50:02<2:58:29, 17.70s/it]

 81%|████████  | 2503/3107 [12:50:17<2:50:01, 16.89s/it]

 81%|████████  | 2504/3107 [12:50:34<2:47:55, 16.71s/it]

 81%|████████  | 2505/3107 [12:50:46<2:34:32, 15.40s/it]

 81%|████████  | 2506/3107 [12:51:01<2:31:35, 15.13s/it]
{'loss': 0.7946, 'grad_norm': 0.14502599954297704, 'learning_rate': 1.9000353749948206e-05, 'epoch': 0.81}

 81%|████████  | 2507/3107 [12:51:13<2:23:47, 14.38s/it]


 81%|████████  | 2509/3107 [12:51:50<2:42:09, 16.27s/it]
{'loss': 0.7025, 'grad_norm': 0.15292089200249145, 'learning_rate': 1.8817311312374564e-05, 'epoch': 0.81}


 81%|████████  | 2511/3107 [12:52:25<2:47:49, 16.90s/it]

 81%|████████  | 2512/3107 [12:52:38<2:34:28, 15.58s/it]

 81%|████████  | 2513/3107 [12:52:56<2:41:02, 16.27s/it]

 81%|████████  | 2514/3107 [12:53:10<2:34:35, 15.64s/it]

 81%|████████  | 2515/3107 [12:53:32<2:52:07, 17.44s/it]

 81%|████████  | 2516/3107 [12:53:59<3:20:50, 20.39s/it]
{'loss': 0.7893, 'grad_norm': 0.13745429226172087, 'learning_rate': 1.8393304476888274e-05, 'epoch': 0.81}


 81%|████████  | 2518/3107 [12:54:27<2:47:35, 17.07s/it]

 81%|████████  | 2519/3107 [12:54:47<2:55:29, 17.91s/it]

 81%|████████  | 2520/3107 [12:55:16<3:28:32, 21.32s/it]

 81%|████████  | 2521/3107 [12:55:37<3:27:45, 21.27s/it]

 81%|████████  | 2522/3107 [12:55:55<3:16:32, 20.16s/it]

 81%|████████  | 2523/3107 [12:56:12<3:08:06, 19.33s/it]

 81%|████████  | 2524/3107 [12:56:29<2:59:44, 18.50s/it]

 81%|████████▏ | 2525/3107 [12:56:44<2:49:44, 17.50s/it]

 81%|████████▏ | 2526/3107 [12:57:06<3:02:29, 18.85s/it]

 81%|████████▏ | 2527/3107 [12:57:23<2:56:20, 18.24s/it]
{'loss': 0.7868, 'grad_norm': 0.15058301132338717, 'learning_rate': 1.773579999127901e-05, 'epoch': 0.81}


 81%|████████▏ | 2529/3107 [12:58:04<3:04:51, 19.19s/it]

 81%|████████▏ | 2530/3107 [12:58:20<2:54:19, 18.13s/it]

 81%|████████▏ | 2531/3107 [12:58:40<2:59:19, 18.68s/it]

 81%|████████▏ | 2532/3107 [12:59:00<3:02:54, 19.09s/it]

 82%|████████▏ | 2533/3107 [12:59:16<2:54:12, 18.21s/it]
{'loss': 0.7082, 'grad_norm': 0.1371541192160517, 'learning_rate': 1.7381716901002177e-05, 'epoch': 0.82}

 82%|████████▏ | 2534/3107 [12:59:36<2:58:03, 18.64s/it]


 82%|████████▏ | 2536/3107 [13:00:11<2:53:34, 18.24s/it]
{'loss': 0.791, 'grad_norm': 0.15555308789837927, 'learning_rate': 1.7205887073541672e-05, 'epoch': 0.82}

 82%|████████▏ | 2537/3107 [13:00:28<2:49:18, 17.82s/it]


 82%|████████▏ | 2539/3107 [13:01:05<2:52:42, 18.24s/it]

 82%|████████▏ | 2540/3107 [13:01:22<2:47:55, 17.77s/it]

 82%|████████▏ | 2541/3107 [13:01:38<2:42:58, 17.28s/it]

 82%|████████▏ | 2542/3107 [13:01:51<2:30:55, 16.03s/it]

 82%|████████▏ | 2543/3107 [13:02:11<2:42:16, 17.26s/it]

 82%|████████▏ | 2544/3107 [13:02:32<2:52:24, 18.37s/it]

 82%|████████▏ | 2545/3107 [13:02:49<2:48:27, 17.99s/it]

 82%|████████▏ | 2546/3107 [13:03:03<2:36:25, 16.73s/it]

 82%|████████▏ | 2547/3107 [13:03:29<3:00:11, 19.31s/it]

 82%|████████▏ | 2548/3107 [13:03:56<3:23:25, 21.83s/it]

 82%|████████▏ | 2549/3107 [13:04:14<3:12:39, 20.72s/it]

 82%|████████▏ | 2550/3107 [13:04:33<3:07:33, 20.20s/it]

 82%|████████▏ | 2551/3107 [13:05:00<3:25:08, 22.14s/it]

 82%|████████▏ | 2552/3107 [13:05:25<3:33:48, 23.11s/it]

 82%|████████▏ | 2553/3107 [13:05:46<3:27:19, 22.45s/it]

 82%|████████▏ | 2554/3107 [13:06:01<3:06:00, 20.18s/it]

 82%|████████▏ | 2555/3107 [13:06:16<2:51:32, 18.65s/it]
{'loss': 0.7797, 'grad_norm': 0.13084824342782608, 'learning_rate': 1.6111181024835e-05, 'epoch': 0.82}


 82%|████████▏ | 2557/3107 [13:06:51<2:48:11, 18.35s/it]

 82%|████████▏ | 2558/3107 [13:07:11<2:51:16, 18.72s/it]

 82%|████████▏ | 2559/3107 [13:07:23<2:32:57, 16.75s/it]

 82%|████████▏ | 2560/3107 [13:07:37<2:25:44, 15.99s/it]

 82%|████████▏ | 2561/3107 [13:07:52<2:21:35, 15.56s/it]

 82%|████████▏ | 2562/3107 [13:08:17<2:47:25, 18.43s/it]

 82%|████████▏ | 2563/3107 [13:08:39<2:57:23, 19.57s/it]

 83%|████████▎ | 2564/3107 [13:08:57<2:54:20, 19.26s/it]

 83%|████████▎ | 2565/3107 [13:09:09<2:34:13, 17.07s/it]

 83%|████████▎ | 2566/3107 [13:09:33<2:51:02, 18.97s/it]

 83%|████████▎ | 2567/3107 [13:09:48<2:39:12, 17.69s/it]

 83%|████████▎ | 2568/3107 [13:10:05<2:39:05, 17.71s/it]
{'loss': 0.7638, 'grad_norm': 0.1394143095166602, 'learning_rate': 1.5381115538175317e-05, 'epoch': 0.83}

 83%|████████▎ | 2569/3107 [13:10:28<2:53:11, 19.32s/it]

 83%|████████▎ | 2570/3107 [13:10:50<3:00:10, 20.13s/it]


 83%|████████▎ | 2572/3107 [13:11:29<2:59:55, 20.18s/it]

 83%|████████▎ | 2573/3107 [13:11:44<2:46:03, 18.66s/it]

 83%|████████▎ | 2574/3107 [13:12:03<2:46:15, 18.72s/it]

 83%|████████▎ | 2575/3107 [13:12:23<2:49:35, 19.13s/it]

 83%|████████▎ | 2576/3107 [13:12:35<2:29:33, 16.90s/it]
{'loss': 0.9617, 'grad_norm': 0.14468886579793766, 'learning_rate': 1.4939563583365779e-05, 'epoch': 0.83}


 83%|████████▎ | 2578/3107 [13:13:12<2:39:50, 18.13s/it]
{'loss': 0.7208, 'grad_norm': 0.1558475058462146, 'learning_rate': 1.4830099158335563e-05, 'epoch': 0.83}


 83%|████████▎ | 2580/3107 [13:13:43<2:23:43, 16.36s/it]

 83%|████████▎ | 2581/3107 [13:13:59<2:21:32, 16.15s/it]

 83%|████████▎ | 2582/3107 [13:14:19<2:31:51, 17.35s/it]

 83%|████████▎ | 2583/3107 [13:14:43<2:49:17, 19.38s/it]

 83%|████████▎ | 2584/3107 [13:15:04<2:51:57, 19.73s/it]

 83%|████████▎ | 2585/3107 [13:15:21<2:46:11, 19.10s/it]

 83%|████████▎ | 2586/3107 [13:15:55<3:24:42, 23.58s/it]

 83%|████████▎ | 2587/3107 [13:16:11<3:04:51, 21.33s/it]

 83%|████████▎ | 2588/3107 [13:16:30<2:57:04, 20.47s/it]
{'loss': 0.8404, 'grad_norm': 0.15124467964415472, 'learning_rate': 1.4288342196475179e-05, 'epoch': 0.83}

 83%|████████▎ | 2589/3107 [13:16:47<2:47:09, 19.36s/it]


 83%|████████▎ | 2591/3107 [13:17:22<2:42:22, 18.88s/it]

 83%|████████▎ | 2592/3107 [13:17:45<2:52:15, 20.07s/it]

 83%|████████▎ | 2593/3107 [13:18:10<3:03:52, 21.46s/it]

 83%|████████▎ | 2594/3107 [13:18:26<2:51:14, 20.03s/it]

 84%|████████▎ | 2595/3107 [13:18:50<3:00:08, 21.11s/it]
{'loss': 0.9188, 'grad_norm': 0.13146187024581768, 'learning_rate': 1.3914653257479416e-05, 'epoch': 0.84}


 84%|████████▎ | 2597/3107 [13:19:32<3:01:25, 21.34s/it]

 84%|████████▎ | 2598/3107 [13:19:44<2:37:40, 18.59s/it]

 84%|████████▎ | 2599/3107 [13:19:56<2:21:39, 16.73s/it]

 84%|████████▎ | 2600/3107 [13:20:12<2:18:51, 16.43s/it]

 84%|████████▎ | 2601/3107 [13:20:26<2:11:51, 15.63s/it]

 84%|████████▎ | 2602/3107 [13:20:38<2:02:05, 14.51s/it]
{'loss': 0.8672, 'grad_norm': 0.1411868894310854, 'learning_rate': 1.3545550221513281e-05, 'epoch': 0.84}


 84%|████████▍ | 2604/3107 [13:21:17<2:24:45, 17.27s/it]

 84%|████████▍ | 2605/3107 [13:21:28<2:08:30, 15.36s/it]
{'loss': 0.9606, 'grad_norm': 0.14690548339164514, 'learning_rate': 1.3388771935321421e-05, 'epoch': 0.84}


 84%|████████▍ | 2607/3107 [13:22:10<2:28:39, 17.84s/it]

 84%|████████▍ | 2608/3107 [13:22:24<2:17:33, 16.54s/it]
{'loss': 0.8367, 'grad_norm': 0.14085888422491333, 'learning_rate': 1.3232841106446792e-05, 'epoch': 0.84}


 84%|████████▍ | 2610/3107 [13:23:12<2:51:05, 20.66s/it]

 84%|████████▍ | 2611/3107 [13:23:37<3:00:00, 21.77s/it]

 84%|████████▍ | 2612/3107 [13:24:00<3:04:06, 22.32s/it]

 84%|████████▍ | 2613/3107 [13:24:16<2:46:56, 20.28s/it]

 84%|████████▍ | 2614/3107 [13:24:36<2:46:41, 20.29s/it]
{'loss': 0.7917, 'grad_norm': 0.13130248595968103, 'learning_rate': 1.2923527915233336e-05, 'epoch': 0.84}


 84%|████████▍ | 2616/3107 [13:25:10<2:28:31, 18.15s/it]

 84%|████████▍ | 2617/3107 [13:25:28<2:27:56, 18.11s/it]
{'loss': 0.7073, 'grad_norm': 0.14127854043064883, 'learning_rate': 1.2770148579404295e-05, 'epoch': 0.84}


 84%|████████▍ | 2619/3107 [13:26:08<2:33:45, 18.90s/it]

 84%|████████▍ | 2620/3107 [13:26:22<2:19:58, 17.25s/it]

 84%|████████▍ | 2621/3107 [13:26:42<2:26:54, 18.14s/it]

 84%|████████▍ | 2622/3107 [13:27:06<2:41:18, 19.96s/it]

 84%|████████▍ | 2623/3107 [13:27:31<2:52:54, 21.43s/it]
{'loss': 0.8629, 'grad_norm': 0.1328629091931739, 'learning_rate': 1.2465951931073694e-05, 'epoch': 0.84}


 84%|████████▍ | 2625/3107 [13:28:20<3:03:50, 22.89s/it]

 85%|████████▍ | 2626/3107 [13:28:36<2:45:33, 20.65s/it]

 85%|████████▍ | 2627/3107 [13:28:53<2:36:11, 19.52s/it]

 85%|████████▍ | 2628/3107 [13:29:14<2:40:27, 20.10s/it]

 85%|████████▍ | 2629/3107 [13:29:38<2:49:05, 21.22s/it]

 85%|████████▍ | 2630/3107 [13:29:53<2:33:54, 19.36s/it]

 85%|████████▍ | 2631/3107 [13:30:05<2:15:34, 17.09s/it]
